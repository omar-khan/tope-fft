//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Sat Apr  7 09:27:48 2012 (1333783668)
// Driver 295.41
//

.version 3.0
.target sm_13, texmode_independent
.address_size 32

.const .align 8 .b8 __internal_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.entry twid1D(
	.param .u32 .ptr .global .align 16 twid1D_param_0,
	.param .u32 twid1D_param_1
)
{
	.local .align 8 .b8 	__local_depot0[40];
	.reg .b32 	%SP;
	.reg .f64 	%fd<178>;
	.reg .pred 	%p<37>;
	.reg .s32 	%r<128>;
	.reg .s64 	%rl<167>;


	mov.u32 	%SP, __local_depot0;
	ld.param.u32 	%r41, [twid1D_param_1];
	// inline asm
	mov.u32 	%r37, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r38, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r39, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r40, %tid.x;
	// inline asm
	add.s32 	%r42, %r40, %r37;
	mad.lo.s32 	%r2, %r39, %r38, %r42;
	cvt.rn.f64.s32 	%fd22, %r2;
	mul.f64 	%fd23, %fd22, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd24, %r41;
	div.rn.f64 	%fd1, %fd23, %fd24;
	setp.eq.f64 	%p1, %fd1, 0d7FF0000000000000;
	setp.eq.f64 	%p2, %fd1, 0dFFF0000000000000;
	or.pred  	%p3, %p1, %p2;
	add.u32 	%r3, %SP, 0;
	@%p3 bra 	BB0_23;

	// inline asm
	abs.f64 	%fd25, %fd1;
	// inline asm
	setp.gt.f64 	%p4, %fd25, 0d41E0000000000000;
	@%p4 bra 	BB0_3;

	mov.f64 	%fd40, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd27, %fd1, %fd40;
	// inline asm
	cvt.rni.s32.f64 	%r43, %fd27;
	// inline asm
	cvt.rn.f64.s32 	%fd41, %r43;
	neg.f64 	%fd37, %fd41;
	mov.f64 	%fd30, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd28, %fd37, %fd30, %fd1;
	// inline asm
	mov.f64 	%fd34, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd32, %fd37, %fd34, %fd28;
	// inline asm
	mov.f64 	%fd38, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd36, %fd37, %fd38, %fd32;
	// inline asm
	mov.u32 	%r124, %r43;
	mov.f64 	%fd172, %fd36;
	bra.uni 	BB0_19;

BB0_3:
	mov.b64 	 %rl1, %fd1;
	and.b64  	%rl150, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl65, %rl3, 2047;
	add.s64 	%rl66, %rl65, 4294966272;
	cvt.u32.u64 	%r5, %rl66;
	shl.b64 	%rl67, %rl1, 11;
	or.b64  	%rl4, %rl67, -9223372036854775808;
	shr.u32 	%r47, %r5, 6;
	mov.u32 	%r48, 16;
	sub.s32 	%r6, %r48, %r47;
	mov.u32 	%r49, 15;
	sub.s32 	%r122, %r49, %r47;
	mov.u32 	%r50, 19;
	sub.s32 	%r8, %r50, %r47;
	mov.u32 	%r45, 18;
	// inline asm
	min.s32 	%r44, %r45, %r8;
	// inline asm
	setp.lt.s32 	%p5, %r122, %r44;
	@%p5 bra 	BB0_5;

	mov.u64 	%rl147, 0;
	bra.uni 	BB0_7;

BB0_5:
	mov.u32 	%r51, 1;
	sub.s32 	%r9, %r51, %r6;
	mov.u64 	%rl147, 0;

BB0_6:
	.pragma "nounroll";
	shl.b32 	%r55, %r122, 3;
	mov.u32 	%r56, __internal_i2opi_d;
	add.s32 	%r57, %r56, %r55;
	ld.const.u64 	%rl71, [%r57];
	mul.lo.s64 	%rl73, %rl71, %rl4;
	// inline asm
	mul.hi.u64 	%rl70, %rl71, %rl4;
	// inline asm
	mad.lo.s64 	%rl74, %rl71, %rl4, %rl147;
	setp.lt.u64 	%p6, %rl74, %rl73;
	selp.u64 	%rl75, 1, 0, %p6;
	add.s64 	%rl147, %rl75, %rl70;
	add.s32 	%r58, %r9, %r122;
	shl.b32 	%r59, %r58, 3;
	add.s32 	%r61, %r3, %r59;
	st.local.u64 	[%r61], %rl74;
	// inline asm
	min.s32 	%r52, %r45, %r8;
	// inline asm
	add.s32 	%r122, %r122, 1;
	setp.lt.s32 	%p7, %r122, %r52;
	@%p7 bra 	BB0_6;

BB0_7:
	mov.u32 	%r62, 1;
	sub.s32 	%r63, %r62, %r6;
	add.s32 	%r64, %r63, %r122;
	shl.b32 	%r65, %r64, 3;
	add.s32 	%r67, %r3, %r65;
	st.local.u64 	[%r67], %rl147;
	ld.local.u64 	%rl148, [%r3+24];
	ld.local.u64 	%rl149, [%r3+16];
	and.b32  	%r68, %r5, 63;
	setp.eq.s32 	%p8, %r68, 0;
	@%p8 bra 	BB0_9;

	and.b64  	%rl76, %rl3, 63;
	cvt.u32.u64 	%r69, %rl76;
	shl.b64 	%rl77, %rl148, %r69;
	neg.s32 	%r70, %r5;
	and.b32  	%r71, %r70, 63;
	shr.u64 	%rl78, %rl149, %r71;
	or.b64  	%rl148, %rl78, %rl77;
	shl.b64 	%rl79, %rl149, %r69;
	ld.local.u64 	%rl80, [%r3+8];
	shr.u64 	%rl81, %rl80, %r71;
	or.b64  	%rl149, %rl81, %rl79;

BB0_9:
	shr.u64 	%rl82, %rl148, 62;
	cvt.u32.u64 	%r72, %rl82;
	shr.u64 	%rl83, %rl149, 62;
	shl.b64 	%rl84, %rl148, 2;
	or.b64  	%rl154, %rl83, %rl84;
	shl.b64 	%rl15, %rl149, 2;
	setp.ne.s64 	%p9, %rl15, 0;
	selp.u64 	%rl85, 1, 0, %p9;
	or.b64  	%rl86, %rl85, %rl154;
	setp.gt.u64 	%p10, %rl86, -9223372036854775808;
	selp.u32 	%r73, 1, 0, %p10;
	add.s32 	%r74, %r73, %r72;
	neg.s32 	%r75, %r74;
	setp.lt.s64 	%p11, %rl1, 0;
	selp.b32 	%r124, %r75, %r74, %p11;
	@%p10 bra 	BB0_11;

	mov.u64 	%rl153, %rl15;
	bra.uni 	BB0_12;

BB0_11:
	not.b64 	%rl87, %rl154;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p12, %rl15, 0;
	selp.u64 	%rl88, 1, 0, %p12;
	add.s64 	%rl154, %rl88, %rl87;
	xor.b64  	%rl150, %rl150, -9223372036854775808;
	mov.u64 	%rl153, %rl16;

BB0_12:
	mov.u64 	%rl152, %rl153;
	setp.gt.s64 	%p13, %rl154, 0;
	@%p13 bra 	BB0_14;

	mov.u32 	%r123, 0;
	bra.uni 	BB0_16;

BB0_14:
	mov.u32 	%r123, 0;

BB0_15:
	shr.u64 	%rl89, %rl152, 63;
	shl.b64 	%rl90, %rl154, 1;
	or.b64  	%rl154, %rl89, %rl90;
	shl.b64 	%rl152, %rl152, 1;
	add.s32 	%r123, %r123, -1;
	setp.gt.s64 	%p14, %rl154, 0;
	@%p14 bra 	BB0_15;

BB0_16:
	mul.lo.s64 	%rl156, %rl154, -3958705157555305931;
	mov.u64 	%rl93, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl91, %rl154, %rl93;
	// inline asm
	setp.gt.s64 	%p15, %rl91, 0;
	mov.u64 	%rl155, %rl91;
	@%p15 bra 	BB0_17;
	bra.uni 	BB0_18;

BB0_17:
	shl.b64 	%rl94, %rl91, 1;
	shr.u64 	%rl95, %rl156, 63;
	or.b64  	%rl155, %rl94, %rl95;
	mul.lo.s64 	%rl156, %rl154, -7917410315110611862;
	add.s32 	%r123, %r123, -1;

BB0_18:
	setp.ne.s64 	%p16, %rl156, 0;
	selp.u64 	%rl96, 1, 0, %p16;
	add.s64 	%rl97, %rl96, %rl155;
	add.s32 	%r78, %r123, 1022;
	cvt.u64.u32 	%rl98, %r78;
	shl.b64 	%rl99, %rl98, 52;
	shr.u64 	%rl100, %rl97, 11;
	shr.u64 	%rl101, %rl97, 10;
	and.b64  	%rl102, %rl101, 1;
	add.s64 	%rl103, %rl99, %rl100;
	add.s64 	%rl104, %rl103, %rl102;
	or.b64  	%rl105, %rl104, %rl150;
	mov.b64 	 %fd172, %rl105;

BB0_19:
	add.s32 	%r20, %r124, 1;
	and.b32  	%r79, %r20, 1;
	setp.eq.s32 	%p17, %r79, 0;
	mul.rn.f64 	%fd5, %fd172, %fd172;
	@%p17 bra 	BB0_21;

	mov.f64 	%fd43, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd45, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd42, %fd43, %fd5, %fd45;
	// inline asm
	mov.f64 	%fd49, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd46, %fd42, %fd5, %fd49;
	// inline asm
	mov.f64 	%fd53, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd50, %fd46, %fd5, %fd53;
	// inline asm
	mov.f64 	%fd57, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd54, %fd50, %fd5, %fd57;
	// inline asm
	mov.f64 	%fd61, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd58, %fd54, %fd5, %fd61;
	// inline asm
	mov.f64 	%fd65, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd62, %fd58, %fd5, %fd65;
	// inline asm
	mov.f64 	%fd69, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd66, %fd62, %fd5, %fd69;
	// inline asm
	mov.f64 	%fd173, %fd66;
	bra.uni 	BB0_22;

BB0_21:
	mov.f64 	%fd71, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd73, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd70, %fd71, %fd5, %fd73;
	// inline asm
	mov.f64 	%fd77, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd74, %fd70, %fd5, %fd77;
	// inline asm
	mov.f64 	%fd81, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd78, %fd74, %fd5, %fd81;
	// inline asm
	mov.f64 	%fd85, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd82, %fd78, %fd5, %fd85;
	// inline asm
	mov.f64 	%fd89, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd86, %fd82, %fd5, %fd89;
	// inline asm
	mul.rn.f64 	%fd91, %fd86, %fd5;
	// inline asm
	fma.rn.f64 	%fd90, %fd91, %fd172, %fd172;
	// inline asm
	mov.f64 	%fd173, %fd90;

BB0_22:
	and.b32  	%r80, %r20, 2;
	setp.eq.s32 	%p18, %r80, 0;
	neg.f64 	%fd94, %fd173;
	selp.f64 	%fd174, %fd173, %fd94, %p18;
	bra.uni 	BB0_24;

BB0_23:
	mov.f64 	%fd174, 0dFFF8000000000000;

BB0_24:
	setp.eq.f64 	%p19, %fd1, 0d0000000000000000;
	or.pred  	%p20, %p2, %p19;
	or.pred  	%p21, %p1, %p20;
	@%p21 bra 	BB0_47;

	// inline asm
	abs.f64 	%fd96, %fd1;
	// inline asm
	setp.gt.f64 	%p22, %fd96, 0d41E0000000000000;
	@%p22 bra 	BB0_27;

	mov.f64 	%fd111, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd98, %fd1, %fd111;
	// inline asm
	cvt.rni.s32.f64 	%r81, %fd98;
	// inline asm
	cvt.rn.f64.s32 	%fd112, %r81;
	neg.f64 	%fd108, %fd112;
	mov.f64 	%fd101, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd99, %fd108, %fd101, %fd1;
	// inline asm
	mov.f64 	%fd105, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd103, %fd108, %fd105, %fd99;
	// inline asm
	mov.f64 	%fd109, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd107, %fd108, %fd109, %fd103;
	// inline asm
	mov.u32 	%r127, %r81;
	mov.f64 	%fd175, %fd107;
	bra.uni 	BB0_43;

BB0_27:
	mov.b64 	 %rl33, %fd1;
	and.b64  	%rl160, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl106, %rl35, 2047;
	add.s64 	%rl107, %rl106, 4294966272;
	cvt.u32.u64 	%r22, %rl107;
	shl.b64 	%rl108, %rl33, 11;
	or.b64  	%rl36, %rl108, -9223372036854775808;
	shr.u32 	%r85, %r22, 6;
	mov.u32 	%r86, 16;
	sub.s32 	%r23, %r86, %r85;
	mov.u32 	%r87, 15;
	sub.s32 	%r125, %r87, %r85;
	mov.u32 	%r88, 19;
	sub.s32 	%r25, %r88, %r85;
	mov.u32 	%r83, 18;
	// inline asm
	min.s32 	%r82, %r83, %r25;
	// inline asm
	setp.lt.s32 	%p23, %r125, %r82;
	@%p23 bra 	BB0_29;

	mov.u64 	%rl157, 0;
	bra.uni 	BB0_31;

BB0_29:
	mov.u32 	%r89, 1;
	sub.s32 	%r26, %r89, %r23;
	mov.u64 	%rl157, 0;

BB0_30:
	.pragma "nounroll";
	shl.b32 	%r93, %r125, 3;
	mov.u32 	%r94, __internal_i2opi_d;
	add.s32 	%r95, %r94, %r93;
	ld.const.u64 	%rl112, [%r95];
	mul.lo.s64 	%rl114, %rl112, %rl36;
	// inline asm
	mul.hi.u64 	%rl111, %rl112, %rl36;
	// inline asm
	mad.lo.s64 	%rl115, %rl112, %rl36, %rl157;
	setp.lt.u64 	%p24, %rl115, %rl114;
	selp.u64 	%rl116, 1, 0, %p24;
	add.s64 	%rl157, %rl116, %rl111;
	add.s32 	%r96, %r26, %r125;
	shl.b32 	%r97, %r96, 3;
	add.s32 	%r99, %r3, %r97;
	st.local.u64 	[%r99], %rl115;
	// inline asm
	min.s32 	%r90, %r83, %r25;
	// inline asm
	add.s32 	%r125, %r125, 1;
	setp.lt.s32 	%p25, %r125, %r90;
	@%p25 bra 	BB0_30;

BB0_31:
	mov.u32 	%r100, 1;
	sub.s32 	%r101, %r100, %r23;
	add.s32 	%r102, %r101, %r125;
	shl.b32 	%r103, %r102, 3;
	add.s32 	%r105, %r3, %r103;
	st.local.u64 	[%r105], %rl157;
	ld.local.u64 	%rl158, [%r3+24];
	ld.local.u64 	%rl159, [%r3+16];
	and.b32  	%r106, %r22, 63;
	setp.eq.s32 	%p26, %r106, 0;
	@%p26 bra 	BB0_33;

	and.b64  	%rl117, %rl35, 63;
	cvt.u32.u64 	%r107, %rl117;
	shl.b64 	%rl118, %rl158, %r107;
	neg.s32 	%r108, %r22;
	and.b32  	%r109, %r108, 63;
	shr.u64 	%rl119, %rl159, %r109;
	or.b64  	%rl158, %rl119, %rl118;
	shl.b64 	%rl120, %rl159, %r107;
	ld.local.u64 	%rl121, [%r3+8];
	shr.u64 	%rl122, %rl121, %r109;
	or.b64  	%rl159, %rl122, %rl120;

BB0_33:
	shr.u64 	%rl123, %rl158, 62;
	cvt.u32.u64 	%r110, %rl123;
	shr.u64 	%rl124, %rl159, 62;
	shl.b64 	%rl125, %rl158, 2;
	or.b64  	%rl164, %rl124, %rl125;
	shl.b64 	%rl47, %rl159, 2;
	setp.ne.s64 	%p27, %rl47, 0;
	selp.u64 	%rl126, 1, 0, %p27;
	or.b64  	%rl127, %rl126, %rl164;
	setp.gt.u64 	%p28, %rl127, -9223372036854775808;
	selp.u32 	%r111, 1, 0, %p28;
	add.s32 	%r112, %r111, %r110;
	neg.s32 	%r113, %r112;
	setp.lt.s64 	%p29, %rl33, 0;
	selp.b32 	%r127, %r113, %r112, %p29;
	@%p28 bra 	BB0_35;

	mov.u64 	%rl163, %rl47;
	bra.uni 	BB0_36;

BB0_35:
	not.b64 	%rl128, %rl164;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p30, %rl47, 0;
	selp.u64 	%rl129, 1, 0, %p30;
	add.s64 	%rl164, %rl129, %rl128;
	xor.b64  	%rl160, %rl160, -9223372036854775808;
	mov.u64 	%rl163, %rl48;

BB0_36:
	mov.u64 	%rl162, %rl163;
	setp.gt.s64 	%p31, %rl164, 0;
	@%p31 bra 	BB0_38;

	mov.u32 	%r126, 0;
	bra.uni 	BB0_40;

BB0_38:
	mov.u32 	%r126, 0;

BB0_39:
	shr.u64 	%rl130, %rl162, 63;
	shl.b64 	%rl131, %rl164, 1;
	or.b64  	%rl164, %rl130, %rl131;
	shl.b64 	%rl162, %rl162, 1;
	add.s32 	%r126, %r126, -1;
	setp.gt.s64 	%p32, %rl164, 0;
	@%p32 bra 	BB0_39;

BB0_40:
	mul.lo.s64 	%rl166, %rl164, -3958705157555305931;
	mov.u64 	%rl134, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl132, %rl164, %rl134;
	// inline asm
	setp.gt.s64 	%p33, %rl132, 0;
	mov.u64 	%rl165, %rl132;
	@%p33 bra 	BB0_41;
	bra.uni 	BB0_42;

BB0_41:
	shl.b64 	%rl135, %rl132, 1;
	shr.u64 	%rl136, %rl166, 63;
	or.b64  	%rl165, %rl135, %rl136;
	mul.lo.s64 	%rl166, %rl164, -7917410315110611862;
	add.s32 	%r126, %r126, -1;

BB0_42:
	setp.ne.s64 	%p34, %rl166, 0;
	selp.u64 	%rl137, 1, 0, %p34;
	add.s64 	%rl138, %rl137, %rl165;
	add.s32 	%r116, %r126, 1022;
	cvt.u64.u32 	%rl139, %r116;
	shl.b64 	%rl140, %rl139, 52;
	shr.u64 	%rl141, %rl138, 11;
	shr.u64 	%rl142, %rl138, 10;
	and.b64  	%rl143, %rl142, 1;
	add.s64 	%rl144, %rl140, %rl141;
	add.s64 	%rl145, %rl144, %rl143;
	or.b64  	%rl146, %rl145, %rl160;
	mov.b64 	 %fd175, %rl146;

BB0_43:
	and.b32  	%r117, %r127, 1;
	setp.eq.s32 	%p35, %r117, 0;
	mul.rn.f64 	%fd15, %fd175, %fd175;
	@%p35 bra 	BB0_45;

	mov.f64 	%fd114, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd116, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd113, %fd114, %fd15, %fd116;
	// inline asm
	mov.f64 	%fd120, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd117, %fd113, %fd15, %fd120;
	// inline asm
	mov.f64 	%fd124, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd121, %fd117, %fd15, %fd124;
	// inline asm
	mov.f64 	%fd128, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd125, %fd121, %fd15, %fd128;
	// inline asm
	mov.f64 	%fd132, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd129, %fd125, %fd15, %fd132;
	// inline asm
	mov.f64 	%fd136, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd133, %fd129, %fd15, %fd136;
	// inline asm
	mov.f64 	%fd140, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd137, %fd133, %fd15, %fd140;
	// inline asm
	mov.f64 	%fd176, %fd137;
	bra.uni 	BB0_46;

BB0_45:
	mov.f64 	%fd142, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd144, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd141, %fd142, %fd15, %fd144;
	// inline asm
	mov.f64 	%fd148, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd145, %fd141, %fd15, %fd148;
	// inline asm
	mov.f64 	%fd152, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd149, %fd145, %fd15, %fd152;
	// inline asm
	mov.f64 	%fd156, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd153, %fd149, %fd15, %fd156;
	// inline asm
	mov.f64 	%fd160, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd157, %fd153, %fd15, %fd160;
	// inline asm
	mul.rn.f64 	%fd162, %fd157, %fd15;
	// inline asm
	fma.rn.f64 	%fd161, %fd162, %fd175, %fd175;
	// inline asm
	mov.f64 	%fd176, %fd161;

BB0_46:
	and.b32  	%r118, %r127, 2;
	setp.eq.s32 	%p36, %r118, 0;
	neg.f64 	%fd165, %fd176;
	selp.f64 	%fd177, %fd176, %fd165, %p36;
	bra.uni 	BB0_48;

BB0_47:
	mov.f64 	%fd166, 0d0000000000000000;
	mul.rn.f64 	%fd177, %fd1, %fd166;

BB0_48:
	neg.f64 	%fd167, %fd177;
	shl.b32 	%r119, %r2, 4;
	ld.param.u32 	%r121, [twid1D_param_0];
	add.s32 	%r120, %r121, %r119;
	st.global.v2.f64 	[%r120], {%fd174, %fd167};
	ret;
}

.entry DIT4C2C(
	.param .u32 .ptr .global .align 8 DIT4C2C_param_0,
	.param .u32 .ptr .global .align 16 DIT4C2C_param_1,
	.param .u32 DIT4C2C_param_2,
	.param .u32 DIT4C2C_param_3,
	.param .u32 DIT4C2C_param_4,
	.param .u32 DIT4C2C_param_5
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<822>;
	.reg .pred 	%p<166>;
	.reg .s32 	%r<468>;
	.reg .s64 	%rl<499>;


	mov.u32 	%SP, __local_depot1;
	ld.param.u32 	%r142, [DIT4C2C_param_3];
	// inline asm
	mov.u32 	%r138, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r139, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r140, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r141, %tid.x;
	// inline asm
	add.s32 	%r143, %r141, %r138;
	mad.lo.s32 	%r6, %r140, %r139, %r143;
	mul.hi.s32 	%r144, %r142, -1840700269;
	add.s32 	%r145, %r144, %r142;
	shr.u32 	%r146, %r145, 31;
	shr.s32 	%r147, %r145, 2;
	add.s32 	%r7, %r147, %r146;
	mul.lo.s32 	%r148, %r7, 7;
	sub.s32 	%r8, %r142, %r148;
	setp.gt.s32 	%p15, %r142, 6;
	@%p15 bra 	BB1_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB1_23;

BB1_2:
	mov.f32 	%f1, 0f40E00000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40800000;
	add.f32 	%f2, %f47, 0f40E00000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r447, 0;
	mov.u32 	%r446, 1;

BB1_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p16, %p3, %p3;
	@%p16 bra 	BB1_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mov.f32 	%f55, 0f40000000;
	mul.rn.f32 	%f56, %f55, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p17, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p18, %f1, %f52;
	and.pred  	%p4, %p17, %p18;
	setp.eq.f32 	%p19, %f48, 0f00000000;
	@%p19 bra 	BB1_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r11, %f58;
	shr.u32 	%r151, %r11, 23;
	and.b32  	%r152, %r151, 255;
	add.s32 	%r448, %r152, -127;
	setp.eq.s32 	%p20, %r152, 0;
	mov.f32 	%f280, %f58;
	@%p20 bra 	BB1_6;
	bra.uni 	BB1_7;

BB1_6:
	and.b32  	%r153, %r11, -2139095041;
	or.b32  	%r154, %r153, 1065353216;
	mov.b32 	 %f60, %r154;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r155, %f61;
	shr.u32 	%r156, %r155, 23;
	and.b32  	%r157, %r156, 255;
	add.s32 	%r448, %r157, -253;
	and.b32  	%r158, %r155, -2139095041;
	or.b32  	%r159, %r158, 1065353216;
	mov.b32 	 %f280, %r159;

BB1_7:
	mov.b32 	 %r160, %f280;
	and.b32  	%r161, %r160, -2139095041;
	or.b32  	%r162, %r161, 1065353216;
	mov.b32 	 %f281, %r162;
	setp.gt.f32 	%p21, %f281, 0f3FB504F3;
	@%p21 bra 	BB1_8;
	bra.uni 	BB1_9;

BB1_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r448, %r448, 1;

BB1_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f55, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r163, %f73;
	and.b32  	%r164, %r163, -4096;
	mov.b32 	 %f81, %r164;
	mov.b32 	 %r165, %f64;
	and.b32  	%r166, %r165, -4096;
	mov.b32 	 %f82, %r166;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f55, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r448;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p22, %f69, 0f77F684DF;
	@%p22 bra 	BB1_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB1_12;

BB1_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB1_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r17, %f131;
	setp.eq.s32 	%p23, %r17, 1118925336;
	@%p23 bra 	BB1_13;
	bra.uni 	BB1_14;

BB1_13:
	add.s32 	%r167, %r17, -1;
	mov.b32 	 %f133, %r167;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB1_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p24, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p24;
	setp.gt.f32 	%p25, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p25;
	setp.neu.f32 	%p26, %f19, %f3;
	@%p26 bra 	BB1_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB1_21;

BB1_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB1_21;

BB1_17:
	@%p3 bra 	BB1_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB1_21;

BB1_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB1_21;

BB1_20:
	mov.f32 	%f283, %f7;

BB1_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r446;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r446, %f157;
	add.s32 	%r447, %r447, 1;
	setp.lt.s32 	%p27, %r447, %r7;
	@%p27 bra 	BB1_3;

	cvt.rn.f32.s32 	%f284, %r446;

BB1_23:
	mov.f32 	%f159, 0f40800000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r8;
	setp.eq.f32 	%p28, %f287, 0f00000000;
	@%p28 bra 	BB1_45;

	setp.nan.f32 	%p29, %f287, %f287;
	@%p29 bra 	BB1_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p30, %f287, 0f7F800000;
	setp.eq.f32 	%p31, %f287, 0fFF800000;
	or.pred  	%p32, %p30, %p31;
	@%p32 bra 	BB1_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p35, %f158, 0f00000000;
	@%p35 bra 	BB1_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r20, %f168;
	shr.u32 	%r168, %r20, 23;
	and.b32  	%r169, %r168, 255;
	add.s32 	%r449, %r169, -127;
	setp.eq.s32 	%p36, %r169, 0;
	mov.f32 	%f285, %f168;
	@%p36 bra 	BB1_28;
	bra.uni 	BB1_29;

BB1_28:
	and.b32  	%r170, %r20, -2139095041;
	or.b32  	%r171, %r170, 1065353216;
	mov.b32 	 %f170, %r171;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r172, %f171;
	shr.u32 	%r173, %r172, 23;
	and.b32  	%r174, %r173, 255;
	add.s32 	%r449, %r174, -253;
	and.b32  	%r175, %r172, -2139095041;
	or.b32  	%r176, %r175, 1065353216;
	mov.b32 	 %f285, %r176;

BB1_29:
	mov.b32 	 %r177, %f285;
	and.b32  	%r178, %r177, -2139095041;
	or.b32  	%r179, %r178, 1065353216;
	mov.b32 	 %f286, %r179;
	setp.gt.f32 	%p37, %f286, 0f3FB504F3;
	@%p37 bra 	BB1_30;
	bra.uni 	BB1_31;

BB1_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r449, %r449, 1;

BB1_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mov.f32 	%f182, 0f40000000;
	mul.rn.f32 	%f183, %f182, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r180, %f183;
	and.b32  	%r181, %r180, -4096;
	mov.b32 	 %f191, %r181;
	mov.b32 	 %r182, %f174;
	and.b32  	%r183, %r182, -4096;
	mov.b32 	 %f192, %r183;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f182, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r449;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p38, %f179, 0f77F684DF;
	@%p38 bra 	BB1_32;
	bra.uni 	BB1_33;

BB1_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB1_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r26, %f241;
	setp.eq.s32 	%p39, %r26, 1118925336;
	@%p39 bra 	BB1_34;
	bra.uni 	BB1_35;

BB1_34:
	add.s32 	%r184, %r26, -1;
	mov.b32 	 %f243, %r184;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB1_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p40, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p40;
	setp.gt.f32 	%p41, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p41;
	setp.neu.f32 	%p42, %f39, %f27;
	@%p42 bra 	BB1_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB1_46;

BB1_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB1_46;

BB1_38:
	setp.lt.f32 	%p43, %f287, 0f00000000;
	@%p43 bra 	BB1_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB1_46;

BB1_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB1_46;

BB1_41:
	setp.lt.f32 	%p44, %f158, 0f3F800000;
	mov.b32 	 %r185, %f287;
	setp.lt.s32 	%p6, %r185, 0;
	@%p44 bra 	BB1_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB1_46;

BB1_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB1_46;

BB1_44:
	add.f32 	%f288, %f287, 0f40800000;
	bra.uni 	BB1_46;

BB1_45:
	mov.f32 	%f288, 0f3F800000;

BB1_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r27, %f267;
	shr.s32 	%r186, %r27, 31;
	shr.u32 	%r187, %r186, 30;
	add.s32 	%r188, %r27, %r187;
	shr.s32 	%r189, %r188, 2;
	div.s32 	%r190, %r6, %r189;
	rem.s32 	%r28, %r6, %r189;
	mad.lo.s32 	%r191, %r190, %r27, %r28;
	shl.b32 	%r192, %r191, 4;
	ld.param.u32 	%r437, [DIT4C2C_param_0];
	add.s32 	%r29, %r437, %r192;
	ld.global.f64 	%fd74, [%r29];
	ld.global.f64 	%fd75, [%r29+8];
	shl.b32 	%r193, %r189, 4;
	add.s32 	%r30, %r29, %r193;
	ld.global.f64 	%fd1, [%r30];
	ld.global.f64 	%fd2, [%r30+8];
	add.s32 	%r31, %r30, %r193;
	ld.global.f64 	%fd3, [%r31];
	ld.global.f64 	%fd4, [%r31+8];
	add.s32 	%r32, %r31, %r193;
	ld.global.f64 	%fd5, [%r32];
	ld.global.f64 	%fd6, [%r32+8];
	ld.param.u32 	%r441, [DIT4C2C_param_2];
	div.s32 	%r194, %r441, %r27;
	mul.lo.s32 	%r33, %r194, %r28;
	shr.s32 	%r195, %r441, 31;
	shr.u32 	%r196, %r195, 30;
	add.s32 	%r197, %r441, %r196;
	shr.s32 	%r34, %r197, 2;
	ld.param.u32 	%r445, [DIT4C2C_param_5];
	setp.eq.s32 	%p45, %r445, 1;
	mov.f64 	%fd812, %fd74;
	mov.f64 	%fd813, %fd75;
	mov.f64 	%fd814, %fd1;
	mov.f64 	%fd815, %fd2;
	mov.f64 	%fd816, %fd3;
	mov.f64 	%fd817, %fd4;
	mov.f64 	%fd818, %fd5;
	mov.f64 	%fd819, %fd6;
	@%p45 bra 	BB1_193;

	setp.eq.s32 	%p46, %r28, 0;
	@%p46 bra 	BB1_232;

	cvt.rn.f64.s32 	%fd7, %r28;
	mul.f64 	%fd76, %fd7, 0d402921FB54442D18;
	cvt.rn.f64.s32 	%fd8, %r27;
	div.rn.f64 	%fd9, %fd76, %fd8;
	mov.f64 	%fd10, 0d7FF0000000000000;
	setp.eq.f64 	%p7, %fd9, 0d7FF0000000000000;
	mov.f64 	%fd11, 0dFFF0000000000000;
	setp.eq.f64 	%p8, %fd9, 0dFFF0000000000000;
	or.pred  	%p47, %p7, %p8;
	add.u32 	%r35, %SP, 0;
	@%p47 bra 	BB1_71;

	// inline asm
	abs.f64 	%fd77, %fd9;
	// inline asm
	setp.gt.f64 	%p48, %fd77, 0d41E0000000000000;
	@%p48 bra 	BB1_51;

	mov.f64 	%fd92, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd79, %fd9, %fd92;
	// inline asm
	cvt.rni.s32.f64 	%r198, %fd79;
	// inline asm
	cvt.rn.f64.s32 	%fd93, %r198;
	neg.f64 	%fd89, %fd93;
	mov.f64 	%fd82, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd80, %fd89, %fd82, %fd9;
	// inline asm
	mov.f64 	%fd86, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd84, %fd89, %fd86, %fd80;
	// inline asm
	mov.f64 	%fd90, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd88, %fd89, %fd90, %fd84;
	// inline asm
	mov.u32 	%r452, %r198;
	mov.f64 	%fd794, %fd88;
	bra.uni 	BB1_67;

BB1_51:
	mov.b64 	 %rl1, %fd9;
	and.b64  	%rl442, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl193, %rl3, 2047;
	add.s64 	%rl194, %rl193, 4294966272;
	cvt.u32.u64 	%r37, %rl194;
	shl.b64 	%rl195, %rl1, 11;
	or.b64  	%rl4, %rl195, -9223372036854775808;
	shr.u32 	%r202, %r37, 6;
	mov.u32 	%r203, 16;
	sub.s32 	%r38, %r203, %r202;
	mov.u32 	%r204, 15;
	sub.s32 	%r450, %r204, %r202;
	mov.u32 	%r205, 19;
	sub.s32 	%r40, %r205, %r202;
	mov.u32 	%r200, 18;
	// inline asm
	min.s32 	%r199, %r200, %r40;
	// inline asm
	setp.lt.s32 	%p49, %r450, %r199;
	@%p49 bra 	BB1_53;

	mov.u64 	%rl439, 0;
	bra.uni 	BB1_55;

BB1_53:
	mov.u32 	%r206, 1;
	sub.s32 	%r41, %r206, %r38;
	mov.u64 	%rl439, 0;

BB1_54:
	.pragma "nounroll";
	shl.b32 	%r210, %r450, 3;
	mov.u32 	%r211, __internal_i2opi_d;
	add.s32 	%r212, %r211, %r210;
	ld.const.u64 	%rl199, [%r212];
	mul.lo.s64 	%rl201, %rl199, %rl4;
	// inline asm
	mul.hi.u64 	%rl198, %rl199, %rl4;
	// inline asm
	mad.lo.s64 	%rl202, %rl199, %rl4, %rl439;
	setp.lt.u64 	%p50, %rl202, %rl201;
	selp.u64 	%rl203, 1, 0, %p50;
	add.s64 	%rl439, %rl203, %rl198;
	add.s32 	%r213, %r41, %r450;
	shl.b32 	%r214, %r213, 3;
	add.s32 	%r216, %r35, %r214;
	st.local.u64 	[%r216], %rl202;
	// inline asm
	min.s32 	%r207, %r200, %r40;
	// inline asm
	add.s32 	%r450, %r450, 1;
	setp.lt.s32 	%p51, %r450, %r207;
	@%p51 bra 	BB1_54;

BB1_55:
	mov.u32 	%r217, 1;
	sub.s32 	%r218, %r217, %r38;
	add.s32 	%r219, %r218, %r450;
	shl.b32 	%r220, %r219, 3;
	add.s32 	%r222, %r35, %r220;
	st.local.u64 	[%r222], %rl439;
	ld.local.u64 	%rl440, [%r35+24];
	ld.local.u64 	%rl441, [%r35+16];
	and.b32  	%r223, %r37, 63;
	setp.eq.s32 	%p52, %r223, 0;
	@%p52 bra 	BB1_57;

	and.b64  	%rl204, %rl3, 63;
	cvt.u32.u64 	%r224, %rl204;
	shl.b64 	%rl205, %rl440, %r224;
	neg.s32 	%r225, %r37;
	and.b32  	%r226, %r225, 63;
	shr.u64 	%rl206, %rl441, %r226;
	or.b64  	%rl440, %rl206, %rl205;
	shl.b64 	%rl207, %rl441, %r224;
	ld.local.u64 	%rl208, [%r35+8];
	shr.u64 	%rl209, %rl208, %r226;
	or.b64  	%rl441, %rl209, %rl207;

BB1_57:
	shr.u64 	%rl210, %rl440, 62;
	cvt.u32.u64 	%r227, %rl210;
	shr.u64 	%rl211, %rl441, 62;
	shl.b64 	%rl212, %rl440, 2;
	or.b64  	%rl446, %rl211, %rl212;
	shl.b64 	%rl15, %rl441, 2;
	setp.ne.s64 	%p53, %rl15, 0;
	selp.u64 	%rl213, 1, 0, %p53;
	or.b64  	%rl214, %rl213, %rl446;
	setp.gt.u64 	%p54, %rl214, -9223372036854775808;
	selp.u32 	%r228, 1, 0, %p54;
	add.s32 	%r229, %r228, %r227;
	neg.s32 	%r230, %r229;
	setp.lt.s64 	%p55, %rl1, 0;
	selp.b32 	%r452, %r230, %r229, %p55;
	@%p54 bra 	BB1_59;

	mov.u64 	%rl445, %rl15;
	bra.uni 	BB1_60;

BB1_59:
	not.b64 	%rl215, %rl446;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p56, %rl15, 0;
	selp.u64 	%rl216, 1, 0, %p56;
	add.s64 	%rl446, %rl216, %rl215;
	xor.b64  	%rl442, %rl442, -9223372036854775808;
	mov.u64 	%rl445, %rl16;

BB1_60:
	mov.u64 	%rl444, %rl445;
	setp.gt.s64 	%p57, %rl446, 0;
	@%p57 bra 	BB1_62;

	mov.u32 	%r451, 0;
	bra.uni 	BB1_64;

BB1_62:
	mov.u32 	%r451, 0;

BB1_63:
	shr.u64 	%rl217, %rl444, 63;
	shl.b64 	%rl218, %rl446, 1;
	or.b64  	%rl446, %rl217, %rl218;
	shl.b64 	%rl444, %rl444, 1;
	add.s32 	%r451, %r451, -1;
	setp.gt.s64 	%p58, %rl446, 0;
	@%p58 bra 	BB1_63;

BB1_64:
	mul.lo.s64 	%rl448, %rl446, -3958705157555305931;
	mov.u64 	%rl221, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl219, %rl446, %rl221;
	// inline asm
	setp.gt.s64 	%p59, %rl219, 0;
	mov.u64 	%rl447, %rl219;
	@%p59 bra 	BB1_65;
	bra.uni 	BB1_66;

BB1_65:
	shl.b64 	%rl222, %rl219, 1;
	shr.u64 	%rl223, %rl448, 63;
	or.b64  	%rl447, %rl222, %rl223;
	mul.lo.s64 	%rl448, %rl446, -7917410315110611862;
	add.s32 	%r451, %r451, -1;

BB1_66:
	setp.ne.s64 	%p60, %rl448, 0;
	selp.u64 	%rl224, 1, 0, %p60;
	add.s64 	%rl225, %rl224, %rl447;
	add.s32 	%r233, %r451, 1022;
	cvt.u64.u32 	%rl226, %r233;
	shl.b64 	%rl227, %rl226, 52;
	shr.u64 	%rl228, %rl225, 11;
	shr.u64 	%rl229, %rl225, 10;
	and.b64  	%rl230, %rl229, 1;
	add.s64 	%rl231, %rl227, %rl228;
	add.s64 	%rl232, %rl231, %rl230;
	or.b64  	%rl233, %rl232, %rl442;
	mov.b64 	 %fd794, %rl233;

BB1_67:
	add.s32 	%r52, %r452, 1;
	and.b32  	%r234, %r52, 1;
	setp.eq.s32 	%p61, %r234, 0;
	mul.rn.f64 	%fd15, %fd794, %fd794;
	@%p61 bra 	BB1_69;

	mov.f64 	%fd95, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd97, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd94, %fd95, %fd15, %fd97;
	// inline asm
	mov.f64 	%fd101, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd98, %fd94, %fd15, %fd101;
	// inline asm
	mov.f64 	%fd105, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd102, %fd98, %fd15, %fd105;
	// inline asm
	mov.f64 	%fd109, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd106, %fd102, %fd15, %fd109;
	// inline asm
	mov.f64 	%fd113, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd110, %fd106, %fd15, %fd113;
	// inline asm
	mov.f64 	%fd117, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd114, %fd110, %fd15, %fd117;
	// inline asm
	mov.f64 	%fd121, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd118, %fd114, %fd15, %fd121;
	// inline asm
	mov.f64 	%fd795, %fd118;
	bra.uni 	BB1_70;

BB1_69:
	mov.f64 	%fd123, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd125, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd122, %fd123, %fd15, %fd125;
	// inline asm
	mov.f64 	%fd129, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd126, %fd122, %fd15, %fd129;
	// inline asm
	mov.f64 	%fd133, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd130, %fd126, %fd15, %fd133;
	// inline asm
	mov.f64 	%fd137, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd134, %fd130, %fd15, %fd137;
	// inline asm
	mov.f64 	%fd141, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd138, %fd134, %fd15, %fd141;
	// inline asm
	mul.rn.f64 	%fd143, %fd138, %fd15;
	// inline asm
	fma.rn.f64 	%fd142, %fd143, %fd794, %fd794;
	// inline asm
	mov.f64 	%fd795, %fd142;

BB1_70:
	and.b32  	%r235, %r52, 2;
	setp.eq.s32 	%p62, %r235, 0;
	neg.f64 	%fd146, %fd795;
	selp.f64 	%fd796, %fd795, %fd146, %p62;
	bra.uni 	BB1_72;

BB1_71:
	mov.f64 	%fd796, 0dFFF8000000000000;

BB1_72:
	setp.eq.f64 	%p63, %fd9, 0d0000000000000000;
	or.pred  	%p64, %p8, %p63;
	or.pred  	%p65, %p7, %p64;
	@%p65 bra 	BB1_95;

	// inline asm
	abs.f64 	%fd147, %fd9;
	// inline asm
	setp.gt.f64 	%p66, %fd147, 0d41E0000000000000;
	@%p66 bra 	BB1_75;

	mov.f64 	%fd162, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd149, %fd9, %fd162;
	// inline asm
	cvt.rni.s32.f64 	%r236, %fd149;
	// inline asm
	cvt.rn.f64.s32 	%fd163, %r236;
	neg.f64 	%fd159, %fd163;
	mov.f64 	%fd152, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd150, %fd159, %fd152, %fd9;
	// inline asm
	mov.f64 	%fd156, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd154, %fd159, %fd156, %fd150;
	// inline asm
	mov.f64 	%fd160, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd158, %fd159, %fd160, %fd154;
	// inline asm
	mov.u32 	%r455, %r236;
	mov.f64 	%fd797, %fd158;
	bra.uni 	BB1_91;

BB1_75:
	mov.b64 	 %rl33, %fd9;
	and.b64  	%rl452, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl234, %rl35, 2047;
	add.s64 	%rl235, %rl234, 4294966272;
	cvt.u32.u64 	%r54, %rl235;
	shl.b64 	%rl236, %rl33, 11;
	or.b64  	%rl36, %rl236, -9223372036854775808;
	shr.u32 	%r240, %r54, 6;
	mov.u32 	%r241, 16;
	sub.s32 	%r55, %r241, %r240;
	mov.u32 	%r242, 15;
	sub.s32 	%r453, %r242, %r240;
	mov.u32 	%r243, 19;
	sub.s32 	%r57, %r243, %r240;
	mov.u32 	%r238, 18;
	// inline asm
	min.s32 	%r237, %r238, %r57;
	// inline asm
	setp.lt.s32 	%p67, %r453, %r237;
	@%p67 bra 	BB1_77;

	mov.u64 	%rl449, 0;
	bra.uni 	BB1_79;

BB1_77:
	mov.u32 	%r244, 1;
	sub.s32 	%r58, %r244, %r55;
	mov.u64 	%rl449, 0;

BB1_78:
	.pragma "nounroll";
	shl.b32 	%r248, %r453, 3;
	mov.u32 	%r249, __internal_i2opi_d;
	add.s32 	%r250, %r249, %r248;
	ld.const.u64 	%rl240, [%r250];
	mul.lo.s64 	%rl242, %rl240, %rl36;
	// inline asm
	mul.hi.u64 	%rl239, %rl240, %rl36;
	// inline asm
	mad.lo.s64 	%rl243, %rl240, %rl36, %rl449;
	setp.lt.u64 	%p68, %rl243, %rl242;
	selp.u64 	%rl244, 1, 0, %p68;
	add.s64 	%rl449, %rl244, %rl239;
	add.s32 	%r251, %r58, %r453;
	shl.b32 	%r252, %r251, 3;
	add.s32 	%r254, %r35, %r252;
	st.local.u64 	[%r254], %rl243;
	// inline asm
	min.s32 	%r245, %r238, %r57;
	// inline asm
	add.s32 	%r453, %r453, 1;
	setp.lt.s32 	%p69, %r453, %r245;
	@%p69 bra 	BB1_78;

BB1_79:
	mov.u32 	%r255, 1;
	sub.s32 	%r256, %r255, %r55;
	add.s32 	%r257, %r256, %r453;
	shl.b32 	%r258, %r257, 3;
	add.s32 	%r260, %r35, %r258;
	st.local.u64 	[%r260], %rl449;
	ld.local.u64 	%rl450, [%r35+24];
	ld.local.u64 	%rl451, [%r35+16];
	and.b32  	%r261, %r54, 63;
	setp.eq.s32 	%p70, %r261, 0;
	@%p70 bra 	BB1_81;

	and.b64  	%rl245, %rl35, 63;
	cvt.u32.u64 	%r262, %rl245;
	shl.b64 	%rl246, %rl450, %r262;
	neg.s32 	%r263, %r54;
	and.b32  	%r264, %r263, 63;
	shr.u64 	%rl247, %rl451, %r264;
	or.b64  	%rl450, %rl247, %rl246;
	shl.b64 	%rl248, %rl451, %r262;
	ld.local.u64 	%rl249, [%r35+8];
	shr.u64 	%rl250, %rl249, %r264;
	or.b64  	%rl451, %rl250, %rl248;

BB1_81:
	shr.u64 	%rl251, %rl450, 62;
	cvt.u32.u64 	%r265, %rl251;
	shr.u64 	%rl252, %rl451, 62;
	shl.b64 	%rl253, %rl450, 2;
	or.b64  	%rl456, %rl252, %rl253;
	shl.b64 	%rl47, %rl451, 2;
	setp.ne.s64 	%p71, %rl47, 0;
	selp.u64 	%rl254, 1, 0, %p71;
	or.b64  	%rl255, %rl254, %rl456;
	setp.gt.u64 	%p72, %rl255, -9223372036854775808;
	selp.u32 	%r266, 1, 0, %p72;
	add.s32 	%r267, %r266, %r265;
	neg.s32 	%r268, %r267;
	setp.lt.s64 	%p73, %rl33, 0;
	selp.b32 	%r455, %r268, %r267, %p73;
	@%p72 bra 	BB1_83;

	mov.u64 	%rl455, %rl47;
	bra.uni 	BB1_84;

BB1_83:
	not.b64 	%rl256, %rl456;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p74, %rl47, 0;
	selp.u64 	%rl257, 1, 0, %p74;
	add.s64 	%rl456, %rl257, %rl256;
	xor.b64  	%rl452, %rl452, -9223372036854775808;
	mov.u64 	%rl455, %rl48;

BB1_84:
	mov.u64 	%rl454, %rl455;
	setp.gt.s64 	%p75, %rl456, 0;
	@%p75 bra 	BB1_86;

	mov.u32 	%r454, 0;
	bra.uni 	BB1_88;

BB1_86:
	mov.u32 	%r454, 0;

BB1_87:
	shr.u64 	%rl258, %rl454, 63;
	shl.b64 	%rl259, %rl456, 1;
	or.b64  	%rl456, %rl258, %rl259;
	shl.b64 	%rl454, %rl454, 1;
	add.s32 	%r454, %r454, -1;
	setp.gt.s64 	%p76, %rl456, 0;
	@%p76 bra 	BB1_87;

BB1_88:
	mul.lo.s64 	%rl458, %rl456, -3958705157555305931;
	mov.u64 	%rl262, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl260, %rl456, %rl262;
	// inline asm
	setp.gt.s64 	%p77, %rl260, 0;
	mov.u64 	%rl457, %rl260;
	@%p77 bra 	BB1_89;
	bra.uni 	BB1_90;

BB1_89:
	shl.b64 	%rl263, %rl260, 1;
	shr.u64 	%rl264, %rl458, 63;
	or.b64  	%rl457, %rl263, %rl264;
	mul.lo.s64 	%rl458, %rl456, -7917410315110611862;
	add.s32 	%r454, %r454, -1;

BB1_90:
	setp.ne.s64 	%p78, %rl458, 0;
	selp.u64 	%rl265, 1, 0, %p78;
	add.s64 	%rl266, %rl265, %rl457;
	add.s32 	%r271, %r454, 1022;
	cvt.u64.u32 	%rl267, %r271;
	shl.b64 	%rl268, %rl267, 52;
	shr.u64 	%rl269, %rl266, 11;
	shr.u64 	%rl270, %rl266, 10;
	and.b64  	%rl271, %rl270, 1;
	add.s64 	%rl272, %rl268, %rl269;
	add.s64 	%rl273, %rl272, %rl271;
	or.b64  	%rl274, %rl273, %rl452;
	mov.b64 	 %fd797, %rl274;

BB1_91:
	and.b32  	%r272, %r455, 1;
	setp.eq.s32 	%p79, %r272, 0;
	mul.rn.f64 	%fd25, %fd797, %fd797;
	@%p79 bra 	BB1_93;

	mov.f64 	%fd165, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd167, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd164, %fd165, %fd25, %fd167;
	// inline asm
	mov.f64 	%fd171, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd168, %fd164, %fd25, %fd171;
	// inline asm
	mov.f64 	%fd175, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd172, %fd168, %fd25, %fd175;
	// inline asm
	mov.f64 	%fd179, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd176, %fd172, %fd25, %fd179;
	// inline asm
	mov.f64 	%fd183, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd180, %fd176, %fd25, %fd183;
	// inline asm
	mov.f64 	%fd187, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd184, %fd180, %fd25, %fd187;
	// inline asm
	mov.f64 	%fd191, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd188, %fd184, %fd25, %fd191;
	// inline asm
	mov.f64 	%fd798, %fd188;
	bra.uni 	BB1_94;

BB1_93:
	mov.f64 	%fd193, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd195, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd192, %fd193, %fd25, %fd195;
	// inline asm
	mov.f64 	%fd199, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd196, %fd192, %fd25, %fd199;
	// inline asm
	mov.f64 	%fd203, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd200, %fd196, %fd25, %fd203;
	// inline asm
	mov.f64 	%fd207, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd204, %fd200, %fd25, %fd207;
	// inline asm
	mov.f64 	%fd211, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd208, %fd204, %fd25, %fd211;
	// inline asm
	mul.rn.f64 	%fd213, %fd208, %fd25;
	// inline asm
	fma.rn.f64 	%fd212, %fd213, %fd797, %fd797;
	// inline asm
	mov.f64 	%fd798, %fd212;

BB1_94:
	and.b32  	%r273, %r455, 2;
	setp.eq.s32 	%p80, %r273, 0;
	neg.f64 	%fd216, %fd798;
	selp.f64 	%fd799, %fd798, %fd216, %p80;
	bra.uni 	BB1_96;

BB1_95:
	mov.f64 	%fd217, 0d0000000000000000;
	mul.rn.f64 	%fd799, %fd9, %fd217;

BB1_96:
	mul.f64 	%fd218, %fd2, %fd799;
	fma.rn.f64 	%fd219, %fd1, %fd796, %fd218;
	mul.f64 	%fd220, %fd2, %fd796;
	neg.f64 	%fd221, %fd1;
	fma.rn.f64 	%fd222, %fd221, %fd799, %fd220;
	mul.f64 	%fd223, %fd7, 0d401921FB54442D18;
	div.rn.f64 	%fd32, %fd223, %fd8;
	setp.eq.f64 	%p9, %fd32, %fd10;
	setp.eq.f64 	%p10, %fd32, %fd11;
	or.pred  	%p81, %p9, %p10;
	@%p81 bra 	BB1_119;

	// inline asm
	abs.f64 	%fd224, %fd32;
	// inline asm
	setp.gt.f64 	%p82, %fd224, 0d41E0000000000000;
	@%p82 bra 	BB1_99;

	mov.f64 	%fd239, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd226, %fd32, %fd239;
	// inline asm
	cvt.rni.s32.f64 	%r274, %fd226;
	// inline asm
	cvt.rn.f64.s32 	%fd240, %r274;
	neg.f64 	%fd236, %fd240;
	mov.f64 	%fd229, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd227, %fd236, %fd229, %fd32;
	// inline asm
	mov.f64 	%fd233, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd231, %fd236, %fd233, %fd227;
	// inline asm
	mov.f64 	%fd237, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd235, %fd236, %fd237, %fd231;
	// inline asm
	mov.u32 	%r458, %r274;
	mov.f64 	%fd800, %fd235;
	bra.uni 	BB1_115;

BB1_99:
	mov.b64 	 %rl65, %fd32;
	and.b64  	%rl462, %rl65, -9223372036854775808;
	shr.u64 	%rl67, %rl65, 52;
	and.b64  	%rl275, %rl67, 2047;
	add.s64 	%rl276, %rl275, 4294966272;
	cvt.u32.u64 	%r70, %rl276;
	shl.b64 	%rl277, %rl65, 11;
	or.b64  	%rl68, %rl277, -9223372036854775808;
	shr.u32 	%r278, %r70, 6;
	mov.u32 	%r279, 16;
	sub.s32 	%r71, %r279, %r278;
	mov.u32 	%r280, 15;
	sub.s32 	%r456, %r280, %r278;
	mov.u32 	%r281, 19;
	sub.s32 	%r73, %r281, %r278;
	mov.u32 	%r276, 18;
	// inline asm
	min.s32 	%r275, %r276, %r73;
	// inline asm
	setp.lt.s32 	%p83, %r456, %r275;
	@%p83 bra 	BB1_101;

	mov.u64 	%rl459, 0;
	bra.uni 	BB1_103;

BB1_101:
	mov.u32 	%r282, 1;
	sub.s32 	%r74, %r282, %r71;
	mov.u64 	%rl459, 0;

BB1_102:
	.pragma "nounroll";
	shl.b32 	%r286, %r456, 3;
	mov.u32 	%r287, __internal_i2opi_d;
	add.s32 	%r288, %r287, %r286;
	ld.const.u64 	%rl281, [%r288];
	mul.lo.s64 	%rl283, %rl281, %rl68;
	// inline asm
	mul.hi.u64 	%rl280, %rl281, %rl68;
	// inline asm
	mad.lo.s64 	%rl284, %rl281, %rl68, %rl459;
	setp.lt.u64 	%p84, %rl284, %rl283;
	selp.u64 	%rl285, 1, 0, %p84;
	add.s64 	%rl459, %rl285, %rl280;
	add.s32 	%r289, %r74, %r456;
	shl.b32 	%r290, %r289, 3;
	add.s32 	%r292, %r35, %r290;
	st.local.u64 	[%r292], %rl284;
	// inline asm
	min.s32 	%r283, %r276, %r73;
	// inline asm
	add.s32 	%r456, %r456, 1;
	setp.lt.s32 	%p85, %r456, %r283;
	@%p85 bra 	BB1_102;

BB1_103:
	mov.u32 	%r293, 1;
	sub.s32 	%r294, %r293, %r71;
	add.s32 	%r295, %r294, %r456;
	shl.b32 	%r296, %r295, 3;
	add.s32 	%r298, %r35, %r296;
	st.local.u64 	[%r298], %rl459;
	ld.local.u64 	%rl460, [%r35+24];
	ld.local.u64 	%rl461, [%r35+16];
	and.b32  	%r299, %r70, 63;
	setp.eq.s32 	%p86, %r299, 0;
	@%p86 bra 	BB1_105;

	and.b64  	%rl286, %rl67, 63;
	cvt.u32.u64 	%r300, %rl286;
	shl.b64 	%rl287, %rl460, %r300;
	neg.s32 	%r301, %r70;
	and.b32  	%r302, %r301, 63;
	shr.u64 	%rl288, %rl461, %r302;
	or.b64  	%rl460, %rl288, %rl287;
	shl.b64 	%rl289, %rl461, %r300;
	ld.local.u64 	%rl290, [%r35+8];
	shr.u64 	%rl291, %rl290, %r302;
	or.b64  	%rl461, %rl291, %rl289;

BB1_105:
	shr.u64 	%rl292, %rl460, 62;
	cvt.u32.u64 	%r303, %rl292;
	shr.u64 	%rl293, %rl461, 62;
	shl.b64 	%rl294, %rl460, 2;
	or.b64  	%rl466, %rl293, %rl294;
	shl.b64 	%rl79, %rl461, 2;
	setp.ne.s64 	%p87, %rl79, 0;
	selp.u64 	%rl295, 1, 0, %p87;
	or.b64  	%rl296, %rl295, %rl466;
	setp.gt.u64 	%p88, %rl296, -9223372036854775808;
	selp.u32 	%r304, 1, 0, %p88;
	add.s32 	%r305, %r304, %r303;
	neg.s32 	%r306, %r305;
	setp.lt.s64 	%p89, %rl65, 0;
	selp.b32 	%r458, %r306, %r305, %p89;
	@%p88 bra 	BB1_107;

	mov.u64 	%rl465, %rl79;
	bra.uni 	BB1_108;

BB1_107:
	not.b64 	%rl297, %rl466;
	neg.s64 	%rl80, %rl79;
	setp.eq.s64 	%p90, %rl79, 0;
	selp.u64 	%rl298, 1, 0, %p90;
	add.s64 	%rl466, %rl298, %rl297;
	xor.b64  	%rl462, %rl462, -9223372036854775808;
	mov.u64 	%rl465, %rl80;

BB1_108:
	mov.u64 	%rl464, %rl465;
	setp.gt.s64 	%p91, %rl466, 0;
	@%p91 bra 	BB1_110;

	mov.u32 	%r457, 0;
	bra.uni 	BB1_112;

BB1_110:
	mov.u32 	%r457, 0;

BB1_111:
	shr.u64 	%rl299, %rl464, 63;
	shl.b64 	%rl300, %rl466, 1;
	or.b64  	%rl466, %rl299, %rl300;
	shl.b64 	%rl464, %rl464, 1;
	add.s32 	%r457, %r457, -1;
	setp.gt.s64 	%p92, %rl466, 0;
	@%p92 bra 	BB1_111;

BB1_112:
	mul.lo.s64 	%rl468, %rl466, -3958705157555305931;
	mov.u64 	%rl303, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl301, %rl466, %rl303;
	// inline asm
	setp.gt.s64 	%p93, %rl301, 0;
	mov.u64 	%rl467, %rl301;
	@%p93 bra 	BB1_113;
	bra.uni 	BB1_114;

BB1_113:
	shl.b64 	%rl304, %rl301, 1;
	shr.u64 	%rl305, %rl468, 63;
	or.b64  	%rl467, %rl304, %rl305;
	mul.lo.s64 	%rl468, %rl466, -7917410315110611862;
	add.s32 	%r457, %r457, -1;

BB1_114:
	setp.ne.s64 	%p94, %rl468, 0;
	selp.u64 	%rl306, 1, 0, %p94;
	add.s64 	%rl307, %rl306, %rl467;
	add.s32 	%r309, %r457, 1022;
	cvt.u64.u32 	%rl308, %r309;
	shl.b64 	%rl309, %rl308, 52;
	shr.u64 	%rl310, %rl307, 11;
	shr.u64 	%rl311, %rl307, 10;
	and.b64  	%rl312, %rl311, 1;
	add.s64 	%rl313, %rl309, %rl310;
	add.s64 	%rl314, %rl313, %rl312;
	or.b64  	%rl315, %rl314, %rl462;
	mov.b64 	 %fd800, %rl315;

BB1_115:
	add.s32 	%r85, %r458, 1;
	and.b32  	%r310, %r85, 1;
	setp.eq.s32 	%p95, %r310, 0;
	mul.rn.f64 	%fd36, %fd800, %fd800;
	@%p95 bra 	BB1_117;

	mov.f64 	%fd242, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd244, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd241, %fd242, %fd36, %fd244;
	// inline asm
	mov.f64 	%fd248, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd245, %fd241, %fd36, %fd248;
	// inline asm
	mov.f64 	%fd252, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd249, %fd245, %fd36, %fd252;
	// inline asm
	mov.f64 	%fd256, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd253, %fd249, %fd36, %fd256;
	// inline asm
	mov.f64 	%fd260, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd257, %fd253, %fd36, %fd260;
	// inline asm
	mov.f64 	%fd264, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd261, %fd257, %fd36, %fd264;
	// inline asm
	mov.f64 	%fd268, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd265, %fd261, %fd36, %fd268;
	// inline asm
	mov.f64 	%fd801, %fd265;
	bra.uni 	BB1_118;

BB1_117:
	mov.f64 	%fd270, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd272, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd269, %fd270, %fd36, %fd272;
	// inline asm
	mov.f64 	%fd276, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd273, %fd269, %fd36, %fd276;
	// inline asm
	mov.f64 	%fd280, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd277, %fd273, %fd36, %fd280;
	// inline asm
	mov.f64 	%fd284, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd281, %fd277, %fd36, %fd284;
	// inline asm
	mov.f64 	%fd288, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd285, %fd281, %fd36, %fd288;
	// inline asm
	mul.rn.f64 	%fd290, %fd285, %fd36;
	// inline asm
	fma.rn.f64 	%fd289, %fd290, %fd800, %fd800;
	// inline asm
	mov.f64 	%fd801, %fd289;

BB1_118:
	and.b32  	%r311, %r85, 2;
	setp.eq.s32 	%p96, %r311, 0;
	neg.f64 	%fd293, %fd801;
	selp.f64 	%fd802, %fd801, %fd293, %p96;
	bra.uni 	BB1_120;

BB1_119:
	mov.f64 	%fd802, 0dFFF8000000000000;

BB1_120:
	setp.eq.f64 	%p97, %fd32, 0d0000000000000000;
	or.pred  	%p98, %p10, %p97;
	or.pred  	%p99, %p9, %p98;
	@%p99 bra 	BB1_143;

	// inline asm
	abs.f64 	%fd294, %fd32;
	// inline asm
	setp.gt.f64 	%p100, %fd294, 0d41E0000000000000;
	@%p100 bra 	BB1_123;

	mov.f64 	%fd309, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd296, %fd32, %fd309;
	// inline asm
	cvt.rni.s32.f64 	%r312, %fd296;
	// inline asm
	cvt.rn.f64.s32 	%fd310, %r312;
	neg.f64 	%fd306, %fd310;
	mov.f64 	%fd299, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd297, %fd306, %fd299, %fd32;
	// inline asm
	mov.f64 	%fd303, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd301, %fd306, %fd303, %fd297;
	// inline asm
	mov.f64 	%fd307, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd305, %fd306, %fd307, %fd301;
	// inline asm
	mov.u32 	%r461, %r312;
	mov.f64 	%fd803, %fd305;
	bra.uni 	BB1_139;

BB1_123:
	mov.b64 	 %rl97, %fd32;
	and.b64  	%rl472, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl316, %rl99, 2047;
	add.s64 	%rl317, %rl316, 4294966272;
	cvt.u32.u64 	%r87, %rl317;
	shl.b64 	%rl318, %rl97, 11;
	or.b64  	%rl100, %rl318, -9223372036854775808;
	shr.u32 	%r316, %r87, 6;
	mov.u32 	%r317, 16;
	sub.s32 	%r88, %r317, %r316;
	mov.u32 	%r318, 15;
	sub.s32 	%r459, %r318, %r316;
	mov.u32 	%r319, 19;
	sub.s32 	%r90, %r319, %r316;
	mov.u32 	%r314, 18;
	// inline asm
	min.s32 	%r313, %r314, %r90;
	// inline asm
	setp.lt.s32 	%p101, %r459, %r313;
	@%p101 bra 	BB1_125;

	mov.u64 	%rl469, 0;
	bra.uni 	BB1_127;

BB1_125:
	mov.u32 	%r320, 1;
	sub.s32 	%r91, %r320, %r88;
	mov.u64 	%rl469, 0;

BB1_126:
	.pragma "nounroll";
	shl.b32 	%r324, %r459, 3;
	mov.u32 	%r325, __internal_i2opi_d;
	add.s32 	%r326, %r325, %r324;
	ld.const.u64 	%rl322, [%r326];
	mul.lo.s64 	%rl324, %rl322, %rl100;
	// inline asm
	mul.hi.u64 	%rl321, %rl322, %rl100;
	// inline asm
	mad.lo.s64 	%rl325, %rl322, %rl100, %rl469;
	setp.lt.u64 	%p102, %rl325, %rl324;
	selp.u64 	%rl326, 1, 0, %p102;
	add.s64 	%rl469, %rl326, %rl321;
	add.s32 	%r327, %r91, %r459;
	shl.b32 	%r328, %r327, 3;
	add.s32 	%r330, %r35, %r328;
	st.local.u64 	[%r330], %rl325;
	// inline asm
	min.s32 	%r321, %r314, %r90;
	// inline asm
	add.s32 	%r459, %r459, 1;
	setp.lt.s32 	%p103, %r459, %r321;
	@%p103 bra 	BB1_126;

BB1_127:
	mov.u32 	%r331, 1;
	sub.s32 	%r332, %r331, %r88;
	add.s32 	%r333, %r332, %r459;
	shl.b32 	%r334, %r333, 3;
	add.s32 	%r336, %r35, %r334;
	st.local.u64 	[%r336], %rl469;
	ld.local.u64 	%rl470, [%r35+24];
	ld.local.u64 	%rl471, [%r35+16];
	and.b32  	%r337, %r87, 63;
	setp.eq.s32 	%p104, %r337, 0;
	@%p104 bra 	BB1_129;

	and.b64  	%rl327, %rl99, 63;
	cvt.u32.u64 	%r338, %rl327;
	shl.b64 	%rl328, %rl470, %r338;
	neg.s32 	%r339, %r87;
	and.b32  	%r340, %r339, 63;
	shr.u64 	%rl329, %rl471, %r340;
	or.b64  	%rl470, %rl329, %rl328;
	shl.b64 	%rl330, %rl471, %r338;
	ld.local.u64 	%rl331, [%r35+8];
	shr.u64 	%rl332, %rl331, %r340;
	or.b64  	%rl471, %rl332, %rl330;

BB1_129:
	shr.u64 	%rl333, %rl470, 62;
	cvt.u32.u64 	%r341, %rl333;
	shr.u64 	%rl334, %rl471, 62;
	shl.b64 	%rl335, %rl470, 2;
	or.b64  	%rl476, %rl334, %rl335;
	shl.b64 	%rl111, %rl471, 2;
	setp.ne.s64 	%p105, %rl111, 0;
	selp.u64 	%rl336, 1, 0, %p105;
	or.b64  	%rl337, %rl336, %rl476;
	setp.gt.u64 	%p106, %rl337, -9223372036854775808;
	selp.u32 	%r342, 1, 0, %p106;
	add.s32 	%r343, %r342, %r341;
	neg.s32 	%r344, %r343;
	setp.lt.s64 	%p107, %rl97, 0;
	selp.b32 	%r461, %r344, %r343, %p107;
	@%p106 bra 	BB1_131;

	mov.u64 	%rl475, %rl111;
	bra.uni 	BB1_132;

BB1_131:
	not.b64 	%rl338, %rl476;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p108, %rl111, 0;
	selp.u64 	%rl339, 1, 0, %p108;
	add.s64 	%rl476, %rl339, %rl338;
	xor.b64  	%rl472, %rl472, -9223372036854775808;
	mov.u64 	%rl475, %rl112;

BB1_132:
	mov.u64 	%rl474, %rl475;
	setp.gt.s64 	%p109, %rl476, 0;
	@%p109 bra 	BB1_134;

	mov.u32 	%r460, 0;
	bra.uni 	BB1_136;

BB1_134:
	mov.u32 	%r460, 0;

BB1_135:
	shr.u64 	%rl340, %rl474, 63;
	shl.b64 	%rl341, %rl476, 1;
	or.b64  	%rl476, %rl340, %rl341;
	shl.b64 	%rl474, %rl474, 1;
	add.s32 	%r460, %r460, -1;
	setp.gt.s64 	%p110, %rl476, 0;
	@%p110 bra 	BB1_135;

BB1_136:
	mul.lo.s64 	%rl478, %rl476, -3958705157555305931;
	mov.u64 	%rl344, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl342, %rl476, %rl344;
	// inline asm
	setp.gt.s64 	%p111, %rl342, 0;
	mov.u64 	%rl477, %rl342;
	@%p111 bra 	BB1_137;
	bra.uni 	BB1_138;

BB1_137:
	shl.b64 	%rl345, %rl342, 1;
	shr.u64 	%rl346, %rl478, 63;
	or.b64  	%rl477, %rl345, %rl346;
	mul.lo.s64 	%rl478, %rl476, -7917410315110611862;
	add.s32 	%r460, %r460, -1;

BB1_138:
	setp.ne.s64 	%p112, %rl478, 0;
	selp.u64 	%rl347, 1, 0, %p112;
	add.s64 	%rl348, %rl347, %rl477;
	add.s32 	%r347, %r460, 1022;
	cvt.u64.u32 	%rl349, %r347;
	shl.b64 	%rl350, %rl349, 52;
	shr.u64 	%rl351, %rl348, 11;
	shr.u64 	%rl352, %rl348, 10;
	and.b64  	%rl353, %rl352, 1;
	add.s64 	%rl354, %rl350, %rl351;
	add.s64 	%rl355, %rl354, %rl353;
	or.b64  	%rl356, %rl355, %rl472;
	mov.b64 	 %fd803, %rl356;

BB1_139:
	and.b32  	%r348, %r461, 1;
	setp.eq.s32 	%p113, %r348, 0;
	mul.rn.f64 	%fd46, %fd803, %fd803;
	@%p113 bra 	BB1_141;

	mov.f64 	%fd312, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd314, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd311, %fd312, %fd46, %fd314;
	// inline asm
	mov.f64 	%fd318, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd315, %fd311, %fd46, %fd318;
	// inline asm
	mov.f64 	%fd322, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd319, %fd315, %fd46, %fd322;
	// inline asm
	mov.f64 	%fd326, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd323, %fd319, %fd46, %fd326;
	// inline asm
	mov.f64 	%fd330, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd327, %fd323, %fd46, %fd330;
	// inline asm
	mov.f64 	%fd334, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd331, %fd327, %fd46, %fd334;
	// inline asm
	mov.f64 	%fd338, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd335, %fd331, %fd46, %fd338;
	// inline asm
	mov.f64 	%fd804, %fd335;
	bra.uni 	BB1_142;

BB1_141:
	mov.f64 	%fd340, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd342, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd339, %fd340, %fd46, %fd342;
	// inline asm
	mov.f64 	%fd346, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd343, %fd339, %fd46, %fd346;
	// inline asm
	mov.f64 	%fd350, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd347, %fd343, %fd46, %fd350;
	// inline asm
	mov.f64 	%fd354, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd351, %fd347, %fd46, %fd354;
	// inline asm
	mov.f64 	%fd358, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd355, %fd351, %fd46, %fd358;
	// inline asm
	mul.rn.f64 	%fd360, %fd355, %fd46;
	// inline asm
	fma.rn.f64 	%fd359, %fd360, %fd803, %fd803;
	// inline asm
	mov.f64 	%fd804, %fd359;

BB1_142:
	and.b32  	%r349, %r461, 2;
	setp.eq.s32 	%p114, %r349, 0;
	neg.f64 	%fd363, %fd804;
	selp.f64 	%fd805, %fd804, %fd363, %p114;
	bra.uni 	BB1_144;

BB1_143:
	mov.f64 	%fd364, 0d0000000000000000;
	mul.rn.f64 	%fd805, %fd32, %fd364;

BB1_144:
	mul.f64 	%fd365, %fd4, %fd805;
	fma.rn.f64 	%fd366, %fd3, %fd802, %fd365;
	mul.f64 	%fd367, %fd4, %fd802;
	neg.f64 	%fd368, %fd3;
	fma.rn.f64 	%fd369, %fd368, %fd805, %fd367;
	mul.f64 	%fd370, %fd7, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd53, %fd370, %fd8;
	setp.eq.f64 	%p11, %fd53, %fd10;
	setp.eq.f64 	%p12, %fd53, %fd11;
	or.pred  	%p115, %p11, %p12;
	@%p115 bra 	BB1_167;

	// inline asm
	abs.f64 	%fd371, %fd53;
	// inline asm
	setp.gt.f64 	%p116, %fd371, 0d41E0000000000000;
	@%p116 bra 	BB1_147;

	mov.f64 	%fd386, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd373, %fd53, %fd386;
	// inline asm
	cvt.rni.s32.f64 	%r350, %fd373;
	// inline asm
	cvt.rn.f64.s32 	%fd387, %r350;
	neg.f64 	%fd383, %fd387;
	mov.f64 	%fd376, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd374, %fd383, %fd376, %fd53;
	// inline asm
	mov.f64 	%fd380, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd378, %fd383, %fd380, %fd374;
	// inline asm
	mov.f64 	%fd384, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd382, %fd383, %fd384, %fd378;
	// inline asm
	mov.u32 	%r464, %r350;
	mov.f64 	%fd806, %fd382;
	bra.uni 	BB1_163;

BB1_147:
	mov.b64 	 %rl129, %fd53;
	and.b64  	%rl482, %rl129, -9223372036854775808;
	shr.u64 	%rl131, %rl129, 52;
	and.b64  	%rl357, %rl131, 2047;
	add.s64 	%rl358, %rl357, 4294966272;
	cvt.u32.u64 	%r103, %rl358;
	shl.b64 	%rl359, %rl129, 11;
	or.b64  	%rl132, %rl359, -9223372036854775808;
	shr.u32 	%r354, %r103, 6;
	mov.u32 	%r355, 16;
	sub.s32 	%r104, %r355, %r354;
	mov.u32 	%r356, 15;
	sub.s32 	%r462, %r356, %r354;
	mov.u32 	%r357, 19;
	sub.s32 	%r106, %r357, %r354;
	mov.u32 	%r352, 18;
	// inline asm
	min.s32 	%r351, %r352, %r106;
	// inline asm
	setp.lt.s32 	%p117, %r462, %r351;
	@%p117 bra 	BB1_149;

	mov.u64 	%rl479, 0;
	bra.uni 	BB1_151;

BB1_149:
	mov.u32 	%r358, 1;
	sub.s32 	%r107, %r358, %r104;
	mov.u64 	%rl479, 0;

BB1_150:
	.pragma "nounroll";
	shl.b32 	%r362, %r462, 3;
	mov.u32 	%r363, __internal_i2opi_d;
	add.s32 	%r364, %r363, %r362;
	ld.const.u64 	%rl363, [%r364];
	mul.lo.s64 	%rl365, %rl363, %rl132;
	// inline asm
	mul.hi.u64 	%rl362, %rl363, %rl132;
	// inline asm
	mad.lo.s64 	%rl366, %rl363, %rl132, %rl479;
	setp.lt.u64 	%p118, %rl366, %rl365;
	selp.u64 	%rl367, 1, 0, %p118;
	add.s64 	%rl479, %rl367, %rl362;
	add.s32 	%r365, %r107, %r462;
	shl.b32 	%r366, %r365, 3;
	add.s32 	%r368, %r35, %r366;
	st.local.u64 	[%r368], %rl366;
	// inline asm
	min.s32 	%r359, %r352, %r106;
	// inline asm
	add.s32 	%r462, %r462, 1;
	setp.lt.s32 	%p119, %r462, %r359;
	@%p119 bra 	BB1_150;

BB1_151:
	mov.u32 	%r369, 1;
	sub.s32 	%r370, %r369, %r104;
	add.s32 	%r371, %r370, %r462;
	shl.b32 	%r372, %r371, 3;
	add.s32 	%r374, %r35, %r372;
	st.local.u64 	[%r374], %rl479;
	ld.local.u64 	%rl480, [%r35+24];
	ld.local.u64 	%rl481, [%r35+16];
	and.b32  	%r375, %r103, 63;
	setp.eq.s32 	%p120, %r375, 0;
	@%p120 bra 	BB1_153;

	and.b64  	%rl368, %rl131, 63;
	cvt.u32.u64 	%r376, %rl368;
	shl.b64 	%rl369, %rl480, %r376;
	neg.s32 	%r377, %r103;
	and.b32  	%r378, %r377, 63;
	shr.u64 	%rl370, %rl481, %r378;
	or.b64  	%rl480, %rl370, %rl369;
	shl.b64 	%rl371, %rl481, %r376;
	ld.local.u64 	%rl372, [%r35+8];
	shr.u64 	%rl373, %rl372, %r378;
	or.b64  	%rl481, %rl373, %rl371;

BB1_153:
	shr.u64 	%rl374, %rl480, 62;
	cvt.u32.u64 	%r379, %rl374;
	shr.u64 	%rl375, %rl481, 62;
	shl.b64 	%rl376, %rl480, 2;
	or.b64  	%rl486, %rl375, %rl376;
	shl.b64 	%rl143, %rl481, 2;
	setp.ne.s64 	%p121, %rl143, 0;
	selp.u64 	%rl377, 1, 0, %p121;
	or.b64  	%rl378, %rl377, %rl486;
	setp.gt.u64 	%p122, %rl378, -9223372036854775808;
	selp.u32 	%r380, 1, 0, %p122;
	add.s32 	%r381, %r380, %r379;
	neg.s32 	%r382, %r381;
	setp.lt.s64 	%p123, %rl129, 0;
	selp.b32 	%r464, %r382, %r381, %p123;
	@%p122 bra 	BB1_155;

	mov.u64 	%rl485, %rl143;
	bra.uni 	BB1_156;

BB1_155:
	not.b64 	%rl379, %rl486;
	neg.s64 	%rl144, %rl143;
	setp.eq.s64 	%p124, %rl143, 0;
	selp.u64 	%rl380, 1, 0, %p124;
	add.s64 	%rl486, %rl380, %rl379;
	xor.b64  	%rl482, %rl482, -9223372036854775808;
	mov.u64 	%rl485, %rl144;

BB1_156:
	mov.u64 	%rl484, %rl485;
	setp.gt.s64 	%p125, %rl486, 0;
	@%p125 bra 	BB1_158;

	mov.u32 	%r463, 0;
	bra.uni 	BB1_160;

BB1_158:
	mov.u32 	%r463, 0;

BB1_159:
	shr.u64 	%rl381, %rl484, 63;
	shl.b64 	%rl382, %rl486, 1;
	or.b64  	%rl486, %rl381, %rl382;
	shl.b64 	%rl484, %rl484, 1;
	add.s32 	%r463, %r463, -1;
	setp.gt.s64 	%p126, %rl486, 0;
	@%p126 bra 	BB1_159;

BB1_160:
	mul.lo.s64 	%rl488, %rl486, -3958705157555305931;
	mov.u64 	%rl385, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl383, %rl486, %rl385;
	// inline asm
	setp.gt.s64 	%p127, %rl383, 0;
	mov.u64 	%rl487, %rl383;
	@%p127 bra 	BB1_161;
	bra.uni 	BB1_162;

BB1_161:
	shl.b64 	%rl386, %rl383, 1;
	shr.u64 	%rl387, %rl488, 63;
	or.b64  	%rl487, %rl386, %rl387;
	mul.lo.s64 	%rl488, %rl486, -7917410315110611862;
	add.s32 	%r463, %r463, -1;

BB1_162:
	setp.ne.s64 	%p128, %rl488, 0;
	selp.u64 	%rl388, 1, 0, %p128;
	add.s64 	%rl389, %rl388, %rl487;
	add.s32 	%r385, %r463, 1022;
	cvt.u64.u32 	%rl390, %r385;
	shl.b64 	%rl391, %rl390, 52;
	shr.u64 	%rl392, %rl389, 11;
	shr.u64 	%rl393, %rl389, 10;
	and.b64  	%rl394, %rl393, 1;
	add.s64 	%rl395, %rl391, %rl392;
	add.s64 	%rl396, %rl395, %rl394;
	or.b64  	%rl397, %rl396, %rl482;
	mov.b64 	 %fd806, %rl397;

BB1_163:
	add.s32 	%r118, %r464, 1;
	and.b32  	%r386, %r118, 1;
	setp.eq.s32 	%p129, %r386, 0;
	mul.rn.f64 	%fd57, %fd806, %fd806;
	@%p129 bra 	BB1_165;

	mov.f64 	%fd389, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd391, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd388, %fd389, %fd57, %fd391;
	// inline asm
	mov.f64 	%fd395, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd392, %fd388, %fd57, %fd395;
	// inline asm
	mov.f64 	%fd399, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd396, %fd392, %fd57, %fd399;
	// inline asm
	mov.f64 	%fd403, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd400, %fd396, %fd57, %fd403;
	// inline asm
	mov.f64 	%fd407, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd404, %fd400, %fd57, %fd407;
	// inline asm
	mov.f64 	%fd411, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd408, %fd404, %fd57, %fd411;
	// inline asm
	mov.f64 	%fd415, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd412, %fd408, %fd57, %fd415;
	// inline asm
	mov.f64 	%fd807, %fd412;
	bra.uni 	BB1_166;

BB1_165:
	mov.f64 	%fd417, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd419, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd416, %fd417, %fd57, %fd419;
	// inline asm
	mov.f64 	%fd423, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd420, %fd416, %fd57, %fd423;
	// inline asm
	mov.f64 	%fd427, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd424, %fd420, %fd57, %fd427;
	// inline asm
	mov.f64 	%fd431, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd428, %fd424, %fd57, %fd431;
	// inline asm
	mov.f64 	%fd435, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd432, %fd428, %fd57, %fd435;
	// inline asm
	mul.rn.f64 	%fd437, %fd432, %fd57;
	// inline asm
	fma.rn.f64 	%fd436, %fd437, %fd806, %fd806;
	// inline asm
	mov.f64 	%fd807, %fd436;

BB1_166:
	and.b32  	%r387, %r118, 2;
	setp.eq.s32 	%p130, %r387, 0;
	neg.f64 	%fd440, %fd807;
	selp.f64 	%fd808, %fd807, %fd440, %p130;
	bra.uni 	BB1_168;

BB1_167:
	mov.f64 	%fd808, 0dFFF8000000000000;

BB1_168:
	setp.eq.f64 	%p131, %fd53, 0d0000000000000000;
	or.pred  	%p132, %p12, %p131;
	or.pred  	%p133, %p11, %p132;
	@%p133 bra 	BB1_191;

	// inline asm
	abs.f64 	%fd441, %fd53;
	// inline asm
	setp.gt.f64 	%p134, %fd441, 0d41E0000000000000;
	@%p134 bra 	BB1_171;

	mov.f64 	%fd456, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd443, %fd53, %fd456;
	// inline asm
	cvt.rni.s32.f64 	%r388, %fd443;
	// inline asm
	cvt.rn.f64.s32 	%fd457, %r388;
	neg.f64 	%fd453, %fd457;
	mov.f64 	%fd446, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd444, %fd453, %fd446, %fd53;
	// inline asm
	mov.f64 	%fd450, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd448, %fd453, %fd450, %fd444;
	// inline asm
	mov.f64 	%fd454, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd452, %fd453, %fd454, %fd448;
	// inline asm
	mov.u32 	%r467, %r388;
	mov.f64 	%fd809, %fd452;
	bra.uni 	BB1_187;

BB1_171:
	mov.b64 	 %rl161, %fd53;
	and.b64  	%rl492, %rl161, -9223372036854775808;
	shr.u64 	%rl163, %rl161, 52;
	and.b64  	%rl398, %rl163, 2047;
	add.s64 	%rl399, %rl398, 4294966272;
	cvt.u32.u64 	%r120, %rl399;
	shl.b64 	%rl400, %rl161, 11;
	or.b64  	%rl164, %rl400, -9223372036854775808;
	shr.u32 	%r392, %r120, 6;
	mov.u32 	%r393, 16;
	sub.s32 	%r121, %r393, %r392;
	mov.u32 	%r394, 15;
	sub.s32 	%r465, %r394, %r392;
	mov.u32 	%r395, 19;
	sub.s32 	%r123, %r395, %r392;
	mov.u32 	%r390, 18;
	// inline asm
	min.s32 	%r389, %r390, %r123;
	// inline asm
	setp.lt.s32 	%p135, %r465, %r389;
	@%p135 bra 	BB1_173;

	mov.u64 	%rl489, 0;
	bra.uni 	BB1_175;

BB1_173:
	mov.u32 	%r396, 1;
	sub.s32 	%r124, %r396, %r121;
	mov.u64 	%rl489, 0;

BB1_174:
	.pragma "nounroll";
	shl.b32 	%r400, %r465, 3;
	mov.u32 	%r401, __internal_i2opi_d;
	add.s32 	%r402, %r401, %r400;
	ld.const.u64 	%rl404, [%r402];
	mul.lo.s64 	%rl406, %rl404, %rl164;
	// inline asm
	mul.hi.u64 	%rl403, %rl404, %rl164;
	// inline asm
	mad.lo.s64 	%rl407, %rl404, %rl164, %rl489;
	setp.lt.u64 	%p136, %rl407, %rl406;
	selp.u64 	%rl408, 1, 0, %p136;
	add.s64 	%rl489, %rl408, %rl403;
	add.s32 	%r403, %r124, %r465;
	shl.b32 	%r404, %r403, 3;
	add.s32 	%r406, %r35, %r404;
	st.local.u64 	[%r406], %rl407;
	// inline asm
	min.s32 	%r397, %r390, %r123;
	// inline asm
	add.s32 	%r465, %r465, 1;
	setp.lt.s32 	%p137, %r465, %r397;
	@%p137 bra 	BB1_174;

BB1_175:
	mov.u32 	%r407, 1;
	sub.s32 	%r408, %r407, %r121;
	add.s32 	%r409, %r408, %r465;
	shl.b32 	%r410, %r409, 3;
	add.s32 	%r412, %r35, %r410;
	st.local.u64 	[%r412], %rl489;
	ld.local.u64 	%rl490, [%r35+24];
	ld.local.u64 	%rl491, [%r35+16];
	and.b32  	%r413, %r120, 63;
	setp.eq.s32 	%p138, %r413, 0;
	@%p138 bra 	BB1_177;

	and.b64  	%rl409, %rl163, 63;
	cvt.u32.u64 	%r414, %rl409;
	shl.b64 	%rl410, %rl490, %r414;
	neg.s32 	%r415, %r120;
	and.b32  	%r416, %r415, 63;
	shr.u64 	%rl411, %rl491, %r416;
	or.b64  	%rl490, %rl411, %rl410;
	shl.b64 	%rl412, %rl491, %r414;
	ld.local.u64 	%rl413, [%r35+8];
	shr.u64 	%rl414, %rl413, %r416;
	or.b64  	%rl491, %rl414, %rl412;

BB1_177:
	shr.u64 	%rl415, %rl490, 62;
	cvt.u32.u64 	%r417, %rl415;
	shr.u64 	%rl416, %rl491, 62;
	shl.b64 	%rl417, %rl490, 2;
	or.b64  	%rl496, %rl416, %rl417;
	shl.b64 	%rl175, %rl491, 2;
	setp.ne.s64 	%p139, %rl175, 0;
	selp.u64 	%rl418, 1, 0, %p139;
	or.b64  	%rl419, %rl418, %rl496;
	setp.gt.u64 	%p140, %rl419, -9223372036854775808;
	selp.u32 	%r418, 1, 0, %p140;
	add.s32 	%r419, %r418, %r417;
	neg.s32 	%r420, %r419;
	setp.lt.s64 	%p141, %rl161, 0;
	selp.b32 	%r467, %r420, %r419, %p141;
	@%p140 bra 	BB1_179;

	mov.u64 	%rl495, %rl175;
	bra.uni 	BB1_180;

BB1_179:
	not.b64 	%rl420, %rl496;
	neg.s64 	%rl176, %rl175;
	setp.eq.s64 	%p142, %rl175, 0;
	selp.u64 	%rl421, 1, 0, %p142;
	add.s64 	%rl496, %rl421, %rl420;
	xor.b64  	%rl492, %rl492, -9223372036854775808;
	mov.u64 	%rl495, %rl176;

BB1_180:
	mov.u64 	%rl494, %rl495;
	setp.gt.s64 	%p143, %rl496, 0;
	@%p143 bra 	BB1_182;

	mov.u32 	%r466, 0;
	bra.uni 	BB1_184;

BB1_182:
	mov.u32 	%r466, 0;

BB1_183:
	shr.u64 	%rl422, %rl494, 63;
	shl.b64 	%rl423, %rl496, 1;
	or.b64  	%rl496, %rl422, %rl423;
	shl.b64 	%rl494, %rl494, 1;
	add.s32 	%r466, %r466, -1;
	setp.gt.s64 	%p144, %rl496, 0;
	@%p144 bra 	BB1_183;

BB1_184:
	mul.lo.s64 	%rl498, %rl496, -3958705157555305931;
	mov.u64 	%rl426, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl424, %rl496, %rl426;
	// inline asm
	setp.gt.s64 	%p145, %rl424, 0;
	mov.u64 	%rl497, %rl424;
	@%p145 bra 	BB1_185;
	bra.uni 	BB1_186;

BB1_185:
	shl.b64 	%rl427, %rl424, 1;
	shr.u64 	%rl428, %rl498, 63;
	or.b64  	%rl497, %rl427, %rl428;
	mul.lo.s64 	%rl498, %rl496, -7917410315110611862;
	add.s32 	%r466, %r466, -1;

BB1_186:
	setp.ne.s64 	%p146, %rl498, 0;
	selp.u64 	%rl429, 1, 0, %p146;
	add.s64 	%rl430, %rl429, %rl497;
	add.s32 	%r423, %r466, 1022;
	cvt.u64.u32 	%rl431, %r423;
	shl.b64 	%rl432, %rl431, 52;
	shr.u64 	%rl433, %rl430, 11;
	shr.u64 	%rl434, %rl430, 10;
	and.b64  	%rl435, %rl434, 1;
	add.s64 	%rl436, %rl432, %rl433;
	add.s64 	%rl437, %rl436, %rl435;
	or.b64  	%rl438, %rl437, %rl492;
	mov.b64 	 %fd809, %rl438;

BB1_187:
	and.b32  	%r424, %r467, 1;
	setp.eq.s32 	%p147, %r424, 0;
	mul.rn.f64 	%fd67, %fd809, %fd809;
	@%p147 bra 	BB1_189;

	mov.f64 	%fd459, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd461, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd458, %fd459, %fd67, %fd461;
	// inline asm
	mov.f64 	%fd465, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd462, %fd458, %fd67, %fd465;
	// inline asm
	mov.f64 	%fd469, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd466, %fd462, %fd67, %fd469;
	// inline asm
	mov.f64 	%fd473, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd470, %fd466, %fd67, %fd473;
	// inline asm
	mov.f64 	%fd477, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd474, %fd470, %fd67, %fd477;
	// inline asm
	mov.f64 	%fd481, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd478, %fd474, %fd67, %fd481;
	// inline asm
	mov.f64 	%fd485, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd482, %fd478, %fd67, %fd485;
	// inline asm
	mov.f64 	%fd810, %fd482;
	bra.uni 	BB1_190;

BB1_189:
	mov.f64 	%fd487, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd489, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd486, %fd487, %fd67, %fd489;
	// inline asm
	mov.f64 	%fd493, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd490, %fd486, %fd67, %fd493;
	// inline asm
	mov.f64 	%fd497, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd494, %fd490, %fd67, %fd497;
	// inline asm
	mov.f64 	%fd501, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd498, %fd494, %fd67, %fd501;
	// inline asm
	mov.f64 	%fd505, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd502, %fd498, %fd67, %fd505;
	// inline asm
	mul.rn.f64 	%fd507, %fd502, %fd67;
	// inline asm
	fma.rn.f64 	%fd506, %fd507, %fd809, %fd809;
	// inline asm
	mov.f64 	%fd810, %fd506;

BB1_190:
	and.b32  	%r425, %r467, 2;
	setp.eq.s32 	%p148, %r425, 0;
	neg.f64 	%fd510, %fd810;
	selp.f64 	%fd811, %fd810, %fd510, %p148;
	bra.uni 	BB1_192;

BB1_191:
	mov.f64 	%fd511, 0d0000000000000000;
	mul.rn.f64 	%fd811, %fd53, %fd511;

BB1_192:
	mul.f64 	%fd512, %fd6, %fd811;
	fma.rn.f64 	%fd513, %fd5, %fd808, %fd512;
	mul.f64 	%fd514, %fd6, %fd808;
	neg.f64 	%fd515, %fd5;
	fma.rn.f64 	%fd516, %fd515, %fd811, %fd514;
	mov.f64 	%fd812, %fd74;
	mov.f64 	%fd813, %fd75;
	mov.f64 	%fd814, %fd219;
	mov.f64 	%fd815, %fd222;
	mov.f64 	%fd816, %fd366;
	mov.f64 	%fd817, %fd369;
	mov.f64 	%fd818, %fd513;
	mov.f64 	%fd819, %fd516;
	bra.uni 	BB1_232;

BB1_193:
	div.s32 	%r426, %r33, %r34;
	rem.s32 	%r427, %r33, %r34;
	shl.b32 	%r428, %r427, 4;
	ld.param.u32 	%r440, [DIT4C2C_param_1];
	add.s32 	%r135, %r440, %r428;
	setp.gt.s32 	%p149, %r426, 1;
	@%p149 bra 	BB1_197;

	setp.eq.s32 	%p152, %r426, 0;
	@%p152 bra 	BB1_201;

	setp.eq.s32 	%p153, %r426, 1;
	@%p153 bra 	BB1_196;
	bra.uni 	BB1_202;

BB1_196:
	ld.global.v2.f64 	{%fd764, %fd765}, [%r135];
	neg.f64 	%fd526, %fd764;
	mov.f64 	%fd820, %fd765;
	mov.f64 	%fd821, %fd526;
	bra.uni 	BB1_202;

BB1_197:
	setp.eq.s32 	%p150, %r426, 2;
	@%p150 bra 	BB1_200;

	setp.ne.s32 	%p151, %r426, 3;
	@%p151 bra 	BB1_202;

	ld.global.v2.f64 	{%fd768, %fd769}, [%r135];
	neg.f64 	%fd518, %fd769;
	mov.f64 	%fd820, %fd518;
	mov.f64 	%fd821, %fd768;
	bra.uni 	BB1_202;

BB1_200:
	ld.global.v2.f64 	{%fd766, %fd767}, [%r135];
	neg.f64 	%fd521, %fd766;
	neg.f64 	%fd523, %fd767;
	mov.f64 	%fd820, %fd521;
	mov.f64 	%fd821, %fd523;
	bra.uni 	BB1_202;

BB1_201:
	ld.global.v2.f64 	{%fd820, %fd821}, [%r135];

BB1_202:
	ld.param.u32 	%r444, [DIT4C2C_param_4];
	setp.eq.s32 	%p13, %r444, 0;
	@%p13 bra 	BB1_203;
	bra.uni 	BB1_204;

BB1_203:
	neg.f64 	%fd528, %fd821;
	mov.f64 	%fd820, %fd820;
	mov.f64 	%fd821, %fd528;

BB1_204:
	setp.eq.s32 	%p14, %r28, 0;
	@%p14 bra 	BB1_206;

	mul.f64 	%fd530, %fd3, %fd820;
	neg.f64 	%fd532, %fd4;
	fma.rn.f64 	%fd533, %fd532, %fd821, %fd530;
	mul.f64 	%fd534, %fd3, %fd821;
	fma.rn.f64 	%fd535, %fd4, %fd820, %fd534;
	mov.f64 	%fd812, %fd74;
	mov.f64 	%fd813, %fd75;
	mov.f64 	%fd814, %fd1;
	mov.f64 	%fd815, %fd2;
	mov.f64 	%fd818, %fd5;
	mov.f64 	%fd819, %fd6;
	mov.f64 	%fd816, %fd533;
	mov.f64 	%fd817, %fd535;

BB1_206:
	shl.b32 	%r430, %r33, 1;
	div.s32 	%r429, %r430, %r34;
	rem.s32 	%r431, %r430, %r34;
	shl.b32 	%r432, %r431, 4;
	ld.param.u32 	%r439, [DIT4C2C_param_1];
	add.s32 	%r136, %r439, %r432;
	setp.gt.s32 	%p154, %r429, 1;
	@%p154 bra 	BB1_210;

	setp.eq.s32 	%p157, %r429, 0;
	@%p157 bra 	BB1_214;

	setp.eq.s32 	%p158, %r429, 1;
	@%p158 bra 	BB1_209;
	bra.uni 	BB1_215;

BB1_209:
	ld.global.v2.f64 	{%fd732, %fd733}, [%r136];
	neg.f64 	%fd545, %fd732;
	mov.f64 	%fd820, %fd733;
	mov.f64 	%fd821, %fd545;
	bra.uni 	BB1_215;

BB1_210:
	setp.eq.s32 	%p155, %r429, 2;
	@%p155 bra 	BB1_213;

	setp.ne.s32 	%p156, %r429, 3;
	@%p156 bra 	BB1_215;

	ld.global.v2.f64 	{%fd736, %fd737}, [%r136];
	neg.f64 	%fd537, %fd737;
	mov.f64 	%fd820, %fd537;
	mov.f64 	%fd821, %fd736;
	bra.uni 	BB1_215;

BB1_213:
	ld.global.v2.f64 	{%fd734, %fd735}, [%r136];
	neg.f64 	%fd540, %fd734;
	neg.f64 	%fd542, %fd735;
	mov.f64 	%fd820, %fd540;
	mov.f64 	%fd821, %fd542;
	bra.uni 	BB1_215;

BB1_214:
	ld.global.v2.f64 	{%fd820, %fd821}, [%r136];

BB1_215:
	@%p13 bra 	BB1_216;
	bra.uni 	BB1_217;

BB1_216:
	neg.f64 	%fd547, %fd821;
	mov.f64 	%fd820, %fd820;
	mov.f64 	%fd821, %fd547;

BB1_217:
	@%p14 bra 	BB1_219;

	mul.f64 	%fd550, %fd814, %fd820;
	neg.f64 	%fd553, %fd815;
	fma.rn.f64 	%fd554, %fd553, %fd821, %fd550;
	mul.f64 	%fd555, %fd814, %fd821;
	fma.rn.f64 	%fd556, %fd815, %fd820, %fd555;
	mov.f64 	%fd814, %fd554;
	mov.f64 	%fd815, %fd556;

BB1_219:
	mul.lo.s32 	%r434, %r33, 3;
	div.s32 	%r433, %r434, %r34;
	rem.s32 	%r435, %r434, %r34;
	shl.b32 	%r436, %r435, 4;
	ld.param.u32 	%r438, [DIT4C2C_param_1];
	add.s32 	%r137, %r438, %r436;
	setp.gt.s32 	%p159, %r433, 1;
	@%p159 bra 	BB1_223;

	setp.eq.s32 	%p162, %r433, 0;
	@%p162 bra 	BB1_227;

	setp.eq.s32 	%p163, %r433, 1;
	@%p163 bra 	BB1_222;
	bra.uni 	BB1_228;

BB1_222:
	ld.global.v2.f64 	{%fd692, %fd693}, [%r137];
	neg.f64 	%fd566, %fd692;
	mov.f64 	%fd820, %fd693;
	mov.f64 	%fd821, %fd566;
	bra.uni 	BB1_228;

BB1_223:
	setp.eq.s32 	%p160, %r433, 2;
	@%p160 bra 	BB1_226;

	setp.ne.s32 	%p161, %r433, 3;
	@%p161 bra 	BB1_228;

	ld.global.v2.f64 	{%fd696, %fd697}, [%r137];
	neg.f64 	%fd558, %fd697;
	mov.f64 	%fd820, %fd558;
	mov.f64 	%fd821, %fd696;
	bra.uni 	BB1_228;

BB1_226:
	ld.global.v2.f64 	{%fd694, %fd695}, [%r137];
	neg.f64 	%fd561, %fd694;
	neg.f64 	%fd563, %fd695;
	mov.f64 	%fd820, %fd561;
	mov.f64 	%fd821, %fd563;
	bra.uni 	BB1_228;

BB1_227:
	ld.global.v2.f64 	{%fd820, %fd821}, [%r137];

BB1_228:
	@%p13 bra 	BB1_229;
	bra.uni 	BB1_230;

BB1_229:
	neg.f64 	%fd568, %fd821;
	mov.f64 	%fd820, %fd820;
	mov.f64 	%fd821, %fd568;

BB1_230:
	@%p14 bra 	BB1_232;

	mul.f64 	%fd571, %fd818, %fd820;
	neg.f64 	%fd574, %fd819;
	fma.rn.f64 	%fd575, %fd574, %fd821, %fd571;
	mul.f64 	%fd576, %fd818, %fd821;
	fma.rn.f64 	%fd577, %fd819, %fd820, %fd576;
	mov.f64 	%fd818, %fd575;
	mov.f64 	%fd819, %fd577;

BB1_232:
	ld.param.u32 	%r443, [DIT4C2C_param_4];
	setp.eq.s32 	%p164, %r443, 1;
	@%p164 bra 	BB1_235;

	ld.param.u32 	%r442, [DIT4C2C_param_4];
	setp.ne.s32 	%p165, %r442, 0;
	@%p165 bra 	BB1_236;

	add.f64 	%fd580, %fd812, %fd814;
	add.f64 	%fd582, %fd580, %fd816;
	add.f64 	%fd584, %fd582, %fd818;
	st.global.f64 	[%r29], %fd584;
	add.f64 	%fd587, %fd813, %fd815;
	add.f64 	%fd589, %fd587, %fd817;
	add.f64 	%fd591, %fd589, %fd819;
	st.global.f64 	[%r29+8], %fd591;
	sub.f64 	%fd592, %fd812, %fd814;
	sub.f64 	%fd593, %fd592, %fd817;
	add.f64 	%fd594, %fd593, %fd819;
	st.global.f64 	[%r30], %fd594;
	sub.f64 	%fd595, %fd813, %fd815;
	add.f64 	%fd596, %fd595, %fd816;
	sub.f64 	%fd597, %fd596, %fd818;
	st.global.f64 	[%r30+8], %fd597;
	sub.f64 	%fd598, %fd580, %fd816;
	sub.f64 	%fd599, %fd598, %fd818;
	st.global.f64 	[%r31], %fd599;
	sub.f64 	%fd600, %fd587, %fd817;
	sub.f64 	%fd601, %fd600, %fd819;
	st.global.f64 	[%r31+8], %fd601;
	add.f64 	%fd602, %fd592, %fd817;
	sub.f64 	%fd603, %fd602, %fd819;
	st.global.f64 	[%r32], %fd603;
	sub.f64 	%fd604, %fd595, %fd816;
	add.f64 	%fd605, %fd604, %fd818;
	st.global.f64 	[%r32+8], %fd605;
	ret;

BB1_235:
	add.f64 	%fd608, %fd812, %fd814;
	add.f64 	%fd610, %fd608, %fd816;
	add.f64 	%fd612, %fd610, %fd818;
	st.global.f64 	[%r29], %fd612;
	add.f64 	%fd615, %fd813, %fd815;
	add.f64 	%fd617, %fd615, %fd817;
	add.f64 	%fd619, %fd617, %fd819;
	st.global.f64 	[%r29+8], %fd619;
	sub.f64 	%fd620, %fd812, %fd814;
	add.f64 	%fd621, %fd620, %fd817;
	sub.f64 	%fd622, %fd621, %fd819;
	st.global.f64 	[%r30], %fd622;
	sub.f64 	%fd623, %fd813, %fd815;
	sub.f64 	%fd624, %fd623, %fd816;
	add.f64 	%fd625, %fd624, %fd818;
	st.global.f64 	[%r30+8], %fd625;
	sub.f64 	%fd626, %fd608, %fd816;
	sub.f64 	%fd627, %fd626, %fd818;
	st.global.f64 	[%r31], %fd627;
	sub.f64 	%fd628, %fd615, %fd817;
	sub.f64 	%fd629, %fd628, %fd819;
	st.global.f64 	[%r31+8], %fd629;
	sub.f64 	%fd630, %fd620, %fd817;
	add.f64 	%fd631, %fd630, %fd819;
	st.global.f64 	[%r32], %fd631;
	add.f64 	%fd632, %fd623, %fd816;
	sub.f64 	%fd633, %fd632, %fd818;
	st.global.f64 	[%r32+8], %fd633;

BB1_236:
	ret;
}

.entry DIT10C2CM(
	.param .u32 .ptr .global .align 8 DIT10C2CM_param_0,
	.param .u32 DIT10C2CM_param_1,
	.param .u32 DIT10C2CM_param_2,
	.param .u32 DIT10C2CM_param_3,
	.param .u32 DIT10C2CM_param_4,
	.param .u32 DIT10C2CM_param_5
)
{
	.local .align 8 .b8 	__local_depot2[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<2211>;
	.reg .pred 	%p<377>;
	.reg .s32 	%r<1197>;
	.reg .s64 	%rl<1533>;


	mov.u32 	%SP, __local_depot2;
	ld.param.u32 	%r351, [DIT10C2CM_param_3];
	// inline asm
	mov.u32 	%r343, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r344, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r345, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r346, %tid.x;
	// inline asm
	add.s32 	%r352, %r346, %r343;
	mad.lo.s32 	%r5, %r345, %r344, %r352;
	// inline asm
	mov.u32 	%r347, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r348, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r349, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r350, %tid.y;
	// inline asm
	add.s32 	%r353, %r350, %r347;
	mad.lo.s32 	%r6, %r349, %r348, %r353;
	mul.hi.s32 	%r354, %r351, -1431655765;
	add.s32 	%r355, %r354, %r351;
	shr.u32 	%r356, %r355, 31;
	shr.s32 	%r357, %r355, 1;
	add.s32 	%r7, %r357, %r356;
	mul.lo.s32 	%r358, %r7, 3;
	sub.s32 	%r8, %r351, %r358;
	setp.gt.s32 	%p21, %r351, 2;
	@%p21 bra 	BB2_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB2_23;

BB2_2:
	mov.f32 	%f1, 0f40400000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f41200000;
	add.f32 	%f2, %f47, 0f40400000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r1136, 0;
	mov.u32 	%r1135, 1;

BB2_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p22, %p3, %p3;
	@%p22 bra 	BB2_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mov.f32 	%f55, 0f40000000;
	mul.rn.f32 	%f56, %f55, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p23, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p24, %f1, %f52;
	and.pred  	%p4, %p23, %p24;
	setp.eq.f32 	%p25, %f48, 0f00000000;
	@%p25 bra 	BB2_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r11, %f58;
	shr.u32 	%r361, %r11, 23;
	and.b32  	%r362, %r361, 255;
	add.s32 	%r1137, %r362, -127;
	setp.eq.s32 	%p26, %r362, 0;
	mov.f32 	%f280, %f58;
	@%p26 bra 	BB2_6;
	bra.uni 	BB2_7;

BB2_6:
	and.b32  	%r363, %r11, -2139095041;
	or.b32  	%r364, %r363, 1065353216;
	mov.b32 	 %f60, %r364;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r365, %f61;
	shr.u32 	%r366, %r365, 23;
	and.b32  	%r367, %r366, 255;
	add.s32 	%r1137, %r367, -253;
	and.b32  	%r368, %r365, -2139095041;
	or.b32  	%r369, %r368, 1065353216;
	mov.b32 	 %f280, %r369;

BB2_7:
	mov.b32 	 %r370, %f280;
	and.b32  	%r371, %r370, -2139095041;
	or.b32  	%r372, %r371, 1065353216;
	mov.b32 	 %f281, %r372;
	setp.gt.f32 	%p27, %f281, 0f3FB504F3;
	@%p27 bra 	BB2_8;
	bra.uni 	BB2_9;

BB2_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r1137, %r1137, 1;

BB2_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f55, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r373, %f73;
	and.b32  	%r374, %r373, -4096;
	mov.b32 	 %f81, %r374;
	mov.b32 	 %r375, %f64;
	and.b32  	%r376, %r375, -4096;
	mov.b32 	 %f82, %r376;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f55, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r1137;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p28, %f69, 0f77F684DF;
	@%p28 bra 	BB2_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB2_12;

BB2_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB2_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r17, %f131;
	setp.eq.s32 	%p29, %r17, 1118925336;
	@%p29 bra 	BB2_13;
	bra.uni 	BB2_14;

BB2_13:
	add.s32 	%r377, %r17, -1;
	mov.b32 	 %f133, %r377;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB2_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p30, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p30;
	setp.gt.f32 	%p31, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p31;
	setp.neu.f32 	%p32, %f19, %f3;
	@%p32 bra 	BB2_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB2_21;

BB2_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB2_21;

BB2_17:
	@%p3 bra 	BB2_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB2_21;

BB2_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB2_21;

BB2_20:
	mov.f32 	%f283, %f7;

BB2_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r1135;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r1135, %f157;
	add.s32 	%r1136, %r1136, 1;
	setp.lt.s32 	%p33, %r1136, %r7;
	@%p33 bra 	BB2_3;

	cvt.rn.f32.s32 	%f284, %r1135;

BB2_23:
	mov.f32 	%f159, 0f41200000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r8;
	setp.eq.f32 	%p34, %f287, 0f00000000;
	@%p34 bra 	BB2_45;

	setp.nan.f32 	%p35, %f287, %f287;
	@%p35 bra 	BB2_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p36, %f287, 0f7F800000;
	setp.eq.f32 	%p37, %f287, 0fFF800000;
	or.pred  	%p38, %p36, %p37;
	@%p38 bra 	BB2_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p41, %f158, 0f00000000;
	@%p41 bra 	BB2_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r20, %f168;
	shr.u32 	%r378, %r20, 23;
	and.b32  	%r379, %r378, 255;
	add.s32 	%r1138, %r379, -127;
	setp.eq.s32 	%p42, %r379, 0;
	mov.f32 	%f285, %f168;
	@%p42 bra 	BB2_28;
	bra.uni 	BB2_29;

BB2_28:
	and.b32  	%r380, %r20, -2139095041;
	or.b32  	%r381, %r380, 1065353216;
	mov.b32 	 %f170, %r381;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r382, %f171;
	shr.u32 	%r383, %r382, 23;
	and.b32  	%r384, %r383, 255;
	add.s32 	%r1138, %r384, -253;
	and.b32  	%r385, %r382, -2139095041;
	or.b32  	%r386, %r385, 1065353216;
	mov.b32 	 %f285, %r386;

BB2_29:
	mov.b32 	 %r387, %f285;
	and.b32  	%r388, %r387, -2139095041;
	or.b32  	%r389, %r388, 1065353216;
	mov.b32 	 %f286, %r389;
	setp.gt.f32 	%p43, %f286, 0f3FB504F3;
	@%p43 bra 	BB2_30;
	bra.uni 	BB2_31;

BB2_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r1138, %r1138, 1;

BB2_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mov.f32 	%f182, 0f40000000;
	mul.rn.f32 	%f183, %f182, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r390, %f183;
	and.b32  	%r391, %r390, -4096;
	mov.b32 	 %f191, %r391;
	mov.b32 	 %r392, %f174;
	and.b32  	%r393, %r392, -4096;
	mov.b32 	 %f192, %r393;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f182, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r1138;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p44, %f179, 0f77F684DF;
	@%p44 bra 	BB2_32;
	bra.uni 	BB2_33;

BB2_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB2_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r26, %f241;
	setp.eq.s32 	%p45, %r26, 1118925336;
	@%p45 bra 	BB2_34;
	bra.uni 	BB2_35;

BB2_34:
	add.s32 	%r394, %r26, -1;
	mov.b32 	 %f243, %r394;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB2_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p46, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p46;
	setp.gt.f32 	%p47, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p47;
	setp.neu.f32 	%p48, %f39, %f27;
	@%p48 bra 	BB2_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB2_46;

BB2_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB2_46;

BB2_38:
	setp.lt.f32 	%p49, %f287, 0f00000000;
	@%p49 bra 	BB2_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB2_46;

BB2_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB2_46;

BB2_41:
	setp.lt.f32 	%p50, %f158, 0f3F800000;
	mov.b32 	 %r395, %f287;
	setp.lt.s32 	%p6, %r395, 0;
	@%p50 bra 	BB2_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB2_46;

BB2_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB2_46;

BB2_44:
	add.f32 	%f288, %f287, 0f41200000;
	bra.uni 	BB2_46;

BB2_45:
	mov.f32 	%f288, 0f3F800000;

BB2_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r27, %f267;
	mul.hi.s32 	%r396, %r27, 1717986919;
	shr.u32 	%r397, %r396, 31;
	shr.s32 	%r398, %r396, 2;
	add.s32 	%r28, %r398, %r397;
	ld.param.u32 	%r1134, [DIT10C2CM_param_5];
	setp.eq.s32 	%p51, %r1134, 0;
	@%p51 bra 	BB2_49;

	ld.param.u32 	%r1133, [DIT10C2CM_param_5];
	setp.ne.s32 	%p52, %r1133, 1;
	@%p52 bra 	BB2_50;

	div.s32 	%r1139, %r6, %r28;
	rem.s32 	%r1140, %r6, %r28;
	ld.param.u32 	%r1142, [DIT10C2CM_param_1];
	mov.u32 	%r1141, %r5;
	bra.uni 	BB2_51;

BB2_49:
	ld.param.u32 	%r1123, [DIT10C2CM_param_1];
	mul.lo.s32 	%r31, %r6, %r1123;
	div.s32 	%r1139, %r5, %r28;
	rem.s32 	%r1140, %r5, %r28;
	mov.u32 	%r1142, 1;
	mov.u32 	%r1141, %r31;
	bra.uni 	BB2_51;

BB2_50:
	mov.u32 	%r1142, 1;
	mov.u32 	%r1141, 0;
	mov.u32 	%r1140, %r404;
	mov.u32 	%r1139, %r405;

BB2_51:
	mad.lo.s32 	%r406, %r1139, %r27, %r1140;
	mad.lo.s32 	%r407, %r1142, %r406, %r1141;
	shl.b32 	%r408, %r407, 4;
	ld.param.u32 	%r1121, [DIT10C2CM_param_0];
	add.s32 	%r38, %r1121, %r408;
	ld.global.f64 	%fd210, [%r38];
	ld.global.f64 	%fd211, [%r38+8];
	mul.lo.s32 	%r409, %r1142, %r28;
	shl.b32 	%r410, %r409, 4;
	add.s32 	%r39, %r38, %r410;
	ld.global.f64 	%fd1, [%r39];
	ld.global.f64 	%fd2, [%r39+8];
	add.s32 	%r40, %r39, %r410;
	ld.global.f64 	%fd3, [%r40];
	ld.global.f64 	%fd4, [%r40+8];
	add.s32 	%r41, %r40, %r410;
	ld.global.f64 	%fd5, [%r41];
	ld.global.f64 	%fd6, [%r41+8];
	add.s32 	%r42, %r41, %r410;
	ld.global.f64 	%fd7, [%r42];
	ld.global.f64 	%fd8, [%r42+8];
	mov.f64 	%fd2181, %fd7;
	mov.f64 	%fd2182, %fd8;
	add.s32 	%r43, %r42, %r410;
	ld.global.f64 	%fd9, [%r43];
	ld.global.f64 	%fd10, [%r43+8];
	mov.f64 	%fd2183, %fd9;
	mov.f64 	%fd2184, %fd10;
	add.s32 	%r44, %r43, %r410;
	ld.global.f64 	%fd11, [%r44];
	ld.global.f64 	%fd12, [%r44+8];
	mov.f64 	%fd2185, %fd11;
	mov.f64 	%fd2186, %fd12;
	add.s32 	%r45, %r44, %r410;
	ld.global.f64 	%fd13, [%r45];
	ld.global.f64 	%fd14, [%r45+8];
	mov.f64 	%fd2187, %fd13;
	mov.f64 	%fd2188, %fd14;
	add.s32 	%r46, %r45, %r410;
	ld.global.f64 	%fd15, [%r46];
	ld.global.f64 	%fd16, [%r46+8];
	mov.f64 	%fd2189, %fd15;
	mov.f64 	%fd2190, %fd16;
	add.s32 	%r47, %r46, %r410;
	ld.global.f64 	%fd17, [%r47];
	ld.global.f64 	%fd18, [%r47+8];
	mov.f64 	%fd2191, %fd17;
	mov.f64 	%fd2192, %fd18;
	setp.eq.s32 	%p53, %r1140, 0;
	mov.f64 	%fd2173, %fd210;
	mov.f64 	%fd2174, %fd211;
	mov.f64 	%fd2175, %fd1;
	mov.f64 	%fd2176, %fd2;
	mov.f64 	%fd2177, %fd3;
	mov.f64 	%fd2178, %fd4;
	mov.f64 	%fd2179, %fd5;
	mov.f64 	%fd2180, %fd6;
	@%p53 bra 	BB2_503;

	cvt.rn.f64.s32 	%fd19, %r1140;
	mul.f64 	%fd212, %fd19, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd20, %r27;
	div.rn.f64 	%fd21, %fd212, %fd20;
	setp.eq.f64 	%p54, %fd21, 0d7FF0000000000000;
	setp.eq.f64 	%p55, %fd21, 0dFFF0000000000000;
	or.pred  	%p56, %p54, %p55;
	add.u32 	%r48, %SP, 0;
	@%p56 bra 	BB2_75;

	// inline asm
	abs.f64 	%fd213, %fd21;
	// inline asm
	setp.gt.f64 	%p57, %fd213, 0d41E0000000000000;
	@%p57 bra 	BB2_55;

	mov.f64 	%fd228, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd215, %fd21, %fd228;
	// inline asm
	cvt.rni.s32.f64 	%r411, %fd215;
	// inline asm
	cvt.rn.f64.s32 	%fd229, %r411;
	neg.f64 	%fd225, %fd229;
	mov.f64 	%fd218, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd216, %fd225, %fd218, %fd21;
	// inline asm
	mov.f64 	%fd222, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd220, %fd225, %fd222, %fd216;
	// inline asm
	mov.f64 	%fd226, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd224, %fd225, %fd226, %fd220;
	// inline asm
	mov.u32 	%r1145, %r411;
	mov.f64 	%fd2119, %fd224;
	bra.uni 	BB2_71;

BB2_55:
	mov.b64 	 %rl1, %fd21;
	shr.u64 	%rl577, %rl1, 52;
	and.b64  	%rl578, %rl577, 2047;
	add.s64 	%rl2, %rl578, 4294966272;
	cvt.u32.u64 	%r415, %rl2;
	shr.u32 	%r416, %r415, 6;
	mov.u32 	%r417, 16;
	sub.s32 	%r50, %r417, %r416;
	mov.u32 	%r418, 15;
	sub.s32 	%r1143, %r418, %r416;
	mov.u32 	%r419, 19;
	sub.s32 	%r414, %r419, %r416;
	mov.u32 	%r413, 18;
	// inline asm
	min.s32 	%r412, %r413, %r414;
	// inline asm
	setp.lt.s32 	%p58, %r1143, %r412;
	@%p58 bra 	BB2_57;

	add.s32 	%r1143, %r50, -1;
	mov.u64 	%rl1353, 0;
	bra.uni 	BB2_59;

BB2_57:
	mov.u32 	%r424, 1;
	sub.s32 	%r53, %r424, %r50;
	mov.u64 	%rl1353, 0;

BB2_58:
	.pragma "nounroll";
	shl.b64 	%rl589, %rl1, 11;
	or.b64  	%rl587, %rl589, -9223372036854775808;
	shl.b32 	%r428, %r1143, 3;
	mov.u32 	%r429, __internal_i2opi_d;
	add.s32 	%r430, %r429, %r428;
	ld.const.u64 	%rl586, [%r430];
	mul.lo.s64 	%rl590, %rl586, %rl587;
	// inline asm
	mul.hi.u64 	%rl585, %rl586, %rl587;
	// inline asm
	mad.lo.s64 	%rl591, %rl586, %rl587, %rl1353;
	setp.lt.u64 	%p59, %rl591, %rl590;
	selp.u64 	%rl592, 1, 0, %p59;
	add.s64 	%rl1353, %rl592, %rl585;
	add.s32 	%r431, %r53, %r1143;
	shl.b32 	%r432, %r431, 3;
	add.s32 	%r434, %r48, %r432;
	st.local.u64 	[%r434], %rl591;
	// inline asm
	min.s32 	%r425, %r413, %r414;
	// inline asm
	add.s32 	%r1143, %r1143, 1;
	setp.lt.s32 	%p60, %r1143, %r425;
	@%p60 bra 	BB2_58;

BB2_59:
	mov.u32 	%r442, 1;
	sub.s32 	%r443, %r442, %r50;
	add.s32 	%r444, %r443, %r1143;
	shl.b32 	%r445, %r444, 3;
	add.s32 	%r447, %r48, %r445;
	st.local.u64 	[%r447], %rl1353;
	and.b32  	%r448, %r415, 63;
	ld.local.u64 	%rl1354, [%r48+24];
	ld.local.u64 	%rl1355, [%r48+16];
	setp.eq.s32 	%p61, %r448, 0;
	@%p61 bra 	BB2_61;

	and.b64  	%rl602, %rl577, 63;
	cvt.u32.u64 	%r449, %rl602;
	shl.b64 	%rl603, %rl1354, %r449;
	add.s64 	%rl604, %rl577, 4294966272;
	cvt.u32.u64 	%r450, %rl604;
	neg.s32 	%r451, %r450;
	and.b32  	%r452, %r451, 63;
	shr.u64 	%rl605, %rl1355, %r452;
	or.b64  	%rl1354, %rl605, %rl603;
	shl.b64 	%rl606, %rl1355, %r449;
	ld.local.u64 	%rl607, [%r48+8];
	shr.u64 	%rl608, %rl607, %r452;
	or.b64  	%rl1355, %rl608, %rl606;

BB2_61:
	shr.u64 	%rl609, %rl1354, 62;
	cvt.u32.u64 	%r453, %rl609;
	shr.u64 	%rl610, %rl1355, 62;
	shl.b64 	%rl611, %rl1354, 2;
	or.b64  	%rl1360, %rl610, %rl611;
	shl.b64 	%rl14, %rl1355, 2;
	setp.ne.s64 	%p62, %rl14, 0;
	selp.u64 	%rl612, 1, 0, %p62;
	or.b64  	%rl613, %rl612, %rl1360;
	setp.gt.u64 	%p63, %rl613, -9223372036854775808;
	selp.u32 	%r454, 1, 0, %p63;
	add.s32 	%r455, %r454, %r453;
	setp.lt.s64 	%p64, %rl1, 0;
	neg.s32 	%r456, %r455;
	selp.b32 	%r1145, %r456, %r455, %p64;
	@%p63 bra 	BB2_63;

	and.b64  	%rl1356, %rl1, -9223372036854775808;
	mov.u64 	%rl1359, %rl14;
	bra.uni 	BB2_64;

BB2_63:
	not.b64 	%rl616, %rl1360;
	neg.s64 	%rl16, %rl14;
	setp.eq.s64 	%p65, %rl14, 0;
	selp.u64 	%rl617, 1, 0, %p65;
	add.s64 	%rl1360, %rl617, %rl616;
	and.b64  	%rl619, %rl1, -9223372036854775808;
	xor.b64  	%rl1356, %rl619, -9223372036854775808;
	mov.u64 	%rl1359, %rl16;

BB2_64:
	mov.u64 	%rl1358, %rl1359;
	setp.gt.s64 	%p66, %rl1360, 0;
	@%p66 bra 	BB2_66;

	mov.u32 	%r1144, 0;
	bra.uni 	BB2_68;

BB2_66:
	mov.u32 	%r1144, 0;

BB2_67:
	shr.u64 	%rl620, %rl1358, 63;
	shl.b64 	%rl621, %rl1360, 1;
	or.b64  	%rl1360, %rl620, %rl621;
	shl.b64 	%rl1358, %rl1358, 1;
	add.s32 	%r1144, %r1144, -1;
	setp.gt.s64 	%p67, %rl1360, 0;
	@%p67 bra 	BB2_67;

BB2_68:
	mul.lo.s64 	%rl1362, %rl1360, -3958705157555305931;
	mov.u64 	%rl624, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl622, %rl1360, %rl624;
	// inline asm
	setp.gt.s64 	%p68, %rl622, 0;
	mov.u64 	%rl1361, %rl622;
	@%p68 bra 	BB2_69;
	bra.uni 	BB2_70;

BB2_69:
	shl.b64 	%rl625, %rl622, 1;
	shr.u64 	%rl626, %rl1362, 63;
	or.b64  	%rl1361, %rl625, %rl626;
	mul.lo.s64 	%rl1362, %rl1360, -7917410315110611862;
	add.s32 	%r1144, %r1144, -1;

BB2_70:
	setp.ne.s64 	%p69, %rl1362, 0;
	selp.u64 	%rl627, 1, 0, %p69;
	add.s64 	%rl628, %rl627, %rl1361;
	add.s32 	%r459, %r1144, 1022;
	cvt.u64.u32 	%rl629, %r459;
	shl.b64 	%rl630, %rl629, 52;
	shr.u64 	%rl631, %rl628, 11;
	shr.u64 	%rl632, %rl628, 10;
	and.b64  	%rl633, %rl632, 1;
	add.s64 	%rl634, %rl630, %rl631;
	add.s64 	%rl635, %rl634, %rl633;
	or.b64  	%rl636, %rl635, %rl1356;
	mov.b64 	 %fd2119, %rl636;

BB2_71:
	add.s32 	%r64, %r1145, 1;
	and.b32  	%r460, %r64, 1;
	setp.eq.s32 	%p70, %r460, 0;
	mul.rn.f64 	%fd25, %fd2119, %fd2119;
	@%p70 bra 	BB2_73;

	mov.f64 	%fd231, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd233, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd230, %fd231, %fd25, %fd233;
	// inline asm
	mov.f64 	%fd237, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd234, %fd230, %fd25, %fd237;
	// inline asm
	mov.f64 	%fd241, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd238, %fd234, %fd25, %fd241;
	// inline asm
	mov.f64 	%fd245, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd242, %fd238, %fd25, %fd245;
	// inline asm
	mov.f64 	%fd249, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd246, %fd242, %fd25, %fd249;
	// inline asm
	mov.f64 	%fd253, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd250, %fd246, %fd25, %fd253;
	// inline asm
	mov.f64 	%fd257, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd254, %fd250, %fd25, %fd257;
	// inline asm
	mov.f64 	%fd2120, %fd254;
	bra.uni 	BB2_74;

BB2_73:
	mov.f64 	%fd259, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd261, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd258, %fd259, %fd25, %fd261;
	// inline asm
	mov.f64 	%fd265, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd262, %fd258, %fd25, %fd265;
	// inline asm
	mov.f64 	%fd269, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd266, %fd262, %fd25, %fd269;
	// inline asm
	mov.f64 	%fd273, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd270, %fd266, %fd25, %fd273;
	// inline asm
	mov.f64 	%fd277, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd274, %fd270, %fd25, %fd277;
	// inline asm
	mul.rn.f64 	%fd279, %fd274, %fd25;
	// inline asm
	fma.rn.f64 	%fd278, %fd279, %fd2119, %fd2119;
	// inline asm
	mov.f64 	%fd2120, %fd278;

BB2_74:
	and.b32  	%r461, %r64, 2;
	setp.eq.s32 	%p71, %r461, 0;
	neg.f64 	%fd282, %fd2120;
	selp.f64 	%fd2121, %fd2120, %fd282, %p71;
	bra.uni 	BB2_76;

BB2_75:
	mov.f64 	%fd2121, 0dFFF8000000000000;

BB2_76:
	setp.eq.f64 	%p73, %fd21, 0d0000000000000000;
	or.pred  	%p74, %p55, %p73;
	or.pred  	%p76, %p54, %p74;
	@%p76 bra 	BB2_99;

	// inline asm
	abs.f64 	%fd284, %fd21;
	// inline asm
	setp.gt.f64 	%p77, %fd284, 0d41E0000000000000;
	@%p77 bra 	BB2_79;

	mov.f64 	%fd299, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd286, %fd21, %fd299;
	// inline asm
	cvt.rni.s32.f64 	%r462, %fd286;
	// inline asm
	cvt.rn.f64.s32 	%fd300, %r462;
	neg.f64 	%fd296, %fd300;
	mov.f64 	%fd289, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd287, %fd296, %fd289, %fd21;
	// inline asm
	mov.f64 	%fd293, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd291, %fd296, %fd293, %fd287;
	// inline asm
	mov.f64 	%fd297, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd295, %fd296, %fd297, %fd291;
	// inline asm
	mov.u32 	%r1148, %r462;
	mov.f64 	%fd2122, %fd295;
	bra.uni 	BB2_95;

BB2_79:
	mov.b64 	 %rl33, %fd21;
	and.b64  	%rl1366, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl637, %rl35, 2047;
	add.s64 	%rl638, %rl637, 4294966272;
	cvt.u32.u64 	%r66, %rl638;
	shl.b64 	%rl639, %rl33, 11;
	or.b64  	%rl36, %rl639, -9223372036854775808;
	shr.u32 	%r466, %r66, 6;
	mov.u32 	%r467, 16;
	sub.s32 	%r67, %r467, %r466;
	mov.u32 	%r468, 15;
	sub.s32 	%r1146, %r468, %r466;
	mov.u32 	%r469, 19;
	sub.s32 	%r69, %r469, %r466;
	mov.u32 	%r464, 18;
	// inline asm
	min.s32 	%r463, %r464, %r69;
	// inline asm
	setp.lt.s32 	%p78, %r1146, %r463;
	@%p78 bra 	BB2_81;

	mov.u64 	%rl1363, 0;
	bra.uni 	BB2_83;

BB2_81:
	mov.u32 	%r470, 1;
	sub.s32 	%r70, %r470, %r67;
	mov.u64 	%rl1363, 0;

BB2_82:
	.pragma "nounroll";
	shl.b32 	%r474, %r1146, 3;
	mov.u32 	%r475, __internal_i2opi_d;
	add.s32 	%r476, %r475, %r474;
	ld.const.u64 	%rl643, [%r476];
	mul.lo.s64 	%rl645, %rl643, %rl36;
	// inline asm
	mul.hi.u64 	%rl642, %rl643, %rl36;
	// inline asm
	mad.lo.s64 	%rl646, %rl643, %rl36, %rl1363;
	setp.lt.u64 	%p79, %rl646, %rl645;
	selp.u64 	%rl647, 1, 0, %p79;
	add.s64 	%rl1363, %rl647, %rl642;
	add.s32 	%r477, %r70, %r1146;
	shl.b32 	%r478, %r477, 3;
	add.s32 	%r480, %r48, %r478;
	st.local.u64 	[%r480], %rl646;
	// inline asm
	min.s32 	%r471, %r464, %r69;
	// inline asm
	add.s32 	%r1146, %r1146, 1;
	setp.lt.s32 	%p80, %r1146, %r471;
	@%p80 bra 	BB2_82;

BB2_83:
	mov.u32 	%r481, 1;
	sub.s32 	%r482, %r481, %r67;
	add.s32 	%r483, %r482, %r1146;
	shl.b32 	%r484, %r483, 3;
	add.s32 	%r486, %r48, %r484;
	st.local.u64 	[%r486], %rl1363;
	ld.local.u64 	%rl1364, [%r48+24];
	ld.local.u64 	%rl1365, [%r48+16];
	and.b32  	%r487, %r66, 63;
	setp.eq.s32 	%p81, %r487, 0;
	@%p81 bra 	BB2_85;

	and.b64  	%rl648, %rl35, 63;
	cvt.u32.u64 	%r488, %rl648;
	shl.b64 	%rl649, %rl1364, %r488;
	neg.s32 	%r489, %r66;
	and.b32  	%r490, %r489, 63;
	shr.u64 	%rl650, %rl1365, %r490;
	or.b64  	%rl1364, %rl650, %rl649;
	shl.b64 	%rl651, %rl1365, %r488;
	ld.local.u64 	%rl652, [%r48+8];
	shr.u64 	%rl653, %rl652, %r490;
	or.b64  	%rl1365, %rl653, %rl651;

BB2_85:
	shr.u64 	%rl654, %rl1364, 62;
	cvt.u32.u64 	%r491, %rl654;
	shr.u64 	%rl655, %rl1365, 62;
	shl.b64 	%rl656, %rl1364, 2;
	or.b64  	%rl1370, %rl655, %rl656;
	shl.b64 	%rl47, %rl1365, 2;
	setp.ne.s64 	%p82, %rl47, 0;
	selp.u64 	%rl657, 1, 0, %p82;
	or.b64  	%rl658, %rl657, %rl1370;
	setp.gt.u64 	%p83, %rl658, -9223372036854775808;
	selp.u32 	%r492, 1, 0, %p83;
	add.s32 	%r493, %r492, %r491;
	neg.s32 	%r494, %r493;
	setp.lt.s64 	%p84, %rl33, 0;
	selp.b32 	%r1148, %r494, %r493, %p84;
	@%p83 bra 	BB2_87;

	mov.u64 	%rl1369, %rl47;
	bra.uni 	BB2_88;

BB2_87:
	not.b64 	%rl659, %rl1370;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p85, %rl47, 0;
	selp.u64 	%rl660, 1, 0, %p85;
	add.s64 	%rl1370, %rl660, %rl659;
	xor.b64  	%rl1366, %rl1366, -9223372036854775808;
	mov.u64 	%rl1369, %rl48;

BB2_88:
	mov.u64 	%rl1368, %rl1369;
	setp.gt.s64 	%p86, %rl1370, 0;
	@%p86 bra 	BB2_90;

	mov.u32 	%r1147, 0;
	bra.uni 	BB2_92;

BB2_90:
	mov.u32 	%r1147, 0;

BB2_91:
	shr.u64 	%rl661, %rl1368, 63;
	shl.b64 	%rl662, %rl1370, 1;
	or.b64  	%rl1370, %rl661, %rl662;
	shl.b64 	%rl1368, %rl1368, 1;
	add.s32 	%r1147, %r1147, -1;
	setp.gt.s64 	%p87, %rl1370, 0;
	@%p87 bra 	BB2_91;

BB2_92:
	mul.lo.s64 	%rl1372, %rl1370, -3958705157555305931;
	mov.u64 	%rl665, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl663, %rl1370, %rl665;
	// inline asm
	setp.gt.s64 	%p88, %rl663, 0;
	mov.u64 	%rl1371, %rl663;
	@%p88 bra 	BB2_93;
	bra.uni 	BB2_94;

BB2_93:
	shl.b64 	%rl666, %rl663, 1;
	shr.u64 	%rl667, %rl1372, 63;
	or.b64  	%rl1371, %rl666, %rl667;
	mul.lo.s64 	%rl1372, %rl1370, -7917410315110611862;
	add.s32 	%r1147, %r1147, -1;

BB2_94:
	setp.ne.s64 	%p89, %rl1372, 0;
	selp.u64 	%rl668, 1, 0, %p89;
	add.s64 	%rl669, %rl668, %rl1371;
	add.s32 	%r497, %r1147, 1022;
	cvt.u64.u32 	%rl670, %r497;
	shl.b64 	%rl671, %rl670, 52;
	shr.u64 	%rl672, %rl669, 11;
	shr.u64 	%rl673, %rl669, 10;
	and.b64  	%rl674, %rl673, 1;
	add.s64 	%rl675, %rl671, %rl672;
	add.s64 	%rl676, %rl675, %rl674;
	or.b64  	%rl677, %rl676, %rl1366;
	mov.b64 	 %fd2122, %rl677;

BB2_95:
	and.b32  	%r498, %r1148, 1;
	setp.eq.s32 	%p90, %r498, 0;
	mul.rn.f64 	%fd35, %fd2122, %fd2122;
	@%p90 bra 	BB2_97;

	mov.f64 	%fd302, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd304, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd301, %fd302, %fd35, %fd304;
	// inline asm
	mov.f64 	%fd308, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd305, %fd301, %fd35, %fd308;
	// inline asm
	mov.f64 	%fd312, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd309, %fd305, %fd35, %fd312;
	// inline asm
	mov.f64 	%fd316, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd313, %fd309, %fd35, %fd316;
	// inline asm
	mov.f64 	%fd320, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd317, %fd313, %fd35, %fd320;
	// inline asm
	mov.f64 	%fd324, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd321, %fd317, %fd35, %fd324;
	// inline asm
	mov.f64 	%fd328, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd325, %fd321, %fd35, %fd328;
	// inline asm
	mov.f64 	%fd2123, %fd325;
	bra.uni 	BB2_98;

BB2_97:
	mov.f64 	%fd330, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd332, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd329, %fd330, %fd35, %fd332;
	// inline asm
	mov.f64 	%fd336, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd333, %fd329, %fd35, %fd336;
	// inline asm
	mov.f64 	%fd340, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd337, %fd333, %fd35, %fd340;
	// inline asm
	mov.f64 	%fd344, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd341, %fd337, %fd35, %fd344;
	// inline asm
	mov.f64 	%fd348, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd345, %fd341, %fd35, %fd348;
	// inline asm
	mul.rn.f64 	%fd350, %fd345, %fd35;
	// inline asm
	fma.rn.f64 	%fd349, %fd350, %fd2122, %fd2122;
	// inline asm
	mov.f64 	%fd2123, %fd349;

BB2_98:
	and.b32  	%r499, %r1148, 2;
	setp.eq.s32 	%p91, %r499, 0;
	neg.f64 	%fd353, %fd2123;
	selp.f64 	%fd2124, %fd2123, %fd353, %p91;
	bra.uni 	BB2_100;

BB2_99:
	mov.f64 	%fd354, 0d0000000000000000;
	mul.rn.f64 	%fd2124, %fd21, %fd354;

BB2_100:
	neg.f64 	%fd355, %fd2124;
	mov.f64 	%fd2209, %fd2121;
	mov.f64 	%fd2210, %fd355;
	ld.param.u32 	%r1132, [DIT10C2CM_param_4];
	setp.eq.s32 	%p92, %r1132, 0;
	@%p92 bra 	BB2_101;
	bra.uni 	BB2_102;

BB2_101:
	mov.f64 	%fd2209, %fd2121;
	mov.f64 	%fd2210, %fd2124;

BB2_102:
	mul.f64 	%fd357, %fd1, %fd2209;
	neg.f64 	%fd359, %fd2;
	fma.rn.f64 	%fd360, %fd359, %fd2210, %fd357;
	mul.f64 	%fd361, %fd2, %fd2209;
	fma.rn.f64 	%fd362, %fd1, %fd2210, %fd361;
	mul.f64 	%fd363, %fd19, 0d402921FB54442D18;
	div.rn.f64 	%fd42, %fd363, %fd20;
	setp.eq.f64 	%p93, %fd42, 0d7FF0000000000000;
	setp.eq.f64 	%p94, %fd42, 0dFFF0000000000000;
	or.pred  	%p95, %p93, %p94;
	@%p95 bra 	BB2_125;

	// inline asm
	abs.f64 	%fd364, %fd42;
	// inline asm
	setp.gt.f64 	%p96, %fd364, 0d41E0000000000000;
	@%p96 bra 	BB2_105;

	mov.f64 	%fd379, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd366, %fd42, %fd379;
	// inline asm
	cvt.rni.s32.f64 	%r500, %fd366;
	// inline asm
	cvt.rn.f64.s32 	%fd380, %r500;
	neg.f64 	%fd376, %fd380;
	mov.f64 	%fd369, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd367, %fd376, %fd369, %fd42;
	// inline asm
	mov.f64 	%fd373, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd371, %fd376, %fd373, %fd367;
	// inline asm
	mov.f64 	%fd377, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd375, %fd376, %fd377, %fd371;
	// inline asm
	mov.u32 	%r1151, %r500;
	mov.f64 	%fd2125, %fd375;
	bra.uni 	BB2_121;

BB2_105:
	mov.b64 	 %rl65, %fd42;
	shr.u64 	%rl678, %rl65, 52;
	and.b64  	%rl679, %rl678, 2047;
	add.s64 	%rl66, %rl679, 4294966272;
	cvt.u32.u64 	%r504, %rl66;
	shr.u32 	%r505, %r504, 6;
	mov.u32 	%r507, 15;
	sub.s32 	%r1149, %r507, %r505;
	mov.u32 	%r508, 19;
	sub.s32 	%r503, %r508, %r505;
	mov.u32 	%r502, 18;
	// inline asm
	min.s32 	%r501, %r502, %r503;
	// inline asm
	setp.lt.s32 	%p97, %r1149, %r501;
	@%p97 bra 	BB2_107;

	mov.u64 	%rl1373, 0;
	bra.uni 	BB2_109;

BB2_107:
	mov.u32 	%r511, 16;
	sub.s32 	%r512, %r511, %r505;
	mov.u32 	%r513, 1;
	sub.s32 	%r84, %r513, %r512;
	mov.u64 	%rl1373, 0;

BB2_108:
	.pragma "nounroll";
	shl.b64 	%rl690, %rl65, 11;
	or.b64  	%rl688, %rl690, -9223372036854775808;
	shl.b32 	%r517, %r1149, 3;
	mov.u32 	%r518, __internal_i2opi_d;
	add.s32 	%r519, %r518, %r517;
	ld.const.u64 	%rl687, [%r519];
	mul.lo.s64 	%rl691, %rl687, %rl688;
	// inline asm
	mul.hi.u64 	%rl686, %rl687, %rl688;
	// inline asm
	mad.lo.s64 	%rl692, %rl687, %rl688, %rl1373;
	setp.lt.u64 	%p98, %rl692, %rl691;
	selp.u64 	%rl693, 1, 0, %p98;
	add.s64 	%rl1373, %rl693, %rl686;
	add.s32 	%r520, %r84, %r1149;
	shl.b32 	%r521, %r520, 3;
	add.s32 	%r523, %r48, %r521;
	st.local.u64 	[%r523], %rl692;
	// inline asm
	min.s32 	%r514, %r502, %r503;
	// inline asm
	add.s32 	%r1149, %r1149, 1;
	setp.lt.s32 	%p99, %r1149, %r514;
	@%p99 bra 	BB2_108;

BB2_109:
	mov.u32 	%r529, 16;
	sub.s32 	%r530, %r529, %r505;
	mov.u32 	%r531, 1;
	sub.s32 	%r532, %r531, %r530;
	add.s32 	%r533, %r532, %r1149;
	shl.b32 	%r534, %r533, 3;
	add.s32 	%r536, %r48, %r534;
	st.local.u64 	[%r536], %rl1373;
	and.b32  	%r537, %r504, 63;
	ld.local.u64 	%rl1374, [%r48+24];
	ld.local.u64 	%rl1375, [%r48+16];
	setp.eq.s32 	%p100, %r537, 0;
	@%p100 bra 	BB2_111;

	and.b64  	%rl703, %rl678, 63;
	cvt.u32.u64 	%r538, %rl703;
	shl.b64 	%rl704, %rl1374, %r538;
	add.s64 	%rl705, %rl678, 4294966272;
	cvt.u32.u64 	%r539, %rl705;
	neg.s32 	%r540, %r539;
	and.b32  	%r541, %r540, 63;
	shr.u64 	%rl706, %rl1375, %r541;
	or.b64  	%rl1374, %rl706, %rl704;
	shl.b64 	%rl707, %rl1375, %r538;
	ld.local.u64 	%rl708, [%r48+8];
	shr.u64 	%rl709, %rl708, %r541;
	or.b64  	%rl1375, %rl709, %rl707;

BB2_111:
	shr.u64 	%rl710, %rl1374, 62;
	cvt.u32.u64 	%r542, %rl710;
	shr.u64 	%rl711, %rl1375, 62;
	shl.b64 	%rl712, %rl1374, 2;
	or.b64  	%rl1380, %rl711, %rl712;
	shl.b64 	%rl78, %rl1375, 2;
	setp.ne.s64 	%p101, %rl78, 0;
	selp.u64 	%rl713, 1, 0, %p101;
	or.b64  	%rl714, %rl713, %rl1380;
	setp.gt.u64 	%p102, %rl714, -9223372036854775808;
	selp.u32 	%r543, 1, 0, %p102;
	add.s32 	%r544, %r543, %r542;
	setp.lt.s64 	%p103, %rl65, 0;
	neg.s32 	%r545, %r544;
	selp.b32 	%r1151, %r545, %r544, %p103;
	@%p102 bra 	BB2_113;

	and.b64  	%rl1376, %rl65, -9223372036854775808;
	mov.u64 	%rl1379, %rl78;
	bra.uni 	BB2_114;

BB2_113:
	not.b64 	%rl717, %rl1380;
	neg.s64 	%rl80, %rl78;
	setp.eq.s64 	%p104, %rl78, 0;
	selp.u64 	%rl718, 1, 0, %p104;
	add.s64 	%rl1380, %rl718, %rl717;
	and.b64  	%rl720, %rl65, -9223372036854775808;
	xor.b64  	%rl1376, %rl720, -9223372036854775808;
	mov.u64 	%rl1379, %rl80;

BB2_114:
	mov.u64 	%rl1378, %rl1379;
	setp.gt.s64 	%p105, %rl1380, 0;
	@%p105 bra 	BB2_116;

	mov.u32 	%r1150, 0;
	bra.uni 	BB2_118;

BB2_116:
	mov.u32 	%r1150, 0;

BB2_117:
	shr.u64 	%rl721, %rl1378, 63;
	shl.b64 	%rl722, %rl1380, 1;
	or.b64  	%rl1380, %rl721, %rl722;
	shl.b64 	%rl1378, %rl1378, 1;
	add.s32 	%r1150, %r1150, -1;
	setp.gt.s64 	%p106, %rl1380, 0;
	@%p106 bra 	BB2_117;

BB2_118:
	mul.lo.s64 	%rl1382, %rl1380, -3958705157555305931;
	mov.u64 	%rl725, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl723, %rl1380, %rl725;
	// inline asm
	setp.gt.s64 	%p107, %rl723, 0;
	mov.u64 	%rl1381, %rl723;
	@%p107 bra 	BB2_119;
	bra.uni 	BB2_120;

BB2_119:
	shl.b64 	%rl726, %rl723, 1;
	shr.u64 	%rl727, %rl1382, 63;
	or.b64  	%rl1381, %rl726, %rl727;
	mul.lo.s64 	%rl1382, %rl1380, -7917410315110611862;
	add.s32 	%r1150, %r1150, -1;

BB2_120:
	setp.ne.s64 	%p108, %rl1382, 0;
	selp.u64 	%rl728, 1, 0, %p108;
	add.s64 	%rl729, %rl728, %rl1381;
	add.s32 	%r548, %r1150, 1022;
	cvt.u64.u32 	%rl730, %r548;
	shl.b64 	%rl731, %rl730, 52;
	shr.u64 	%rl732, %rl729, 11;
	shr.u64 	%rl733, %rl729, 10;
	and.b64  	%rl734, %rl733, 1;
	add.s64 	%rl735, %rl731, %rl732;
	add.s64 	%rl736, %rl735, %rl734;
	or.b64  	%rl737, %rl736, %rl1376;
	mov.b64 	 %fd2125, %rl737;

BB2_121:
	add.s32 	%r95, %r1151, 1;
	and.b32  	%r549, %r95, 1;
	setp.eq.s32 	%p109, %r549, 0;
	mul.rn.f64 	%fd46, %fd2125, %fd2125;
	@%p109 bra 	BB2_123;

	mov.f64 	%fd382, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd384, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd381, %fd382, %fd46, %fd384;
	// inline asm
	mov.f64 	%fd388, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd385, %fd381, %fd46, %fd388;
	// inline asm
	mov.f64 	%fd392, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd389, %fd385, %fd46, %fd392;
	// inline asm
	mov.f64 	%fd396, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd393, %fd389, %fd46, %fd396;
	// inline asm
	mov.f64 	%fd400, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd397, %fd393, %fd46, %fd400;
	// inline asm
	mov.f64 	%fd404, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd401, %fd397, %fd46, %fd404;
	// inline asm
	mov.f64 	%fd408, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd405, %fd401, %fd46, %fd408;
	// inline asm
	mov.f64 	%fd2126, %fd405;
	bra.uni 	BB2_124;

BB2_123:
	mov.f64 	%fd410, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd412, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd409, %fd410, %fd46, %fd412;
	// inline asm
	mov.f64 	%fd416, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd413, %fd409, %fd46, %fd416;
	// inline asm
	mov.f64 	%fd420, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd417, %fd413, %fd46, %fd420;
	// inline asm
	mov.f64 	%fd424, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd421, %fd417, %fd46, %fd424;
	// inline asm
	mov.f64 	%fd428, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd425, %fd421, %fd46, %fd428;
	// inline asm
	mul.rn.f64 	%fd430, %fd425, %fd46;
	// inline asm
	fma.rn.f64 	%fd429, %fd430, %fd2125, %fd2125;
	// inline asm
	mov.f64 	%fd2126, %fd429;

BB2_124:
	and.b32  	%r550, %r95, 2;
	setp.eq.s32 	%p110, %r550, 0;
	neg.f64 	%fd433, %fd2126;
	selp.f64 	%fd2127, %fd2126, %fd433, %p110;
	bra.uni 	BB2_126;

BB2_125:
	mov.f64 	%fd2127, 0dFFF8000000000000;

BB2_126:
	setp.eq.f64 	%p112, %fd42, 0d0000000000000000;
	or.pred  	%p113, %p94, %p112;
	or.pred  	%p115, %p93, %p113;
	@%p115 bra 	BB2_149;

	// inline asm
	abs.f64 	%fd435, %fd42;
	// inline asm
	setp.gt.f64 	%p116, %fd435, 0d41E0000000000000;
	@%p116 bra 	BB2_129;

	mov.f64 	%fd450, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd437, %fd42, %fd450;
	// inline asm
	cvt.rni.s32.f64 	%r551, %fd437;
	// inline asm
	cvt.rn.f64.s32 	%fd451, %r551;
	neg.f64 	%fd447, %fd451;
	mov.f64 	%fd440, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd438, %fd447, %fd440, %fd42;
	// inline asm
	mov.f64 	%fd444, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd442, %fd447, %fd444, %fd438;
	// inline asm
	mov.f64 	%fd448, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd446, %fd447, %fd448, %fd442;
	// inline asm
	mov.u32 	%r1154, %r551;
	mov.f64 	%fd2128, %fd446;
	bra.uni 	BB2_145;

BB2_129:
	mov.b64 	 %rl97, %fd42;
	and.b64  	%rl1386, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl738, %rl99, 2047;
	add.s64 	%rl739, %rl738, 4294966272;
	cvt.u32.u64 	%r97, %rl739;
	shl.b64 	%rl740, %rl97, 11;
	or.b64  	%rl100, %rl740, -9223372036854775808;
	shr.u32 	%r555, %r97, 6;
	mov.u32 	%r556, 16;
	sub.s32 	%r98, %r556, %r555;
	mov.u32 	%r557, 15;
	sub.s32 	%r1152, %r557, %r555;
	mov.u32 	%r558, 19;
	sub.s32 	%r100, %r558, %r555;
	mov.u32 	%r553, 18;
	// inline asm
	min.s32 	%r552, %r553, %r100;
	// inline asm
	setp.lt.s32 	%p117, %r1152, %r552;
	@%p117 bra 	BB2_131;

	mov.u64 	%rl1383, 0;
	bra.uni 	BB2_133;

BB2_131:
	mov.u32 	%r559, 1;
	sub.s32 	%r101, %r559, %r98;
	mov.u64 	%rl1383, 0;

BB2_132:
	.pragma "nounroll";
	shl.b32 	%r563, %r1152, 3;
	mov.u32 	%r564, __internal_i2opi_d;
	add.s32 	%r565, %r564, %r563;
	ld.const.u64 	%rl744, [%r565];
	mul.lo.s64 	%rl746, %rl744, %rl100;
	// inline asm
	mul.hi.u64 	%rl743, %rl744, %rl100;
	// inline asm
	mad.lo.s64 	%rl747, %rl744, %rl100, %rl1383;
	setp.lt.u64 	%p118, %rl747, %rl746;
	selp.u64 	%rl748, 1, 0, %p118;
	add.s64 	%rl1383, %rl748, %rl743;
	add.s32 	%r566, %r101, %r1152;
	shl.b32 	%r567, %r566, 3;
	add.s32 	%r569, %r48, %r567;
	st.local.u64 	[%r569], %rl747;
	// inline asm
	min.s32 	%r560, %r553, %r100;
	// inline asm
	add.s32 	%r1152, %r1152, 1;
	setp.lt.s32 	%p119, %r1152, %r560;
	@%p119 bra 	BB2_132;

BB2_133:
	mov.u32 	%r570, 1;
	sub.s32 	%r571, %r570, %r98;
	add.s32 	%r572, %r571, %r1152;
	shl.b32 	%r573, %r572, 3;
	add.s32 	%r575, %r48, %r573;
	st.local.u64 	[%r575], %rl1383;
	ld.local.u64 	%rl1384, [%r48+24];
	ld.local.u64 	%rl1385, [%r48+16];
	and.b32  	%r576, %r97, 63;
	setp.eq.s32 	%p120, %r576, 0;
	@%p120 bra 	BB2_135;

	and.b64  	%rl749, %rl99, 63;
	cvt.u32.u64 	%r577, %rl749;
	shl.b64 	%rl750, %rl1384, %r577;
	neg.s32 	%r578, %r97;
	and.b32  	%r579, %r578, 63;
	shr.u64 	%rl751, %rl1385, %r579;
	or.b64  	%rl1384, %rl751, %rl750;
	shl.b64 	%rl752, %rl1385, %r577;
	ld.local.u64 	%rl753, [%r48+8];
	shr.u64 	%rl754, %rl753, %r579;
	or.b64  	%rl1385, %rl754, %rl752;

BB2_135:
	shr.u64 	%rl755, %rl1384, 62;
	cvt.u32.u64 	%r580, %rl755;
	shr.u64 	%rl756, %rl1385, 62;
	shl.b64 	%rl757, %rl1384, 2;
	or.b64  	%rl1390, %rl756, %rl757;
	shl.b64 	%rl111, %rl1385, 2;
	setp.ne.s64 	%p121, %rl111, 0;
	selp.u64 	%rl758, 1, 0, %p121;
	or.b64  	%rl759, %rl758, %rl1390;
	setp.gt.u64 	%p122, %rl759, -9223372036854775808;
	selp.u32 	%r581, 1, 0, %p122;
	add.s32 	%r582, %r581, %r580;
	neg.s32 	%r583, %r582;
	setp.lt.s64 	%p123, %rl97, 0;
	selp.b32 	%r1154, %r583, %r582, %p123;
	@%p122 bra 	BB2_137;

	mov.u64 	%rl1389, %rl111;
	bra.uni 	BB2_138;

BB2_137:
	not.b64 	%rl760, %rl1390;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p124, %rl111, 0;
	selp.u64 	%rl761, 1, 0, %p124;
	add.s64 	%rl1390, %rl761, %rl760;
	xor.b64  	%rl1386, %rl1386, -9223372036854775808;
	mov.u64 	%rl1389, %rl112;

BB2_138:
	mov.u64 	%rl1388, %rl1389;
	setp.gt.s64 	%p125, %rl1390, 0;
	@%p125 bra 	BB2_140;

	mov.u32 	%r1153, 0;
	bra.uni 	BB2_142;

BB2_140:
	mov.u32 	%r1153, 0;

BB2_141:
	shr.u64 	%rl762, %rl1388, 63;
	shl.b64 	%rl763, %rl1390, 1;
	or.b64  	%rl1390, %rl762, %rl763;
	shl.b64 	%rl1388, %rl1388, 1;
	add.s32 	%r1153, %r1153, -1;
	setp.gt.s64 	%p126, %rl1390, 0;
	@%p126 bra 	BB2_141;

BB2_142:
	mul.lo.s64 	%rl1392, %rl1390, -3958705157555305931;
	mov.u64 	%rl766, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl764, %rl1390, %rl766;
	// inline asm
	setp.gt.s64 	%p127, %rl764, 0;
	mov.u64 	%rl1391, %rl764;
	@%p127 bra 	BB2_143;
	bra.uni 	BB2_144;

BB2_143:
	shl.b64 	%rl767, %rl764, 1;
	shr.u64 	%rl768, %rl1392, 63;
	or.b64  	%rl1391, %rl767, %rl768;
	mul.lo.s64 	%rl1392, %rl1390, -7917410315110611862;
	add.s32 	%r1153, %r1153, -1;

BB2_144:
	setp.ne.s64 	%p128, %rl1392, 0;
	selp.u64 	%rl769, 1, 0, %p128;
	add.s64 	%rl770, %rl769, %rl1391;
	add.s32 	%r586, %r1153, 1022;
	cvt.u64.u32 	%rl771, %r586;
	shl.b64 	%rl772, %rl771, 52;
	shr.u64 	%rl773, %rl770, 11;
	shr.u64 	%rl774, %rl770, 10;
	and.b64  	%rl775, %rl774, 1;
	add.s64 	%rl776, %rl772, %rl773;
	add.s64 	%rl777, %rl776, %rl775;
	or.b64  	%rl778, %rl777, %rl1386;
	mov.b64 	 %fd2128, %rl778;

BB2_145:
	and.b32  	%r587, %r1154, 1;
	setp.eq.s32 	%p129, %r587, 0;
	mul.rn.f64 	%fd56, %fd2128, %fd2128;
	@%p129 bra 	BB2_147;

	mov.f64 	%fd453, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd455, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd452, %fd453, %fd56, %fd455;
	// inline asm
	mov.f64 	%fd459, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd456, %fd452, %fd56, %fd459;
	// inline asm
	mov.f64 	%fd463, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd460, %fd456, %fd56, %fd463;
	// inline asm
	mov.f64 	%fd467, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd464, %fd460, %fd56, %fd467;
	// inline asm
	mov.f64 	%fd471, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd468, %fd464, %fd56, %fd471;
	// inline asm
	mov.f64 	%fd475, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd472, %fd468, %fd56, %fd475;
	// inline asm
	mov.f64 	%fd479, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd476, %fd472, %fd56, %fd479;
	// inline asm
	mov.f64 	%fd2129, %fd476;
	bra.uni 	BB2_148;

BB2_147:
	mov.f64 	%fd481, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd483, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd480, %fd481, %fd56, %fd483;
	// inline asm
	mov.f64 	%fd487, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd484, %fd480, %fd56, %fd487;
	// inline asm
	mov.f64 	%fd491, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd488, %fd484, %fd56, %fd491;
	// inline asm
	mov.f64 	%fd495, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd492, %fd488, %fd56, %fd495;
	// inline asm
	mov.f64 	%fd499, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd496, %fd492, %fd56, %fd499;
	// inline asm
	mul.rn.f64 	%fd501, %fd496, %fd56;
	// inline asm
	fma.rn.f64 	%fd500, %fd501, %fd2128, %fd2128;
	// inline asm
	mov.f64 	%fd2129, %fd500;

BB2_148:
	and.b32  	%r588, %r1154, 2;
	setp.eq.s32 	%p130, %r588, 0;
	neg.f64 	%fd504, %fd2129;
	selp.f64 	%fd2130, %fd2129, %fd504, %p130;
	bra.uni 	BB2_150;

BB2_149:
	mov.f64 	%fd505, 0d0000000000000000;
	mul.rn.f64 	%fd2130, %fd42, %fd505;

BB2_150:
	neg.f64 	%fd506, %fd2130;
	mov.f64 	%fd2207, %fd2127;
	mov.f64 	%fd2208, %fd506;
	ld.param.u32 	%r1131, [DIT10C2CM_param_4];
	setp.eq.s32 	%p131, %r1131, 0;
	@%p131 bra 	BB2_151;
	bra.uni 	BB2_152;

BB2_151:
	mov.f64 	%fd2207, %fd2127;
	mov.f64 	%fd2208, %fd2130;

BB2_152:
	mul.f64 	%fd508, %fd3, %fd2207;
	neg.f64 	%fd510, %fd4;
	fma.rn.f64 	%fd511, %fd510, %fd2208, %fd508;
	mul.f64 	%fd512, %fd4, %fd2207;
	fma.rn.f64 	%fd513, %fd3, %fd2208, %fd512;
	mul.f64 	%fd514, %fd19, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd63, %fd514, %fd20;
	setp.eq.f64 	%p7, %fd63, 0d7FF0000000000000;
	setp.eq.f64 	%p8, %fd63, 0dFFF0000000000000;
	or.pred  	%p132, %p7, %p8;
	@%p132 bra 	BB2_175;

	// inline asm
	abs.f64 	%fd515, %fd63;
	// inline asm
	setp.gt.f64 	%p133, %fd515, 0d41E0000000000000;
	@%p133 bra 	BB2_155;

	mov.f64 	%fd530, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd517, %fd63, %fd530;
	// inline asm
	cvt.rni.s32.f64 	%r589, %fd517;
	// inline asm
	cvt.rn.f64.s32 	%fd531, %r589;
	neg.f64 	%fd527, %fd531;
	mov.f64 	%fd520, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd518, %fd527, %fd520, %fd63;
	// inline asm
	mov.f64 	%fd524, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd522, %fd527, %fd524, %fd518;
	// inline asm
	mov.f64 	%fd528, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd526, %fd527, %fd528, %fd522;
	// inline asm
	mov.u32 	%r1157, %r589;
	mov.f64 	%fd2131, %fd526;
	bra.uni 	BB2_171;

BB2_155:
	mov.b64 	 %rl129, %fd63;
	and.b64  	%rl1396, %rl129, -9223372036854775808;
	shr.u64 	%rl131, %rl129, 52;
	and.b64  	%rl779, %rl131, 2047;
	add.s64 	%rl780, %rl779, 4294966272;
	cvt.u32.u64 	%r113, %rl780;
	shl.b64 	%rl781, %rl129, 11;
	or.b64  	%rl132, %rl781, -9223372036854775808;
	shr.u32 	%r593, %r113, 6;
	mov.u32 	%r594, 16;
	sub.s32 	%r114, %r594, %r593;
	mov.u32 	%r595, 15;
	sub.s32 	%r1155, %r595, %r593;
	mov.u32 	%r596, 19;
	sub.s32 	%r116, %r596, %r593;
	mov.u32 	%r591, 18;
	// inline asm
	min.s32 	%r590, %r591, %r116;
	// inline asm
	setp.lt.s32 	%p134, %r1155, %r590;
	@%p134 bra 	BB2_157;

	mov.u64 	%rl1393, 0;
	bra.uni 	BB2_159;

BB2_157:
	mov.u32 	%r597, 1;
	sub.s32 	%r117, %r597, %r114;
	mov.u64 	%rl1393, 0;

BB2_158:
	.pragma "nounroll";
	shl.b32 	%r601, %r1155, 3;
	mov.u32 	%r602, __internal_i2opi_d;
	add.s32 	%r603, %r602, %r601;
	ld.const.u64 	%rl785, [%r603];
	mul.lo.s64 	%rl787, %rl785, %rl132;
	// inline asm
	mul.hi.u64 	%rl784, %rl785, %rl132;
	// inline asm
	mad.lo.s64 	%rl788, %rl785, %rl132, %rl1393;
	setp.lt.u64 	%p135, %rl788, %rl787;
	selp.u64 	%rl789, 1, 0, %p135;
	add.s64 	%rl1393, %rl789, %rl784;
	add.s32 	%r604, %r117, %r1155;
	shl.b32 	%r605, %r604, 3;
	add.s32 	%r607, %r48, %r605;
	st.local.u64 	[%r607], %rl788;
	// inline asm
	min.s32 	%r598, %r591, %r116;
	// inline asm
	add.s32 	%r1155, %r1155, 1;
	setp.lt.s32 	%p136, %r1155, %r598;
	@%p136 bra 	BB2_158;

BB2_159:
	mov.u32 	%r608, 1;
	sub.s32 	%r609, %r608, %r114;
	add.s32 	%r610, %r609, %r1155;
	shl.b32 	%r611, %r610, 3;
	add.s32 	%r613, %r48, %r611;
	st.local.u64 	[%r613], %rl1393;
	ld.local.u64 	%rl1394, [%r48+24];
	ld.local.u64 	%rl1395, [%r48+16];
	and.b32  	%r614, %r113, 63;
	setp.eq.s32 	%p137, %r614, 0;
	@%p137 bra 	BB2_161;

	and.b64  	%rl790, %rl131, 63;
	cvt.u32.u64 	%r615, %rl790;
	shl.b64 	%rl791, %rl1394, %r615;
	neg.s32 	%r616, %r113;
	and.b32  	%r617, %r616, 63;
	shr.u64 	%rl792, %rl1395, %r617;
	or.b64  	%rl1394, %rl792, %rl791;
	shl.b64 	%rl793, %rl1395, %r615;
	ld.local.u64 	%rl794, [%r48+8];
	shr.u64 	%rl795, %rl794, %r617;
	or.b64  	%rl1395, %rl795, %rl793;

BB2_161:
	shr.u64 	%rl796, %rl1394, 62;
	cvt.u32.u64 	%r618, %rl796;
	shr.u64 	%rl797, %rl1395, 62;
	shl.b64 	%rl798, %rl1394, 2;
	or.b64  	%rl1400, %rl797, %rl798;
	shl.b64 	%rl143, %rl1395, 2;
	setp.ne.s64 	%p138, %rl143, 0;
	selp.u64 	%rl799, 1, 0, %p138;
	or.b64  	%rl800, %rl799, %rl1400;
	setp.gt.u64 	%p139, %rl800, -9223372036854775808;
	selp.u32 	%r619, 1, 0, %p139;
	add.s32 	%r620, %r619, %r618;
	neg.s32 	%r621, %r620;
	setp.lt.s64 	%p140, %rl129, 0;
	selp.b32 	%r1157, %r621, %r620, %p140;
	@%p139 bra 	BB2_163;

	mov.u64 	%rl1399, %rl143;
	bra.uni 	BB2_164;

BB2_163:
	not.b64 	%rl801, %rl1400;
	neg.s64 	%rl144, %rl143;
	setp.eq.s64 	%p141, %rl143, 0;
	selp.u64 	%rl802, 1, 0, %p141;
	add.s64 	%rl1400, %rl802, %rl801;
	xor.b64  	%rl1396, %rl1396, -9223372036854775808;
	mov.u64 	%rl1399, %rl144;

BB2_164:
	mov.u64 	%rl1398, %rl1399;
	setp.gt.s64 	%p142, %rl1400, 0;
	@%p142 bra 	BB2_166;

	mov.u32 	%r1156, 0;
	bra.uni 	BB2_168;

BB2_166:
	mov.u32 	%r1156, 0;

BB2_167:
	shr.u64 	%rl803, %rl1398, 63;
	shl.b64 	%rl804, %rl1400, 1;
	or.b64  	%rl1400, %rl803, %rl804;
	shl.b64 	%rl1398, %rl1398, 1;
	add.s32 	%r1156, %r1156, -1;
	setp.gt.s64 	%p143, %rl1400, 0;
	@%p143 bra 	BB2_167;

BB2_168:
	mul.lo.s64 	%rl1402, %rl1400, -3958705157555305931;
	mov.u64 	%rl807, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl805, %rl1400, %rl807;
	// inline asm
	setp.gt.s64 	%p144, %rl805, 0;
	mov.u64 	%rl1401, %rl805;
	@%p144 bra 	BB2_169;
	bra.uni 	BB2_170;

BB2_169:
	shl.b64 	%rl808, %rl805, 1;
	shr.u64 	%rl809, %rl1402, 63;
	or.b64  	%rl1401, %rl808, %rl809;
	mul.lo.s64 	%rl1402, %rl1400, -7917410315110611862;
	add.s32 	%r1156, %r1156, -1;

BB2_170:
	setp.ne.s64 	%p145, %rl1402, 0;
	selp.u64 	%rl810, 1, 0, %p145;
	add.s64 	%rl811, %rl810, %rl1401;
	add.s32 	%r624, %r1156, 1022;
	cvt.u64.u32 	%rl812, %r624;
	shl.b64 	%rl813, %rl812, 52;
	shr.u64 	%rl814, %rl811, 11;
	shr.u64 	%rl815, %rl811, 10;
	and.b64  	%rl816, %rl815, 1;
	add.s64 	%rl817, %rl813, %rl814;
	add.s64 	%rl818, %rl817, %rl816;
	or.b64  	%rl819, %rl818, %rl1396;
	mov.b64 	 %fd2131, %rl819;

BB2_171:
	add.s32 	%r128, %r1157, 1;
	and.b32  	%r625, %r128, 1;
	setp.eq.s32 	%p146, %r625, 0;
	mul.rn.f64 	%fd67, %fd2131, %fd2131;
	@%p146 bra 	BB2_173;

	mov.f64 	%fd533, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd535, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd532, %fd533, %fd67, %fd535;
	// inline asm
	mov.f64 	%fd539, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd536, %fd532, %fd67, %fd539;
	// inline asm
	mov.f64 	%fd543, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd540, %fd536, %fd67, %fd543;
	// inline asm
	mov.f64 	%fd547, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd544, %fd540, %fd67, %fd547;
	// inline asm
	mov.f64 	%fd551, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd548, %fd544, %fd67, %fd551;
	// inline asm
	mov.f64 	%fd555, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd552, %fd548, %fd67, %fd555;
	// inline asm
	mov.f64 	%fd559, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd556, %fd552, %fd67, %fd559;
	// inline asm
	mov.f64 	%fd2132, %fd556;
	bra.uni 	BB2_174;

BB2_173:
	mov.f64 	%fd561, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd563, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd560, %fd561, %fd67, %fd563;
	// inline asm
	mov.f64 	%fd567, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd564, %fd560, %fd67, %fd567;
	// inline asm
	mov.f64 	%fd571, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd568, %fd564, %fd67, %fd571;
	// inline asm
	mov.f64 	%fd575, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd572, %fd568, %fd67, %fd575;
	// inline asm
	mov.f64 	%fd579, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd576, %fd572, %fd67, %fd579;
	// inline asm
	mul.rn.f64 	%fd581, %fd576, %fd67;
	// inline asm
	fma.rn.f64 	%fd580, %fd581, %fd2131, %fd2131;
	// inline asm
	mov.f64 	%fd2132, %fd580;

BB2_174:
	and.b32  	%r626, %r128, 2;
	setp.eq.s32 	%p147, %r626, 0;
	neg.f64 	%fd584, %fd2132;
	selp.f64 	%fd2133, %fd2132, %fd584, %p147;
	bra.uni 	BB2_176;

BB2_175:
	mov.f64 	%fd2133, 0dFFF8000000000000;

BB2_176:
	setp.eq.f64 	%p148, %fd63, 0d0000000000000000;
	or.pred  	%p149, %p8, %p148;
	or.pred  	%p150, %p7, %p149;
	@%p150 bra 	BB2_199;

	// inline asm
	abs.f64 	%fd586, %fd63;
	// inline asm
	setp.gt.f64 	%p151, %fd586, 0d41E0000000000000;
	@%p151 bra 	BB2_179;

	mov.f64 	%fd601, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd588, %fd63, %fd601;
	// inline asm
	cvt.rni.s32.f64 	%r627, %fd588;
	// inline asm
	cvt.rn.f64.s32 	%fd602, %r627;
	neg.f64 	%fd598, %fd602;
	mov.f64 	%fd591, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd589, %fd598, %fd591, %fd63;
	// inline asm
	mov.f64 	%fd595, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd593, %fd598, %fd595, %fd589;
	// inline asm
	mov.f64 	%fd599, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd597, %fd598, %fd599, %fd593;
	// inline asm
	mov.u32 	%r1160, %r627;
	mov.f64 	%fd2134, %fd597;
	bra.uni 	BB2_195;

BB2_179:
	mov.b64 	 %rl161, %fd63;
	and.b64  	%rl1406, %rl161, -9223372036854775808;
	shr.u64 	%rl163, %rl161, 52;
	and.b64  	%rl820, %rl163, 2047;
	add.s64 	%rl821, %rl820, 4294966272;
	cvt.u32.u64 	%r130, %rl821;
	shl.b64 	%rl822, %rl161, 11;
	or.b64  	%rl164, %rl822, -9223372036854775808;
	shr.u32 	%r631, %r130, 6;
	mov.u32 	%r632, 16;
	sub.s32 	%r131, %r632, %r631;
	mov.u32 	%r633, 15;
	sub.s32 	%r1158, %r633, %r631;
	mov.u32 	%r634, 19;
	sub.s32 	%r133, %r634, %r631;
	mov.u32 	%r629, 18;
	// inline asm
	min.s32 	%r628, %r629, %r133;
	// inline asm
	setp.lt.s32 	%p152, %r1158, %r628;
	@%p152 bra 	BB2_181;

	mov.u64 	%rl1403, 0;
	bra.uni 	BB2_183;

BB2_181:
	mov.u32 	%r635, 1;
	sub.s32 	%r134, %r635, %r131;
	mov.u64 	%rl1403, 0;

BB2_182:
	.pragma "nounroll";
	shl.b32 	%r639, %r1158, 3;
	mov.u32 	%r640, __internal_i2opi_d;
	add.s32 	%r641, %r640, %r639;
	ld.const.u64 	%rl826, [%r641];
	mul.lo.s64 	%rl828, %rl826, %rl164;
	// inline asm
	mul.hi.u64 	%rl825, %rl826, %rl164;
	// inline asm
	mad.lo.s64 	%rl829, %rl826, %rl164, %rl1403;
	setp.lt.u64 	%p153, %rl829, %rl828;
	selp.u64 	%rl830, 1, 0, %p153;
	add.s64 	%rl1403, %rl830, %rl825;
	add.s32 	%r642, %r134, %r1158;
	shl.b32 	%r643, %r642, 3;
	add.s32 	%r645, %r48, %r643;
	st.local.u64 	[%r645], %rl829;
	// inline asm
	min.s32 	%r636, %r629, %r133;
	// inline asm
	add.s32 	%r1158, %r1158, 1;
	setp.lt.s32 	%p154, %r1158, %r636;
	@%p154 bra 	BB2_182;

BB2_183:
	mov.u32 	%r646, 1;
	sub.s32 	%r647, %r646, %r131;
	add.s32 	%r648, %r647, %r1158;
	shl.b32 	%r649, %r648, 3;
	add.s32 	%r651, %r48, %r649;
	st.local.u64 	[%r651], %rl1403;
	ld.local.u64 	%rl1404, [%r48+24];
	ld.local.u64 	%rl1405, [%r48+16];
	and.b32  	%r652, %r130, 63;
	setp.eq.s32 	%p155, %r652, 0;
	@%p155 bra 	BB2_185;

	and.b64  	%rl831, %rl163, 63;
	cvt.u32.u64 	%r653, %rl831;
	shl.b64 	%rl832, %rl1404, %r653;
	neg.s32 	%r654, %r130;
	and.b32  	%r655, %r654, 63;
	shr.u64 	%rl833, %rl1405, %r655;
	or.b64  	%rl1404, %rl833, %rl832;
	shl.b64 	%rl834, %rl1405, %r653;
	ld.local.u64 	%rl835, [%r48+8];
	shr.u64 	%rl836, %rl835, %r655;
	or.b64  	%rl1405, %rl836, %rl834;

BB2_185:
	shr.u64 	%rl837, %rl1404, 62;
	cvt.u32.u64 	%r656, %rl837;
	shr.u64 	%rl838, %rl1405, 62;
	shl.b64 	%rl839, %rl1404, 2;
	or.b64  	%rl1410, %rl838, %rl839;
	shl.b64 	%rl175, %rl1405, 2;
	setp.ne.s64 	%p156, %rl175, 0;
	selp.u64 	%rl840, 1, 0, %p156;
	or.b64  	%rl841, %rl840, %rl1410;
	setp.gt.u64 	%p157, %rl841, -9223372036854775808;
	selp.u32 	%r657, 1, 0, %p157;
	add.s32 	%r658, %r657, %r656;
	neg.s32 	%r659, %r658;
	setp.lt.s64 	%p158, %rl161, 0;
	selp.b32 	%r1160, %r659, %r658, %p158;
	@%p157 bra 	BB2_187;

	mov.u64 	%rl1409, %rl175;
	bra.uni 	BB2_188;

BB2_187:
	not.b64 	%rl842, %rl1410;
	neg.s64 	%rl176, %rl175;
	setp.eq.s64 	%p159, %rl175, 0;
	selp.u64 	%rl843, 1, 0, %p159;
	add.s64 	%rl1410, %rl843, %rl842;
	xor.b64  	%rl1406, %rl1406, -9223372036854775808;
	mov.u64 	%rl1409, %rl176;

BB2_188:
	mov.u64 	%rl1408, %rl1409;
	setp.gt.s64 	%p160, %rl1410, 0;
	@%p160 bra 	BB2_190;

	mov.u32 	%r1159, 0;
	bra.uni 	BB2_192;

BB2_190:
	mov.u32 	%r1159, 0;

BB2_191:
	shr.u64 	%rl844, %rl1408, 63;
	shl.b64 	%rl845, %rl1410, 1;
	or.b64  	%rl1410, %rl844, %rl845;
	shl.b64 	%rl1408, %rl1408, 1;
	add.s32 	%r1159, %r1159, -1;
	setp.gt.s64 	%p161, %rl1410, 0;
	@%p161 bra 	BB2_191;

BB2_192:
	mul.lo.s64 	%rl1412, %rl1410, -3958705157555305931;
	mov.u64 	%rl848, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl846, %rl1410, %rl848;
	// inline asm
	setp.gt.s64 	%p162, %rl846, 0;
	mov.u64 	%rl1411, %rl846;
	@%p162 bra 	BB2_193;
	bra.uni 	BB2_194;

BB2_193:
	shl.b64 	%rl849, %rl846, 1;
	shr.u64 	%rl850, %rl1412, 63;
	or.b64  	%rl1411, %rl849, %rl850;
	mul.lo.s64 	%rl1412, %rl1410, -7917410315110611862;
	add.s32 	%r1159, %r1159, -1;

BB2_194:
	setp.ne.s64 	%p163, %rl1412, 0;
	selp.u64 	%rl851, 1, 0, %p163;
	add.s64 	%rl852, %rl851, %rl1411;
	add.s32 	%r662, %r1159, 1022;
	cvt.u64.u32 	%rl853, %r662;
	shl.b64 	%rl854, %rl853, 52;
	shr.u64 	%rl855, %rl852, 11;
	shr.u64 	%rl856, %rl852, 10;
	and.b64  	%rl857, %rl856, 1;
	add.s64 	%rl858, %rl854, %rl855;
	add.s64 	%rl859, %rl858, %rl857;
	or.b64  	%rl860, %rl859, %rl1406;
	mov.b64 	 %fd2134, %rl860;

BB2_195:
	and.b32  	%r663, %r1160, 1;
	setp.eq.s32 	%p164, %r663, 0;
	mul.rn.f64 	%fd77, %fd2134, %fd2134;
	@%p164 bra 	BB2_197;

	mov.f64 	%fd604, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd606, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd603, %fd604, %fd77, %fd606;
	// inline asm
	mov.f64 	%fd610, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd607, %fd603, %fd77, %fd610;
	// inline asm
	mov.f64 	%fd614, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd611, %fd607, %fd77, %fd614;
	// inline asm
	mov.f64 	%fd618, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd615, %fd611, %fd77, %fd618;
	// inline asm
	mov.f64 	%fd622, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd619, %fd615, %fd77, %fd622;
	// inline asm
	mov.f64 	%fd626, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd623, %fd619, %fd77, %fd626;
	// inline asm
	mov.f64 	%fd630, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd627, %fd623, %fd77, %fd630;
	// inline asm
	mov.f64 	%fd2135, %fd627;
	bra.uni 	BB2_198;

BB2_197:
	mov.f64 	%fd632, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd634, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd631, %fd632, %fd77, %fd634;
	// inline asm
	mov.f64 	%fd638, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd635, %fd631, %fd77, %fd638;
	// inline asm
	mov.f64 	%fd642, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd639, %fd635, %fd77, %fd642;
	// inline asm
	mov.f64 	%fd646, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd643, %fd639, %fd77, %fd646;
	// inline asm
	mov.f64 	%fd650, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd647, %fd643, %fd77, %fd650;
	// inline asm
	mul.rn.f64 	%fd652, %fd647, %fd77;
	// inline asm
	fma.rn.f64 	%fd651, %fd652, %fd2134, %fd2134;
	// inline asm
	mov.f64 	%fd2135, %fd651;

BB2_198:
	and.b32  	%r664, %r1160, 2;
	setp.eq.s32 	%p165, %r664, 0;
	neg.f64 	%fd655, %fd2135;
	selp.f64 	%fd2136, %fd2135, %fd655, %p165;
	bra.uni 	BB2_200;

BB2_199:
	mov.f64 	%fd656, 0d0000000000000000;
	mul.rn.f64 	%fd2136, %fd63, %fd656;

BB2_200:
	neg.f64 	%fd657, %fd2136;
	mov.f64 	%fd2205, %fd2133;
	mov.f64 	%fd2206, %fd657;
	ld.param.u32 	%r1130, [DIT10C2CM_param_4];
	setp.eq.s32 	%p166, %r1130, 0;
	@%p166 bra 	BB2_201;
	bra.uni 	BB2_202;

BB2_201:
	mov.f64 	%fd2205, %fd2133;
	mov.f64 	%fd2206, %fd2136;

BB2_202:
	mul.f64 	%fd659, %fd5, %fd2205;
	neg.f64 	%fd661, %fd6;
	fma.rn.f64 	%fd662, %fd661, %fd2206, %fd659;
	mul.f64 	%fd663, %fd6, %fd2205;
	fma.rn.f64 	%fd664, %fd5, %fd2206, %fd663;
	mul.f64 	%fd665, %fd19, 0d403921FB54442D18;
	div.rn.f64 	%fd84, %fd665, %fd20;
	setp.eq.f64 	%p9, %fd84, 0d7FF0000000000000;
	setp.eq.f64 	%p10, %fd84, 0dFFF0000000000000;
	or.pred  	%p167, %p9, %p10;
	mov.f64 	%fd2173, %fd210;
	mov.f64 	%fd2174, %fd211;
	mov.f64 	%fd2175, %fd360;
	mov.f64 	%fd2176, %fd362;
	mov.f64 	%fd2177, %fd511;
	mov.f64 	%fd2178, %fd513;
	mov.f64 	%fd2179, %fd662;
	mov.f64 	%fd2180, %fd664;
	@%p167 bra 	BB2_225;

	// inline asm
	abs.f64 	%fd666, %fd84;
	// inline asm
	setp.gt.f64 	%p168, %fd666, 0d41E0000000000000;
	@%p168 bra 	BB2_205;

	mov.f64 	%fd681, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd668, %fd84, %fd681;
	// inline asm
	cvt.rni.s32.f64 	%r665, %fd668;
	// inline asm
	cvt.rn.f64.s32 	%fd682, %r665;
	neg.f64 	%fd678, %fd682;
	mov.f64 	%fd671, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd669, %fd678, %fd671, %fd84;
	// inline asm
	mov.f64 	%fd675, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd673, %fd678, %fd675, %fd669;
	// inline asm
	mov.f64 	%fd679, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd677, %fd678, %fd679, %fd673;
	// inline asm
	mov.u32 	%r1163, %r665;
	mov.f64 	%fd2137, %fd677;
	bra.uni 	BB2_221;

BB2_205:
	mov.b64 	 %rl193, %fd84;
	and.b64  	%rl1416, %rl193, -9223372036854775808;
	shr.u64 	%rl195, %rl193, 52;
	and.b64  	%rl861, %rl195, 2047;
	add.s64 	%rl862, %rl861, 4294966272;
	cvt.u32.u64 	%r146, %rl862;
	shl.b64 	%rl863, %rl193, 11;
	or.b64  	%rl196, %rl863, -9223372036854775808;
	shr.u32 	%r669, %r146, 6;
	mov.u32 	%r670, 16;
	sub.s32 	%r147, %r670, %r669;
	mov.u32 	%r671, 15;
	sub.s32 	%r1161, %r671, %r669;
	mov.u32 	%r672, 19;
	sub.s32 	%r149, %r672, %r669;
	mov.u32 	%r667, 18;
	// inline asm
	min.s32 	%r666, %r667, %r149;
	// inline asm
	setp.lt.s32 	%p169, %r1161, %r666;
	@%p169 bra 	BB2_207;

	mov.u64 	%rl1413, 0;
	bra.uni 	BB2_209;

BB2_207:
	mov.u32 	%r673, 1;
	sub.s32 	%r150, %r673, %r147;
	mov.u64 	%rl1413, 0;

BB2_208:
	.pragma "nounroll";
	shl.b32 	%r677, %r1161, 3;
	mov.u32 	%r678, __internal_i2opi_d;
	add.s32 	%r679, %r678, %r677;
	ld.const.u64 	%rl867, [%r679];
	mul.lo.s64 	%rl869, %rl867, %rl196;
	// inline asm
	mul.hi.u64 	%rl866, %rl867, %rl196;
	// inline asm
	mad.lo.s64 	%rl870, %rl867, %rl196, %rl1413;
	setp.lt.u64 	%p170, %rl870, %rl869;
	selp.u64 	%rl871, 1, 0, %p170;
	add.s64 	%rl1413, %rl871, %rl866;
	add.s32 	%r680, %r150, %r1161;
	shl.b32 	%r681, %r680, 3;
	add.s32 	%r683, %r48, %r681;
	st.local.u64 	[%r683], %rl870;
	// inline asm
	min.s32 	%r674, %r667, %r149;
	// inline asm
	add.s32 	%r1161, %r1161, 1;
	setp.lt.s32 	%p171, %r1161, %r674;
	@%p171 bra 	BB2_208;

BB2_209:
	mov.u32 	%r684, 1;
	sub.s32 	%r685, %r684, %r147;
	add.s32 	%r686, %r685, %r1161;
	shl.b32 	%r687, %r686, 3;
	add.s32 	%r689, %r48, %r687;
	st.local.u64 	[%r689], %rl1413;
	ld.local.u64 	%rl1414, [%r48+24];
	ld.local.u64 	%rl1415, [%r48+16];
	and.b32  	%r690, %r146, 63;
	setp.eq.s32 	%p172, %r690, 0;
	@%p172 bra 	BB2_211;

	and.b64  	%rl872, %rl195, 63;
	cvt.u32.u64 	%r691, %rl872;
	shl.b64 	%rl873, %rl1414, %r691;
	neg.s32 	%r692, %r146;
	and.b32  	%r693, %r692, 63;
	shr.u64 	%rl874, %rl1415, %r693;
	or.b64  	%rl1414, %rl874, %rl873;
	shl.b64 	%rl875, %rl1415, %r691;
	ld.local.u64 	%rl876, [%r48+8];
	shr.u64 	%rl877, %rl876, %r693;
	or.b64  	%rl1415, %rl877, %rl875;

BB2_211:
	shr.u64 	%rl878, %rl1414, 62;
	cvt.u32.u64 	%r694, %rl878;
	shr.u64 	%rl879, %rl1415, 62;
	shl.b64 	%rl880, %rl1414, 2;
	or.b64  	%rl1420, %rl879, %rl880;
	shl.b64 	%rl207, %rl1415, 2;
	setp.ne.s64 	%p173, %rl207, 0;
	selp.u64 	%rl881, 1, 0, %p173;
	or.b64  	%rl882, %rl881, %rl1420;
	setp.gt.u64 	%p174, %rl882, -9223372036854775808;
	selp.u32 	%r695, 1, 0, %p174;
	add.s32 	%r696, %r695, %r694;
	neg.s32 	%r697, %r696;
	setp.lt.s64 	%p175, %rl193, 0;
	selp.b32 	%r1163, %r697, %r696, %p175;
	@%p174 bra 	BB2_213;

	mov.u64 	%rl1419, %rl207;
	bra.uni 	BB2_214;

BB2_213:
	not.b64 	%rl883, %rl1420;
	neg.s64 	%rl208, %rl207;
	setp.eq.s64 	%p176, %rl207, 0;
	selp.u64 	%rl884, 1, 0, %p176;
	add.s64 	%rl1420, %rl884, %rl883;
	xor.b64  	%rl1416, %rl1416, -9223372036854775808;
	mov.u64 	%rl1419, %rl208;

BB2_214:
	mov.u64 	%rl1418, %rl1419;
	setp.gt.s64 	%p177, %rl1420, 0;
	@%p177 bra 	BB2_216;

	mov.u32 	%r1162, 0;
	bra.uni 	BB2_218;

BB2_216:
	mov.u32 	%r1162, 0;

BB2_217:
	shr.u64 	%rl885, %rl1418, 63;
	shl.b64 	%rl886, %rl1420, 1;
	or.b64  	%rl1420, %rl885, %rl886;
	shl.b64 	%rl1418, %rl1418, 1;
	add.s32 	%r1162, %r1162, -1;
	setp.gt.s64 	%p178, %rl1420, 0;
	@%p178 bra 	BB2_217;

BB2_218:
	mul.lo.s64 	%rl1422, %rl1420, -3958705157555305931;
	mov.u64 	%rl889, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl887, %rl1420, %rl889;
	// inline asm
	setp.gt.s64 	%p179, %rl887, 0;
	mov.u64 	%rl1421, %rl887;
	@%p179 bra 	BB2_219;
	bra.uni 	BB2_220;

BB2_219:
	shl.b64 	%rl890, %rl887, 1;
	shr.u64 	%rl891, %rl1422, 63;
	or.b64  	%rl1421, %rl890, %rl891;
	mul.lo.s64 	%rl1422, %rl1420, -7917410315110611862;
	add.s32 	%r1162, %r1162, -1;

BB2_220:
	setp.ne.s64 	%p180, %rl1422, 0;
	selp.u64 	%rl892, 1, 0, %p180;
	add.s64 	%rl893, %rl892, %rl1421;
	add.s32 	%r700, %r1162, 1022;
	cvt.u64.u32 	%rl894, %r700;
	shl.b64 	%rl895, %rl894, 52;
	shr.u64 	%rl896, %rl893, 11;
	shr.u64 	%rl897, %rl893, 10;
	and.b64  	%rl898, %rl897, 1;
	add.s64 	%rl899, %rl895, %rl896;
	add.s64 	%rl900, %rl899, %rl898;
	or.b64  	%rl901, %rl900, %rl1416;
	mov.b64 	 %fd2137, %rl901;

BB2_221:
	add.s32 	%r161, %r1163, 1;
	and.b32  	%r701, %r161, 1;
	setp.eq.s32 	%p181, %r701, 0;
	mul.rn.f64 	%fd88, %fd2137, %fd2137;
	@%p181 bra 	BB2_223;

	mov.f64 	%fd684, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd686, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd683, %fd684, %fd88, %fd686;
	// inline asm
	mov.f64 	%fd690, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd687, %fd683, %fd88, %fd690;
	// inline asm
	mov.f64 	%fd694, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd691, %fd687, %fd88, %fd694;
	// inline asm
	mov.f64 	%fd698, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd695, %fd691, %fd88, %fd698;
	// inline asm
	mov.f64 	%fd702, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd699, %fd695, %fd88, %fd702;
	// inline asm
	mov.f64 	%fd706, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd703, %fd699, %fd88, %fd706;
	// inline asm
	mov.f64 	%fd710, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd707, %fd703, %fd88, %fd710;
	// inline asm
	mov.f64 	%fd2138, %fd707;
	bra.uni 	BB2_224;

BB2_223:
	mov.f64 	%fd712, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd714, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd711, %fd712, %fd88, %fd714;
	// inline asm
	mov.f64 	%fd718, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd715, %fd711, %fd88, %fd718;
	// inline asm
	mov.f64 	%fd722, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd719, %fd715, %fd88, %fd722;
	// inline asm
	mov.f64 	%fd726, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd723, %fd719, %fd88, %fd726;
	// inline asm
	mov.f64 	%fd730, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd727, %fd723, %fd88, %fd730;
	// inline asm
	mul.rn.f64 	%fd732, %fd727, %fd88;
	// inline asm
	fma.rn.f64 	%fd731, %fd732, %fd2137, %fd2137;
	// inline asm
	mov.f64 	%fd2138, %fd731;

BB2_224:
	and.b32  	%r702, %r161, 2;
	setp.eq.s32 	%p182, %r702, 0;
	neg.f64 	%fd735, %fd2138;
	selp.f64 	%fd2139, %fd2138, %fd735, %p182;
	bra.uni 	BB2_226;

BB2_225:
	mov.f64 	%fd2139, 0dFFF8000000000000;

BB2_226:
	setp.eq.f64 	%p183, %fd84, 0d0000000000000000;
	or.pred  	%p184, %p10, %p183;
	or.pred  	%p185, %p9, %p184;
	@%p185 bra 	BB2_249;

	// inline asm
	abs.f64 	%fd737, %fd84;
	// inline asm
	setp.gt.f64 	%p186, %fd737, 0d41E0000000000000;
	@%p186 bra 	BB2_229;

	mov.f64 	%fd752, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd739, %fd84, %fd752;
	// inline asm
	cvt.rni.s32.f64 	%r703, %fd739;
	// inline asm
	cvt.rn.f64.s32 	%fd753, %r703;
	neg.f64 	%fd749, %fd753;
	mov.f64 	%fd742, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd740, %fd749, %fd742, %fd84;
	// inline asm
	mov.f64 	%fd746, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd744, %fd749, %fd746, %fd740;
	// inline asm
	mov.f64 	%fd750, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd748, %fd749, %fd750, %fd744;
	// inline asm
	mov.u32 	%r1166, %r703;
	mov.f64 	%fd2140, %fd748;
	bra.uni 	BB2_245;

BB2_229:
	mov.b64 	 %rl225, %fd84;
	and.b64  	%rl1426, %rl225, -9223372036854775808;
	shr.u64 	%rl227, %rl225, 52;
	and.b64  	%rl902, %rl227, 2047;
	add.s64 	%rl903, %rl902, 4294966272;
	cvt.u32.u64 	%r163, %rl903;
	shl.b64 	%rl904, %rl225, 11;
	or.b64  	%rl228, %rl904, -9223372036854775808;
	shr.u32 	%r707, %r163, 6;
	mov.u32 	%r708, 16;
	sub.s32 	%r164, %r708, %r707;
	mov.u32 	%r709, 15;
	sub.s32 	%r1164, %r709, %r707;
	mov.u32 	%r710, 19;
	sub.s32 	%r166, %r710, %r707;
	mov.u32 	%r705, 18;
	// inline asm
	min.s32 	%r704, %r705, %r166;
	// inline asm
	setp.lt.s32 	%p187, %r1164, %r704;
	@%p187 bra 	BB2_231;

	mov.u64 	%rl1423, 0;
	bra.uni 	BB2_233;

BB2_231:
	mov.u32 	%r711, 1;
	sub.s32 	%r167, %r711, %r164;
	mov.u64 	%rl1423, 0;

BB2_232:
	.pragma "nounroll";
	shl.b32 	%r715, %r1164, 3;
	mov.u32 	%r716, __internal_i2opi_d;
	add.s32 	%r717, %r716, %r715;
	ld.const.u64 	%rl908, [%r717];
	mul.lo.s64 	%rl910, %rl908, %rl228;
	// inline asm
	mul.hi.u64 	%rl907, %rl908, %rl228;
	// inline asm
	mad.lo.s64 	%rl911, %rl908, %rl228, %rl1423;
	setp.lt.u64 	%p188, %rl911, %rl910;
	selp.u64 	%rl912, 1, 0, %p188;
	add.s64 	%rl1423, %rl912, %rl907;
	add.s32 	%r718, %r167, %r1164;
	shl.b32 	%r719, %r718, 3;
	add.s32 	%r721, %r48, %r719;
	st.local.u64 	[%r721], %rl911;
	// inline asm
	min.s32 	%r712, %r705, %r166;
	// inline asm
	add.s32 	%r1164, %r1164, 1;
	setp.lt.s32 	%p189, %r1164, %r712;
	@%p189 bra 	BB2_232;

BB2_233:
	mov.u32 	%r722, 1;
	sub.s32 	%r723, %r722, %r164;
	add.s32 	%r724, %r723, %r1164;
	shl.b32 	%r725, %r724, 3;
	add.s32 	%r727, %r48, %r725;
	st.local.u64 	[%r727], %rl1423;
	ld.local.u64 	%rl1424, [%r48+24];
	ld.local.u64 	%rl1425, [%r48+16];
	and.b32  	%r728, %r163, 63;
	setp.eq.s32 	%p190, %r728, 0;
	@%p190 bra 	BB2_235;

	and.b64  	%rl913, %rl227, 63;
	cvt.u32.u64 	%r729, %rl913;
	shl.b64 	%rl914, %rl1424, %r729;
	neg.s32 	%r730, %r163;
	and.b32  	%r731, %r730, 63;
	shr.u64 	%rl915, %rl1425, %r731;
	or.b64  	%rl1424, %rl915, %rl914;
	shl.b64 	%rl916, %rl1425, %r729;
	ld.local.u64 	%rl917, [%r48+8];
	shr.u64 	%rl918, %rl917, %r731;
	or.b64  	%rl1425, %rl918, %rl916;

BB2_235:
	shr.u64 	%rl919, %rl1424, 62;
	cvt.u32.u64 	%r732, %rl919;
	shr.u64 	%rl920, %rl1425, 62;
	shl.b64 	%rl921, %rl1424, 2;
	or.b64  	%rl1430, %rl920, %rl921;
	shl.b64 	%rl239, %rl1425, 2;
	setp.ne.s64 	%p191, %rl239, 0;
	selp.u64 	%rl922, 1, 0, %p191;
	or.b64  	%rl923, %rl922, %rl1430;
	setp.gt.u64 	%p192, %rl923, -9223372036854775808;
	selp.u32 	%r733, 1, 0, %p192;
	add.s32 	%r734, %r733, %r732;
	neg.s32 	%r735, %r734;
	setp.lt.s64 	%p193, %rl225, 0;
	selp.b32 	%r1166, %r735, %r734, %p193;
	@%p192 bra 	BB2_237;

	mov.u64 	%rl1429, %rl239;
	bra.uni 	BB2_238;

BB2_237:
	not.b64 	%rl924, %rl1430;
	neg.s64 	%rl240, %rl239;
	setp.eq.s64 	%p194, %rl239, 0;
	selp.u64 	%rl925, 1, 0, %p194;
	add.s64 	%rl1430, %rl925, %rl924;
	xor.b64  	%rl1426, %rl1426, -9223372036854775808;
	mov.u64 	%rl1429, %rl240;

BB2_238:
	mov.u64 	%rl1428, %rl1429;
	setp.gt.s64 	%p195, %rl1430, 0;
	@%p195 bra 	BB2_240;

	mov.u32 	%r1165, 0;
	bra.uni 	BB2_242;

BB2_240:
	mov.u32 	%r1165, 0;

BB2_241:
	shr.u64 	%rl926, %rl1428, 63;
	shl.b64 	%rl927, %rl1430, 1;
	or.b64  	%rl1430, %rl926, %rl927;
	shl.b64 	%rl1428, %rl1428, 1;
	add.s32 	%r1165, %r1165, -1;
	setp.gt.s64 	%p196, %rl1430, 0;
	@%p196 bra 	BB2_241;

BB2_242:
	mul.lo.s64 	%rl1432, %rl1430, -3958705157555305931;
	mov.u64 	%rl930, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl928, %rl1430, %rl930;
	// inline asm
	setp.gt.s64 	%p197, %rl928, 0;
	mov.u64 	%rl1431, %rl928;
	@%p197 bra 	BB2_243;
	bra.uni 	BB2_244;

BB2_243:
	shl.b64 	%rl931, %rl928, 1;
	shr.u64 	%rl932, %rl1432, 63;
	or.b64  	%rl1431, %rl931, %rl932;
	mul.lo.s64 	%rl1432, %rl1430, -7917410315110611862;
	add.s32 	%r1165, %r1165, -1;

BB2_244:
	setp.ne.s64 	%p198, %rl1432, 0;
	selp.u64 	%rl933, 1, 0, %p198;
	add.s64 	%rl934, %rl933, %rl1431;
	add.s32 	%r738, %r1165, 1022;
	cvt.u64.u32 	%rl935, %r738;
	shl.b64 	%rl936, %rl935, 52;
	shr.u64 	%rl937, %rl934, 11;
	shr.u64 	%rl938, %rl934, 10;
	and.b64  	%rl939, %rl938, 1;
	add.s64 	%rl940, %rl936, %rl937;
	add.s64 	%rl941, %rl940, %rl939;
	or.b64  	%rl942, %rl941, %rl1426;
	mov.b64 	 %fd2140, %rl942;

BB2_245:
	and.b32  	%r739, %r1166, 1;
	setp.eq.s32 	%p199, %r739, 0;
	mul.rn.f64 	%fd98, %fd2140, %fd2140;
	@%p199 bra 	BB2_247;

	mov.f64 	%fd755, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd757, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd754, %fd755, %fd98, %fd757;
	// inline asm
	mov.f64 	%fd761, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd758, %fd754, %fd98, %fd761;
	// inline asm
	mov.f64 	%fd765, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd762, %fd758, %fd98, %fd765;
	// inline asm
	mov.f64 	%fd769, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd766, %fd762, %fd98, %fd769;
	// inline asm
	mov.f64 	%fd773, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd770, %fd766, %fd98, %fd773;
	// inline asm
	mov.f64 	%fd777, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd774, %fd770, %fd98, %fd777;
	// inline asm
	mov.f64 	%fd781, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd778, %fd774, %fd98, %fd781;
	// inline asm
	mov.f64 	%fd2141, %fd778;
	bra.uni 	BB2_248;

BB2_247:
	mov.f64 	%fd783, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd785, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd782, %fd783, %fd98, %fd785;
	// inline asm
	mov.f64 	%fd789, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd786, %fd782, %fd98, %fd789;
	// inline asm
	mov.f64 	%fd793, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd790, %fd786, %fd98, %fd793;
	// inline asm
	mov.f64 	%fd797, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd794, %fd790, %fd98, %fd797;
	// inline asm
	mov.f64 	%fd801, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd798, %fd794, %fd98, %fd801;
	// inline asm
	mul.rn.f64 	%fd803, %fd798, %fd98;
	// inline asm
	fma.rn.f64 	%fd802, %fd803, %fd2140, %fd2140;
	// inline asm
	mov.f64 	%fd2141, %fd802;

BB2_248:
	and.b32  	%r740, %r1166, 2;
	setp.eq.s32 	%p200, %r740, 0;
	neg.f64 	%fd806, %fd2141;
	selp.f64 	%fd2142, %fd2141, %fd806, %p200;
	bra.uni 	BB2_250;

BB2_249:
	mov.f64 	%fd807, 0d0000000000000000;
	mul.rn.f64 	%fd2142, %fd84, %fd807;

BB2_250:
	neg.f64 	%fd808, %fd2142;
	mov.f64 	%fd2203, %fd2139;
	mov.f64 	%fd2204, %fd808;
	ld.param.u32 	%r1129, [DIT10C2CM_param_4];
	setp.eq.s32 	%p201, %r1129, 0;
	@%p201 bra 	BB2_251;
	bra.uni 	BB2_252;

BB2_251:
	mov.f64 	%fd2203, %fd2139;
	mov.f64 	%fd2204, %fd2142;

BB2_252:
	mul.f64 	%fd810, %fd7, %fd2203;
	neg.f64 	%fd812, %fd8;
	fma.rn.f64 	%fd813, %fd812, %fd2204, %fd810;
	mul.f64 	%fd814, %fd8, %fd2203;
	fma.rn.f64 	%fd815, %fd7, %fd2204, %fd814;
	mov.f64 	%fd2181, %fd813;
	mov.f64 	%fd2182, %fd815;
	mul.f64 	%fd816, %fd19, 0d403F6A7A2955385E;
	div.rn.f64 	%fd105, %fd816, %fd20;
	setp.eq.f64 	%p11, %fd105, 0d7FF0000000000000;
	setp.eq.f64 	%p12, %fd105, 0dFFF0000000000000;
	or.pred  	%p202, %p11, %p12;
	@%p202 bra 	BB2_275;

	// inline asm
	abs.f64 	%fd817, %fd105;
	// inline asm
	setp.gt.f64 	%p203, %fd817, 0d41E0000000000000;
	@%p203 bra 	BB2_255;

	mov.f64 	%fd832, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd819, %fd105, %fd832;
	// inline asm
	cvt.rni.s32.f64 	%r741, %fd819;
	// inline asm
	cvt.rn.f64.s32 	%fd833, %r741;
	neg.f64 	%fd829, %fd833;
	mov.f64 	%fd822, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd820, %fd829, %fd822, %fd105;
	// inline asm
	mov.f64 	%fd826, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd824, %fd829, %fd826, %fd820;
	// inline asm
	mov.f64 	%fd830, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd828, %fd829, %fd830, %fd824;
	// inline asm
	mov.u32 	%r1169, %r741;
	mov.f64 	%fd2143, %fd828;
	bra.uni 	BB2_271;

BB2_255:
	mov.b64 	 %rl257, %fd105;
	and.b64  	%rl1436, %rl257, -9223372036854775808;
	shr.u64 	%rl259, %rl257, 52;
	and.b64  	%rl943, %rl259, 2047;
	add.s64 	%rl944, %rl943, 4294966272;
	cvt.u32.u64 	%r179, %rl944;
	shl.b64 	%rl945, %rl257, 11;
	or.b64  	%rl260, %rl945, -9223372036854775808;
	shr.u32 	%r745, %r179, 6;
	mov.u32 	%r746, 16;
	sub.s32 	%r180, %r746, %r745;
	mov.u32 	%r747, 15;
	sub.s32 	%r1167, %r747, %r745;
	mov.u32 	%r748, 19;
	sub.s32 	%r182, %r748, %r745;
	mov.u32 	%r743, 18;
	// inline asm
	min.s32 	%r742, %r743, %r182;
	// inline asm
	setp.lt.s32 	%p204, %r1167, %r742;
	@%p204 bra 	BB2_257;

	mov.u64 	%rl1433, 0;
	bra.uni 	BB2_259;

BB2_257:
	mov.u32 	%r749, 1;
	sub.s32 	%r183, %r749, %r180;
	mov.u64 	%rl1433, 0;

BB2_258:
	.pragma "nounroll";
	shl.b32 	%r753, %r1167, 3;
	mov.u32 	%r754, __internal_i2opi_d;
	add.s32 	%r755, %r754, %r753;
	ld.const.u64 	%rl949, [%r755];
	mul.lo.s64 	%rl951, %rl949, %rl260;
	// inline asm
	mul.hi.u64 	%rl948, %rl949, %rl260;
	// inline asm
	mad.lo.s64 	%rl952, %rl949, %rl260, %rl1433;
	setp.lt.u64 	%p205, %rl952, %rl951;
	selp.u64 	%rl953, 1, 0, %p205;
	add.s64 	%rl1433, %rl953, %rl948;
	add.s32 	%r756, %r183, %r1167;
	shl.b32 	%r757, %r756, 3;
	add.s32 	%r759, %r48, %r757;
	st.local.u64 	[%r759], %rl952;
	// inline asm
	min.s32 	%r750, %r743, %r182;
	// inline asm
	add.s32 	%r1167, %r1167, 1;
	setp.lt.s32 	%p206, %r1167, %r750;
	@%p206 bra 	BB2_258;

BB2_259:
	mov.u32 	%r760, 1;
	sub.s32 	%r761, %r760, %r180;
	add.s32 	%r762, %r761, %r1167;
	shl.b32 	%r763, %r762, 3;
	add.s32 	%r765, %r48, %r763;
	st.local.u64 	[%r765], %rl1433;
	ld.local.u64 	%rl1434, [%r48+24];
	ld.local.u64 	%rl1435, [%r48+16];
	and.b32  	%r766, %r179, 63;
	setp.eq.s32 	%p207, %r766, 0;
	@%p207 bra 	BB2_261;

	and.b64  	%rl954, %rl259, 63;
	cvt.u32.u64 	%r767, %rl954;
	shl.b64 	%rl955, %rl1434, %r767;
	neg.s32 	%r768, %r179;
	and.b32  	%r769, %r768, 63;
	shr.u64 	%rl956, %rl1435, %r769;
	or.b64  	%rl1434, %rl956, %rl955;
	shl.b64 	%rl957, %rl1435, %r767;
	ld.local.u64 	%rl958, [%r48+8];
	shr.u64 	%rl959, %rl958, %r769;
	or.b64  	%rl1435, %rl959, %rl957;

BB2_261:
	shr.u64 	%rl960, %rl1434, 62;
	cvt.u32.u64 	%r770, %rl960;
	shr.u64 	%rl961, %rl1435, 62;
	shl.b64 	%rl962, %rl1434, 2;
	or.b64  	%rl1440, %rl961, %rl962;
	shl.b64 	%rl271, %rl1435, 2;
	setp.ne.s64 	%p208, %rl271, 0;
	selp.u64 	%rl963, 1, 0, %p208;
	or.b64  	%rl964, %rl963, %rl1440;
	setp.gt.u64 	%p209, %rl964, -9223372036854775808;
	selp.u32 	%r771, 1, 0, %p209;
	add.s32 	%r772, %r771, %r770;
	neg.s32 	%r773, %r772;
	setp.lt.s64 	%p210, %rl257, 0;
	selp.b32 	%r1169, %r773, %r772, %p210;
	@%p209 bra 	BB2_263;

	mov.u64 	%rl1439, %rl271;
	bra.uni 	BB2_264;

BB2_263:
	not.b64 	%rl965, %rl1440;
	neg.s64 	%rl272, %rl271;
	setp.eq.s64 	%p211, %rl271, 0;
	selp.u64 	%rl966, 1, 0, %p211;
	add.s64 	%rl1440, %rl966, %rl965;
	xor.b64  	%rl1436, %rl1436, -9223372036854775808;
	mov.u64 	%rl1439, %rl272;

BB2_264:
	mov.u64 	%rl1438, %rl1439;
	setp.gt.s64 	%p212, %rl1440, 0;
	@%p212 bra 	BB2_266;

	mov.u32 	%r1168, 0;
	bra.uni 	BB2_268;

BB2_266:
	mov.u32 	%r1168, 0;

BB2_267:
	shr.u64 	%rl967, %rl1438, 63;
	shl.b64 	%rl968, %rl1440, 1;
	or.b64  	%rl1440, %rl967, %rl968;
	shl.b64 	%rl1438, %rl1438, 1;
	add.s32 	%r1168, %r1168, -1;
	setp.gt.s64 	%p213, %rl1440, 0;
	@%p213 bra 	BB2_267;

BB2_268:
	mul.lo.s64 	%rl1442, %rl1440, -3958705157555305931;
	mov.u64 	%rl971, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl969, %rl1440, %rl971;
	// inline asm
	setp.gt.s64 	%p214, %rl969, 0;
	mov.u64 	%rl1441, %rl969;
	@%p214 bra 	BB2_269;
	bra.uni 	BB2_270;

BB2_269:
	shl.b64 	%rl972, %rl969, 1;
	shr.u64 	%rl973, %rl1442, 63;
	or.b64  	%rl1441, %rl972, %rl973;
	mul.lo.s64 	%rl1442, %rl1440, -7917410315110611862;
	add.s32 	%r1168, %r1168, -1;

BB2_270:
	setp.ne.s64 	%p215, %rl1442, 0;
	selp.u64 	%rl974, 1, 0, %p215;
	add.s64 	%rl975, %rl974, %rl1441;
	add.s32 	%r776, %r1168, 1022;
	cvt.u64.u32 	%rl976, %r776;
	shl.b64 	%rl977, %rl976, 52;
	shr.u64 	%rl978, %rl975, 11;
	shr.u64 	%rl979, %rl975, 10;
	and.b64  	%rl980, %rl979, 1;
	add.s64 	%rl981, %rl977, %rl978;
	add.s64 	%rl982, %rl981, %rl980;
	or.b64  	%rl983, %rl982, %rl1436;
	mov.b64 	 %fd2143, %rl983;

BB2_271:
	add.s32 	%r194, %r1169, 1;
	and.b32  	%r777, %r194, 1;
	setp.eq.s32 	%p216, %r777, 0;
	mul.rn.f64 	%fd109, %fd2143, %fd2143;
	@%p216 bra 	BB2_273;

	mov.f64 	%fd835, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd837, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd834, %fd835, %fd109, %fd837;
	// inline asm
	mov.f64 	%fd841, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd838, %fd834, %fd109, %fd841;
	// inline asm
	mov.f64 	%fd845, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd842, %fd838, %fd109, %fd845;
	// inline asm
	mov.f64 	%fd849, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd846, %fd842, %fd109, %fd849;
	// inline asm
	mov.f64 	%fd853, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd850, %fd846, %fd109, %fd853;
	// inline asm
	mov.f64 	%fd857, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd854, %fd850, %fd109, %fd857;
	// inline asm
	mov.f64 	%fd861, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd858, %fd854, %fd109, %fd861;
	// inline asm
	mov.f64 	%fd2144, %fd858;
	bra.uni 	BB2_274;

BB2_273:
	mov.f64 	%fd863, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd865, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd862, %fd863, %fd109, %fd865;
	// inline asm
	mov.f64 	%fd869, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd866, %fd862, %fd109, %fd869;
	// inline asm
	mov.f64 	%fd873, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd870, %fd866, %fd109, %fd873;
	// inline asm
	mov.f64 	%fd877, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd874, %fd870, %fd109, %fd877;
	// inline asm
	mov.f64 	%fd881, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd878, %fd874, %fd109, %fd881;
	// inline asm
	mul.rn.f64 	%fd883, %fd878, %fd109;
	// inline asm
	fma.rn.f64 	%fd882, %fd883, %fd2143, %fd2143;
	// inline asm
	mov.f64 	%fd2144, %fd882;

BB2_274:
	and.b32  	%r778, %r194, 2;
	setp.eq.s32 	%p217, %r778, 0;
	neg.f64 	%fd886, %fd2144;
	selp.f64 	%fd2145, %fd2144, %fd886, %p217;
	bra.uni 	BB2_276;

BB2_275:
	mov.f64 	%fd2145, 0dFFF8000000000000;

BB2_276:
	setp.eq.f64 	%p218, %fd105, 0d0000000000000000;
	or.pred  	%p219, %p12, %p218;
	or.pred  	%p220, %p11, %p219;
	@%p220 bra 	BB2_299;

	// inline asm
	abs.f64 	%fd888, %fd105;
	// inline asm
	setp.gt.f64 	%p221, %fd888, 0d41E0000000000000;
	@%p221 bra 	BB2_279;

	mov.f64 	%fd903, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd890, %fd105, %fd903;
	// inline asm
	cvt.rni.s32.f64 	%r779, %fd890;
	// inline asm
	cvt.rn.f64.s32 	%fd904, %r779;
	neg.f64 	%fd900, %fd904;
	mov.f64 	%fd893, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd891, %fd900, %fd893, %fd105;
	// inline asm
	mov.f64 	%fd897, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd895, %fd900, %fd897, %fd891;
	// inline asm
	mov.f64 	%fd901, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd899, %fd900, %fd901, %fd895;
	// inline asm
	mov.u32 	%r1172, %r779;
	mov.f64 	%fd2146, %fd899;
	bra.uni 	BB2_295;

BB2_279:
	mov.b64 	 %rl289, %fd105;
	and.b64  	%rl1446, %rl289, -9223372036854775808;
	shr.u64 	%rl291, %rl289, 52;
	and.b64  	%rl984, %rl291, 2047;
	add.s64 	%rl985, %rl984, 4294966272;
	cvt.u32.u64 	%r196, %rl985;
	shl.b64 	%rl986, %rl289, 11;
	or.b64  	%rl292, %rl986, -9223372036854775808;
	shr.u32 	%r783, %r196, 6;
	mov.u32 	%r784, 16;
	sub.s32 	%r197, %r784, %r783;
	mov.u32 	%r785, 15;
	sub.s32 	%r1170, %r785, %r783;
	mov.u32 	%r786, 19;
	sub.s32 	%r199, %r786, %r783;
	mov.u32 	%r781, 18;
	// inline asm
	min.s32 	%r780, %r781, %r199;
	// inline asm
	setp.lt.s32 	%p222, %r1170, %r780;
	@%p222 bra 	BB2_281;

	mov.u64 	%rl1443, 0;
	bra.uni 	BB2_283;

BB2_281:
	mov.u32 	%r787, 1;
	sub.s32 	%r200, %r787, %r197;
	mov.u64 	%rl1443, 0;

BB2_282:
	.pragma "nounroll";
	shl.b32 	%r791, %r1170, 3;
	mov.u32 	%r792, __internal_i2opi_d;
	add.s32 	%r793, %r792, %r791;
	ld.const.u64 	%rl990, [%r793];
	mul.lo.s64 	%rl992, %rl990, %rl292;
	// inline asm
	mul.hi.u64 	%rl989, %rl990, %rl292;
	// inline asm
	mad.lo.s64 	%rl993, %rl990, %rl292, %rl1443;
	setp.lt.u64 	%p223, %rl993, %rl992;
	selp.u64 	%rl994, 1, 0, %p223;
	add.s64 	%rl1443, %rl994, %rl989;
	add.s32 	%r794, %r200, %r1170;
	shl.b32 	%r795, %r794, 3;
	add.s32 	%r797, %r48, %r795;
	st.local.u64 	[%r797], %rl993;
	// inline asm
	min.s32 	%r788, %r781, %r199;
	// inline asm
	add.s32 	%r1170, %r1170, 1;
	setp.lt.s32 	%p224, %r1170, %r788;
	@%p224 bra 	BB2_282;

BB2_283:
	mov.u32 	%r798, 1;
	sub.s32 	%r799, %r798, %r197;
	add.s32 	%r800, %r799, %r1170;
	shl.b32 	%r801, %r800, 3;
	add.s32 	%r803, %r48, %r801;
	st.local.u64 	[%r803], %rl1443;
	ld.local.u64 	%rl1444, [%r48+24];
	ld.local.u64 	%rl1445, [%r48+16];
	and.b32  	%r804, %r196, 63;
	setp.eq.s32 	%p225, %r804, 0;
	@%p225 bra 	BB2_285;

	and.b64  	%rl995, %rl291, 63;
	cvt.u32.u64 	%r805, %rl995;
	shl.b64 	%rl996, %rl1444, %r805;
	neg.s32 	%r806, %r196;
	and.b32  	%r807, %r806, 63;
	shr.u64 	%rl997, %rl1445, %r807;
	or.b64  	%rl1444, %rl997, %rl996;
	shl.b64 	%rl998, %rl1445, %r805;
	ld.local.u64 	%rl999, [%r48+8];
	shr.u64 	%rl1000, %rl999, %r807;
	or.b64  	%rl1445, %rl1000, %rl998;

BB2_285:
	shr.u64 	%rl1001, %rl1444, 62;
	cvt.u32.u64 	%r808, %rl1001;
	shr.u64 	%rl1002, %rl1445, 62;
	shl.b64 	%rl1003, %rl1444, 2;
	or.b64  	%rl1450, %rl1002, %rl1003;
	shl.b64 	%rl303, %rl1445, 2;
	setp.ne.s64 	%p226, %rl303, 0;
	selp.u64 	%rl1004, 1, 0, %p226;
	or.b64  	%rl1005, %rl1004, %rl1450;
	setp.gt.u64 	%p227, %rl1005, -9223372036854775808;
	selp.u32 	%r809, 1, 0, %p227;
	add.s32 	%r810, %r809, %r808;
	neg.s32 	%r811, %r810;
	setp.lt.s64 	%p228, %rl289, 0;
	selp.b32 	%r1172, %r811, %r810, %p228;
	@%p227 bra 	BB2_287;

	mov.u64 	%rl1449, %rl303;
	bra.uni 	BB2_288;

BB2_287:
	not.b64 	%rl1006, %rl1450;
	neg.s64 	%rl304, %rl303;
	setp.eq.s64 	%p229, %rl303, 0;
	selp.u64 	%rl1007, 1, 0, %p229;
	add.s64 	%rl1450, %rl1007, %rl1006;
	xor.b64  	%rl1446, %rl1446, -9223372036854775808;
	mov.u64 	%rl1449, %rl304;

BB2_288:
	mov.u64 	%rl1448, %rl1449;
	setp.gt.s64 	%p230, %rl1450, 0;
	@%p230 bra 	BB2_290;

	mov.u32 	%r1171, 0;
	bra.uni 	BB2_292;

BB2_290:
	mov.u32 	%r1171, 0;

BB2_291:
	shr.u64 	%rl1008, %rl1448, 63;
	shl.b64 	%rl1009, %rl1450, 1;
	or.b64  	%rl1450, %rl1008, %rl1009;
	shl.b64 	%rl1448, %rl1448, 1;
	add.s32 	%r1171, %r1171, -1;
	setp.gt.s64 	%p231, %rl1450, 0;
	@%p231 bra 	BB2_291;

BB2_292:
	mul.lo.s64 	%rl1452, %rl1450, -3958705157555305931;
	mov.u64 	%rl1012, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1010, %rl1450, %rl1012;
	// inline asm
	setp.gt.s64 	%p232, %rl1010, 0;
	mov.u64 	%rl1451, %rl1010;
	@%p232 bra 	BB2_293;
	bra.uni 	BB2_294;

BB2_293:
	shl.b64 	%rl1013, %rl1010, 1;
	shr.u64 	%rl1014, %rl1452, 63;
	or.b64  	%rl1451, %rl1013, %rl1014;
	mul.lo.s64 	%rl1452, %rl1450, -7917410315110611862;
	add.s32 	%r1171, %r1171, -1;

BB2_294:
	setp.ne.s64 	%p233, %rl1452, 0;
	selp.u64 	%rl1015, 1, 0, %p233;
	add.s64 	%rl1016, %rl1015, %rl1451;
	add.s32 	%r814, %r1171, 1022;
	cvt.u64.u32 	%rl1017, %r814;
	shl.b64 	%rl1018, %rl1017, 52;
	shr.u64 	%rl1019, %rl1016, 11;
	shr.u64 	%rl1020, %rl1016, 10;
	and.b64  	%rl1021, %rl1020, 1;
	add.s64 	%rl1022, %rl1018, %rl1019;
	add.s64 	%rl1023, %rl1022, %rl1021;
	or.b64  	%rl1024, %rl1023, %rl1446;
	mov.b64 	 %fd2146, %rl1024;

BB2_295:
	and.b32  	%r815, %r1172, 1;
	setp.eq.s32 	%p234, %r815, 0;
	mul.rn.f64 	%fd119, %fd2146, %fd2146;
	@%p234 bra 	BB2_297;

	mov.f64 	%fd906, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd908, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd905, %fd906, %fd119, %fd908;
	// inline asm
	mov.f64 	%fd912, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd909, %fd905, %fd119, %fd912;
	// inline asm
	mov.f64 	%fd916, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd913, %fd909, %fd119, %fd916;
	// inline asm
	mov.f64 	%fd920, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd917, %fd913, %fd119, %fd920;
	// inline asm
	mov.f64 	%fd924, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd921, %fd917, %fd119, %fd924;
	// inline asm
	mov.f64 	%fd928, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd925, %fd921, %fd119, %fd928;
	// inline asm
	mov.f64 	%fd932, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd929, %fd925, %fd119, %fd932;
	// inline asm
	mov.f64 	%fd2147, %fd929;
	bra.uni 	BB2_298;

BB2_297:
	mov.f64 	%fd934, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd936, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd933, %fd934, %fd119, %fd936;
	// inline asm
	mov.f64 	%fd940, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd937, %fd933, %fd119, %fd940;
	// inline asm
	mov.f64 	%fd944, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd941, %fd937, %fd119, %fd944;
	// inline asm
	mov.f64 	%fd948, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd945, %fd941, %fd119, %fd948;
	// inline asm
	mov.f64 	%fd952, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd949, %fd945, %fd119, %fd952;
	// inline asm
	mul.rn.f64 	%fd954, %fd949, %fd119;
	// inline asm
	fma.rn.f64 	%fd953, %fd954, %fd2146, %fd2146;
	// inline asm
	mov.f64 	%fd2147, %fd953;

BB2_298:
	and.b32  	%r816, %r1172, 2;
	setp.eq.s32 	%p235, %r816, 0;
	neg.f64 	%fd957, %fd2147;
	selp.f64 	%fd2148, %fd2147, %fd957, %p235;
	bra.uni 	BB2_300;

BB2_299:
	mov.f64 	%fd958, 0d0000000000000000;
	mul.rn.f64 	%fd2148, %fd105, %fd958;

BB2_300:
	neg.f64 	%fd959, %fd2148;
	mov.f64 	%fd2201, %fd2145;
	mov.f64 	%fd2202, %fd959;
	ld.param.u32 	%r1128, [DIT10C2CM_param_4];
	setp.eq.s32 	%p236, %r1128, 0;
	@%p236 bra 	BB2_301;
	bra.uni 	BB2_302;

BB2_301:
	mov.f64 	%fd2201, %fd2145;
	mov.f64 	%fd2202, %fd2148;

BB2_302:
	mul.f64 	%fd961, %fd9, %fd2201;
	neg.f64 	%fd963, %fd10;
	fma.rn.f64 	%fd964, %fd963, %fd2202, %fd961;
	mul.f64 	%fd965, %fd10, %fd2201;
	fma.rn.f64 	%fd966, %fd9, %fd2202, %fd965;
	mov.f64 	%fd2183, %fd964;
	mov.f64 	%fd2184, %fd966;
	mul.f64 	%fd967, %fd19, 0d4042D97C7F3321D2;
	div.rn.f64 	%fd126, %fd967, %fd20;
	setp.eq.f64 	%p13, %fd126, 0d7FF0000000000000;
	setp.eq.f64 	%p14, %fd126, 0dFFF0000000000000;
	or.pred  	%p237, %p13, %p14;
	@%p237 bra 	BB2_325;

	// inline asm
	abs.f64 	%fd968, %fd126;
	// inline asm
	setp.gt.f64 	%p238, %fd968, 0d41E0000000000000;
	@%p238 bra 	BB2_305;

	mov.f64 	%fd983, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd970, %fd126, %fd983;
	// inline asm
	cvt.rni.s32.f64 	%r817, %fd970;
	// inline asm
	cvt.rn.f64.s32 	%fd984, %r817;
	neg.f64 	%fd980, %fd984;
	mov.f64 	%fd973, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd971, %fd980, %fd973, %fd126;
	// inline asm
	mov.f64 	%fd977, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd975, %fd980, %fd977, %fd971;
	// inline asm
	mov.f64 	%fd981, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd979, %fd980, %fd981, %fd975;
	// inline asm
	mov.u32 	%r1175, %r817;
	mov.f64 	%fd2149, %fd979;
	bra.uni 	BB2_321;

BB2_305:
	mov.b64 	 %rl321, %fd126;
	and.b64  	%rl1456, %rl321, -9223372036854775808;
	shr.u64 	%rl323, %rl321, 52;
	and.b64  	%rl1025, %rl323, 2047;
	add.s64 	%rl1026, %rl1025, 4294966272;
	cvt.u32.u64 	%r212, %rl1026;
	shl.b64 	%rl1027, %rl321, 11;
	or.b64  	%rl324, %rl1027, -9223372036854775808;
	shr.u32 	%r821, %r212, 6;
	mov.u32 	%r822, 16;
	sub.s32 	%r213, %r822, %r821;
	mov.u32 	%r823, 15;
	sub.s32 	%r1173, %r823, %r821;
	mov.u32 	%r824, 19;
	sub.s32 	%r215, %r824, %r821;
	mov.u32 	%r819, 18;
	// inline asm
	min.s32 	%r818, %r819, %r215;
	// inline asm
	setp.lt.s32 	%p239, %r1173, %r818;
	@%p239 bra 	BB2_307;

	mov.u64 	%rl1453, 0;
	bra.uni 	BB2_309;

BB2_307:
	mov.u32 	%r825, 1;
	sub.s32 	%r216, %r825, %r213;
	mov.u64 	%rl1453, 0;

BB2_308:
	.pragma "nounroll";
	shl.b32 	%r829, %r1173, 3;
	mov.u32 	%r830, __internal_i2opi_d;
	add.s32 	%r831, %r830, %r829;
	ld.const.u64 	%rl1031, [%r831];
	mul.lo.s64 	%rl1033, %rl1031, %rl324;
	// inline asm
	mul.hi.u64 	%rl1030, %rl1031, %rl324;
	// inline asm
	mad.lo.s64 	%rl1034, %rl1031, %rl324, %rl1453;
	setp.lt.u64 	%p240, %rl1034, %rl1033;
	selp.u64 	%rl1035, 1, 0, %p240;
	add.s64 	%rl1453, %rl1035, %rl1030;
	add.s32 	%r832, %r216, %r1173;
	shl.b32 	%r833, %r832, 3;
	add.s32 	%r835, %r48, %r833;
	st.local.u64 	[%r835], %rl1034;
	// inline asm
	min.s32 	%r826, %r819, %r215;
	// inline asm
	add.s32 	%r1173, %r1173, 1;
	setp.lt.s32 	%p241, %r1173, %r826;
	@%p241 bra 	BB2_308;

BB2_309:
	mov.u32 	%r836, 1;
	sub.s32 	%r837, %r836, %r213;
	add.s32 	%r838, %r837, %r1173;
	shl.b32 	%r839, %r838, 3;
	add.s32 	%r841, %r48, %r839;
	st.local.u64 	[%r841], %rl1453;
	ld.local.u64 	%rl1454, [%r48+24];
	ld.local.u64 	%rl1455, [%r48+16];
	and.b32  	%r842, %r212, 63;
	setp.eq.s32 	%p242, %r842, 0;
	@%p242 bra 	BB2_311;

	and.b64  	%rl1036, %rl323, 63;
	cvt.u32.u64 	%r843, %rl1036;
	shl.b64 	%rl1037, %rl1454, %r843;
	neg.s32 	%r844, %r212;
	and.b32  	%r845, %r844, 63;
	shr.u64 	%rl1038, %rl1455, %r845;
	or.b64  	%rl1454, %rl1038, %rl1037;
	shl.b64 	%rl1039, %rl1455, %r843;
	ld.local.u64 	%rl1040, [%r48+8];
	shr.u64 	%rl1041, %rl1040, %r845;
	or.b64  	%rl1455, %rl1041, %rl1039;

BB2_311:
	shr.u64 	%rl1042, %rl1454, 62;
	cvt.u32.u64 	%r846, %rl1042;
	shr.u64 	%rl1043, %rl1455, 62;
	shl.b64 	%rl1044, %rl1454, 2;
	or.b64  	%rl1460, %rl1043, %rl1044;
	shl.b64 	%rl335, %rl1455, 2;
	setp.ne.s64 	%p243, %rl335, 0;
	selp.u64 	%rl1045, 1, 0, %p243;
	or.b64  	%rl1046, %rl1045, %rl1460;
	setp.gt.u64 	%p244, %rl1046, -9223372036854775808;
	selp.u32 	%r847, 1, 0, %p244;
	add.s32 	%r848, %r847, %r846;
	neg.s32 	%r849, %r848;
	setp.lt.s64 	%p245, %rl321, 0;
	selp.b32 	%r1175, %r849, %r848, %p245;
	@%p244 bra 	BB2_313;

	mov.u64 	%rl1459, %rl335;
	bra.uni 	BB2_314;

BB2_313:
	not.b64 	%rl1047, %rl1460;
	neg.s64 	%rl336, %rl335;
	setp.eq.s64 	%p246, %rl335, 0;
	selp.u64 	%rl1048, 1, 0, %p246;
	add.s64 	%rl1460, %rl1048, %rl1047;
	xor.b64  	%rl1456, %rl1456, -9223372036854775808;
	mov.u64 	%rl1459, %rl336;

BB2_314:
	mov.u64 	%rl1458, %rl1459;
	setp.gt.s64 	%p247, %rl1460, 0;
	@%p247 bra 	BB2_316;

	mov.u32 	%r1174, 0;
	bra.uni 	BB2_318;

BB2_316:
	mov.u32 	%r1174, 0;

BB2_317:
	shr.u64 	%rl1049, %rl1458, 63;
	shl.b64 	%rl1050, %rl1460, 1;
	or.b64  	%rl1460, %rl1049, %rl1050;
	shl.b64 	%rl1458, %rl1458, 1;
	add.s32 	%r1174, %r1174, -1;
	setp.gt.s64 	%p248, %rl1460, 0;
	@%p248 bra 	BB2_317;

BB2_318:
	mul.lo.s64 	%rl1462, %rl1460, -3958705157555305931;
	mov.u64 	%rl1053, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1051, %rl1460, %rl1053;
	// inline asm
	setp.gt.s64 	%p249, %rl1051, 0;
	mov.u64 	%rl1461, %rl1051;
	@%p249 bra 	BB2_319;
	bra.uni 	BB2_320;

BB2_319:
	shl.b64 	%rl1054, %rl1051, 1;
	shr.u64 	%rl1055, %rl1462, 63;
	or.b64  	%rl1461, %rl1054, %rl1055;
	mul.lo.s64 	%rl1462, %rl1460, -7917410315110611862;
	add.s32 	%r1174, %r1174, -1;

BB2_320:
	setp.ne.s64 	%p250, %rl1462, 0;
	selp.u64 	%rl1056, 1, 0, %p250;
	add.s64 	%rl1057, %rl1056, %rl1461;
	add.s32 	%r852, %r1174, 1022;
	cvt.u64.u32 	%rl1058, %r852;
	shl.b64 	%rl1059, %rl1058, 52;
	shr.u64 	%rl1060, %rl1057, 11;
	shr.u64 	%rl1061, %rl1057, 10;
	and.b64  	%rl1062, %rl1061, 1;
	add.s64 	%rl1063, %rl1059, %rl1060;
	add.s64 	%rl1064, %rl1063, %rl1062;
	or.b64  	%rl1065, %rl1064, %rl1456;
	mov.b64 	 %fd2149, %rl1065;

BB2_321:
	add.s32 	%r227, %r1175, 1;
	and.b32  	%r853, %r227, 1;
	setp.eq.s32 	%p251, %r853, 0;
	mul.rn.f64 	%fd130, %fd2149, %fd2149;
	@%p251 bra 	BB2_323;

	mov.f64 	%fd986, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd988, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd985, %fd986, %fd130, %fd988;
	// inline asm
	mov.f64 	%fd992, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd989, %fd985, %fd130, %fd992;
	// inline asm
	mov.f64 	%fd996, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd993, %fd989, %fd130, %fd996;
	// inline asm
	mov.f64 	%fd1000, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd997, %fd993, %fd130, %fd1000;
	// inline asm
	mov.f64 	%fd1004, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1001, %fd997, %fd130, %fd1004;
	// inline asm
	mov.f64 	%fd1008, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1005, %fd1001, %fd130, %fd1008;
	// inline asm
	mov.f64 	%fd1012, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1009, %fd1005, %fd130, %fd1012;
	// inline asm
	mov.f64 	%fd2150, %fd1009;
	bra.uni 	BB2_324;

BB2_323:
	mov.f64 	%fd1014, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1016, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1013, %fd1014, %fd130, %fd1016;
	// inline asm
	mov.f64 	%fd1020, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1017, %fd1013, %fd130, %fd1020;
	// inline asm
	mov.f64 	%fd1024, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1021, %fd1017, %fd130, %fd1024;
	// inline asm
	mov.f64 	%fd1028, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1025, %fd1021, %fd130, %fd1028;
	// inline asm
	mov.f64 	%fd1032, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1029, %fd1025, %fd130, %fd1032;
	// inline asm
	mul.rn.f64 	%fd1034, %fd1029, %fd130;
	// inline asm
	fma.rn.f64 	%fd1033, %fd1034, %fd2149, %fd2149;
	// inline asm
	mov.f64 	%fd2150, %fd1033;

BB2_324:
	and.b32  	%r854, %r227, 2;
	setp.eq.s32 	%p252, %r854, 0;
	neg.f64 	%fd1037, %fd2150;
	selp.f64 	%fd2151, %fd2150, %fd1037, %p252;
	bra.uni 	BB2_326;

BB2_325:
	mov.f64 	%fd2151, 0dFFF8000000000000;

BB2_326:
	setp.eq.f64 	%p253, %fd126, 0d0000000000000000;
	or.pred  	%p254, %p14, %p253;
	or.pred  	%p255, %p13, %p254;
	@%p255 bra 	BB2_349;

	// inline asm
	abs.f64 	%fd1039, %fd126;
	// inline asm
	setp.gt.f64 	%p256, %fd1039, 0d41E0000000000000;
	@%p256 bra 	BB2_329;

	mov.f64 	%fd1054, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1041, %fd126, %fd1054;
	// inline asm
	cvt.rni.s32.f64 	%r855, %fd1041;
	// inline asm
	cvt.rn.f64.s32 	%fd1055, %r855;
	neg.f64 	%fd1051, %fd1055;
	mov.f64 	%fd1044, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1042, %fd1051, %fd1044, %fd126;
	// inline asm
	mov.f64 	%fd1048, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1046, %fd1051, %fd1048, %fd1042;
	// inline asm
	mov.f64 	%fd1052, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1050, %fd1051, %fd1052, %fd1046;
	// inline asm
	mov.u32 	%r1178, %r855;
	mov.f64 	%fd2152, %fd1050;
	bra.uni 	BB2_345;

BB2_329:
	mov.b64 	 %rl353, %fd126;
	and.b64  	%rl1466, %rl353, -9223372036854775808;
	shr.u64 	%rl355, %rl353, 52;
	and.b64  	%rl1066, %rl355, 2047;
	add.s64 	%rl1067, %rl1066, 4294966272;
	cvt.u32.u64 	%r229, %rl1067;
	shl.b64 	%rl1068, %rl353, 11;
	or.b64  	%rl356, %rl1068, -9223372036854775808;
	shr.u32 	%r859, %r229, 6;
	mov.u32 	%r860, 16;
	sub.s32 	%r230, %r860, %r859;
	mov.u32 	%r861, 15;
	sub.s32 	%r1176, %r861, %r859;
	mov.u32 	%r862, 19;
	sub.s32 	%r232, %r862, %r859;
	mov.u32 	%r857, 18;
	// inline asm
	min.s32 	%r856, %r857, %r232;
	// inline asm
	setp.lt.s32 	%p257, %r1176, %r856;
	@%p257 bra 	BB2_331;

	mov.u64 	%rl1463, 0;
	bra.uni 	BB2_333;

BB2_331:
	mov.u32 	%r863, 1;
	sub.s32 	%r233, %r863, %r230;
	mov.u64 	%rl1463, 0;

BB2_332:
	.pragma "nounroll";
	shl.b32 	%r867, %r1176, 3;
	mov.u32 	%r868, __internal_i2opi_d;
	add.s32 	%r869, %r868, %r867;
	ld.const.u64 	%rl1072, [%r869];
	mul.lo.s64 	%rl1074, %rl1072, %rl356;
	// inline asm
	mul.hi.u64 	%rl1071, %rl1072, %rl356;
	// inline asm
	mad.lo.s64 	%rl1075, %rl1072, %rl356, %rl1463;
	setp.lt.u64 	%p258, %rl1075, %rl1074;
	selp.u64 	%rl1076, 1, 0, %p258;
	add.s64 	%rl1463, %rl1076, %rl1071;
	add.s32 	%r870, %r233, %r1176;
	shl.b32 	%r871, %r870, 3;
	add.s32 	%r873, %r48, %r871;
	st.local.u64 	[%r873], %rl1075;
	// inline asm
	min.s32 	%r864, %r857, %r232;
	// inline asm
	add.s32 	%r1176, %r1176, 1;
	setp.lt.s32 	%p259, %r1176, %r864;
	@%p259 bra 	BB2_332;

BB2_333:
	mov.u32 	%r874, 1;
	sub.s32 	%r875, %r874, %r230;
	add.s32 	%r876, %r875, %r1176;
	shl.b32 	%r877, %r876, 3;
	add.s32 	%r879, %r48, %r877;
	st.local.u64 	[%r879], %rl1463;
	ld.local.u64 	%rl1464, [%r48+24];
	ld.local.u64 	%rl1465, [%r48+16];
	and.b32  	%r880, %r229, 63;
	setp.eq.s32 	%p260, %r880, 0;
	@%p260 bra 	BB2_335;

	and.b64  	%rl1077, %rl355, 63;
	cvt.u32.u64 	%r881, %rl1077;
	shl.b64 	%rl1078, %rl1464, %r881;
	neg.s32 	%r882, %r229;
	and.b32  	%r883, %r882, 63;
	shr.u64 	%rl1079, %rl1465, %r883;
	or.b64  	%rl1464, %rl1079, %rl1078;
	shl.b64 	%rl1080, %rl1465, %r881;
	ld.local.u64 	%rl1081, [%r48+8];
	shr.u64 	%rl1082, %rl1081, %r883;
	or.b64  	%rl1465, %rl1082, %rl1080;

BB2_335:
	shr.u64 	%rl1083, %rl1464, 62;
	cvt.u32.u64 	%r884, %rl1083;
	shr.u64 	%rl1084, %rl1465, 62;
	shl.b64 	%rl1085, %rl1464, 2;
	or.b64  	%rl1470, %rl1084, %rl1085;
	shl.b64 	%rl367, %rl1465, 2;
	setp.ne.s64 	%p261, %rl367, 0;
	selp.u64 	%rl1086, 1, 0, %p261;
	or.b64  	%rl1087, %rl1086, %rl1470;
	setp.gt.u64 	%p262, %rl1087, -9223372036854775808;
	selp.u32 	%r885, 1, 0, %p262;
	add.s32 	%r886, %r885, %r884;
	neg.s32 	%r887, %r886;
	setp.lt.s64 	%p263, %rl353, 0;
	selp.b32 	%r1178, %r887, %r886, %p263;
	@%p262 bra 	BB2_337;

	mov.u64 	%rl1469, %rl367;
	bra.uni 	BB2_338;

BB2_337:
	not.b64 	%rl1088, %rl1470;
	neg.s64 	%rl368, %rl367;
	setp.eq.s64 	%p264, %rl367, 0;
	selp.u64 	%rl1089, 1, 0, %p264;
	add.s64 	%rl1470, %rl1089, %rl1088;
	xor.b64  	%rl1466, %rl1466, -9223372036854775808;
	mov.u64 	%rl1469, %rl368;

BB2_338:
	mov.u64 	%rl1468, %rl1469;
	setp.gt.s64 	%p265, %rl1470, 0;
	@%p265 bra 	BB2_340;

	mov.u32 	%r1177, 0;
	bra.uni 	BB2_342;

BB2_340:
	mov.u32 	%r1177, 0;

BB2_341:
	shr.u64 	%rl1090, %rl1468, 63;
	shl.b64 	%rl1091, %rl1470, 1;
	or.b64  	%rl1470, %rl1090, %rl1091;
	shl.b64 	%rl1468, %rl1468, 1;
	add.s32 	%r1177, %r1177, -1;
	setp.gt.s64 	%p266, %rl1470, 0;
	@%p266 bra 	BB2_341;

BB2_342:
	mul.lo.s64 	%rl1472, %rl1470, -3958705157555305931;
	mov.u64 	%rl1094, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1092, %rl1470, %rl1094;
	// inline asm
	setp.gt.s64 	%p267, %rl1092, 0;
	mov.u64 	%rl1471, %rl1092;
	@%p267 bra 	BB2_343;
	bra.uni 	BB2_344;

BB2_343:
	shl.b64 	%rl1095, %rl1092, 1;
	shr.u64 	%rl1096, %rl1472, 63;
	or.b64  	%rl1471, %rl1095, %rl1096;
	mul.lo.s64 	%rl1472, %rl1470, -7917410315110611862;
	add.s32 	%r1177, %r1177, -1;

BB2_344:
	setp.ne.s64 	%p268, %rl1472, 0;
	selp.u64 	%rl1097, 1, 0, %p268;
	add.s64 	%rl1098, %rl1097, %rl1471;
	add.s32 	%r890, %r1177, 1022;
	cvt.u64.u32 	%rl1099, %r890;
	shl.b64 	%rl1100, %rl1099, 52;
	shr.u64 	%rl1101, %rl1098, 11;
	shr.u64 	%rl1102, %rl1098, 10;
	and.b64  	%rl1103, %rl1102, 1;
	add.s64 	%rl1104, %rl1100, %rl1101;
	add.s64 	%rl1105, %rl1104, %rl1103;
	or.b64  	%rl1106, %rl1105, %rl1466;
	mov.b64 	 %fd2152, %rl1106;

BB2_345:
	and.b32  	%r891, %r1178, 1;
	setp.eq.s32 	%p269, %r891, 0;
	mul.rn.f64 	%fd140, %fd2152, %fd2152;
	@%p269 bra 	BB2_347;

	mov.f64 	%fd1057, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1059, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1056, %fd1057, %fd140, %fd1059;
	// inline asm
	mov.f64 	%fd1063, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1060, %fd1056, %fd140, %fd1063;
	// inline asm
	mov.f64 	%fd1067, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1064, %fd1060, %fd140, %fd1067;
	// inline asm
	mov.f64 	%fd1071, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1068, %fd1064, %fd140, %fd1071;
	// inline asm
	mov.f64 	%fd1075, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1072, %fd1068, %fd140, %fd1075;
	// inline asm
	mov.f64 	%fd1079, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1076, %fd1072, %fd140, %fd1079;
	// inline asm
	mov.f64 	%fd1083, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1080, %fd1076, %fd140, %fd1083;
	// inline asm
	mov.f64 	%fd2153, %fd1080;
	bra.uni 	BB2_348;

BB2_347:
	mov.f64 	%fd1085, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1087, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1084, %fd1085, %fd140, %fd1087;
	// inline asm
	mov.f64 	%fd1091, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1088, %fd1084, %fd140, %fd1091;
	// inline asm
	mov.f64 	%fd1095, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1092, %fd1088, %fd140, %fd1095;
	// inline asm
	mov.f64 	%fd1099, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1096, %fd1092, %fd140, %fd1099;
	// inline asm
	mov.f64 	%fd1103, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1100, %fd1096, %fd140, %fd1103;
	// inline asm
	mul.rn.f64 	%fd1105, %fd1100, %fd140;
	// inline asm
	fma.rn.f64 	%fd1104, %fd1105, %fd2152, %fd2152;
	// inline asm
	mov.f64 	%fd2153, %fd1104;

BB2_348:
	and.b32  	%r892, %r1178, 2;
	setp.eq.s32 	%p270, %r892, 0;
	neg.f64 	%fd1108, %fd2153;
	selp.f64 	%fd2154, %fd2153, %fd1108, %p270;
	bra.uni 	BB2_350;

BB2_349:
	mov.f64 	%fd1109, 0d0000000000000000;
	mul.rn.f64 	%fd2154, %fd126, %fd1109;

BB2_350:
	neg.f64 	%fd1110, %fd2154;
	mov.f64 	%fd2199, %fd2151;
	mov.f64 	%fd2200, %fd1110;
	ld.param.u32 	%r1127, [DIT10C2CM_param_4];
	setp.eq.s32 	%p271, %r1127, 0;
	@%p271 bra 	BB2_351;
	bra.uni 	BB2_352;

BB2_351:
	mov.f64 	%fd2199, %fd2151;
	mov.f64 	%fd2200, %fd2154;

BB2_352:
	mul.f64 	%fd1112, %fd11, %fd2199;
	neg.f64 	%fd1114, %fd12;
	fma.rn.f64 	%fd1115, %fd1114, %fd2200, %fd1112;
	mul.f64 	%fd1116, %fd12, %fd2199;
	fma.rn.f64 	%fd1117, %fd11, %fd2200, %fd1116;
	mov.f64 	%fd2185, %fd1115;
	mov.f64 	%fd2186, %fd1117;
	mul.f64 	%fd1118, %fd19, 0d4045FDBBE9BBA775;
	div.rn.f64 	%fd147, %fd1118, %fd20;
	setp.eq.f64 	%p15, %fd147, 0d7FF0000000000000;
	setp.eq.f64 	%p16, %fd147, 0dFFF0000000000000;
	or.pred  	%p272, %p15, %p16;
	@%p272 bra 	BB2_375;

	// inline asm
	abs.f64 	%fd1119, %fd147;
	// inline asm
	setp.gt.f64 	%p273, %fd1119, 0d41E0000000000000;
	@%p273 bra 	BB2_355;

	mov.f64 	%fd1134, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1121, %fd147, %fd1134;
	// inline asm
	cvt.rni.s32.f64 	%r893, %fd1121;
	// inline asm
	cvt.rn.f64.s32 	%fd1135, %r893;
	neg.f64 	%fd1131, %fd1135;
	mov.f64 	%fd1124, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1122, %fd1131, %fd1124, %fd147;
	// inline asm
	mov.f64 	%fd1128, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1126, %fd1131, %fd1128, %fd1122;
	// inline asm
	mov.f64 	%fd1132, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1130, %fd1131, %fd1132, %fd1126;
	// inline asm
	mov.u32 	%r1181, %r893;
	mov.f64 	%fd2155, %fd1130;
	bra.uni 	BB2_371;

BB2_355:
	mov.b64 	 %rl385, %fd147;
	and.b64  	%rl1476, %rl385, -9223372036854775808;
	shr.u64 	%rl387, %rl385, 52;
	and.b64  	%rl1107, %rl387, 2047;
	add.s64 	%rl1108, %rl1107, 4294966272;
	cvt.u32.u64 	%r245, %rl1108;
	shl.b64 	%rl1109, %rl385, 11;
	or.b64  	%rl388, %rl1109, -9223372036854775808;
	shr.u32 	%r897, %r245, 6;
	mov.u32 	%r898, 16;
	sub.s32 	%r246, %r898, %r897;
	mov.u32 	%r899, 15;
	sub.s32 	%r1179, %r899, %r897;
	mov.u32 	%r900, 19;
	sub.s32 	%r248, %r900, %r897;
	mov.u32 	%r895, 18;
	// inline asm
	min.s32 	%r894, %r895, %r248;
	// inline asm
	setp.lt.s32 	%p274, %r1179, %r894;
	@%p274 bra 	BB2_357;

	mov.u64 	%rl1473, 0;
	bra.uni 	BB2_359;

BB2_357:
	mov.u32 	%r901, 1;
	sub.s32 	%r249, %r901, %r246;
	mov.u64 	%rl1473, 0;

BB2_358:
	.pragma "nounroll";
	shl.b32 	%r905, %r1179, 3;
	mov.u32 	%r906, __internal_i2opi_d;
	add.s32 	%r907, %r906, %r905;
	ld.const.u64 	%rl1113, [%r907];
	mul.lo.s64 	%rl1115, %rl1113, %rl388;
	// inline asm
	mul.hi.u64 	%rl1112, %rl1113, %rl388;
	// inline asm
	mad.lo.s64 	%rl1116, %rl1113, %rl388, %rl1473;
	setp.lt.u64 	%p275, %rl1116, %rl1115;
	selp.u64 	%rl1117, 1, 0, %p275;
	add.s64 	%rl1473, %rl1117, %rl1112;
	add.s32 	%r908, %r249, %r1179;
	shl.b32 	%r909, %r908, 3;
	add.s32 	%r911, %r48, %r909;
	st.local.u64 	[%r911], %rl1116;
	// inline asm
	min.s32 	%r902, %r895, %r248;
	// inline asm
	add.s32 	%r1179, %r1179, 1;
	setp.lt.s32 	%p276, %r1179, %r902;
	@%p276 bra 	BB2_358;

BB2_359:
	mov.u32 	%r912, 1;
	sub.s32 	%r913, %r912, %r246;
	add.s32 	%r914, %r913, %r1179;
	shl.b32 	%r915, %r914, 3;
	add.s32 	%r917, %r48, %r915;
	st.local.u64 	[%r917], %rl1473;
	ld.local.u64 	%rl1474, [%r48+24];
	ld.local.u64 	%rl1475, [%r48+16];
	and.b32  	%r918, %r245, 63;
	setp.eq.s32 	%p277, %r918, 0;
	@%p277 bra 	BB2_361;

	and.b64  	%rl1118, %rl387, 63;
	cvt.u32.u64 	%r919, %rl1118;
	shl.b64 	%rl1119, %rl1474, %r919;
	neg.s32 	%r920, %r245;
	and.b32  	%r921, %r920, 63;
	shr.u64 	%rl1120, %rl1475, %r921;
	or.b64  	%rl1474, %rl1120, %rl1119;
	shl.b64 	%rl1121, %rl1475, %r919;
	ld.local.u64 	%rl1122, [%r48+8];
	shr.u64 	%rl1123, %rl1122, %r921;
	or.b64  	%rl1475, %rl1123, %rl1121;

BB2_361:
	shr.u64 	%rl1124, %rl1474, 62;
	cvt.u32.u64 	%r922, %rl1124;
	shr.u64 	%rl1125, %rl1475, 62;
	shl.b64 	%rl1126, %rl1474, 2;
	or.b64  	%rl1480, %rl1125, %rl1126;
	shl.b64 	%rl399, %rl1475, 2;
	setp.ne.s64 	%p278, %rl399, 0;
	selp.u64 	%rl1127, 1, 0, %p278;
	or.b64  	%rl1128, %rl1127, %rl1480;
	setp.gt.u64 	%p279, %rl1128, -9223372036854775808;
	selp.u32 	%r923, 1, 0, %p279;
	add.s32 	%r924, %r923, %r922;
	neg.s32 	%r925, %r924;
	setp.lt.s64 	%p280, %rl385, 0;
	selp.b32 	%r1181, %r925, %r924, %p280;
	@%p279 bra 	BB2_363;

	mov.u64 	%rl1479, %rl399;
	bra.uni 	BB2_364;

BB2_363:
	not.b64 	%rl1129, %rl1480;
	neg.s64 	%rl400, %rl399;
	setp.eq.s64 	%p281, %rl399, 0;
	selp.u64 	%rl1130, 1, 0, %p281;
	add.s64 	%rl1480, %rl1130, %rl1129;
	xor.b64  	%rl1476, %rl1476, -9223372036854775808;
	mov.u64 	%rl1479, %rl400;

BB2_364:
	mov.u64 	%rl1478, %rl1479;
	setp.gt.s64 	%p282, %rl1480, 0;
	@%p282 bra 	BB2_366;

	mov.u32 	%r1180, 0;
	bra.uni 	BB2_368;

BB2_366:
	mov.u32 	%r1180, 0;

BB2_367:
	shr.u64 	%rl1131, %rl1478, 63;
	shl.b64 	%rl1132, %rl1480, 1;
	or.b64  	%rl1480, %rl1131, %rl1132;
	shl.b64 	%rl1478, %rl1478, 1;
	add.s32 	%r1180, %r1180, -1;
	setp.gt.s64 	%p283, %rl1480, 0;
	@%p283 bra 	BB2_367;

BB2_368:
	mul.lo.s64 	%rl1482, %rl1480, -3958705157555305931;
	mov.u64 	%rl1135, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1133, %rl1480, %rl1135;
	// inline asm
	setp.gt.s64 	%p284, %rl1133, 0;
	mov.u64 	%rl1481, %rl1133;
	@%p284 bra 	BB2_369;
	bra.uni 	BB2_370;

BB2_369:
	shl.b64 	%rl1136, %rl1133, 1;
	shr.u64 	%rl1137, %rl1482, 63;
	or.b64  	%rl1481, %rl1136, %rl1137;
	mul.lo.s64 	%rl1482, %rl1480, -7917410315110611862;
	add.s32 	%r1180, %r1180, -1;

BB2_370:
	setp.ne.s64 	%p285, %rl1482, 0;
	selp.u64 	%rl1138, 1, 0, %p285;
	add.s64 	%rl1139, %rl1138, %rl1481;
	add.s32 	%r928, %r1180, 1022;
	cvt.u64.u32 	%rl1140, %r928;
	shl.b64 	%rl1141, %rl1140, 52;
	shr.u64 	%rl1142, %rl1139, 11;
	shr.u64 	%rl1143, %rl1139, 10;
	and.b64  	%rl1144, %rl1143, 1;
	add.s64 	%rl1145, %rl1141, %rl1142;
	add.s64 	%rl1146, %rl1145, %rl1144;
	or.b64  	%rl1147, %rl1146, %rl1476;
	mov.b64 	 %fd2155, %rl1147;

BB2_371:
	add.s32 	%r260, %r1181, 1;
	and.b32  	%r929, %r260, 1;
	setp.eq.s32 	%p286, %r929, 0;
	mul.rn.f64 	%fd151, %fd2155, %fd2155;
	@%p286 bra 	BB2_373;

	mov.f64 	%fd1137, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1139, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1136, %fd1137, %fd151, %fd1139;
	// inline asm
	mov.f64 	%fd1143, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1140, %fd1136, %fd151, %fd1143;
	// inline asm
	mov.f64 	%fd1147, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1144, %fd1140, %fd151, %fd1147;
	// inline asm
	mov.f64 	%fd1151, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1148, %fd1144, %fd151, %fd1151;
	// inline asm
	mov.f64 	%fd1155, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1152, %fd1148, %fd151, %fd1155;
	// inline asm
	mov.f64 	%fd1159, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1156, %fd1152, %fd151, %fd1159;
	// inline asm
	mov.f64 	%fd1163, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1160, %fd1156, %fd151, %fd1163;
	// inline asm
	mov.f64 	%fd2156, %fd1160;
	bra.uni 	BB2_374;

BB2_373:
	mov.f64 	%fd1165, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1167, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1164, %fd1165, %fd151, %fd1167;
	// inline asm
	mov.f64 	%fd1171, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1168, %fd1164, %fd151, %fd1171;
	// inline asm
	mov.f64 	%fd1175, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1172, %fd1168, %fd151, %fd1175;
	// inline asm
	mov.f64 	%fd1179, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1176, %fd1172, %fd151, %fd1179;
	// inline asm
	mov.f64 	%fd1183, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1180, %fd1176, %fd151, %fd1183;
	// inline asm
	mul.rn.f64 	%fd1185, %fd1180, %fd151;
	// inline asm
	fma.rn.f64 	%fd1184, %fd1185, %fd2155, %fd2155;
	// inline asm
	mov.f64 	%fd2156, %fd1184;

BB2_374:
	and.b32  	%r930, %r260, 2;
	setp.eq.s32 	%p287, %r930, 0;
	neg.f64 	%fd1188, %fd2156;
	selp.f64 	%fd2157, %fd2156, %fd1188, %p287;
	bra.uni 	BB2_376;

BB2_375:
	mov.f64 	%fd2157, 0dFFF8000000000000;

BB2_376:
	setp.eq.f64 	%p288, %fd147, 0d0000000000000000;
	or.pred  	%p289, %p16, %p288;
	or.pred  	%p290, %p15, %p289;
	@%p290 bra 	BB2_399;

	// inline asm
	abs.f64 	%fd1190, %fd147;
	// inline asm
	setp.gt.f64 	%p291, %fd1190, 0d41E0000000000000;
	@%p291 bra 	BB2_379;

	mov.f64 	%fd1205, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1192, %fd147, %fd1205;
	// inline asm
	cvt.rni.s32.f64 	%r931, %fd1192;
	// inline asm
	cvt.rn.f64.s32 	%fd1206, %r931;
	neg.f64 	%fd1202, %fd1206;
	mov.f64 	%fd1195, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1193, %fd1202, %fd1195, %fd147;
	// inline asm
	mov.f64 	%fd1199, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1197, %fd1202, %fd1199, %fd1193;
	// inline asm
	mov.f64 	%fd1203, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1201, %fd1202, %fd1203, %fd1197;
	// inline asm
	mov.u32 	%r1184, %r931;
	mov.f64 	%fd2158, %fd1201;
	bra.uni 	BB2_395;

BB2_379:
	mov.b64 	 %rl417, %fd147;
	and.b64  	%rl1486, %rl417, -9223372036854775808;
	shr.u64 	%rl419, %rl417, 52;
	and.b64  	%rl1148, %rl419, 2047;
	add.s64 	%rl1149, %rl1148, 4294966272;
	cvt.u32.u64 	%r262, %rl1149;
	shl.b64 	%rl1150, %rl417, 11;
	or.b64  	%rl420, %rl1150, -9223372036854775808;
	shr.u32 	%r935, %r262, 6;
	mov.u32 	%r936, 16;
	sub.s32 	%r263, %r936, %r935;
	mov.u32 	%r937, 15;
	sub.s32 	%r1182, %r937, %r935;
	mov.u32 	%r938, 19;
	sub.s32 	%r265, %r938, %r935;
	mov.u32 	%r933, 18;
	// inline asm
	min.s32 	%r932, %r933, %r265;
	// inline asm
	setp.lt.s32 	%p292, %r1182, %r932;
	@%p292 bra 	BB2_381;

	mov.u64 	%rl1483, 0;
	bra.uni 	BB2_383;

BB2_381:
	mov.u32 	%r939, 1;
	sub.s32 	%r266, %r939, %r263;
	mov.u64 	%rl1483, 0;

BB2_382:
	.pragma "nounroll";
	shl.b32 	%r943, %r1182, 3;
	mov.u32 	%r944, __internal_i2opi_d;
	add.s32 	%r945, %r944, %r943;
	ld.const.u64 	%rl1154, [%r945];
	mul.lo.s64 	%rl1156, %rl1154, %rl420;
	// inline asm
	mul.hi.u64 	%rl1153, %rl1154, %rl420;
	// inline asm
	mad.lo.s64 	%rl1157, %rl1154, %rl420, %rl1483;
	setp.lt.u64 	%p293, %rl1157, %rl1156;
	selp.u64 	%rl1158, 1, 0, %p293;
	add.s64 	%rl1483, %rl1158, %rl1153;
	add.s32 	%r946, %r266, %r1182;
	shl.b32 	%r947, %r946, 3;
	add.s32 	%r949, %r48, %r947;
	st.local.u64 	[%r949], %rl1157;
	// inline asm
	min.s32 	%r940, %r933, %r265;
	// inline asm
	add.s32 	%r1182, %r1182, 1;
	setp.lt.s32 	%p294, %r1182, %r940;
	@%p294 bra 	BB2_382;

BB2_383:
	mov.u32 	%r950, 1;
	sub.s32 	%r951, %r950, %r263;
	add.s32 	%r952, %r951, %r1182;
	shl.b32 	%r953, %r952, 3;
	add.s32 	%r955, %r48, %r953;
	st.local.u64 	[%r955], %rl1483;
	ld.local.u64 	%rl1484, [%r48+24];
	ld.local.u64 	%rl1485, [%r48+16];
	and.b32  	%r956, %r262, 63;
	setp.eq.s32 	%p295, %r956, 0;
	@%p295 bra 	BB2_385;

	and.b64  	%rl1159, %rl419, 63;
	cvt.u32.u64 	%r957, %rl1159;
	shl.b64 	%rl1160, %rl1484, %r957;
	neg.s32 	%r958, %r262;
	and.b32  	%r959, %r958, 63;
	shr.u64 	%rl1161, %rl1485, %r959;
	or.b64  	%rl1484, %rl1161, %rl1160;
	shl.b64 	%rl1162, %rl1485, %r957;
	ld.local.u64 	%rl1163, [%r48+8];
	shr.u64 	%rl1164, %rl1163, %r959;
	or.b64  	%rl1485, %rl1164, %rl1162;

BB2_385:
	shr.u64 	%rl1165, %rl1484, 62;
	cvt.u32.u64 	%r960, %rl1165;
	shr.u64 	%rl1166, %rl1485, 62;
	shl.b64 	%rl1167, %rl1484, 2;
	or.b64  	%rl1490, %rl1166, %rl1167;
	shl.b64 	%rl431, %rl1485, 2;
	setp.ne.s64 	%p296, %rl431, 0;
	selp.u64 	%rl1168, 1, 0, %p296;
	or.b64  	%rl1169, %rl1168, %rl1490;
	setp.gt.u64 	%p297, %rl1169, -9223372036854775808;
	selp.u32 	%r961, 1, 0, %p297;
	add.s32 	%r962, %r961, %r960;
	neg.s32 	%r963, %r962;
	setp.lt.s64 	%p298, %rl417, 0;
	selp.b32 	%r1184, %r963, %r962, %p298;
	@%p297 bra 	BB2_387;

	mov.u64 	%rl1489, %rl431;
	bra.uni 	BB2_388;

BB2_387:
	not.b64 	%rl1170, %rl1490;
	neg.s64 	%rl432, %rl431;
	setp.eq.s64 	%p299, %rl431, 0;
	selp.u64 	%rl1171, 1, 0, %p299;
	add.s64 	%rl1490, %rl1171, %rl1170;
	xor.b64  	%rl1486, %rl1486, -9223372036854775808;
	mov.u64 	%rl1489, %rl432;

BB2_388:
	mov.u64 	%rl1488, %rl1489;
	setp.gt.s64 	%p300, %rl1490, 0;
	@%p300 bra 	BB2_390;

	mov.u32 	%r1183, 0;
	bra.uni 	BB2_392;

BB2_390:
	mov.u32 	%r1183, 0;

BB2_391:
	shr.u64 	%rl1172, %rl1488, 63;
	shl.b64 	%rl1173, %rl1490, 1;
	or.b64  	%rl1490, %rl1172, %rl1173;
	shl.b64 	%rl1488, %rl1488, 1;
	add.s32 	%r1183, %r1183, -1;
	setp.gt.s64 	%p301, %rl1490, 0;
	@%p301 bra 	BB2_391;

BB2_392:
	mul.lo.s64 	%rl1492, %rl1490, -3958705157555305931;
	mov.u64 	%rl1176, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1174, %rl1490, %rl1176;
	// inline asm
	setp.gt.s64 	%p302, %rl1174, 0;
	mov.u64 	%rl1491, %rl1174;
	@%p302 bra 	BB2_393;
	bra.uni 	BB2_394;

BB2_393:
	shl.b64 	%rl1177, %rl1174, 1;
	shr.u64 	%rl1178, %rl1492, 63;
	or.b64  	%rl1491, %rl1177, %rl1178;
	mul.lo.s64 	%rl1492, %rl1490, -7917410315110611862;
	add.s32 	%r1183, %r1183, -1;

BB2_394:
	setp.ne.s64 	%p303, %rl1492, 0;
	selp.u64 	%rl1179, 1, 0, %p303;
	add.s64 	%rl1180, %rl1179, %rl1491;
	add.s32 	%r966, %r1183, 1022;
	cvt.u64.u32 	%rl1181, %r966;
	shl.b64 	%rl1182, %rl1181, 52;
	shr.u64 	%rl1183, %rl1180, 11;
	shr.u64 	%rl1184, %rl1180, 10;
	and.b64  	%rl1185, %rl1184, 1;
	add.s64 	%rl1186, %rl1182, %rl1183;
	add.s64 	%rl1187, %rl1186, %rl1185;
	or.b64  	%rl1188, %rl1187, %rl1486;
	mov.b64 	 %fd2158, %rl1188;

BB2_395:
	and.b32  	%r967, %r1184, 1;
	setp.eq.s32 	%p304, %r967, 0;
	mul.rn.f64 	%fd161, %fd2158, %fd2158;
	@%p304 bra 	BB2_397;

	mov.f64 	%fd1208, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1210, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1207, %fd1208, %fd161, %fd1210;
	// inline asm
	mov.f64 	%fd1214, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1211, %fd1207, %fd161, %fd1214;
	// inline asm
	mov.f64 	%fd1218, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1215, %fd1211, %fd161, %fd1218;
	// inline asm
	mov.f64 	%fd1222, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1219, %fd1215, %fd161, %fd1222;
	// inline asm
	mov.f64 	%fd1226, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1223, %fd1219, %fd161, %fd1226;
	// inline asm
	mov.f64 	%fd1230, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1227, %fd1223, %fd161, %fd1230;
	// inline asm
	mov.f64 	%fd1234, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1231, %fd1227, %fd161, %fd1234;
	// inline asm
	mov.f64 	%fd2159, %fd1231;
	bra.uni 	BB2_398;

BB2_397:
	mov.f64 	%fd1236, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1238, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1235, %fd1236, %fd161, %fd1238;
	// inline asm
	mov.f64 	%fd1242, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1239, %fd1235, %fd161, %fd1242;
	// inline asm
	mov.f64 	%fd1246, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1243, %fd1239, %fd161, %fd1246;
	// inline asm
	mov.f64 	%fd1250, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1247, %fd1243, %fd161, %fd1250;
	// inline asm
	mov.f64 	%fd1254, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1251, %fd1247, %fd161, %fd1254;
	// inline asm
	mul.rn.f64 	%fd1256, %fd1251, %fd161;
	// inline asm
	fma.rn.f64 	%fd1255, %fd1256, %fd2158, %fd2158;
	// inline asm
	mov.f64 	%fd2159, %fd1255;

BB2_398:
	and.b32  	%r968, %r1184, 2;
	setp.eq.s32 	%p305, %r968, 0;
	neg.f64 	%fd1259, %fd2159;
	selp.f64 	%fd2160, %fd2159, %fd1259, %p305;
	bra.uni 	BB2_400;

BB2_399:
	mov.f64 	%fd1260, 0d0000000000000000;
	mul.rn.f64 	%fd2160, %fd147, %fd1260;

BB2_400:
	neg.f64 	%fd1261, %fd2160;
	mov.f64 	%fd2197, %fd2157;
	mov.f64 	%fd2198, %fd1261;
	ld.param.u32 	%r1126, [DIT10C2CM_param_4];
	setp.eq.s32 	%p306, %r1126, 0;
	@%p306 bra 	BB2_401;
	bra.uni 	BB2_402;

BB2_401:
	mov.f64 	%fd2197, %fd2157;
	mov.f64 	%fd2198, %fd2160;

BB2_402:
	mul.f64 	%fd1263, %fd13, %fd2197;
	neg.f64 	%fd1265, %fd14;
	fma.rn.f64 	%fd1266, %fd1265, %fd2198, %fd1263;
	mul.f64 	%fd1267, %fd14, %fd2197;
	fma.rn.f64 	%fd1268, %fd13, %fd2198, %fd1267;
	mov.f64 	%fd2187, %fd1266;
	mov.f64 	%fd2188, %fd1268;
	mul.f64 	%fd1269, %fd19, 0d404921FB54442D18;
	div.rn.f64 	%fd168, %fd1269, %fd20;
	setp.eq.f64 	%p17, %fd168, 0d7FF0000000000000;
	setp.eq.f64 	%p18, %fd168, 0dFFF0000000000000;
	or.pred  	%p307, %p17, %p18;
	@%p307 bra 	BB2_425;

	// inline asm
	abs.f64 	%fd1270, %fd168;
	// inline asm
	setp.gt.f64 	%p308, %fd1270, 0d41E0000000000000;
	@%p308 bra 	BB2_405;

	mov.f64 	%fd1285, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1272, %fd168, %fd1285;
	// inline asm
	cvt.rni.s32.f64 	%r969, %fd1272;
	// inline asm
	cvt.rn.f64.s32 	%fd1286, %r969;
	neg.f64 	%fd1282, %fd1286;
	mov.f64 	%fd1275, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1273, %fd1282, %fd1275, %fd168;
	// inline asm
	mov.f64 	%fd1279, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1277, %fd1282, %fd1279, %fd1273;
	// inline asm
	mov.f64 	%fd1283, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1281, %fd1282, %fd1283, %fd1277;
	// inline asm
	mov.u32 	%r1187, %r969;
	mov.f64 	%fd2161, %fd1281;
	bra.uni 	BB2_421;

BB2_405:
	mov.b64 	 %rl449, %fd168;
	and.b64  	%rl1496, %rl449, -9223372036854775808;
	shr.u64 	%rl451, %rl449, 52;
	and.b64  	%rl1189, %rl451, 2047;
	add.s64 	%rl1190, %rl1189, 4294966272;
	cvt.u32.u64 	%r278, %rl1190;
	shl.b64 	%rl1191, %rl449, 11;
	or.b64  	%rl452, %rl1191, -9223372036854775808;
	shr.u32 	%r973, %r278, 6;
	mov.u32 	%r974, 16;
	sub.s32 	%r279, %r974, %r973;
	mov.u32 	%r975, 15;
	sub.s32 	%r1185, %r975, %r973;
	mov.u32 	%r976, 19;
	sub.s32 	%r281, %r976, %r973;
	mov.u32 	%r971, 18;
	// inline asm
	min.s32 	%r970, %r971, %r281;
	// inline asm
	setp.lt.s32 	%p309, %r1185, %r970;
	@%p309 bra 	BB2_407;

	mov.u64 	%rl1493, 0;
	bra.uni 	BB2_409;

BB2_407:
	mov.u32 	%r977, 1;
	sub.s32 	%r282, %r977, %r279;
	mov.u64 	%rl1493, 0;

BB2_408:
	.pragma "nounroll";
	shl.b32 	%r981, %r1185, 3;
	mov.u32 	%r982, __internal_i2opi_d;
	add.s32 	%r983, %r982, %r981;
	ld.const.u64 	%rl1195, [%r983];
	mul.lo.s64 	%rl1197, %rl1195, %rl452;
	// inline asm
	mul.hi.u64 	%rl1194, %rl1195, %rl452;
	// inline asm
	mad.lo.s64 	%rl1198, %rl1195, %rl452, %rl1493;
	setp.lt.u64 	%p310, %rl1198, %rl1197;
	selp.u64 	%rl1199, 1, 0, %p310;
	add.s64 	%rl1493, %rl1199, %rl1194;
	add.s32 	%r984, %r282, %r1185;
	shl.b32 	%r985, %r984, 3;
	add.s32 	%r987, %r48, %r985;
	st.local.u64 	[%r987], %rl1198;
	// inline asm
	min.s32 	%r978, %r971, %r281;
	// inline asm
	add.s32 	%r1185, %r1185, 1;
	setp.lt.s32 	%p311, %r1185, %r978;
	@%p311 bra 	BB2_408;

BB2_409:
	mov.u32 	%r988, 1;
	sub.s32 	%r989, %r988, %r279;
	add.s32 	%r990, %r989, %r1185;
	shl.b32 	%r991, %r990, 3;
	add.s32 	%r993, %r48, %r991;
	st.local.u64 	[%r993], %rl1493;
	ld.local.u64 	%rl1494, [%r48+24];
	ld.local.u64 	%rl1495, [%r48+16];
	and.b32  	%r994, %r278, 63;
	setp.eq.s32 	%p312, %r994, 0;
	@%p312 bra 	BB2_411;

	and.b64  	%rl1200, %rl451, 63;
	cvt.u32.u64 	%r995, %rl1200;
	shl.b64 	%rl1201, %rl1494, %r995;
	neg.s32 	%r996, %r278;
	and.b32  	%r997, %r996, 63;
	shr.u64 	%rl1202, %rl1495, %r997;
	or.b64  	%rl1494, %rl1202, %rl1201;
	shl.b64 	%rl1203, %rl1495, %r995;
	ld.local.u64 	%rl1204, [%r48+8];
	shr.u64 	%rl1205, %rl1204, %r997;
	or.b64  	%rl1495, %rl1205, %rl1203;

BB2_411:
	shr.u64 	%rl1206, %rl1494, 62;
	cvt.u32.u64 	%r998, %rl1206;
	shr.u64 	%rl1207, %rl1495, 62;
	shl.b64 	%rl1208, %rl1494, 2;
	or.b64  	%rl1500, %rl1207, %rl1208;
	shl.b64 	%rl463, %rl1495, 2;
	setp.ne.s64 	%p313, %rl463, 0;
	selp.u64 	%rl1209, 1, 0, %p313;
	or.b64  	%rl1210, %rl1209, %rl1500;
	setp.gt.u64 	%p314, %rl1210, -9223372036854775808;
	selp.u32 	%r999, 1, 0, %p314;
	add.s32 	%r1000, %r999, %r998;
	neg.s32 	%r1001, %r1000;
	setp.lt.s64 	%p315, %rl449, 0;
	selp.b32 	%r1187, %r1001, %r1000, %p315;
	@%p314 bra 	BB2_413;

	mov.u64 	%rl1499, %rl463;
	bra.uni 	BB2_414;

BB2_413:
	not.b64 	%rl1211, %rl1500;
	neg.s64 	%rl464, %rl463;
	setp.eq.s64 	%p316, %rl463, 0;
	selp.u64 	%rl1212, 1, 0, %p316;
	add.s64 	%rl1500, %rl1212, %rl1211;
	xor.b64  	%rl1496, %rl1496, -9223372036854775808;
	mov.u64 	%rl1499, %rl464;

BB2_414:
	mov.u64 	%rl1498, %rl1499;
	setp.gt.s64 	%p317, %rl1500, 0;
	@%p317 bra 	BB2_416;

	mov.u32 	%r1186, 0;
	bra.uni 	BB2_418;

BB2_416:
	mov.u32 	%r1186, 0;

BB2_417:
	shr.u64 	%rl1213, %rl1498, 63;
	shl.b64 	%rl1214, %rl1500, 1;
	or.b64  	%rl1500, %rl1213, %rl1214;
	shl.b64 	%rl1498, %rl1498, 1;
	add.s32 	%r1186, %r1186, -1;
	setp.gt.s64 	%p318, %rl1500, 0;
	@%p318 bra 	BB2_417;

BB2_418:
	mul.lo.s64 	%rl1502, %rl1500, -3958705157555305931;
	mov.u64 	%rl1217, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1215, %rl1500, %rl1217;
	// inline asm
	setp.gt.s64 	%p319, %rl1215, 0;
	mov.u64 	%rl1501, %rl1215;
	@%p319 bra 	BB2_419;
	bra.uni 	BB2_420;

BB2_419:
	shl.b64 	%rl1218, %rl1215, 1;
	shr.u64 	%rl1219, %rl1502, 63;
	or.b64  	%rl1501, %rl1218, %rl1219;
	mul.lo.s64 	%rl1502, %rl1500, -7917410315110611862;
	add.s32 	%r1186, %r1186, -1;

BB2_420:
	setp.ne.s64 	%p320, %rl1502, 0;
	selp.u64 	%rl1220, 1, 0, %p320;
	add.s64 	%rl1221, %rl1220, %rl1501;
	add.s32 	%r1004, %r1186, 1022;
	cvt.u64.u32 	%rl1222, %r1004;
	shl.b64 	%rl1223, %rl1222, 52;
	shr.u64 	%rl1224, %rl1221, 11;
	shr.u64 	%rl1225, %rl1221, 10;
	and.b64  	%rl1226, %rl1225, 1;
	add.s64 	%rl1227, %rl1223, %rl1224;
	add.s64 	%rl1228, %rl1227, %rl1226;
	or.b64  	%rl1229, %rl1228, %rl1496;
	mov.b64 	 %fd2161, %rl1229;

BB2_421:
	add.s32 	%r293, %r1187, 1;
	and.b32  	%r1005, %r293, 1;
	setp.eq.s32 	%p321, %r1005, 0;
	mul.rn.f64 	%fd172, %fd2161, %fd2161;
	@%p321 bra 	BB2_423;

	mov.f64 	%fd1288, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1290, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1287, %fd1288, %fd172, %fd1290;
	// inline asm
	mov.f64 	%fd1294, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1291, %fd1287, %fd172, %fd1294;
	// inline asm
	mov.f64 	%fd1298, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1295, %fd1291, %fd172, %fd1298;
	// inline asm
	mov.f64 	%fd1302, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1299, %fd1295, %fd172, %fd1302;
	// inline asm
	mov.f64 	%fd1306, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1303, %fd1299, %fd172, %fd1306;
	// inline asm
	mov.f64 	%fd1310, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1307, %fd1303, %fd172, %fd1310;
	// inline asm
	mov.f64 	%fd1314, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1311, %fd1307, %fd172, %fd1314;
	// inline asm
	mov.f64 	%fd2162, %fd1311;
	bra.uni 	BB2_424;

BB2_423:
	mov.f64 	%fd1316, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1318, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1315, %fd1316, %fd172, %fd1318;
	// inline asm
	mov.f64 	%fd1322, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1319, %fd1315, %fd172, %fd1322;
	// inline asm
	mov.f64 	%fd1326, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1323, %fd1319, %fd172, %fd1326;
	// inline asm
	mov.f64 	%fd1330, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1327, %fd1323, %fd172, %fd1330;
	// inline asm
	mov.f64 	%fd1334, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1331, %fd1327, %fd172, %fd1334;
	// inline asm
	mul.rn.f64 	%fd1336, %fd1331, %fd172;
	// inline asm
	fma.rn.f64 	%fd1335, %fd1336, %fd2161, %fd2161;
	// inline asm
	mov.f64 	%fd2162, %fd1335;

BB2_424:
	and.b32  	%r1006, %r293, 2;
	setp.eq.s32 	%p322, %r1006, 0;
	neg.f64 	%fd1339, %fd2162;
	selp.f64 	%fd2163, %fd2162, %fd1339, %p322;
	bra.uni 	BB2_426;

BB2_425:
	mov.f64 	%fd2163, 0dFFF8000000000000;

BB2_426:
	setp.eq.f64 	%p323, %fd168, 0d0000000000000000;
	or.pred  	%p324, %p18, %p323;
	or.pred  	%p325, %p17, %p324;
	@%p325 bra 	BB2_449;

	// inline asm
	abs.f64 	%fd1341, %fd168;
	// inline asm
	setp.gt.f64 	%p326, %fd1341, 0d41E0000000000000;
	@%p326 bra 	BB2_429;

	mov.f64 	%fd1356, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1343, %fd168, %fd1356;
	// inline asm
	cvt.rni.s32.f64 	%r1007, %fd1343;
	// inline asm
	cvt.rn.f64.s32 	%fd1357, %r1007;
	neg.f64 	%fd1353, %fd1357;
	mov.f64 	%fd1346, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1344, %fd1353, %fd1346, %fd168;
	// inline asm
	mov.f64 	%fd1350, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1348, %fd1353, %fd1350, %fd1344;
	// inline asm
	mov.f64 	%fd1354, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1352, %fd1353, %fd1354, %fd1348;
	// inline asm
	mov.u32 	%r1190, %r1007;
	mov.f64 	%fd2164, %fd1352;
	bra.uni 	BB2_445;

BB2_429:
	mov.b64 	 %rl481, %fd168;
	and.b64  	%rl1506, %rl481, -9223372036854775808;
	shr.u64 	%rl483, %rl481, 52;
	and.b64  	%rl1230, %rl483, 2047;
	add.s64 	%rl1231, %rl1230, 4294966272;
	cvt.u32.u64 	%r295, %rl1231;
	shl.b64 	%rl1232, %rl481, 11;
	or.b64  	%rl484, %rl1232, -9223372036854775808;
	shr.u32 	%r1011, %r295, 6;
	mov.u32 	%r1012, 16;
	sub.s32 	%r296, %r1012, %r1011;
	mov.u32 	%r1013, 15;
	sub.s32 	%r1188, %r1013, %r1011;
	mov.u32 	%r1014, 19;
	sub.s32 	%r298, %r1014, %r1011;
	mov.u32 	%r1009, 18;
	// inline asm
	min.s32 	%r1008, %r1009, %r298;
	// inline asm
	setp.lt.s32 	%p327, %r1188, %r1008;
	@%p327 bra 	BB2_431;

	mov.u64 	%rl1503, 0;
	bra.uni 	BB2_433;

BB2_431:
	mov.u32 	%r1015, 1;
	sub.s32 	%r299, %r1015, %r296;
	mov.u64 	%rl1503, 0;

BB2_432:
	.pragma "nounroll";
	shl.b32 	%r1019, %r1188, 3;
	mov.u32 	%r1020, __internal_i2opi_d;
	add.s32 	%r1021, %r1020, %r1019;
	ld.const.u64 	%rl1236, [%r1021];
	mul.lo.s64 	%rl1238, %rl1236, %rl484;
	// inline asm
	mul.hi.u64 	%rl1235, %rl1236, %rl484;
	// inline asm
	mad.lo.s64 	%rl1239, %rl1236, %rl484, %rl1503;
	setp.lt.u64 	%p328, %rl1239, %rl1238;
	selp.u64 	%rl1240, 1, 0, %p328;
	add.s64 	%rl1503, %rl1240, %rl1235;
	add.s32 	%r1022, %r299, %r1188;
	shl.b32 	%r1023, %r1022, 3;
	add.s32 	%r1025, %r48, %r1023;
	st.local.u64 	[%r1025], %rl1239;
	// inline asm
	min.s32 	%r1016, %r1009, %r298;
	// inline asm
	add.s32 	%r1188, %r1188, 1;
	setp.lt.s32 	%p329, %r1188, %r1016;
	@%p329 bra 	BB2_432;

BB2_433:
	mov.u32 	%r1026, 1;
	sub.s32 	%r1027, %r1026, %r296;
	add.s32 	%r1028, %r1027, %r1188;
	shl.b32 	%r1029, %r1028, 3;
	add.s32 	%r1031, %r48, %r1029;
	st.local.u64 	[%r1031], %rl1503;
	ld.local.u64 	%rl1504, [%r48+24];
	ld.local.u64 	%rl1505, [%r48+16];
	and.b32  	%r1032, %r295, 63;
	setp.eq.s32 	%p330, %r1032, 0;
	@%p330 bra 	BB2_435;

	and.b64  	%rl1241, %rl483, 63;
	cvt.u32.u64 	%r1033, %rl1241;
	shl.b64 	%rl1242, %rl1504, %r1033;
	neg.s32 	%r1034, %r295;
	and.b32  	%r1035, %r1034, 63;
	shr.u64 	%rl1243, %rl1505, %r1035;
	or.b64  	%rl1504, %rl1243, %rl1242;
	shl.b64 	%rl1244, %rl1505, %r1033;
	ld.local.u64 	%rl1245, [%r48+8];
	shr.u64 	%rl1246, %rl1245, %r1035;
	or.b64  	%rl1505, %rl1246, %rl1244;

BB2_435:
	shr.u64 	%rl1247, %rl1504, 62;
	cvt.u32.u64 	%r1036, %rl1247;
	shr.u64 	%rl1248, %rl1505, 62;
	shl.b64 	%rl1249, %rl1504, 2;
	or.b64  	%rl1510, %rl1248, %rl1249;
	shl.b64 	%rl495, %rl1505, 2;
	setp.ne.s64 	%p331, %rl495, 0;
	selp.u64 	%rl1250, 1, 0, %p331;
	or.b64  	%rl1251, %rl1250, %rl1510;
	setp.gt.u64 	%p332, %rl1251, -9223372036854775808;
	selp.u32 	%r1037, 1, 0, %p332;
	add.s32 	%r1038, %r1037, %r1036;
	neg.s32 	%r1039, %r1038;
	setp.lt.s64 	%p333, %rl481, 0;
	selp.b32 	%r1190, %r1039, %r1038, %p333;
	@%p332 bra 	BB2_437;

	mov.u64 	%rl1509, %rl495;
	bra.uni 	BB2_438;

BB2_437:
	not.b64 	%rl1252, %rl1510;
	neg.s64 	%rl496, %rl495;
	setp.eq.s64 	%p334, %rl495, 0;
	selp.u64 	%rl1253, 1, 0, %p334;
	add.s64 	%rl1510, %rl1253, %rl1252;
	xor.b64  	%rl1506, %rl1506, -9223372036854775808;
	mov.u64 	%rl1509, %rl496;

BB2_438:
	mov.u64 	%rl1508, %rl1509;
	setp.gt.s64 	%p335, %rl1510, 0;
	@%p335 bra 	BB2_440;

	mov.u32 	%r1189, 0;
	bra.uni 	BB2_442;

BB2_440:
	mov.u32 	%r1189, 0;

BB2_441:
	shr.u64 	%rl1254, %rl1508, 63;
	shl.b64 	%rl1255, %rl1510, 1;
	or.b64  	%rl1510, %rl1254, %rl1255;
	shl.b64 	%rl1508, %rl1508, 1;
	add.s32 	%r1189, %r1189, -1;
	setp.gt.s64 	%p336, %rl1510, 0;
	@%p336 bra 	BB2_441;

BB2_442:
	mul.lo.s64 	%rl1512, %rl1510, -3958705157555305931;
	mov.u64 	%rl1258, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1256, %rl1510, %rl1258;
	// inline asm
	setp.gt.s64 	%p337, %rl1256, 0;
	mov.u64 	%rl1511, %rl1256;
	@%p337 bra 	BB2_443;
	bra.uni 	BB2_444;

BB2_443:
	shl.b64 	%rl1259, %rl1256, 1;
	shr.u64 	%rl1260, %rl1512, 63;
	or.b64  	%rl1511, %rl1259, %rl1260;
	mul.lo.s64 	%rl1512, %rl1510, -7917410315110611862;
	add.s32 	%r1189, %r1189, -1;

BB2_444:
	setp.ne.s64 	%p338, %rl1512, 0;
	selp.u64 	%rl1261, 1, 0, %p338;
	add.s64 	%rl1262, %rl1261, %rl1511;
	add.s32 	%r1042, %r1189, 1022;
	cvt.u64.u32 	%rl1263, %r1042;
	shl.b64 	%rl1264, %rl1263, 52;
	shr.u64 	%rl1265, %rl1262, 11;
	shr.u64 	%rl1266, %rl1262, 10;
	and.b64  	%rl1267, %rl1266, 1;
	add.s64 	%rl1268, %rl1264, %rl1265;
	add.s64 	%rl1269, %rl1268, %rl1267;
	or.b64  	%rl1270, %rl1269, %rl1506;
	mov.b64 	 %fd2164, %rl1270;

BB2_445:
	and.b32  	%r1043, %r1190, 1;
	setp.eq.s32 	%p339, %r1043, 0;
	mul.rn.f64 	%fd182, %fd2164, %fd2164;
	@%p339 bra 	BB2_447;

	mov.f64 	%fd1359, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1361, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1358, %fd1359, %fd182, %fd1361;
	// inline asm
	mov.f64 	%fd1365, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1362, %fd1358, %fd182, %fd1365;
	// inline asm
	mov.f64 	%fd1369, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1366, %fd1362, %fd182, %fd1369;
	// inline asm
	mov.f64 	%fd1373, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1370, %fd1366, %fd182, %fd1373;
	// inline asm
	mov.f64 	%fd1377, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1374, %fd1370, %fd182, %fd1377;
	// inline asm
	mov.f64 	%fd1381, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1378, %fd1374, %fd182, %fd1381;
	// inline asm
	mov.f64 	%fd1385, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1382, %fd1378, %fd182, %fd1385;
	// inline asm
	mov.f64 	%fd2165, %fd1382;
	bra.uni 	BB2_448;

BB2_447:
	mov.f64 	%fd1387, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1389, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1386, %fd1387, %fd182, %fd1389;
	// inline asm
	mov.f64 	%fd1393, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1390, %fd1386, %fd182, %fd1393;
	// inline asm
	mov.f64 	%fd1397, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1394, %fd1390, %fd182, %fd1397;
	// inline asm
	mov.f64 	%fd1401, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1398, %fd1394, %fd182, %fd1401;
	// inline asm
	mov.f64 	%fd1405, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1402, %fd1398, %fd182, %fd1405;
	// inline asm
	mul.rn.f64 	%fd1407, %fd1402, %fd182;
	// inline asm
	fma.rn.f64 	%fd1406, %fd1407, %fd2164, %fd2164;
	// inline asm
	mov.f64 	%fd2165, %fd1406;

BB2_448:
	and.b32  	%r1044, %r1190, 2;
	setp.eq.s32 	%p340, %r1044, 0;
	neg.f64 	%fd1410, %fd2165;
	selp.f64 	%fd2166, %fd2165, %fd1410, %p340;
	bra.uni 	BB2_450;

BB2_449:
	mov.f64 	%fd1411, 0d0000000000000000;
	mul.rn.f64 	%fd2166, %fd168, %fd1411;

BB2_450:
	neg.f64 	%fd1412, %fd2166;
	mov.f64 	%fd2195, %fd2163;
	mov.f64 	%fd2196, %fd1412;
	ld.param.u32 	%r1125, [DIT10C2CM_param_4];
	setp.eq.s32 	%p341, %r1125, 0;
	@%p341 bra 	BB2_451;
	bra.uni 	BB2_452;

BB2_451:
	mov.f64 	%fd2195, %fd2163;
	mov.f64 	%fd2196, %fd2166;

BB2_452:
	mul.f64 	%fd1414, %fd15, %fd2195;
	neg.f64 	%fd1416, %fd16;
	fma.rn.f64 	%fd1417, %fd1416, %fd2196, %fd1414;
	mul.f64 	%fd1418, %fd16, %fd2195;
	fma.rn.f64 	%fd1419, %fd15, %fd2196, %fd1418;
	mov.f64 	%fd2189, %fd1417;
	mov.f64 	%fd2190, %fd1419;
	mul.f64 	%fd1420, %fd19, 0d404C463ABECCB2BB;
	div.rn.f64 	%fd189, %fd1420, %fd20;
	setp.eq.f64 	%p19, %fd189, 0d7FF0000000000000;
	setp.eq.f64 	%p20, %fd189, 0dFFF0000000000000;
	or.pred  	%p342, %p19, %p20;
	@%p342 bra 	BB2_475;

	// inline asm
	abs.f64 	%fd1421, %fd189;
	// inline asm
	setp.gt.f64 	%p343, %fd1421, 0d41E0000000000000;
	@%p343 bra 	BB2_455;

	mov.f64 	%fd1436, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1423, %fd189, %fd1436;
	// inline asm
	cvt.rni.s32.f64 	%r1045, %fd1423;
	// inline asm
	cvt.rn.f64.s32 	%fd1437, %r1045;
	neg.f64 	%fd1433, %fd1437;
	mov.f64 	%fd1426, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1424, %fd1433, %fd1426, %fd189;
	// inline asm
	mov.f64 	%fd1430, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1428, %fd1433, %fd1430, %fd1424;
	// inline asm
	mov.f64 	%fd1434, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1432, %fd1433, %fd1434, %fd1428;
	// inline asm
	mov.u32 	%r1193, %r1045;
	mov.f64 	%fd2167, %fd1432;
	bra.uni 	BB2_471;

BB2_455:
	mov.b64 	 %rl513, %fd189;
	and.b64  	%rl1516, %rl513, -9223372036854775808;
	shr.u64 	%rl515, %rl513, 52;
	and.b64  	%rl1271, %rl515, 2047;
	add.s64 	%rl1272, %rl1271, 4294966272;
	cvt.u32.u64 	%r311, %rl1272;
	shl.b64 	%rl1273, %rl513, 11;
	or.b64  	%rl516, %rl1273, -9223372036854775808;
	shr.u32 	%r1049, %r311, 6;
	mov.u32 	%r1050, 16;
	sub.s32 	%r312, %r1050, %r1049;
	mov.u32 	%r1051, 15;
	sub.s32 	%r1191, %r1051, %r1049;
	mov.u32 	%r1052, 19;
	sub.s32 	%r314, %r1052, %r1049;
	mov.u32 	%r1047, 18;
	// inline asm
	min.s32 	%r1046, %r1047, %r314;
	// inline asm
	setp.lt.s32 	%p344, %r1191, %r1046;
	@%p344 bra 	BB2_457;

	mov.u64 	%rl1513, 0;
	bra.uni 	BB2_459;

BB2_457:
	mov.u32 	%r1053, 1;
	sub.s32 	%r315, %r1053, %r312;
	mov.u64 	%rl1513, 0;

BB2_458:
	.pragma "nounroll";
	shl.b32 	%r1057, %r1191, 3;
	mov.u32 	%r1058, __internal_i2opi_d;
	add.s32 	%r1059, %r1058, %r1057;
	ld.const.u64 	%rl1277, [%r1059];
	mul.lo.s64 	%rl1279, %rl1277, %rl516;
	// inline asm
	mul.hi.u64 	%rl1276, %rl1277, %rl516;
	// inline asm
	mad.lo.s64 	%rl1280, %rl1277, %rl516, %rl1513;
	setp.lt.u64 	%p345, %rl1280, %rl1279;
	selp.u64 	%rl1281, 1, 0, %p345;
	add.s64 	%rl1513, %rl1281, %rl1276;
	add.s32 	%r1060, %r315, %r1191;
	shl.b32 	%r1061, %r1060, 3;
	add.s32 	%r1063, %r48, %r1061;
	st.local.u64 	[%r1063], %rl1280;
	// inline asm
	min.s32 	%r1054, %r1047, %r314;
	// inline asm
	add.s32 	%r1191, %r1191, 1;
	setp.lt.s32 	%p346, %r1191, %r1054;
	@%p346 bra 	BB2_458;

BB2_459:
	mov.u32 	%r1064, 1;
	sub.s32 	%r1065, %r1064, %r312;
	add.s32 	%r1066, %r1065, %r1191;
	shl.b32 	%r1067, %r1066, 3;
	add.s32 	%r1069, %r48, %r1067;
	st.local.u64 	[%r1069], %rl1513;
	ld.local.u64 	%rl1514, [%r48+24];
	ld.local.u64 	%rl1515, [%r48+16];
	and.b32  	%r1070, %r311, 63;
	setp.eq.s32 	%p347, %r1070, 0;
	@%p347 bra 	BB2_461;

	and.b64  	%rl1282, %rl515, 63;
	cvt.u32.u64 	%r1071, %rl1282;
	shl.b64 	%rl1283, %rl1514, %r1071;
	neg.s32 	%r1072, %r311;
	and.b32  	%r1073, %r1072, 63;
	shr.u64 	%rl1284, %rl1515, %r1073;
	or.b64  	%rl1514, %rl1284, %rl1283;
	shl.b64 	%rl1285, %rl1515, %r1071;
	ld.local.u64 	%rl1286, [%r48+8];
	shr.u64 	%rl1287, %rl1286, %r1073;
	or.b64  	%rl1515, %rl1287, %rl1285;

BB2_461:
	shr.u64 	%rl1288, %rl1514, 62;
	cvt.u32.u64 	%r1074, %rl1288;
	shr.u64 	%rl1289, %rl1515, 62;
	shl.b64 	%rl1290, %rl1514, 2;
	or.b64  	%rl1520, %rl1289, %rl1290;
	shl.b64 	%rl527, %rl1515, 2;
	setp.ne.s64 	%p348, %rl527, 0;
	selp.u64 	%rl1291, 1, 0, %p348;
	or.b64  	%rl1292, %rl1291, %rl1520;
	setp.gt.u64 	%p349, %rl1292, -9223372036854775808;
	selp.u32 	%r1075, 1, 0, %p349;
	add.s32 	%r1076, %r1075, %r1074;
	neg.s32 	%r1077, %r1076;
	setp.lt.s64 	%p350, %rl513, 0;
	selp.b32 	%r1193, %r1077, %r1076, %p350;
	@%p349 bra 	BB2_463;

	mov.u64 	%rl1519, %rl527;
	bra.uni 	BB2_464;

BB2_463:
	not.b64 	%rl1293, %rl1520;
	neg.s64 	%rl528, %rl527;
	setp.eq.s64 	%p351, %rl527, 0;
	selp.u64 	%rl1294, 1, 0, %p351;
	add.s64 	%rl1520, %rl1294, %rl1293;
	xor.b64  	%rl1516, %rl1516, -9223372036854775808;
	mov.u64 	%rl1519, %rl528;

BB2_464:
	mov.u64 	%rl1518, %rl1519;
	setp.gt.s64 	%p352, %rl1520, 0;
	@%p352 bra 	BB2_466;

	mov.u32 	%r1192, 0;
	bra.uni 	BB2_468;

BB2_466:
	mov.u32 	%r1192, 0;

BB2_467:
	shr.u64 	%rl1295, %rl1518, 63;
	shl.b64 	%rl1296, %rl1520, 1;
	or.b64  	%rl1520, %rl1295, %rl1296;
	shl.b64 	%rl1518, %rl1518, 1;
	add.s32 	%r1192, %r1192, -1;
	setp.gt.s64 	%p353, %rl1520, 0;
	@%p353 bra 	BB2_467;

BB2_468:
	mul.lo.s64 	%rl1522, %rl1520, -3958705157555305931;
	mov.u64 	%rl1299, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1297, %rl1520, %rl1299;
	// inline asm
	setp.gt.s64 	%p354, %rl1297, 0;
	mov.u64 	%rl1521, %rl1297;
	@%p354 bra 	BB2_469;
	bra.uni 	BB2_470;

BB2_469:
	shl.b64 	%rl1300, %rl1297, 1;
	shr.u64 	%rl1301, %rl1522, 63;
	or.b64  	%rl1521, %rl1300, %rl1301;
	mul.lo.s64 	%rl1522, %rl1520, -7917410315110611862;
	add.s32 	%r1192, %r1192, -1;

BB2_470:
	setp.ne.s64 	%p355, %rl1522, 0;
	selp.u64 	%rl1302, 1, 0, %p355;
	add.s64 	%rl1303, %rl1302, %rl1521;
	add.s32 	%r1080, %r1192, 1022;
	cvt.u64.u32 	%rl1304, %r1080;
	shl.b64 	%rl1305, %rl1304, 52;
	shr.u64 	%rl1306, %rl1303, 11;
	shr.u64 	%rl1307, %rl1303, 10;
	and.b64  	%rl1308, %rl1307, 1;
	add.s64 	%rl1309, %rl1305, %rl1306;
	add.s64 	%rl1310, %rl1309, %rl1308;
	or.b64  	%rl1311, %rl1310, %rl1516;
	mov.b64 	 %fd2167, %rl1311;

BB2_471:
	add.s32 	%r326, %r1193, 1;
	and.b32  	%r1081, %r326, 1;
	setp.eq.s32 	%p356, %r1081, 0;
	mul.rn.f64 	%fd193, %fd2167, %fd2167;
	@%p356 bra 	BB2_473;

	mov.f64 	%fd1439, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1441, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1438, %fd1439, %fd193, %fd1441;
	// inline asm
	mov.f64 	%fd1445, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1442, %fd1438, %fd193, %fd1445;
	// inline asm
	mov.f64 	%fd1449, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1446, %fd1442, %fd193, %fd1449;
	// inline asm
	mov.f64 	%fd1453, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1450, %fd1446, %fd193, %fd1453;
	// inline asm
	mov.f64 	%fd1457, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1454, %fd1450, %fd193, %fd1457;
	// inline asm
	mov.f64 	%fd1461, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1458, %fd1454, %fd193, %fd1461;
	// inline asm
	mov.f64 	%fd1465, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1462, %fd1458, %fd193, %fd1465;
	// inline asm
	mov.f64 	%fd2168, %fd1462;
	bra.uni 	BB2_474;

BB2_473:
	mov.f64 	%fd1467, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1469, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1466, %fd1467, %fd193, %fd1469;
	// inline asm
	mov.f64 	%fd1473, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1470, %fd1466, %fd193, %fd1473;
	// inline asm
	mov.f64 	%fd1477, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1474, %fd1470, %fd193, %fd1477;
	// inline asm
	mov.f64 	%fd1481, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1478, %fd1474, %fd193, %fd1481;
	// inline asm
	mov.f64 	%fd1485, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1482, %fd1478, %fd193, %fd1485;
	// inline asm
	mul.rn.f64 	%fd1487, %fd1482, %fd193;
	// inline asm
	fma.rn.f64 	%fd1486, %fd1487, %fd2167, %fd2167;
	// inline asm
	mov.f64 	%fd2168, %fd1486;

BB2_474:
	and.b32  	%r1082, %r326, 2;
	setp.eq.s32 	%p357, %r1082, 0;
	neg.f64 	%fd1490, %fd2168;
	selp.f64 	%fd2169, %fd2168, %fd1490, %p357;
	bra.uni 	BB2_476;

BB2_475:
	mov.f64 	%fd2169, 0dFFF8000000000000;

BB2_476:
	setp.eq.f64 	%p358, %fd189, 0d0000000000000000;
	or.pred  	%p359, %p20, %p358;
	or.pred  	%p360, %p19, %p359;
	@%p360 bra 	BB2_499;

	// inline asm
	abs.f64 	%fd1492, %fd189;
	// inline asm
	setp.gt.f64 	%p361, %fd1492, 0d41E0000000000000;
	@%p361 bra 	BB2_479;

	mov.f64 	%fd1507, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1494, %fd189, %fd1507;
	// inline asm
	cvt.rni.s32.f64 	%r1083, %fd1494;
	// inline asm
	cvt.rn.f64.s32 	%fd1508, %r1083;
	neg.f64 	%fd1504, %fd1508;
	mov.f64 	%fd1497, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1495, %fd1504, %fd1497, %fd189;
	// inline asm
	mov.f64 	%fd1501, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1499, %fd1504, %fd1501, %fd1495;
	// inline asm
	mov.f64 	%fd1505, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1503, %fd1504, %fd1505, %fd1499;
	// inline asm
	mov.u32 	%r1196, %r1083;
	mov.f64 	%fd2170, %fd1503;
	bra.uni 	BB2_495;

BB2_479:
	mov.b64 	 %rl545, %fd189;
	and.b64  	%rl1526, %rl545, -9223372036854775808;
	shr.u64 	%rl547, %rl545, 52;
	and.b64  	%rl1312, %rl547, 2047;
	add.s64 	%rl1313, %rl1312, 4294966272;
	cvt.u32.u64 	%r328, %rl1313;
	shl.b64 	%rl1314, %rl545, 11;
	or.b64  	%rl548, %rl1314, -9223372036854775808;
	shr.u32 	%r1087, %r328, 6;
	mov.u32 	%r1088, 16;
	sub.s32 	%r329, %r1088, %r1087;
	mov.u32 	%r1089, 15;
	sub.s32 	%r1194, %r1089, %r1087;
	mov.u32 	%r1090, 19;
	sub.s32 	%r331, %r1090, %r1087;
	mov.u32 	%r1085, 18;
	// inline asm
	min.s32 	%r1084, %r1085, %r331;
	// inline asm
	setp.lt.s32 	%p362, %r1194, %r1084;
	@%p362 bra 	BB2_481;

	mov.u64 	%rl1523, 0;
	bra.uni 	BB2_483;

BB2_481:
	mov.u32 	%r1091, 1;
	sub.s32 	%r332, %r1091, %r329;
	mov.u64 	%rl1523, 0;

BB2_482:
	.pragma "nounroll";
	shl.b32 	%r1095, %r1194, 3;
	mov.u32 	%r1096, __internal_i2opi_d;
	add.s32 	%r1097, %r1096, %r1095;
	ld.const.u64 	%rl1318, [%r1097];
	mul.lo.s64 	%rl1320, %rl1318, %rl548;
	// inline asm
	mul.hi.u64 	%rl1317, %rl1318, %rl548;
	// inline asm
	mad.lo.s64 	%rl1321, %rl1318, %rl548, %rl1523;
	setp.lt.u64 	%p363, %rl1321, %rl1320;
	selp.u64 	%rl1322, 1, 0, %p363;
	add.s64 	%rl1523, %rl1322, %rl1317;
	add.s32 	%r1098, %r332, %r1194;
	shl.b32 	%r1099, %r1098, 3;
	add.s32 	%r1101, %r48, %r1099;
	st.local.u64 	[%r1101], %rl1321;
	// inline asm
	min.s32 	%r1092, %r1085, %r331;
	// inline asm
	add.s32 	%r1194, %r1194, 1;
	setp.lt.s32 	%p364, %r1194, %r1092;
	@%p364 bra 	BB2_482;

BB2_483:
	mov.u32 	%r1102, 1;
	sub.s32 	%r1103, %r1102, %r329;
	add.s32 	%r1104, %r1103, %r1194;
	shl.b32 	%r1105, %r1104, 3;
	add.s32 	%r1107, %r48, %r1105;
	st.local.u64 	[%r1107], %rl1523;
	ld.local.u64 	%rl1524, [%r48+24];
	ld.local.u64 	%rl1525, [%r48+16];
	and.b32  	%r1108, %r328, 63;
	setp.eq.s32 	%p365, %r1108, 0;
	@%p365 bra 	BB2_485;

	and.b64  	%rl1323, %rl547, 63;
	cvt.u32.u64 	%r1109, %rl1323;
	shl.b64 	%rl1324, %rl1524, %r1109;
	neg.s32 	%r1110, %r328;
	and.b32  	%r1111, %r1110, 63;
	shr.u64 	%rl1325, %rl1525, %r1111;
	or.b64  	%rl1524, %rl1325, %rl1324;
	shl.b64 	%rl1326, %rl1525, %r1109;
	ld.local.u64 	%rl1327, [%r48+8];
	shr.u64 	%rl1328, %rl1327, %r1111;
	or.b64  	%rl1525, %rl1328, %rl1326;

BB2_485:
	shr.u64 	%rl1329, %rl1524, 62;
	cvt.u32.u64 	%r1112, %rl1329;
	shr.u64 	%rl1330, %rl1525, 62;
	shl.b64 	%rl1331, %rl1524, 2;
	or.b64  	%rl1530, %rl1330, %rl1331;
	shl.b64 	%rl559, %rl1525, 2;
	setp.ne.s64 	%p366, %rl559, 0;
	selp.u64 	%rl1332, 1, 0, %p366;
	or.b64  	%rl1333, %rl1332, %rl1530;
	setp.gt.u64 	%p367, %rl1333, -9223372036854775808;
	selp.u32 	%r1113, 1, 0, %p367;
	add.s32 	%r1114, %r1113, %r1112;
	neg.s32 	%r1115, %r1114;
	setp.lt.s64 	%p368, %rl545, 0;
	selp.b32 	%r1196, %r1115, %r1114, %p368;
	@%p367 bra 	BB2_487;

	mov.u64 	%rl1529, %rl559;
	bra.uni 	BB2_488;

BB2_487:
	not.b64 	%rl1334, %rl1530;
	neg.s64 	%rl560, %rl559;
	setp.eq.s64 	%p369, %rl559, 0;
	selp.u64 	%rl1335, 1, 0, %p369;
	add.s64 	%rl1530, %rl1335, %rl1334;
	xor.b64  	%rl1526, %rl1526, -9223372036854775808;
	mov.u64 	%rl1529, %rl560;

BB2_488:
	mov.u64 	%rl1528, %rl1529;
	setp.gt.s64 	%p370, %rl1530, 0;
	@%p370 bra 	BB2_490;

	mov.u32 	%r1195, 0;
	bra.uni 	BB2_492;

BB2_490:
	mov.u32 	%r1195, 0;

BB2_491:
	shr.u64 	%rl1336, %rl1528, 63;
	shl.b64 	%rl1337, %rl1530, 1;
	or.b64  	%rl1530, %rl1336, %rl1337;
	shl.b64 	%rl1528, %rl1528, 1;
	add.s32 	%r1195, %r1195, -1;
	setp.gt.s64 	%p371, %rl1530, 0;
	@%p371 bra 	BB2_491;

BB2_492:
	mul.lo.s64 	%rl1532, %rl1530, -3958705157555305931;
	mov.u64 	%rl1340, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1338, %rl1530, %rl1340;
	// inline asm
	setp.gt.s64 	%p372, %rl1338, 0;
	mov.u64 	%rl1531, %rl1338;
	@%p372 bra 	BB2_493;
	bra.uni 	BB2_494;

BB2_493:
	shl.b64 	%rl1341, %rl1338, 1;
	shr.u64 	%rl1342, %rl1532, 63;
	or.b64  	%rl1531, %rl1341, %rl1342;
	mul.lo.s64 	%rl1532, %rl1530, -7917410315110611862;
	add.s32 	%r1195, %r1195, -1;

BB2_494:
	setp.ne.s64 	%p373, %rl1532, 0;
	selp.u64 	%rl1343, 1, 0, %p373;
	add.s64 	%rl1344, %rl1343, %rl1531;
	add.s32 	%r1118, %r1195, 1022;
	cvt.u64.u32 	%rl1345, %r1118;
	shl.b64 	%rl1346, %rl1345, 52;
	shr.u64 	%rl1347, %rl1344, 11;
	shr.u64 	%rl1348, %rl1344, 10;
	and.b64  	%rl1349, %rl1348, 1;
	add.s64 	%rl1350, %rl1346, %rl1347;
	add.s64 	%rl1351, %rl1350, %rl1349;
	or.b64  	%rl1352, %rl1351, %rl1526;
	mov.b64 	 %fd2170, %rl1352;

BB2_495:
	and.b32  	%r1119, %r1196, 1;
	setp.eq.s32 	%p374, %r1119, 0;
	mul.rn.f64 	%fd203, %fd2170, %fd2170;
	@%p374 bra 	BB2_497;

	mov.f64 	%fd1510, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1512, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1509, %fd1510, %fd203, %fd1512;
	// inline asm
	mov.f64 	%fd1516, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1513, %fd1509, %fd203, %fd1516;
	// inline asm
	mov.f64 	%fd1520, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1517, %fd1513, %fd203, %fd1520;
	// inline asm
	mov.f64 	%fd1524, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1521, %fd1517, %fd203, %fd1524;
	// inline asm
	mov.f64 	%fd1528, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1525, %fd1521, %fd203, %fd1528;
	// inline asm
	mov.f64 	%fd1532, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1529, %fd1525, %fd203, %fd1532;
	// inline asm
	mov.f64 	%fd1536, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1533, %fd1529, %fd203, %fd1536;
	// inline asm
	mov.f64 	%fd2171, %fd1533;
	bra.uni 	BB2_498;

BB2_497:
	mov.f64 	%fd1538, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1540, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1537, %fd1538, %fd203, %fd1540;
	// inline asm
	mov.f64 	%fd1544, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1541, %fd1537, %fd203, %fd1544;
	// inline asm
	mov.f64 	%fd1548, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1545, %fd1541, %fd203, %fd1548;
	// inline asm
	mov.f64 	%fd1552, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1549, %fd1545, %fd203, %fd1552;
	// inline asm
	mov.f64 	%fd1556, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1553, %fd1549, %fd203, %fd1556;
	// inline asm
	mul.rn.f64 	%fd1558, %fd1553, %fd203;
	// inline asm
	fma.rn.f64 	%fd1557, %fd1558, %fd2170, %fd2170;
	// inline asm
	mov.f64 	%fd2171, %fd1557;

BB2_498:
	and.b32  	%r1120, %r1196, 2;
	setp.eq.s32 	%p375, %r1120, 0;
	neg.f64 	%fd1561, %fd2171;
	selp.f64 	%fd2172, %fd2171, %fd1561, %p375;
	bra.uni 	BB2_500;

BB2_499:
	mov.f64 	%fd1562, 0d0000000000000000;
	mul.rn.f64 	%fd2172, %fd189, %fd1562;

BB2_500:
	neg.f64 	%fd1563, %fd2172;
	mov.f64 	%fd2193, %fd2169;
	mov.f64 	%fd2194, %fd1563;
	ld.param.u32 	%r1124, [DIT10C2CM_param_4];
	setp.eq.s32 	%p376, %r1124, 0;
	@%p376 bra 	BB2_501;
	bra.uni 	BB2_502;

BB2_501:
	mov.f64 	%fd2193, %fd2169;
	mov.f64 	%fd2194, %fd2172;

BB2_502:
	mul.f64 	%fd1565, %fd17, %fd2193;
	neg.f64 	%fd1567, %fd18;
	fma.rn.f64 	%fd1568, %fd1567, %fd2194, %fd1565;
	mul.f64 	%fd1569, %fd18, %fd2193;
	fma.rn.f64 	%fd1570, %fd17, %fd2194, %fd1569;
	mov.f64 	%fd2191, %fd1568;
	mov.f64 	%fd2192, %fd1570;

BB2_503:
	add.f64 	%fd1573, %fd2173, %fd2175;
	add.f64 	%fd1575, %fd1573, %fd2177;
	add.f64 	%fd1577, %fd1575, %fd2179;
	add.f64 	%fd1579, %fd1577, %fd2181;
	add.f64 	%fd1581, %fd1579, %fd2183;
	add.f64 	%fd1583, %fd1581, %fd2185;
	add.f64 	%fd1585, %fd1583, %fd2187;
	add.f64 	%fd1587, %fd1585, %fd2189;
	add.f64 	%fd1589, %fd1587, %fd2191;
	st.global.f64 	[%r38], %fd1589;
	add.f64 	%fd1592, %fd2174, %fd2176;
	add.f64 	%fd1594, %fd1592, %fd2178;
	add.f64 	%fd1596, %fd1594, %fd2180;
	add.f64 	%fd1598, %fd1596, %fd2182;
	add.f64 	%fd1600, %fd1598, %fd2184;
	add.f64 	%fd1602, %fd1600, %fd2186;
	add.f64 	%fd1604, %fd1602, %fd2188;
	add.f64 	%fd1606, %fd1604, %fd2190;
	add.f64 	%fd1608, %fd1606, %fd2192;
	st.global.f64 	[%r38+8], %fd1608;
	mul.f64 	%fd1609, %fd2175, 0d3FE9E3779B97F4A4;
	mul.f64 	%fd1610, %fd2176, 0d3FE2CF2304755A5D;
	mov.f64 	%fd1611, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1612, %fd2175, 0d3FE9E3779B97F4A4, %fd1610;
	add.f64 	%fd1613, %fd2173, %fd1612;
	mul.f64 	%fd1614, %fd2177, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1615, %fd2178, 0d3FEE6F0E13445503;
	mov.f64 	%fd1616, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1617, %fd2177, 0d3FD3C6EF372FE948, %fd1615;
	add.f64 	%fd1618, %fd1613, %fd1617;
	mul.f64 	%fd1619, %fd2179, 0dBFD3C6EF372FE948;
	mul.f64 	%fd1620, %fd2180, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1621, %fd2179, 0dBFD3C6EF372FE948, %fd1620;
	add.f64 	%fd1622, %fd1618, %fd1621;
	mul.f64 	%fd1623, %fd2181, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1624, %fd2182, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1625, %fd2181, 0dBFE9E3779B97F4A4, %fd1624;
	add.f64 	%fd1626, %fd1622, %fd1625;
	sub.f64 	%fd1627, %fd1626, %fd2183;
	mul.f64 	%fd1628, %fd2185, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1629, %fd2186, 0d3FE2CF2304755A5D;
	neg.f64 	%fd1630, %fd2186;
	fma.rn.f64 	%fd1631, %fd1630, %fd1611, %fd1628;
	add.f64 	%fd1632, %fd1627, %fd1631;
	mul.f64 	%fd1633, %fd2187, 0dBFD3C6EF372FE948;
	mul.f64 	%fd1634, %fd2188, 0d3FEE6F0E13445503;
	neg.f64 	%fd1635, %fd2188;
	fma.rn.f64 	%fd1636, %fd1635, %fd1616, %fd1633;
	add.f64 	%fd1637, %fd1632, %fd1636;
	mul.f64 	%fd1638, %fd2189, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1639, %fd2190, 0d3FEE6F0E13445503;
	neg.f64 	%fd1640, %fd2190;
	fma.rn.f64 	%fd1641, %fd1640, %fd1616, %fd1638;
	add.f64 	%fd1642, %fd1637, %fd1641;
	mul.f64 	%fd1643, %fd2191, 0d3FE9E3779B97F4A4;
	mul.f64 	%fd1644, %fd2192, 0d3FE2CF2304755A5D;
	neg.f64 	%fd1645, %fd2192;
	fma.rn.f64 	%fd1646, %fd1645, %fd1611, %fd1643;
	add.f64 	%fd1647, %fd1642, %fd1646;
	st.global.f64 	[%r39], %fd1647;
	mul.f64 	%fd1648, %fd2176, 0d3FE9E3779B97F4A4;
	mul.f64 	%fd1649, %fd2175, 0d3FE2CF2304755A5D;
	neg.f64 	%fd1650, %fd2175;
	fma.rn.f64 	%fd1651, %fd1650, %fd1611, %fd1648;
	add.f64 	%fd1652, %fd2174, %fd1651;
	mul.f64 	%fd1653, %fd2178, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1654, %fd2177, 0d3FEE6F0E13445503;
	neg.f64 	%fd1655, %fd2177;
	fma.rn.f64 	%fd1656, %fd1655, %fd1616, %fd1653;
	add.f64 	%fd1657, %fd1652, %fd1656;
	mul.f64 	%fd1658, %fd2180, 0dBFD3C6EF372FE948;
	mul.f64 	%fd1659, %fd2179, 0d3FEE6F0E13445503;
	neg.f64 	%fd1660, %fd2179;
	fma.rn.f64 	%fd1661, %fd1660, %fd1616, %fd1658;
	add.f64 	%fd1662, %fd1657, %fd1661;
	mul.f64 	%fd1663, %fd2182, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1664, %fd2181, 0d3FE2CF2304755A5D;
	neg.f64 	%fd1665, %fd2181;
	fma.rn.f64 	%fd1666, %fd1665, %fd1611, %fd1663;
	add.f64 	%fd1667, %fd1662, %fd1666;
	sub.f64 	%fd1668, %fd1667, %fd2184;
	mul.f64 	%fd1669, %fd2186, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1670, %fd2185, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1671, %fd2186, 0dBFE9E3779B97F4A4, %fd1670;
	add.f64 	%fd1672, %fd1668, %fd1671;
	mul.f64 	%fd1673, %fd2188, 0dBFD3C6EF372FE948;
	mul.f64 	%fd1674, %fd2187, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1675, %fd2188, 0dBFD3C6EF372FE948, %fd1674;
	add.f64 	%fd1676, %fd1672, %fd1675;
	mul.f64 	%fd1677, %fd2190, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1678, %fd2189, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1679, %fd2190, 0d3FD3C6EF372FE948, %fd1678;
	add.f64 	%fd1680, %fd1676, %fd1679;
	mul.f64 	%fd1681, %fd2192, 0d3FE9E3779B97F4A4;
	mul.f64 	%fd1682, %fd2191, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1683, %fd2192, 0d3FE9E3779B97F4A4, %fd1682;
	add.f64 	%fd1684, %fd1680, %fd1683;
	st.global.f64 	[%r39+8], %fd1684;
	mul.f64 	%fd1685, %fd2175, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1686, %fd2176, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1687, %fd2175, 0d3FD3C6EF372FE948, %fd1686;
	add.f64 	%fd1688, %fd2173, %fd1687;
	mul.f64 	%fd1689, %fd2177, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1690, %fd2178, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1691, %fd2177, 0dBFE9E3779B97F4A4, %fd1690;
	add.f64 	%fd1692, %fd1688, %fd1691;
	mul.f64 	%fd1693, %fd2179, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1694, %fd2180, 0d3FE2CF2304755A5D;
	neg.f64 	%fd1695, %fd2180;
	fma.rn.f64 	%fd1696, %fd1695, %fd1611, %fd1693;
	add.f64 	%fd1697, %fd1692, %fd1696;
	mul.f64 	%fd1698, %fd2181, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1699, %fd2182, 0d3FEE6F0E13445503;
	neg.f64 	%fd1700, %fd2182;
	fma.rn.f64 	%fd1701, %fd1700, %fd1616, %fd1698;
	add.f64 	%fd1702, %fd1697, %fd1701;
	add.f64 	%fd1703, %fd1702, %fd2183;
	mul.f64 	%fd1704, %fd2185, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1705, %fd2186, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1706, %fd2185, 0d3FD3C6EF372FE948, %fd1705;
	add.f64 	%fd1707, %fd1703, %fd1706;
	mul.f64 	%fd1708, %fd2187, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1709, %fd2188, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1710, %fd2187, 0dBFE9E3779B97F4A4, %fd1709;
	add.f64 	%fd1711, %fd1707, %fd1710;
	mul.f64 	%fd1712, %fd2189, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1713, %fd2190, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1714, %fd1640, %fd1611, %fd1712;
	add.f64 	%fd1715, %fd1711, %fd1714;
	mul.f64 	%fd1716, %fd2191, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1717, %fd2192, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1718, %fd1645, %fd1616, %fd1716;
	add.f64 	%fd1719, %fd1715, %fd1718;
	st.global.f64 	[%r40], %fd1719;
	mul.f64 	%fd1720, %fd2176, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1721, %fd2175, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1722, %fd1650, %fd1616, %fd1720;
	add.f64 	%fd1723, %fd2174, %fd1722;
	mul.f64 	%fd1724, %fd2178, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1725, %fd2177, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1726, %fd1655, %fd1611, %fd1724;
	add.f64 	%fd1727, %fd1723, %fd1726;
	mul.f64 	%fd1728, %fd2180, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1729, %fd2179, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1730, %fd2180, 0dBFE9E3779B97F4A4, %fd1729;
	add.f64 	%fd1731, %fd1727, %fd1730;
	mul.f64 	%fd1732, %fd2182, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1733, %fd2181, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1734, %fd2182, 0d3FD3C6EF372FE948, %fd1733;
	add.f64 	%fd1735, %fd1731, %fd1734;
	add.f64 	%fd1736, %fd1735, %fd2184;
	mul.f64 	%fd1737, %fd2186, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1738, %fd2185, 0d3FEE6F0E13445503;
	neg.f64 	%fd1739, %fd2185;
	fma.rn.f64 	%fd1740, %fd1739, %fd1616, %fd1737;
	add.f64 	%fd1741, %fd1736, %fd1740;
	mul.f64 	%fd1742, %fd2188, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1743, %fd2187, 0d3FE2CF2304755A5D;
	neg.f64 	%fd1744, %fd2187;
	fma.rn.f64 	%fd1745, %fd1744, %fd1611, %fd1742;
	add.f64 	%fd1746, %fd1741, %fd1745;
	mul.f64 	%fd1747, %fd2190, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd1748, %fd2189, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd1749, %fd2190, 0dBFE9E3779B97F4A4, %fd1748;
	add.f64 	%fd1750, %fd1746, %fd1749;
	mul.f64 	%fd1751, %fd2192, 0d3FD3C6EF372FE948;
	mul.f64 	%fd1752, %fd2191, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd1753, %fd2192, 0d3FD3C6EF372FE948, %fd1752;
	add.f64 	%fd1754, %fd1750, %fd1753;
	st.global.f64 	[%r40+8], %fd1754;
	mul.f64 	%fd1755, %fd2175, 0dBFD3C6EF372FE948;
	fma.rn.f64 	%fd1756, %fd2175, 0dBFD3C6EF372FE948, %fd1686;
	add.f64 	%fd1757, %fd2173, %fd1756;
	neg.f64 	%fd1758, %fd2178;
	fma.rn.f64 	%fd1759, %fd1758, %fd1611, %fd1689;
	add.f64 	%fd1760, %fd1757, %fd1759;
	mul.f64 	%fd1761, %fd2179, 0d3FE9E3779B97F4A4;
	fma.rn.f64 	%fd1762, %fd1695, %fd1611, %fd1761;
	add.f64 	%fd1763, %fd1760, %fd1762;
	fma.rn.f64 	%fd1764, %fd2181, 0d3FD3C6EF372FE948, %fd1699;
	add.f64 	%fd1765, %fd1763, %fd1764;
	sub.f64 	%fd1766, %fd1765, %fd2183;
	fma.rn.f64 	%fd1767, %fd1630, %fd1616, %fd1704;
	add.f64 	%fd1768, %fd1766, %fd1767;
	mul.f64 	%fd1769, %fd2187, 0d3FE9E3779B97F4A4;
	fma.rn.f64 	%fd1770, %fd2187, 0d3FE9E3779B97F4A4, %fd1709;
	add.f64 	%fd1771, %fd1768, %fd1770;
	fma.rn.f64 	%fd1772, %fd2189, 0dBFE9E3779B97F4A4, %fd1713;
	add.f64 	%fd1773, %fd1771, %fd1772;
	mul.f64 	%fd1774, %fd2191, 0dBFD3C6EF372FE948;
	fma.rn.f64 	%fd1775, %fd1645, %fd1616, %fd1774;
	add.f64 	%fd1776, %fd1773, %fd1775;
	st.global.f64 	[%r41], %fd1776;
	mul.f64 	%fd1777, %fd2176, 0dBFD3C6EF372FE948;
	fma.rn.f64 	%fd1778, %fd1650, %fd1616, %fd1777;
	add.f64 	%fd1779, %fd2174, %fd1778;
	fma.rn.f64 	%fd1780, %fd2178, 0dBFE9E3779B97F4A4, %fd1725;
	add.f64 	%fd1781, %fd1779, %fd1780;
	mul.f64 	%fd1782, %fd2180, 0d3FE9E3779B97F4A4;
	fma.rn.f64 	%fd1783, %fd2180, 0d3FE9E3779B97F4A4, %fd1729;
	add.f64 	%fd1784, %fd1781, %fd1783;
	fma.rn.f64 	%fd1785, %fd1665, %fd1616, %fd1732;
	add.f64 	%fd1786, %fd1784, %fd1785;
	sub.f64 	%fd1787, %fd1786, %fd2184;
	fma.rn.f64 	%fd1788, %fd2186, 0d3FD3C6EF372FE948, %fd1738;
	add.f64 	%fd1789, %fd1787, %fd1788;
	mul.f64 	%fd1790, %fd2188, 0d3FE9E3779B97F4A4;
	fma.rn.f64 	%fd1791, %fd1744, %fd1611, %fd1790;
	add.f64 	%fd1792, %fd1789, %fd1791;
	neg.f64 	%fd1793, %fd2189;
	fma.rn.f64 	%fd1794, %fd1793, %fd1611, %fd1747;
	add.f64 	%fd1795, %fd1792, %fd1794;
	mul.f64 	%fd1796, %fd2192, 0dBFD3C6EF372FE948;
	fma.rn.f64 	%fd1797, %fd2192, 0dBFD3C6EF372FE948, %fd1752;
	add.f64 	%fd1798, %fd1795, %fd1797;
	st.global.f64 	[%r41+8], %fd1798;
	mul.f64 	%fd1799, %fd2175, 0dBFE9E3779B97F4A4;
	fma.rn.f64 	%fd1800, %fd2175, 0dBFE9E3779B97F4A4, %fd1610;
	add.f64 	%fd1801, %fd2173, %fd1800;
	fma.rn.f64 	%fd1802, %fd1758, %fd1616, %fd1614;
	add.f64 	%fd1803, %fd1801, %fd1802;
	mul.f64 	%fd1804, %fd2179, 0d3FD3C6EF372FE948;
	fma.rn.f64 	%fd1805, %fd2179, 0d3FD3C6EF372FE948, %fd1620;
	add.f64 	%fd1806, %fd1803, %fd1805;
	fma.rn.f64 	%fd1807, %fd1700, %fd1611, %fd1623;
	add.f64 	%fd1808, %fd1806, %fd1807;
	add.f64 	%fd1809, %fd1808, %fd2183;
	fma.rn.f64 	%fd1810, %fd2185, 0dBFE9E3779B97F4A4, %fd1629;
	add.f64 	%fd1811, %fd1809, %fd1810;
	mul.f64 	%fd1812, %fd2187, 0d3FD3C6EF372FE948;
	fma.rn.f64 	%fd1813, %fd1635, %fd1616, %fd1812;
	add.f64 	%fd1814, %fd1811, %fd1813;
	fma.rn.f64 	%fd1815, %fd2189, 0d3FD3C6EF372FE948, %fd1639;
	add.f64 	%fd1816, %fd1814, %fd1815;
	mul.f64 	%fd1817, %fd2191, 0dBFE9E3779B97F4A4;
	fma.rn.f64 	%fd1818, %fd1645, %fd1611, %fd1817;
	add.f64 	%fd1819, %fd1816, %fd1818;
	st.global.f64 	[%r42], %fd1819;
	mul.f64 	%fd1820, %fd2176, 0dBFE9E3779B97F4A4;
	fma.rn.f64 	%fd1821, %fd1650, %fd1611, %fd1820;
	add.f64 	%fd1822, %fd2174, %fd1821;
	fma.rn.f64 	%fd1823, %fd2178, 0d3FD3C6EF372FE948, %fd1654;
	add.f64 	%fd1824, %fd1822, %fd1823;
	mul.f64 	%fd1825, %fd2180, 0d3FD3C6EF372FE948;
	fma.rn.f64 	%fd1826, %fd1660, %fd1616, %fd1825;
	add.f64 	%fd1827, %fd1824, %fd1826;
	fma.rn.f64 	%fd1828, %fd2182, 0dBFE9E3779B97F4A4, %fd1664;
	add.f64 	%fd1829, %fd1827, %fd1828;
	add.f64 	%fd1830, %fd1829, %fd2184;
	fma.rn.f64 	%fd1831, %fd1739, %fd1611, %fd1669;
	add.f64 	%fd1832, %fd1830, %fd1831;
	mul.f64 	%fd1833, %fd2188, 0d3FD3C6EF372FE948;
	fma.rn.f64 	%fd1834, %fd2188, 0d3FD3C6EF372FE948, %fd1674;
	add.f64 	%fd1835, %fd1832, %fd1834;
	fma.rn.f64 	%fd1836, %fd1793, %fd1616, %fd1677;
	add.f64 	%fd1837, %fd1835, %fd1836;
	mul.f64 	%fd1838, %fd2192, 0dBFE9E3779B97F4A4;
	fma.rn.f64 	%fd1839, %fd2192, 0dBFE9E3779B97F4A4, %fd1682;
	add.f64 	%fd1840, %fd1837, %fd1839;
	st.global.f64 	[%r42+8], %fd1840;
	sub.f64 	%fd1841, %fd2173, %fd2175;
	add.f64 	%fd1842, %fd1841, %fd2177;
	sub.f64 	%fd1843, %fd1842, %fd2179;
	add.f64 	%fd1844, %fd1843, %fd2181;
	sub.f64 	%fd1845, %fd1844, %fd2183;
	add.f64 	%fd1846, %fd1845, %fd2185;
	sub.f64 	%fd1847, %fd1846, %fd2187;
	add.f64 	%fd1848, %fd1847, %fd2189;
	sub.f64 	%fd1849, %fd1848, %fd2191;
	st.global.f64 	[%r43], %fd1849;
	sub.f64 	%fd1850, %fd2174, %fd2176;
	add.f64 	%fd1851, %fd1850, %fd2178;
	sub.f64 	%fd1852, %fd1851, %fd2180;
	add.f64 	%fd1853, %fd1852, %fd2182;
	sub.f64 	%fd1854, %fd1853, %fd2184;
	add.f64 	%fd1855, %fd1854, %fd2186;
	sub.f64 	%fd1856, %fd1855, %fd2188;
	add.f64 	%fd1857, %fd1856, %fd2190;
	sub.f64 	%fd1858, %fd1857, %fd2192;
	st.global.f64 	[%r43+8], %fd1858;
	neg.f64 	%fd1859, %fd2176;
	fma.rn.f64 	%fd1860, %fd1859, %fd1611, %fd1799;
	add.f64 	%fd1861, %fd2173, %fd1860;
	add.f64 	%fd1862, %fd1861, %fd1617;
	fma.rn.f64 	%fd1863, %fd1695, %fd1616, %fd1804;
	add.f64 	%fd1864, %fd1862, %fd1863;
	add.f64 	%fd1865, %fd1864, %fd1625;
	add.f64 	%fd1866, %fd1865, %fd2183;
	add.f64 	%fd1867, %fd1866, %fd1631;
	fma.rn.f64 	%fd1868, %fd2187, 0d3FD3C6EF372FE948, %fd1634;
	add.f64 	%fd1869, %fd1867, %fd1868;
	add.f64 	%fd1870, %fd1869, %fd1641;
	fma.rn.f64 	%fd1871, %fd2191, 0dBFE9E3779B97F4A4, %fd1644;
	add.f64 	%fd1872, %fd1870, %fd1871;
	st.global.f64 	[%r44], %fd1872;
	fma.rn.f64 	%fd1873, %fd2176, 0dBFE9E3779B97F4A4, %fd1649;
	add.f64 	%fd1874, %fd2174, %fd1873;
	add.f64 	%fd1875, %fd1874, %fd1656;
	fma.rn.f64 	%fd1876, %fd2180, 0d3FD3C6EF372FE948, %fd1659;
	add.f64 	%fd1877, %fd1875, %fd1876;
	add.f64 	%fd1878, %fd1877, %fd1666;
	add.f64 	%fd1879, %fd1878, %fd2184;
	add.f64 	%fd1880, %fd1879, %fd1671;
	fma.rn.f64 	%fd1881, %fd1744, %fd1616, %fd1833;
	add.f64 	%fd1882, %fd1880, %fd1881;
	add.f64 	%fd1883, %fd1882, %fd1679;
	neg.f64 	%fd1884, %fd2191;
	fma.rn.f64 	%fd1885, %fd1884, %fd1611, %fd1838;
	add.f64 	%fd1886, %fd1883, %fd1885;
	st.global.f64 	[%r44+8], %fd1886;
	fma.rn.f64 	%fd1887, %fd1859, %fd1616, %fd1755;
	add.f64 	%fd1888, %fd2173, %fd1887;
	add.f64 	%fd1889, %fd1888, %fd1691;
	fma.rn.f64 	%fd1890, %fd2179, 0d3FE9E3779B97F4A4, %fd1694;
	add.f64 	%fd1891, %fd1889, %fd1890;
	add.f64 	%fd1892, %fd1891, %fd1701;
	sub.f64 	%fd1893, %fd1892, %fd2183;
	add.f64 	%fd1894, %fd1893, %fd1706;
	fma.rn.f64 	%fd1895, %fd1635, %fd1611, %fd1769;
	add.f64 	%fd1896, %fd1894, %fd1895;
	add.f64 	%fd1897, %fd1896, %fd1714;
	fma.rn.f64 	%fd1898, %fd2191, 0dBFD3C6EF372FE948, %fd1717;
	add.f64 	%fd1899, %fd1897, %fd1898;
	st.global.f64 	[%r45], %fd1899;
	fma.rn.f64 	%fd1900, %fd2176, 0dBFD3C6EF372FE948, %fd1721;
	add.f64 	%fd1901, %fd2174, %fd1900;
	add.f64 	%fd1902, %fd1901, %fd1726;
	fma.rn.f64 	%fd1903, %fd1660, %fd1611, %fd1782;
	add.f64 	%fd1904, %fd1902, %fd1903;
	add.f64 	%fd1905, %fd1904, %fd1734;
	sub.f64 	%fd1906, %fd1905, %fd2184;
	add.f64 	%fd1907, %fd1906, %fd1740;
	fma.rn.f64 	%fd1908, %fd2188, 0d3FE9E3779B97F4A4, %fd1743;
	add.f64 	%fd1909, %fd1907, %fd1908;
	add.f64 	%fd1910, %fd1909, %fd1749;
	fma.rn.f64 	%fd1911, %fd1884, %fd1616, %fd1796;
	add.f64 	%fd1912, %fd1910, %fd1911;
	st.global.f64 	[%r45+8], %fd1912;
	fma.rn.f64 	%fd1913, %fd1859, %fd1616, %fd1685;
	add.f64 	%fd1914, %fd2173, %fd1913;
	add.f64 	%fd1915, %fd1914, %fd1759;
	fma.rn.f64 	%fd1916, %fd2179, 0dBFE9E3779B97F4A4, %fd1694;
	add.f64 	%fd1917, %fd1915, %fd1916;
	add.f64 	%fd1918, %fd1917, %fd1764;
	add.f64 	%fd1919, %fd1918, %fd2183;
	add.f64 	%fd1920, %fd1919, %fd1767;
	fma.rn.f64 	%fd1921, %fd1635, %fd1611, %fd1708;
	add.f64 	%fd1922, %fd1920, %fd1921;
	add.f64 	%fd1923, %fd1922, %fd1772;
	fma.rn.f64 	%fd1924, %fd2191, 0d3FD3C6EF372FE948, %fd1717;
	add.f64 	%fd1925, %fd1923, %fd1924;
	st.global.f64 	[%r46], %fd1925;
	fma.rn.f64 	%fd1926, %fd2176, 0d3FD3C6EF372FE948, %fd1721;
	add.f64 	%fd1927, %fd2174, %fd1926;
	add.f64 	%fd1928, %fd1927, %fd1780;
	fma.rn.f64 	%fd1929, %fd1660, %fd1611, %fd1728;
	add.f64 	%fd1930, %fd1928, %fd1929;
	add.f64 	%fd1931, %fd1930, %fd1785;
	add.f64 	%fd1932, %fd1931, %fd2184;
	add.f64 	%fd1933, %fd1932, %fd1788;
	fma.rn.f64 	%fd1934, %fd2188, 0dBFE9E3779B97F4A4, %fd1743;
	add.f64 	%fd1935, %fd1933, %fd1934;
	add.f64 	%fd1936, %fd1935, %fd1794;
	fma.rn.f64 	%fd1937, %fd1884, %fd1616, %fd1751;
	add.f64 	%fd1938, %fd1936, %fd1937;
	st.global.f64 	[%r46+8], %fd1938;
	fma.rn.f64 	%fd1939, %fd1859, %fd1611, %fd1609;
	add.f64 	%fd1940, %fd2173, %fd1939;
	add.f64 	%fd1941, %fd1940, %fd1802;
	fma.rn.f64 	%fd1942, %fd1695, %fd1616, %fd1619;
	add.f64 	%fd1943, %fd1941, %fd1942;
	add.f64 	%fd1944, %fd1943, %fd1807;
	sub.f64 	%fd1945, %fd1944, %fd2183;
	add.f64 	%fd1946, %fd1945, %fd1810;
	fma.rn.f64 	%fd1947, %fd2187, 0dBFD3C6EF372FE948, %fd1634;
	add.f64 	%fd1948, %fd1946, %fd1947;
	add.f64 	%fd1949, %fd1948, %fd1815;
	fma.rn.f64 	%fd1950, %fd2191, 0d3FE9E3779B97F4A4, %fd1644;
	add.f64 	%fd1951, %fd1949, %fd1950;
	st.global.f64 	[%r47], %fd1951;
	fma.rn.f64 	%fd1952, %fd2176, 0d3FE9E3779B97F4A4, %fd1649;
	add.f64 	%fd1953, %fd2174, %fd1952;
	add.f64 	%fd1954, %fd1953, %fd1823;
	fma.rn.f64 	%fd1955, %fd2180, 0dBFD3C6EF372FE948, %fd1659;
	add.f64 	%fd1956, %fd1954, %fd1955;
	add.f64 	%fd1957, %fd1956, %fd1828;
	sub.f64 	%fd1958, %fd1957, %fd2184;
	add.f64 	%fd1959, %fd1958, %fd1831;
	fma.rn.f64 	%fd1960, %fd1744, %fd1616, %fd1673;
	add.f64 	%fd1961, %fd1959, %fd1960;
	add.f64 	%fd1962, %fd1961, %fd1836;
	fma.rn.f64 	%fd1963, %fd1884, %fd1611, %fd1681;
	add.f64 	%fd1964, %fd1962, %fd1963;
	st.global.f64 	[%r47+8], %fd1964;
	ret;
}

.entry DIT10C2C(
	.param .u32 .ptr .global .align 8 DIT10C2C_param_0,
	.param .u32 DIT10C2C_param_1,
	.param .u32 DIT10C2C_param_2,
	.param .u32 DIT10C2C_param_3
)
{
	.local .align 8 .b8 	__local_depot3[40];
	.reg .b32 	%SP;
	.reg .f64 	%fd<2717>;
	.reg .pred 	%p<378>;
	.reg .s32 	%r<1201>;
	.reg .s64 	%rl<1533>;


	mov.u32 	%SP, __local_depot3;
	ld.param.u32 	%r347, [DIT10C2C_param_2];
	// inline asm
	mov.u32 	%r343, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r344, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r345, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r346, %tid.x;
	// inline asm
	add.s32 	%r348, %r346, %r343;
	mad.lo.s32 	%r3, %r345, %r344, %r348;
	mul.hi.u32 	%r349, %r347, -1431655765;
	shr.u32 	%r4, %r349, 1;
	mul.lo.s32 	%r350, %r4, 3;
	sub.s32 	%r5, %r347, %r350;
	setp.gt.u32 	%p17, %r347, 2;
	@%p17 bra 	BB3_2;

	mov.f64 	%fd2620, 0d3FF0000000000000;
	bra.uni 	BB3_24;

BB3_2:
	mov.f64 	%fd1, 0d4008000000000000;
	mov.f64 	%fd3, 0d7FF0000000000000;
	mov.pred 	%p2, 0;
	mov.f64 	%fd4, 0dFFF0000000000000;
	mov.f64 	%fd5, 0d3FB5555555555555;
	mov.f64 	%fd6, 0dBC46A4CB00B9E7B0;
	mov.u32 	%r1138, 1;
	mov.u32 	%r1137, 0;

BB3_3:
	@%p2 bra 	BB3_21;

	@%p2 bra 	BB3_20;

	mov.f64 	%fd266, 0d4024000000000000;
	// inline asm
	abs.f64 	%fd265, %fd266;
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r353}, %fd265; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r354, hi}, %fd265; 
	}
	// inline asm
	shr.u32 	%r355, %r353, 20;
	and.b32  	%r1141, %r355, 2047;
	setp.eq.s32 	%p18, %r1141, 0;
	mov.u32 	%r1139, %r354;
	mov.u32 	%r1140, %r353;
	@%p18 bra 	BB3_6;
	bra.uni 	BB3_7;

BB3_6:
	mov.f64 	%fd271, 0d4350000000000000;
	mul.rn.f64 	%fd270, %fd265, %fd271;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r356}, %fd270; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r357, hi}, %fd270; 
	}
	// inline asm
	shr.u32 	%r358, %r356, 20;
	and.b32  	%r359, %r358, 2047;
	add.s32 	%r1141, %r359, -54;
	mov.u32 	%r1139, %r357;
	mov.u32 	%r1140, %r356;

BB3_7:
	add.s32 	%r1142, %r1141, -1023;
	and.b32  	%r362, %r1140, -2146435073;
	or.b32  	%r361, %r362, 1072693248;
	// inline asm
	mov.b64 	%fd272, {%r1139, %r361};
	// inline asm
	setp.gt.u32 	%p19, %r361, 1073127582;
	mov.f64 	%fd2614, %fd272;
	@%p19 bra 	BB3_8;
	bra.uni 	BB3_9;

BB3_8:
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r363, hi}, %fd272; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r364}, %fd272; 
	}
	// inline asm
	add.s32 	%r366, %r364, -1048576;
	// inline asm
	mov.b64 	%fd275, {%r363, %r366};
	// inline asm
	add.s32 	%r1142, %r1141, -1022;
	mov.f64 	%fd2614, %fd275;

BB3_9:
	add.f64 	%fd354, %fd2614, 0d3FF0000000000000;
	rcp.rn.f64 	%fd355, %fd354;
	add.f64 	%fd302, %fd2614, 0dBFF0000000000000;
	mul.rn.f64 	%fd356, %fd302, %fd355;
	add.f64 	%fd350, %fd356, %fd356;
	mul.rn.f64 	%fd298, %fd350, %fd350;
	mov.f64 	%fd277, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd279, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd276, %fd277, %fd298, %fd279;
	// inline asm
	mov.f64 	%fd283, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd280, %fd276, %fd298, %fd283;
	// inline asm
	mov.f64 	%fd287, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd284, %fd280, %fd298, %fd287;
	// inline asm
	mov.f64 	%fd291, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd288, %fd284, %fd298, %fd291;
	// inline asm
	mov.f64 	%fd295, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd292, %fd288, %fd298, %fd295;
	// inline asm
	mov.f64 	%fd299, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd296, %fd292, %fd298, %fd299;
	// inline asm
	mul.rn.f64 	%fd357, %fd296, %fd298;
	sub.f64 	%fd358, %fd302, %fd350;
	mov.f64 	%fd359, 0d4000000000000000;
	mul.rn.f64 	%fd303, %fd359, %fd358;
	neg.f64 	%fd301, %fd350;
	// inline asm
	fma.rn.f64 	%fd300, %fd301, %fd302, %fd303;
	// inline asm
	mul.rn.f64 	%fd346, %fd355, %fd300;
	add.f64 	%fd360, %fd5, %fd357;
	sub.f64 	%fd361, %fd5, %fd360;
	add.f64 	%fd362, %fd361, %fd357;
	add.f64 	%fd363, %fd362, 0d0000000000000000;
	add.f64 	%fd364, %fd363, %fd6;
	add.f64 	%fd313, %fd360, %fd364;
	sub.f64 	%fd365, %fd360, %fd313;
	add.f64 	%fd317, %fd365, %fd364;
	mul.rn.f64 	%fd366, %fd313, %fd350;
	neg.f64 	%fd307, %fd366;
	// inline asm
	fma.rn.f64 	%fd304, %fd313, %fd350, %fd307;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd308, %fd317, %fd346, %fd304;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd312, %fd313, %fd346, %fd308;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd316, %fd317, %fd350, %fd312;
	// inline asm
	add.f64 	%fd329, %fd366, %fd316;
	sub.f64 	%fd367, %fd366, %fd329;
	add.f64 	%fd333, %fd367, %fd316;
	mul.rn.f64 	%fd368, %fd329, %fd350;
	neg.f64 	%fd323, %fd368;
	// inline asm
	fma.rn.f64 	%fd320, %fd329, %fd350, %fd323;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd324, %fd333, %fd346, %fd320;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd328, %fd329, %fd346, %fd324;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd332, %fd333, %fd350, %fd328;
	// inline asm
	add.f64 	%fd345, %fd368, %fd332;
	sub.f64 	%fd369, %fd368, %fd345;
	add.f64 	%fd349, %fd369, %fd332;
	mul.rn.f64 	%fd370, %fd345, %fd350;
	neg.f64 	%fd339, %fd370;
	// inline asm
	fma.rn.f64 	%fd336, %fd345, %fd350, %fd339;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd340, %fd349, %fd346, %fd336;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd344, %fd345, %fd346, %fd340;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd348, %fd349, %fd350, %fd344;
	// inline asm
	add.f64 	%fd371, %fd370, %fd348;
	sub.f64 	%fd372, %fd370, %fd371;
	add.f64 	%fd373, %fd372, %fd348;
	add.f64 	%fd374, %fd350, %fd371;
	sub.f64 	%fd375, %fd350, %fd374;
	add.f64 	%fd376, %fd375, %fd371;
	add.f64 	%fd377, %fd376, %fd373;
	add.f64 	%fd378, %fd377, %fd346;
	add.f64 	%fd379, %fd374, %fd378;
	sub.f64 	%fd380, %fd374, %fd379;
	add.f64 	%fd381, %fd380, %fd378;
	cvt.rn.f64.s32 	%fd382, %r1142;
	mov.f64 	%fd383, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd384, %fd382, %fd383;
	mov.f64 	%fd385, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd386, %fd382, %fd385;
	add.f64 	%fd387, %fd384, %fd379;
	sub.f64 	%fd388, %fd384, %fd387;
	add.f64 	%fd389, %fd388, %fd379;
	add.f64 	%fd390, %fd389, %fd381;
	add.f64 	%fd391, %fd390, %fd386;
	add.f64 	%fd11, %fd387, %fd391;
	sub.f64 	%fd392, %fd387, %fd11;
	add.f64 	%fd12, %fd392, %fd391;
	// inline asm
	abs.f64 	%fd352, %fd1;
	// inline asm
	setp.gt.f64 	%p20, %fd352, 0d7F0D2A1BE4048F90;
	@%p20 bra 	BB3_11;

	mov.f64 	%fd2615, %fd1;
	bra.uni 	BB3_12;

BB3_11:
	mov.f64 	%fd393, 0d3F20000000000000;
	mul.rn.f64 	%fd13, %fd1, %fd393;
	mov.f64 	%fd2615, %fd13;

BB3_12:
	mov.f64 	%fd14, %fd2615;
	mul.rn.f64 	%fd403, %fd11, %fd14;
	neg.f64 	%fd397, %fd403;
	// inline asm
	fma.rn.f64 	%fd394, %fd11, %fd14, %fd397;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd398, %fd12, %fd14, %fd394;
	// inline asm
	add.f64 	%fd402, %fd403, %fd398;
	sub.f64 	%fd404, %fd403, %fd402;
	add.f64 	%fd16, %fd404, %fd398;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r367}, %fd402; 
	}
	// inline asm
	setp.lt.u32 	%p21, %r367, 1082535491;
	setp.lt.s32 	%p22, %r367, -1064875759;
	or.pred  	%p23, %p21, %p22;
	@%p23 bra 	BB3_14;

	setp.lt.s32 	%p24, %r367, 0;
	selp.f64 	%fd405, 0d0000000000000000, %fd3, %p24;
	add.f64 	%fd406, %fd402, %fd402;
	setp.nan.f64 	%p25, %fd402, %fd402;
	selp.f64 	%fd2619, %fd406, %fd405, %p25;
	bra.uni 	BB3_17;

BB3_14:
	mov.f64 	%fd409, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd408, %fd402, %fd409;
	// inline asm
	abs.f64 	%fd407, %fd408;
	// inline asm
	setp.gt.f64 	%p26, %fd407, 0d4330000000000000;
	mov.f64 	%fd2616, %fd408;
	@%p26 bra 	BB3_16;

	add.f64 	%fd411, %fd407, 0d3FE0000000000000;
	// inline asm
	cvt.rmi.f64.f64 	%fd410, %fd411;
	// inline asm
	setp.lt.f64 	%p27, %fd407, 0d3FE0000000000000;
	selp.f64 	%fd414, 0d0000000000000000, %fd410, %p27;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r368}, %fd408; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r369, hi}, %fd414; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r370}, %fd414; 
	}
	// inline asm
	and.b32  	%r373, %r368, -2147483648;
	and.b32  	%r374, %r370, 2147483647;
	or.b32  	%r372, %r374, %r373;
	// inline asm
	mov.b64 	%fd415, {%r369, %r372};
	// inline asm
	mov.f64 	%fd2616, %fd415;

BB3_16:
	mov.f64 	%fd418, 0dBFE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd416, %fd2616, %fd418, %fd402;
	// inline asm
	mov.f64 	%fd422, 0dBC7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd420, %fd2616, %fd422, %fd416;
	// inline asm
	cvt.rzi.s32.f64 	%r377, %fd2616;
	add.s32 	%r378, %r377, 55;
	setp.lt.s32 	%p28, %r377, -1020;
	selp.b32 	%r379, %r378, %r377, %p28;
	selp.f64 	 %fd473, 0d3C90000000000000, 0d4000000000000000, %p28;
	mov.f64 	%fd425, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd427, 0d3E5AFD81DA6C3BAF;
	// inline asm
	fma.rn.f64 	%fd424, %fd425, %fd420, %fd427;
	// inline asm
	mov.f64 	%fd431, 0d3E927E55F60F80E6;
	// inline asm
	fma.rn.f64 	%fd428, %fd424, %fd420, %fd431;
	// inline asm
	mov.f64 	%fd435, 0d3EC71DDA8F02D666;
	// inline asm
	fma.rn.f64 	%fd432, %fd428, %fd420, %fd435;
	// inline asm
	mov.f64 	%fd439, 0d3EFA01A013B894E0;
	// inline asm
	fma.rn.f64 	%fd436, %fd432, %fd420, %fd439;
	// inline asm
	mov.f64 	%fd443, 0d3F2A01A01D3AF788;
	// inline asm
	fma.rn.f64 	%fd440, %fd436, %fd420, %fd443;
	// inline asm
	mov.f64 	%fd447, 0d3F56C16C16C3A1EC;
	// inline asm
	fma.rn.f64 	%fd444, %fd440, %fd420, %fd447;
	// inline asm
	mov.f64 	%fd451, 0d3F81111111109161;
	// inline asm
	fma.rn.f64 	%fd448, %fd444, %fd420, %fd451;
	// inline asm
	mov.f64 	%fd455, 0d3FA55555555554C1;
	// inline asm
	fma.rn.f64 	%fd452, %fd448, %fd420, %fd455;
	// inline asm
	mov.f64 	%fd459, 0d3FC555555555556F;
	// inline asm
	fma.rn.f64 	%fd456, %fd452, %fd420, %fd459;
	// inline asm
	mov.f64 	%fd463, 0d3FE0000000000000;
	// inline asm
	fma.rn.f64 	%fd460, %fd456, %fd420, %fd463;
	// inline asm
	mul.rn.f64 	%fd465, %fd460, %fd420;
	// inline asm
	fma.rn.f64 	%fd464, %fd465, %fd420, %fd420;
	// inline asm
	shl.b32 	%r380, %r379, 20;
	add.s32 	%r376, %r380, 1071644672;
	mov.u32 	%r375, 0;
	// inline asm
	mov.b64 	%fd468, {%r375, %r376};
	// inline asm
	// inline asm
	fma.rn.f64 	%fd469, %fd464, %fd468, %fd468;
	// inline asm
	mul.rn.f64 	%fd2619, %fd469, %fd473;

BB3_17:
	mov.f64 	%fd23, %fd2619;
	setp.eq.f64 	%p29, %fd23, %fd4;
	setp.eq.f64 	%p30, %fd23, %fd3;
	or.pred  	%p31, %p30, %p29;
	@%p31 bra 	BB3_19;

	// inline asm
	fma.rn.f64 	%fd474, %fd23, %fd16, %fd23;
	// inline asm
	mov.f64 	%fd24, %fd474;
	mov.f64 	%fd2618, %fd24;
	bra.uni 	BB3_22;

BB3_19:
	mov.f64 	%fd2618, %fd23;
	bra.uni 	BB3_22;

BB3_20:
	mov.f64 	%fd2618, %fd478;
	bra.uni 	BB3_22;

BB3_21:
	mov.f64 	%fd2618, %fd3;

BB3_22:
	mov.f64 	%fd25, %fd2618;
	cvt.rn.f64.s32 	%fd479, %r1138;
	mul.f64 	%fd480, %fd479, %fd25;
	cvt.rzi.s32.f64 	%r1138, %fd480;
	add.s32 	%r1137, %r1137, 1;
	setp.lt.s32 	%p32, %r1137, %r4;
	@%p32 bra 	BB3_3;

	cvt.rn.f64.s32 	%fd2620, %r1138;

BB3_24:
	cvt.rn.f64.s32 	%fd2622, %r5;
	setp.eq.s32 	%p33, %r5, 0;
	@%p33 bra 	BB3_42;

	mov.f64 	%fd29, 0d7FF0000000000000;
	mov.pred 	%p34, 0;
	@%p34 bra 	BB3_43;

	setp.eq.f64 	%p35, %fd2622, %fd29;
	mov.f64 	%fd30, 0dFFF0000000000000;
	setp.eq.f64 	%p36, %fd2622, 0dFFF0000000000000;
	or.pred  	%p37, %p35, %p36;
	@%p37 bra 	BB3_41;

	neg.f64 	%fd485, %fd29;
	setp.eq.f64 	%p38, %fd485, 0d4024000000000000;
	@%p38 bra 	BB3_43;

	mov.f64 	%fd487, 0d4024000000000000;
	// inline asm
	abs.f64 	%fd486, %fd487;
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r381}, %fd486; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r382, hi}, %fd486; 
	}
	// inline asm
	shr.u32 	%r383, %r381, 20;
	and.b32  	%r1145, %r383, 2047;
	setp.eq.s32 	%p39, %r1145, 0;
	mov.u32 	%r1143, %r382;
	mov.u32 	%r1144, %r381;
	@%p39 bra 	BB3_29;
	bra.uni 	BB3_30;

BB3_29:
	mov.f64 	%fd492, 0d4350000000000000;
	mul.rn.f64 	%fd491, %fd486, %fd492;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r384}, %fd491; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r385, hi}, %fd491; 
	}
	// inline asm
	shr.u32 	%r386, %r384, 20;
	and.b32  	%r387, %r386, 2047;
	add.s32 	%r1145, %r387, -54;
	mov.u32 	%r1143, %r385;
	mov.u32 	%r1144, %r384;

BB3_30:
	add.s32 	%r1146, %r1145, -1023;
	and.b32  	%r390, %r1144, -2146435073;
	or.b32  	%r389, %r390, 1072693248;
	// inline asm
	mov.b64 	%fd493, {%r1143, %r389};
	// inline asm
	setp.gt.u32 	%p40, %r389, 1073127582;
	mov.f64 	%fd2621, %fd493;
	@%p40 bra 	BB3_31;
	bra.uni 	BB3_32;

BB3_31:
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r391, hi}, %fd493; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r392}, %fd493; 
	}
	// inline asm
	add.s32 	%r394, %r392, -1048576;
	// inline asm
	mov.b64 	%fd496, {%r391, %r394};
	// inline asm
	add.s32 	%r1146, %r1145, -1022;
	mov.f64 	%fd2621, %fd496;

BB3_32:
	add.f64 	%fd575, %fd2621, 0d3FF0000000000000;
	rcp.rn.f64 	%fd576, %fd575;
	add.f64 	%fd523, %fd2621, 0dBFF0000000000000;
	mul.rn.f64 	%fd577, %fd523, %fd576;
	add.f64 	%fd571, %fd577, %fd577;
	mul.rn.f64 	%fd519, %fd571, %fd571;
	mov.f64 	%fd498, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd500, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd497, %fd498, %fd519, %fd500;
	// inline asm
	mov.f64 	%fd504, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd501, %fd497, %fd519, %fd504;
	// inline asm
	mov.f64 	%fd508, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd505, %fd501, %fd519, %fd508;
	// inline asm
	mov.f64 	%fd512, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd509, %fd505, %fd519, %fd512;
	// inline asm
	mov.f64 	%fd516, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd513, %fd509, %fd519, %fd516;
	// inline asm
	mov.f64 	%fd520, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd517, %fd513, %fd519, %fd520;
	// inline asm
	mul.rn.f64 	%fd578, %fd517, %fd519;
	sub.f64 	%fd579, %fd523, %fd571;
	mov.f64 	%fd580, 0d4000000000000000;
	mul.rn.f64 	%fd524, %fd580, %fd579;
	neg.f64 	%fd522, %fd571;
	// inline asm
	fma.rn.f64 	%fd521, %fd522, %fd523, %fd524;
	// inline asm
	mul.rn.f64 	%fd567, %fd576, %fd521;
	mov.f64 	%fd581, 0d3FB5555555555555;
	add.f64 	%fd582, %fd578, 0d3FB5555555555555;
	sub.f64 	%fd583, %fd581, %fd582;
	add.f64 	%fd584, %fd583, %fd578;
	add.f64 	%fd585, %fd584, 0d0000000000000000;
	add.f64 	%fd586, %fd585, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd534, %fd582, %fd586;
	sub.f64 	%fd587, %fd582, %fd534;
	add.f64 	%fd538, %fd587, %fd586;
	mul.rn.f64 	%fd588, %fd534, %fd571;
	neg.f64 	%fd528, %fd588;
	// inline asm
	fma.rn.f64 	%fd525, %fd534, %fd571, %fd528;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd529, %fd538, %fd567, %fd525;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd533, %fd534, %fd567, %fd529;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd537, %fd538, %fd571, %fd533;
	// inline asm
	add.f64 	%fd550, %fd588, %fd537;
	sub.f64 	%fd589, %fd588, %fd550;
	add.f64 	%fd554, %fd589, %fd537;
	mul.rn.f64 	%fd590, %fd550, %fd571;
	neg.f64 	%fd544, %fd590;
	// inline asm
	fma.rn.f64 	%fd541, %fd550, %fd571, %fd544;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd545, %fd554, %fd567, %fd541;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd549, %fd550, %fd567, %fd545;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd553, %fd554, %fd571, %fd549;
	// inline asm
	add.f64 	%fd566, %fd590, %fd553;
	sub.f64 	%fd591, %fd590, %fd566;
	add.f64 	%fd570, %fd591, %fd553;
	mul.rn.f64 	%fd592, %fd566, %fd571;
	neg.f64 	%fd560, %fd592;
	// inline asm
	fma.rn.f64 	%fd557, %fd566, %fd571, %fd560;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd561, %fd570, %fd567, %fd557;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd565, %fd566, %fd567, %fd561;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd569, %fd570, %fd571, %fd565;
	// inline asm
	add.f64 	%fd593, %fd592, %fd569;
	sub.f64 	%fd594, %fd592, %fd593;
	add.f64 	%fd595, %fd594, %fd569;
	add.f64 	%fd596, %fd571, %fd593;
	sub.f64 	%fd597, %fd571, %fd596;
	add.f64 	%fd598, %fd597, %fd593;
	add.f64 	%fd599, %fd598, %fd595;
	add.f64 	%fd600, %fd599, %fd567;
	add.f64 	%fd601, %fd596, %fd600;
	sub.f64 	%fd602, %fd596, %fd601;
	add.f64 	%fd603, %fd602, %fd600;
	cvt.rn.f64.s32 	%fd604, %r1146;
	mov.f64 	%fd605, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd606, %fd604, %fd605;
	mov.f64 	%fd607, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd608, %fd604, %fd607;
	add.f64 	%fd609, %fd606, %fd601;
	sub.f64 	%fd610, %fd606, %fd609;
	add.f64 	%fd611, %fd610, %fd601;
	add.f64 	%fd612, %fd611, %fd603;
	add.f64 	%fd613, %fd612, %fd608;
	add.f64 	%fd35, %fd609, %fd613;
	sub.f64 	%fd614, %fd609, %fd35;
	add.f64 	%fd36, %fd614, %fd613;
	// inline asm
	abs.f64 	%fd573, %fd2622;
	// inline asm
	setp.gt.f64 	%p41, %fd573, 0d7F0D2A1BE4048F90;
	@%p41 bra 	BB3_33;
	bra.uni 	BB3_34;

BB3_33:
	mov.f64 	%fd615, 0d3F20000000000000;
	mul.rn.f64 	%fd2622, %fd2622, %fd615;

BB3_34:
	mul.rn.f64 	%fd625, %fd35, %fd2622;
	neg.f64 	%fd619, %fd625;
	// inline asm
	fma.rn.f64 	%fd616, %fd35, %fd2622, %fd619;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd620, %fd36, %fd2622, %fd616;
	// inline asm
	add.f64 	%fd624, %fd625, %fd620;
	sub.f64 	%fd626, %fd625, %fd624;
	add.f64 	%fd40, %fd626, %fd620;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r395}, %fd624; 
	}
	// inline asm
	setp.lt.u32 	%p42, %r395, 1082535491;
	setp.lt.s32 	%p43, %r395, -1064875759;
	or.pred  	%p44, %p42, %p43;
	@%p44 bra 	BB3_36;

	setp.lt.s32 	%p45, %r395, 0;
	selp.f64 	%fd627, 0d0000000000000000, %fd29, %p45;
	add.f64 	%fd628, %fd624, %fd624;
	setp.nan.f64 	%p46, %fd624, %fd624;
	selp.f64 	%fd2624, %fd628, %fd627, %p46;
	bra.uni 	BB3_39;

BB3_36:
	mov.f64 	%fd631, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd630, %fd624, %fd631;
	// inline asm
	abs.f64 	%fd629, %fd630;
	// inline asm
	setp.gt.f64 	%p47, %fd629, 0d4330000000000000;
	mov.f64 	%fd2623, %fd630;
	@%p47 bra 	BB3_38;

	add.f64 	%fd633, %fd629, 0d3FE0000000000000;
	// inline asm
	cvt.rmi.f64.f64 	%fd632, %fd633;
	// inline asm
	setp.lt.f64 	%p48, %fd629, 0d3FE0000000000000;
	selp.f64 	%fd636, 0d0000000000000000, %fd632, %p48;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r396}, %fd630; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r397, hi}, %fd636; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r398}, %fd636; 
	}
	// inline asm
	and.b32  	%r401, %r396, -2147483648;
	and.b32  	%r402, %r398, 2147483647;
	or.b32  	%r400, %r402, %r401;
	// inline asm
	mov.b64 	%fd637, {%r397, %r400};
	// inline asm
	mov.f64 	%fd2623, %fd637;

BB3_38:
	mov.f64 	%fd640, 0dBFE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd638, %fd2623, %fd640, %fd624;
	// inline asm
	mov.f64 	%fd644, 0dBC7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd642, %fd2623, %fd644, %fd638;
	// inline asm
	cvt.rzi.s32.f64 	%r405, %fd2623;
	add.s32 	%r406, %r405, 55;
	setp.lt.s32 	%p49, %r405, -1020;
	selp.b32 	%r407, %r406, %r405, %p49;
	selp.f64 	 %fd695, 0d3C90000000000000, 0d4000000000000000, %p49;
	mov.f64 	%fd647, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd649, 0d3E5AFD81DA6C3BAF;
	// inline asm
	fma.rn.f64 	%fd646, %fd647, %fd642, %fd649;
	// inline asm
	mov.f64 	%fd653, 0d3E927E55F60F80E6;
	// inline asm
	fma.rn.f64 	%fd650, %fd646, %fd642, %fd653;
	// inline asm
	mov.f64 	%fd657, 0d3EC71DDA8F02D666;
	// inline asm
	fma.rn.f64 	%fd654, %fd650, %fd642, %fd657;
	// inline asm
	mov.f64 	%fd661, 0d3EFA01A013B894E0;
	// inline asm
	fma.rn.f64 	%fd658, %fd654, %fd642, %fd661;
	// inline asm
	mov.f64 	%fd665, 0d3F2A01A01D3AF788;
	// inline asm
	fma.rn.f64 	%fd662, %fd658, %fd642, %fd665;
	// inline asm
	mov.f64 	%fd669, 0d3F56C16C16C3A1EC;
	// inline asm
	fma.rn.f64 	%fd666, %fd662, %fd642, %fd669;
	// inline asm
	mov.f64 	%fd673, 0d3F81111111109161;
	// inline asm
	fma.rn.f64 	%fd670, %fd666, %fd642, %fd673;
	// inline asm
	mov.f64 	%fd677, 0d3FA55555555554C1;
	// inline asm
	fma.rn.f64 	%fd674, %fd670, %fd642, %fd677;
	// inline asm
	mov.f64 	%fd681, 0d3FC555555555556F;
	// inline asm
	fma.rn.f64 	%fd678, %fd674, %fd642, %fd681;
	// inline asm
	mov.f64 	%fd685, 0d3FE0000000000000;
	// inline asm
	fma.rn.f64 	%fd682, %fd678, %fd642, %fd685;
	// inline asm
	mul.rn.f64 	%fd687, %fd682, %fd642;
	// inline asm
	fma.rn.f64 	%fd686, %fd687, %fd642, %fd642;
	// inline asm
	shl.b32 	%r408, %r407, 20;
	add.s32 	%r404, %r408, 1071644672;
	mov.u32 	%r403, 0;
	// inline asm
	mov.b64 	%fd690, {%r403, %r404};
	// inline asm
	// inline asm
	fma.rn.f64 	%fd691, %fd686, %fd690, %fd690;
	// inline asm
	mul.rn.f64 	%fd2624, %fd691, %fd695;

BB3_39:
	setp.eq.f64 	%p50, %fd2624, %fd30;
	setp.eq.f64 	%p51, %fd2624, %fd29;
	or.pred  	%p52, %p51, %p50;
	@%p52 bra 	BB3_43;

	// inline asm
	fma.rn.f64 	%fd696, %fd2624, %fd40, %fd2624;
	// inline asm
	mov.f64 	%fd2624, %fd696;
	bra.uni 	BB3_43;

BB3_41:
	mov.f64 	%fd701, 0d4024000000000000;
	// inline asm
	abs.f64 	%fd700, %fd701;
	// inline asm
	setp.gt.f64 	%p53, %fd700, 0d3FF0000000000000;
	selp.f64 	%fd2624, %fd29, 0d0000000000000000, %p53;
	bra.uni 	BB3_43;

BB3_42:
	mov.f64 	%fd2624, 0d3FF0000000000000;

BB3_43:
	mul.f64 	%fd704, %fd2620, %fd2624;
	cvt.rzi.s32.f64 	%r36, %fd704;
	mul.hi.s32 	%r409, %r36, 1717986919;
	shr.u32 	%r410, %r409, 31;
	shr.s32 	%r411, %r409, 2;
	add.s32 	%r412, %r411, %r410;
	div.s32 	%r413, %r3, %r412;
	rem.s32 	%r37, %r3, %r412;
	mad.lo.s32 	%r414, %r413, %r36, %r37;
	shl.b32 	%r415, %r414, 4;
	ld.param.u32 	%r1127, [DIT10C2C_param_0];
	add.s32 	%r38, %r1127, %r415;
	ld.global.f64 	%fd705, [%r38];
	ld.global.f64 	%fd706, [%r38+8];
	shl.b32 	%r416, %r412, 4;
	add.s32 	%r39, %r38, %r416;
	ld.global.f64 	%fd51, [%r39];
	ld.global.f64 	%fd52, [%r39+8];
	add.s32 	%r40, %r39, %r416;
	ld.global.f64 	%fd53, [%r40];
	ld.global.f64 	%fd54, [%r40+8];
	add.s32 	%r41, %r40, %r416;
	ld.global.f64 	%fd55, [%r41];
	ld.global.f64 	%fd56, [%r41+8];
	add.s32 	%r42, %r41, %r416;
	ld.global.f64 	%fd57, [%r42];
	ld.global.f64 	%fd58, [%r42+8];
	mov.f64 	%fd2687, %fd57;
	mov.f64 	%fd2688, %fd58;
	add.s32 	%r43, %r42, %r416;
	ld.global.f64 	%fd59, [%r43];
	ld.global.f64 	%fd60, [%r43+8];
	mov.f64 	%fd2689, %fd59;
	mov.f64 	%fd2690, %fd60;
	add.s32 	%r44, %r43, %r416;
	ld.global.f64 	%fd61, [%r44];
	ld.global.f64 	%fd62, [%r44+8];
	mov.f64 	%fd2691, %fd61;
	mov.f64 	%fd2692, %fd62;
	add.s32 	%r45, %r44, %r416;
	ld.global.f64 	%fd63, [%r45];
	ld.global.f64 	%fd64, [%r45+8];
	mov.f64 	%fd2693, %fd63;
	mov.f64 	%fd2694, %fd64;
	add.s32 	%r46, %r45, %r416;
	ld.global.f64 	%fd65, [%r46];
	ld.global.f64 	%fd66, [%r46+8];
	mov.f64 	%fd2695, %fd65;
	mov.f64 	%fd2696, %fd66;
	add.s32 	%r47, %r46, %r416;
	ld.global.f64 	%fd67, [%r47];
	ld.global.f64 	%fd68, [%r47+8];
	mov.f64 	%fd2697, %fd67;
	mov.f64 	%fd2698, %fd68;
	setp.eq.s32 	%p54, %r37, 0;
	mov.f64 	%fd2679, %fd705;
	mov.f64 	%fd2680, %fd706;
	mov.f64 	%fd2681, %fd51;
	mov.f64 	%fd2682, %fd52;
	mov.f64 	%fd2683, %fd53;
	mov.f64 	%fd2684, %fd54;
	mov.f64 	%fd2685, %fd55;
	mov.f64 	%fd2686, %fd56;
	@%p54 bra 	BB3_495;

	cvt.rn.f64.s32 	%fd69, %r37;
	mul.f64 	%fd707, %fd69, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd70, %r36;
	div.rn.f64 	%fd71, %fd707, %fd70;
	setp.eq.f64 	%p55, %fd71, 0d7FF0000000000000;
	setp.eq.f64 	%p56, %fd71, 0dFFF0000000000000;
	or.pred  	%p57, %p55, %p56;
	add.u32 	%r48, %SP, 0;
	@%p57 bra 	BB3_67;

	// inline asm
	abs.f64 	%fd708, %fd71;
	// inline asm
	setp.gt.f64 	%p58, %fd708, 0d41E0000000000000;
	@%p58 bra 	BB3_47;

	mov.f64 	%fd723, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd710, %fd71, %fd723;
	// inline asm
	cvt.rni.s32.f64 	%r417, %fd710;
	// inline asm
	cvt.rn.f64.s32 	%fd724, %r417;
	neg.f64 	%fd720, %fd724;
	mov.f64 	%fd713, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd711, %fd720, %fd713, %fd71;
	// inline asm
	mov.f64 	%fd717, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd715, %fd720, %fd717, %fd711;
	// inline asm
	mov.f64 	%fd721, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd719, %fd720, %fd721, %fd715;
	// inline asm
	mov.u32 	%r1149, %r417;
	mov.f64 	%fd2625, %fd719;
	bra.uni 	BB3_63;

BB3_47:
	mov.b64 	 %rl1, %fd71;
	shr.u64 	%rl577, %rl1, 52;
	and.b64  	%rl578, %rl577, 2047;
	add.s64 	%rl2, %rl578, 4294966272;
	cvt.u32.u64 	%r421, %rl2;
	shr.u32 	%r422, %r421, 6;
	mov.u32 	%r423, 16;
	sub.s32 	%r50, %r423, %r422;
	mov.u32 	%r424, 15;
	sub.s32 	%r1147, %r424, %r422;
	mov.u32 	%r425, 19;
	sub.s32 	%r420, %r425, %r422;
	mov.u32 	%r419, 18;
	// inline asm
	min.s32 	%r418, %r419, %r420;
	// inline asm
	setp.lt.s32 	%p59, %r1147, %r418;
	@%p59 bra 	BB3_49;

	add.s32 	%r1147, %r50, -1;
	mov.u64 	%rl1353, 0;
	bra.uni 	BB3_51;

BB3_49:
	mov.u32 	%r430, 1;
	sub.s32 	%r53, %r430, %r50;
	mov.u64 	%rl1353, 0;

BB3_50:
	.pragma "nounroll";
	shl.b64 	%rl589, %rl1, 11;
	or.b64  	%rl587, %rl589, -9223372036854775808;
	shl.b32 	%r434, %r1147, 3;
	mov.u32 	%r435, __internal_i2opi_d;
	add.s32 	%r436, %r435, %r434;
	ld.const.u64 	%rl586, [%r436];
	mul.lo.s64 	%rl590, %rl586, %rl587;
	// inline asm
	mul.hi.u64 	%rl585, %rl586, %rl587;
	// inline asm
	mad.lo.s64 	%rl591, %rl586, %rl587, %rl1353;
	setp.lt.u64 	%p60, %rl591, %rl590;
	selp.u64 	%rl592, 1, 0, %p60;
	add.s64 	%rl1353, %rl592, %rl585;
	add.s32 	%r437, %r53, %r1147;
	shl.b32 	%r438, %r437, 3;
	add.s32 	%r440, %r48, %r438;
	st.local.u64 	[%r440], %rl591;
	// inline asm
	min.s32 	%r431, %r419, %r420;
	// inline asm
	add.s32 	%r1147, %r1147, 1;
	setp.lt.s32 	%p61, %r1147, %r431;
	@%p61 bra 	BB3_50;

BB3_51:
	mov.u32 	%r448, 1;
	sub.s32 	%r449, %r448, %r50;
	add.s32 	%r450, %r449, %r1147;
	shl.b32 	%r451, %r450, 3;
	add.s32 	%r453, %r48, %r451;
	st.local.u64 	[%r453], %rl1353;
	and.b32  	%r454, %r421, 63;
	ld.local.u64 	%rl1354, [%r48+24];
	ld.local.u64 	%rl1355, [%r48+16];
	setp.eq.s32 	%p62, %r454, 0;
	@%p62 bra 	BB3_53;

	and.b64  	%rl602, %rl577, 63;
	cvt.u32.u64 	%r455, %rl602;
	shl.b64 	%rl603, %rl1354, %r455;
	add.s64 	%rl604, %rl577, 4294966272;
	cvt.u32.u64 	%r456, %rl604;
	neg.s32 	%r457, %r456;
	and.b32  	%r458, %r457, 63;
	shr.u64 	%rl605, %rl1355, %r458;
	or.b64  	%rl1354, %rl605, %rl603;
	shl.b64 	%rl606, %rl1355, %r455;
	ld.local.u64 	%rl607, [%r48+8];
	shr.u64 	%rl608, %rl607, %r458;
	or.b64  	%rl1355, %rl608, %rl606;

BB3_53:
	shr.u64 	%rl609, %rl1354, 62;
	cvt.u32.u64 	%r459, %rl609;
	shr.u64 	%rl610, %rl1355, 62;
	shl.b64 	%rl611, %rl1354, 2;
	or.b64  	%rl1360, %rl610, %rl611;
	shl.b64 	%rl14, %rl1355, 2;
	setp.ne.s64 	%p63, %rl14, 0;
	selp.u64 	%rl612, 1, 0, %p63;
	or.b64  	%rl613, %rl612, %rl1360;
	setp.gt.u64 	%p64, %rl613, -9223372036854775808;
	selp.u32 	%r460, 1, 0, %p64;
	add.s32 	%r461, %r460, %r459;
	setp.lt.s64 	%p65, %rl1, 0;
	neg.s32 	%r462, %r461;
	selp.b32 	%r1149, %r462, %r461, %p65;
	@%p64 bra 	BB3_55;

	and.b64  	%rl1356, %rl1, -9223372036854775808;
	mov.u64 	%rl1359, %rl14;
	bra.uni 	BB3_56;

BB3_55:
	not.b64 	%rl616, %rl1360;
	neg.s64 	%rl16, %rl14;
	setp.eq.s64 	%p66, %rl14, 0;
	selp.u64 	%rl617, 1, 0, %p66;
	add.s64 	%rl1360, %rl617, %rl616;
	and.b64  	%rl619, %rl1, -9223372036854775808;
	xor.b64  	%rl1356, %rl619, -9223372036854775808;
	mov.u64 	%rl1359, %rl16;

BB3_56:
	mov.u64 	%rl1358, %rl1359;
	setp.gt.s64 	%p67, %rl1360, 0;
	@%p67 bra 	BB3_58;

	mov.u32 	%r1148, 0;
	bra.uni 	BB3_60;

BB3_58:
	mov.u32 	%r1148, 0;

BB3_59:
	shr.u64 	%rl620, %rl1358, 63;
	shl.b64 	%rl621, %rl1360, 1;
	or.b64  	%rl1360, %rl620, %rl621;
	shl.b64 	%rl1358, %rl1358, 1;
	add.s32 	%r1148, %r1148, -1;
	setp.gt.s64 	%p68, %rl1360, 0;
	@%p68 bra 	BB3_59;

BB3_60:
	mul.lo.s64 	%rl1362, %rl1360, -3958705157555305931;
	mov.u64 	%rl624, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl622, %rl1360, %rl624;
	// inline asm
	setp.gt.s64 	%p69, %rl622, 0;
	mov.u64 	%rl1361, %rl622;
	@%p69 bra 	BB3_61;
	bra.uni 	BB3_62;

BB3_61:
	shl.b64 	%rl625, %rl622, 1;
	shr.u64 	%rl626, %rl1362, 63;
	or.b64  	%rl1361, %rl625, %rl626;
	mul.lo.s64 	%rl1362, %rl1360, -7917410315110611862;
	add.s32 	%r1148, %r1148, -1;

BB3_62:
	setp.ne.s64 	%p70, %rl1362, 0;
	selp.u64 	%rl627, 1, 0, %p70;
	add.s64 	%rl628, %rl627, %rl1361;
	add.s32 	%r465, %r1148, 1022;
	cvt.u64.u32 	%rl629, %r465;
	shl.b64 	%rl630, %rl629, 52;
	shr.u64 	%rl631, %rl628, 11;
	shr.u64 	%rl632, %rl628, 10;
	and.b64  	%rl633, %rl632, 1;
	add.s64 	%rl634, %rl630, %rl631;
	add.s64 	%rl635, %rl634, %rl633;
	or.b64  	%rl636, %rl635, %rl1356;
	mov.b64 	 %fd2625, %rl636;

BB3_63:
	add.s32 	%r64, %r1149, 1;
	and.b32  	%r466, %r64, 1;
	setp.eq.s32 	%p71, %r466, 0;
	mul.rn.f64 	%fd75, %fd2625, %fd2625;
	@%p71 bra 	BB3_65;

	mov.f64 	%fd726, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd728, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd725, %fd726, %fd75, %fd728;
	// inline asm
	mov.f64 	%fd732, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd729, %fd725, %fd75, %fd732;
	// inline asm
	mov.f64 	%fd736, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd733, %fd729, %fd75, %fd736;
	// inline asm
	mov.f64 	%fd740, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd737, %fd733, %fd75, %fd740;
	// inline asm
	mov.f64 	%fd744, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd741, %fd737, %fd75, %fd744;
	// inline asm
	mov.f64 	%fd748, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd745, %fd741, %fd75, %fd748;
	// inline asm
	mov.f64 	%fd752, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd749, %fd745, %fd75, %fd752;
	// inline asm
	mov.f64 	%fd2626, %fd749;
	bra.uni 	BB3_66;

BB3_65:
	mov.f64 	%fd754, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd756, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd753, %fd754, %fd75, %fd756;
	// inline asm
	mov.f64 	%fd760, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd757, %fd753, %fd75, %fd760;
	// inline asm
	mov.f64 	%fd764, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd761, %fd757, %fd75, %fd764;
	// inline asm
	mov.f64 	%fd768, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd765, %fd761, %fd75, %fd768;
	// inline asm
	mov.f64 	%fd772, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd769, %fd765, %fd75, %fd772;
	// inline asm
	mul.rn.f64 	%fd774, %fd769, %fd75;
	// inline asm
	fma.rn.f64 	%fd773, %fd774, %fd2625, %fd2625;
	// inline asm
	mov.f64 	%fd2626, %fd773;

BB3_66:
	and.b32  	%r467, %r64, 2;
	setp.eq.s32 	%p72, %r467, 0;
	neg.f64 	%fd777, %fd2626;
	selp.f64 	%fd2627, %fd2626, %fd777, %p72;
	bra.uni 	BB3_68;

BB3_67:
	mov.f64 	%fd2627, 0dFFF8000000000000;

BB3_68:
	setp.eq.f64 	%p74, %fd71, 0d0000000000000000;
	or.pred  	%p75, %p56, %p74;
	or.pred  	%p77, %p55, %p75;
	@%p77 bra 	BB3_91;

	// inline asm
	abs.f64 	%fd779, %fd71;
	// inline asm
	setp.gt.f64 	%p78, %fd779, 0d41E0000000000000;
	@%p78 bra 	BB3_71;

	mov.f64 	%fd794, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd781, %fd71, %fd794;
	// inline asm
	cvt.rni.s32.f64 	%r468, %fd781;
	// inline asm
	cvt.rn.f64.s32 	%fd795, %r468;
	neg.f64 	%fd791, %fd795;
	mov.f64 	%fd784, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd782, %fd791, %fd784, %fd71;
	// inline asm
	mov.f64 	%fd788, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd786, %fd791, %fd788, %fd782;
	// inline asm
	mov.f64 	%fd792, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd790, %fd791, %fd792, %fd786;
	// inline asm
	mov.u32 	%r1152, %r468;
	mov.f64 	%fd2628, %fd790;
	bra.uni 	BB3_87;

BB3_71:
	mov.b64 	 %rl33, %fd71;
	and.b64  	%rl1366, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl637, %rl35, 2047;
	add.s64 	%rl638, %rl637, 4294966272;
	cvt.u32.u64 	%r66, %rl638;
	shl.b64 	%rl639, %rl33, 11;
	or.b64  	%rl36, %rl639, -9223372036854775808;
	shr.u32 	%r472, %r66, 6;
	mov.u32 	%r473, 16;
	sub.s32 	%r67, %r473, %r472;
	mov.u32 	%r474, 15;
	sub.s32 	%r1150, %r474, %r472;
	mov.u32 	%r475, 19;
	sub.s32 	%r69, %r475, %r472;
	mov.u32 	%r470, 18;
	// inline asm
	min.s32 	%r469, %r470, %r69;
	// inline asm
	setp.lt.s32 	%p79, %r1150, %r469;
	@%p79 bra 	BB3_73;

	mov.u64 	%rl1363, 0;
	bra.uni 	BB3_75;

BB3_73:
	mov.u32 	%r476, 1;
	sub.s32 	%r70, %r476, %r67;
	mov.u64 	%rl1363, 0;

BB3_74:
	.pragma "nounroll";
	shl.b32 	%r480, %r1150, 3;
	mov.u32 	%r481, __internal_i2opi_d;
	add.s32 	%r482, %r481, %r480;
	ld.const.u64 	%rl643, [%r482];
	mul.lo.s64 	%rl645, %rl643, %rl36;
	// inline asm
	mul.hi.u64 	%rl642, %rl643, %rl36;
	// inline asm
	mad.lo.s64 	%rl646, %rl643, %rl36, %rl1363;
	setp.lt.u64 	%p80, %rl646, %rl645;
	selp.u64 	%rl647, 1, 0, %p80;
	add.s64 	%rl1363, %rl647, %rl642;
	add.s32 	%r483, %r70, %r1150;
	shl.b32 	%r484, %r483, 3;
	add.s32 	%r486, %r48, %r484;
	st.local.u64 	[%r486], %rl646;
	// inline asm
	min.s32 	%r477, %r470, %r69;
	// inline asm
	add.s32 	%r1150, %r1150, 1;
	setp.lt.s32 	%p81, %r1150, %r477;
	@%p81 bra 	BB3_74;

BB3_75:
	mov.u32 	%r487, 1;
	sub.s32 	%r488, %r487, %r67;
	add.s32 	%r489, %r488, %r1150;
	shl.b32 	%r490, %r489, 3;
	add.s32 	%r492, %r48, %r490;
	st.local.u64 	[%r492], %rl1363;
	ld.local.u64 	%rl1364, [%r48+24];
	ld.local.u64 	%rl1365, [%r48+16];
	and.b32  	%r493, %r66, 63;
	setp.eq.s32 	%p82, %r493, 0;
	@%p82 bra 	BB3_77;

	and.b64  	%rl648, %rl35, 63;
	cvt.u32.u64 	%r494, %rl648;
	shl.b64 	%rl649, %rl1364, %r494;
	neg.s32 	%r495, %r66;
	and.b32  	%r496, %r495, 63;
	shr.u64 	%rl650, %rl1365, %r496;
	or.b64  	%rl1364, %rl650, %rl649;
	shl.b64 	%rl651, %rl1365, %r494;
	ld.local.u64 	%rl652, [%r48+8];
	shr.u64 	%rl653, %rl652, %r496;
	or.b64  	%rl1365, %rl653, %rl651;

BB3_77:
	shr.u64 	%rl654, %rl1364, 62;
	cvt.u32.u64 	%r497, %rl654;
	shr.u64 	%rl655, %rl1365, 62;
	shl.b64 	%rl656, %rl1364, 2;
	or.b64  	%rl1370, %rl655, %rl656;
	shl.b64 	%rl47, %rl1365, 2;
	setp.ne.s64 	%p83, %rl47, 0;
	selp.u64 	%rl657, 1, 0, %p83;
	or.b64  	%rl658, %rl657, %rl1370;
	setp.gt.u64 	%p84, %rl658, -9223372036854775808;
	selp.u32 	%r498, 1, 0, %p84;
	add.s32 	%r499, %r498, %r497;
	neg.s32 	%r500, %r499;
	setp.lt.s64 	%p85, %rl33, 0;
	selp.b32 	%r1152, %r500, %r499, %p85;
	@%p84 bra 	BB3_79;

	mov.u64 	%rl1369, %rl47;
	bra.uni 	BB3_80;

BB3_79:
	not.b64 	%rl659, %rl1370;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p86, %rl47, 0;
	selp.u64 	%rl660, 1, 0, %p86;
	add.s64 	%rl1370, %rl660, %rl659;
	xor.b64  	%rl1366, %rl1366, -9223372036854775808;
	mov.u64 	%rl1369, %rl48;

BB3_80:
	mov.u64 	%rl1368, %rl1369;
	setp.gt.s64 	%p87, %rl1370, 0;
	@%p87 bra 	BB3_82;

	mov.u32 	%r1151, 0;
	bra.uni 	BB3_84;

BB3_82:
	mov.u32 	%r1151, 0;

BB3_83:
	shr.u64 	%rl661, %rl1368, 63;
	shl.b64 	%rl662, %rl1370, 1;
	or.b64  	%rl1370, %rl661, %rl662;
	shl.b64 	%rl1368, %rl1368, 1;
	add.s32 	%r1151, %r1151, -1;
	setp.gt.s64 	%p88, %rl1370, 0;
	@%p88 bra 	BB3_83;

BB3_84:
	mul.lo.s64 	%rl1372, %rl1370, -3958705157555305931;
	mov.u64 	%rl665, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl663, %rl1370, %rl665;
	// inline asm
	setp.gt.s64 	%p89, %rl663, 0;
	mov.u64 	%rl1371, %rl663;
	@%p89 bra 	BB3_85;
	bra.uni 	BB3_86;

BB3_85:
	shl.b64 	%rl666, %rl663, 1;
	shr.u64 	%rl667, %rl1372, 63;
	or.b64  	%rl1371, %rl666, %rl667;
	mul.lo.s64 	%rl1372, %rl1370, -7917410315110611862;
	add.s32 	%r1151, %r1151, -1;

BB3_86:
	setp.ne.s64 	%p90, %rl1372, 0;
	selp.u64 	%rl668, 1, 0, %p90;
	add.s64 	%rl669, %rl668, %rl1371;
	add.s32 	%r503, %r1151, 1022;
	cvt.u64.u32 	%rl670, %r503;
	shl.b64 	%rl671, %rl670, 52;
	shr.u64 	%rl672, %rl669, 11;
	shr.u64 	%rl673, %rl669, 10;
	and.b64  	%rl674, %rl673, 1;
	add.s64 	%rl675, %rl671, %rl672;
	add.s64 	%rl676, %rl675, %rl674;
	or.b64  	%rl677, %rl676, %rl1366;
	mov.b64 	 %fd2628, %rl677;

BB3_87:
	and.b32  	%r504, %r1152, 1;
	setp.eq.s32 	%p91, %r504, 0;
	mul.rn.f64 	%fd85, %fd2628, %fd2628;
	@%p91 bra 	BB3_89;

	mov.f64 	%fd797, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd799, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd796, %fd797, %fd85, %fd799;
	// inline asm
	mov.f64 	%fd803, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd800, %fd796, %fd85, %fd803;
	// inline asm
	mov.f64 	%fd807, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd804, %fd800, %fd85, %fd807;
	// inline asm
	mov.f64 	%fd811, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd808, %fd804, %fd85, %fd811;
	// inline asm
	mov.f64 	%fd815, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd812, %fd808, %fd85, %fd815;
	// inline asm
	mov.f64 	%fd819, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd816, %fd812, %fd85, %fd819;
	// inline asm
	mov.f64 	%fd823, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd820, %fd816, %fd85, %fd823;
	// inline asm
	mov.f64 	%fd2629, %fd820;
	bra.uni 	BB3_90;

BB3_89:
	mov.f64 	%fd825, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd827, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd824, %fd825, %fd85, %fd827;
	// inline asm
	mov.f64 	%fd831, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd828, %fd824, %fd85, %fd831;
	// inline asm
	mov.f64 	%fd835, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd832, %fd828, %fd85, %fd835;
	// inline asm
	mov.f64 	%fd839, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd836, %fd832, %fd85, %fd839;
	// inline asm
	mov.f64 	%fd843, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd840, %fd836, %fd85, %fd843;
	// inline asm
	mul.rn.f64 	%fd845, %fd840, %fd85;
	// inline asm
	fma.rn.f64 	%fd844, %fd845, %fd2628, %fd2628;
	// inline asm
	mov.f64 	%fd2629, %fd844;

BB3_90:
	and.b32  	%r505, %r1152, 2;
	setp.eq.s32 	%p92, %r505, 0;
	neg.f64 	%fd848, %fd2629;
	selp.f64 	%fd2630, %fd2629, %fd848, %p92;
	bra.uni 	BB3_92;

BB3_91:
	mov.f64 	%fd849, 0d0000000000000000;
	mul.rn.f64 	%fd2630, %fd71, %fd849;

BB3_92:
	neg.f64 	%fd850, %fd2630;
	mov.f64 	%fd2715, %fd2627;
	mov.f64 	%fd2716, %fd850;
	ld.param.u32 	%r1136, [DIT10C2C_param_3];
	setp.eq.s32 	%p93, %r1136, 0;
	@%p93 bra 	BB3_93;
	bra.uni 	BB3_94;

BB3_93:
	mov.f64 	%fd2715, %fd2627;
	mov.f64 	%fd2716, %fd2630;

BB3_94:
	mul.f64 	%fd852, %fd51, %fd2715;
	neg.f64 	%fd854, %fd52;
	fma.rn.f64 	%fd855, %fd854, %fd2716, %fd852;
	mul.f64 	%fd856, %fd52, %fd2715;
	fma.rn.f64 	%fd857, %fd51, %fd2716, %fd856;
	mul.f64 	%fd858, %fd69, 0d402921FB54442D18;
	div.rn.f64 	%fd92, %fd858, %fd70;
	setp.eq.f64 	%p94, %fd92, 0d7FF0000000000000;
	setp.eq.f64 	%p95, %fd92, 0dFFF0000000000000;
	or.pred  	%p96, %p94, %p95;
	@%p96 bra 	BB3_117;

	// inline asm
	abs.f64 	%fd859, %fd92;
	// inline asm
	setp.gt.f64 	%p97, %fd859, 0d41E0000000000000;
	@%p97 bra 	BB3_97;

	mov.f64 	%fd874, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd861, %fd92, %fd874;
	// inline asm
	cvt.rni.s32.f64 	%r506, %fd861;
	// inline asm
	cvt.rn.f64.s32 	%fd875, %r506;
	neg.f64 	%fd871, %fd875;
	mov.f64 	%fd864, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd862, %fd871, %fd864, %fd92;
	// inline asm
	mov.f64 	%fd868, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd866, %fd871, %fd868, %fd862;
	// inline asm
	mov.f64 	%fd872, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd870, %fd871, %fd872, %fd866;
	// inline asm
	mov.u32 	%r1155, %r506;
	mov.f64 	%fd2631, %fd870;
	bra.uni 	BB3_113;

BB3_97:
	mov.b64 	 %rl65, %fd92;
	shr.u64 	%rl678, %rl65, 52;
	and.b64  	%rl679, %rl678, 2047;
	add.s64 	%rl66, %rl679, 4294966272;
	cvt.u32.u64 	%r510, %rl66;
	shr.u32 	%r511, %r510, 6;
	mov.u32 	%r513, 15;
	sub.s32 	%r1153, %r513, %r511;
	mov.u32 	%r514, 19;
	sub.s32 	%r509, %r514, %r511;
	mov.u32 	%r508, 18;
	// inline asm
	min.s32 	%r507, %r508, %r509;
	// inline asm
	setp.lt.s32 	%p98, %r1153, %r507;
	@%p98 bra 	BB3_99;

	mov.u64 	%rl1373, 0;
	bra.uni 	BB3_101;

BB3_99:
	mov.u32 	%r517, 16;
	sub.s32 	%r518, %r517, %r511;
	mov.u32 	%r519, 1;
	sub.s32 	%r84, %r519, %r518;
	mov.u64 	%rl1373, 0;

BB3_100:
	.pragma "nounroll";
	shl.b64 	%rl690, %rl65, 11;
	or.b64  	%rl688, %rl690, -9223372036854775808;
	shl.b32 	%r523, %r1153, 3;
	mov.u32 	%r524, __internal_i2opi_d;
	add.s32 	%r525, %r524, %r523;
	ld.const.u64 	%rl687, [%r525];
	mul.lo.s64 	%rl691, %rl687, %rl688;
	// inline asm
	mul.hi.u64 	%rl686, %rl687, %rl688;
	// inline asm
	mad.lo.s64 	%rl692, %rl687, %rl688, %rl1373;
	setp.lt.u64 	%p99, %rl692, %rl691;
	selp.u64 	%rl693, 1, 0, %p99;
	add.s64 	%rl1373, %rl693, %rl686;
	add.s32 	%r526, %r84, %r1153;
	shl.b32 	%r527, %r526, 3;
	add.s32 	%r529, %r48, %r527;
	st.local.u64 	[%r529], %rl692;
	// inline asm
	min.s32 	%r520, %r508, %r509;
	// inline asm
	add.s32 	%r1153, %r1153, 1;
	setp.lt.s32 	%p100, %r1153, %r520;
	@%p100 bra 	BB3_100;

BB3_101:
	mov.u32 	%r535, 16;
	sub.s32 	%r536, %r535, %r511;
	mov.u32 	%r537, 1;
	sub.s32 	%r538, %r537, %r536;
	add.s32 	%r539, %r538, %r1153;
	shl.b32 	%r540, %r539, 3;
	add.s32 	%r542, %r48, %r540;
	st.local.u64 	[%r542], %rl1373;
	and.b32  	%r543, %r510, 63;
	ld.local.u64 	%rl1374, [%r48+24];
	ld.local.u64 	%rl1375, [%r48+16];
	setp.eq.s32 	%p101, %r543, 0;
	@%p101 bra 	BB3_103;

	and.b64  	%rl703, %rl678, 63;
	cvt.u32.u64 	%r544, %rl703;
	shl.b64 	%rl704, %rl1374, %r544;
	add.s64 	%rl705, %rl678, 4294966272;
	cvt.u32.u64 	%r545, %rl705;
	neg.s32 	%r546, %r545;
	and.b32  	%r547, %r546, 63;
	shr.u64 	%rl706, %rl1375, %r547;
	or.b64  	%rl1374, %rl706, %rl704;
	shl.b64 	%rl707, %rl1375, %r544;
	ld.local.u64 	%rl708, [%r48+8];
	shr.u64 	%rl709, %rl708, %r547;
	or.b64  	%rl1375, %rl709, %rl707;

BB3_103:
	shr.u64 	%rl710, %rl1374, 62;
	cvt.u32.u64 	%r548, %rl710;
	shr.u64 	%rl711, %rl1375, 62;
	shl.b64 	%rl712, %rl1374, 2;
	or.b64  	%rl1380, %rl711, %rl712;
	shl.b64 	%rl78, %rl1375, 2;
	setp.ne.s64 	%p102, %rl78, 0;
	selp.u64 	%rl713, 1, 0, %p102;
	or.b64  	%rl714, %rl713, %rl1380;
	setp.gt.u64 	%p103, %rl714, -9223372036854775808;
	selp.u32 	%r549, 1, 0, %p103;
	add.s32 	%r550, %r549, %r548;
	setp.lt.s64 	%p104, %rl65, 0;
	neg.s32 	%r551, %r550;
	selp.b32 	%r1155, %r551, %r550, %p104;
	@%p103 bra 	BB3_105;

	and.b64  	%rl1376, %rl65, -9223372036854775808;
	mov.u64 	%rl1379, %rl78;
	bra.uni 	BB3_106;

BB3_105:
	not.b64 	%rl717, %rl1380;
	neg.s64 	%rl80, %rl78;
	setp.eq.s64 	%p105, %rl78, 0;
	selp.u64 	%rl718, 1, 0, %p105;
	add.s64 	%rl1380, %rl718, %rl717;
	and.b64  	%rl720, %rl65, -9223372036854775808;
	xor.b64  	%rl1376, %rl720, -9223372036854775808;
	mov.u64 	%rl1379, %rl80;

BB3_106:
	mov.u64 	%rl1378, %rl1379;
	setp.gt.s64 	%p106, %rl1380, 0;
	@%p106 bra 	BB3_108;

	mov.u32 	%r1154, 0;
	bra.uni 	BB3_110;

BB3_108:
	mov.u32 	%r1154, 0;

BB3_109:
	shr.u64 	%rl721, %rl1378, 63;
	shl.b64 	%rl722, %rl1380, 1;
	or.b64  	%rl1380, %rl721, %rl722;
	shl.b64 	%rl1378, %rl1378, 1;
	add.s32 	%r1154, %r1154, -1;
	setp.gt.s64 	%p107, %rl1380, 0;
	@%p107 bra 	BB3_109;

BB3_110:
	mul.lo.s64 	%rl1382, %rl1380, -3958705157555305931;
	mov.u64 	%rl725, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl723, %rl1380, %rl725;
	// inline asm
	setp.gt.s64 	%p108, %rl723, 0;
	mov.u64 	%rl1381, %rl723;
	@%p108 bra 	BB3_111;
	bra.uni 	BB3_112;

BB3_111:
	shl.b64 	%rl726, %rl723, 1;
	shr.u64 	%rl727, %rl1382, 63;
	or.b64  	%rl1381, %rl726, %rl727;
	mul.lo.s64 	%rl1382, %rl1380, -7917410315110611862;
	add.s32 	%r1154, %r1154, -1;

BB3_112:
	setp.ne.s64 	%p109, %rl1382, 0;
	selp.u64 	%rl728, 1, 0, %p109;
	add.s64 	%rl729, %rl728, %rl1381;
	add.s32 	%r554, %r1154, 1022;
	cvt.u64.u32 	%rl730, %r554;
	shl.b64 	%rl731, %rl730, 52;
	shr.u64 	%rl732, %rl729, 11;
	shr.u64 	%rl733, %rl729, 10;
	and.b64  	%rl734, %rl733, 1;
	add.s64 	%rl735, %rl731, %rl732;
	add.s64 	%rl736, %rl735, %rl734;
	or.b64  	%rl737, %rl736, %rl1376;
	mov.b64 	 %fd2631, %rl737;

BB3_113:
	add.s32 	%r95, %r1155, 1;
	and.b32  	%r555, %r95, 1;
	setp.eq.s32 	%p110, %r555, 0;
	mul.rn.f64 	%fd96, %fd2631, %fd2631;
	@%p110 bra 	BB3_115;

	mov.f64 	%fd877, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd879, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd876, %fd877, %fd96, %fd879;
	// inline asm
	mov.f64 	%fd883, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd880, %fd876, %fd96, %fd883;
	// inline asm
	mov.f64 	%fd887, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd884, %fd880, %fd96, %fd887;
	// inline asm
	mov.f64 	%fd891, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd888, %fd884, %fd96, %fd891;
	// inline asm
	mov.f64 	%fd895, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd892, %fd888, %fd96, %fd895;
	// inline asm
	mov.f64 	%fd899, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd896, %fd892, %fd96, %fd899;
	// inline asm
	mov.f64 	%fd903, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd900, %fd896, %fd96, %fd903;
	// inline asm
	mov.f64 	%fd2632, %fd900;
	bra.uni 	BB3_116;

BB3_115:
	mov.f64 	%fd905, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd907, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd904, %fd905, %fd96, %fd907;
	// inline asm
	mov.f64 	%fd911, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd908, %fd904, %fd96, %fd911;
	// inline asm
	mov.f64 	%fd915, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd912, %fd908, %fd96, %fd915;
	// inline asm
	mov.f64 	%fd919, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd916, %fd912, %fd96, %fd919;
	// inline asm
	mov.f64 	%fd923, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd920, %fd916, %fd96, %fd923;
	// inline asm
	mul.rn.f64 	%fd925, %fd920, %fd96;
	// inline asm
	fma.rn.f64 	%fd924, %fd925, %fd2631, %fd2631;
	// inline asm
	mov.f64 	%fd2632, %fd924;

BB3_116:
	and.b32  	%r556, %r95, 2;
	setp.eq.s32 	%p111, %r556, 0;
	neg.f64 	%fd928, %fd2632;
	selp.f64 	%fd2633, %fd2632, %fd928, %p111;
	bra.uni 	BB3_118;

BB3_117:
	mov.f64 	%fd2633, 0dFFF8000000000000;

BB3_118:
	setp.eq.f64 	%p113, %fd92, 0d0000000000000000;
	or.pred  	%p114, %p95, %p113;
	or.pred  	%p116, %p94, %p114;
	@%p116 bra 	BB3_141;

	// inline asm
	abs.f64 	%fd930, %fd92;
	// inline asm
	setp.gt.f64 	%p117, %fd930, 0d41E0000000000000;
	@%p117 bra 	BB3_121;

	mov.f64 	%fd945, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd932, %fd92, %fd945;
	// inline asm
	cvt.rni.s32.f64 	%r557, %fd932;
	// inline asm
	cvt.rn.f64.s32 	%fd946, %r557;
	neg.f64 	%fd942, %fd946;
	mov.f64 	%fd935, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd933, %fd942, %fd935, %fd92;
	// inline asm
	mov.f64 	%fd939, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd937, %fd942, %fd939, %fd933;
	// inline asm
	mov.f64 	%fd943, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd941, %fd942, %fd943, %fd937;
	// inline asm
	mov.u32 	%r1158, %r557;
	mov.f64 	%fd2634, %fd941;
	bra.uni 	BB3_137;

BB3_121:
	mov.b64 	 %rl97, %fd92;
	and.b64  	%rl1386, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl738, %rl99, 2047;
	add.s64 	%rl739, %rl738, 4294966272;
	cvt.u32.u64 	%r97, %rl739;
	shl.b64 	%rl740, %rl97, 11;
	or.b64  	%rl100, %rl740, -9223372036854775808;
	shr.u32 	%r561, %r97, 6;
	mov.u32 	%r562, 16;
	sub.s32 	%r98, %r562, %r561;
	mov.u32 	%r563, 15;
	sub.s32 	%r1156, %r563, %r561;
	mov.u32 	%r564, 19;
	sub.s32 	%r100, %r564, %r561;
	mov.u32 	%r559, 18;
	// inline asm
	min.s32 	%r558, %r559, %r100;
	// inline asm
	setp.lt.s32 	%p118, %r1156, %r558;
	@%p118 bra 	BB3_123;

	mov.u64 	%rl1383, 0;
	bra.uni 	BB3_125;

BB3_123:
	mov.u32 	%r565, 1;
	sub.s32 	%r101, %r565, %r98;
	mov.u64 	%rl1383, 0;

BB3_124:
	.pragma "nounroll";
	shl.b32 	%r569, %r1156, 3;
	mov.u32 	%r570, __internal_i2opi_d;
	add.s32 	%r571, %r570, %r569;
	ld.const.u64 	%rl744, [%r571];
	mul.lo.s64 	%rl746, %rl744, %rl100;
	// inline asm
	mul.hi.u64 	%rl743, %rl744, %rl100;
	// inline asm
	mad.lo.s64 	%rl747, %rl744, %rl100, %rl1383;
	setp.lt.u64 	%p119, %rl747, %rl746;
	selp.u64 	%rl748, 1, 0, %p119;
	add.s64 	%rl1383, %rl748, %rl743;
	add.s32 	%r572, %r101, %r1156;
	shl.b32 	%r573, %r572, 3;
	add.s32 	%r575, %r48, %r573;
	st.local.u64 	[%r575], %rl747;
	// inline asm
	min.s32 	%r566, %r559, %r100;
	// inline asm
	add.s32 	%r1156, %r1156, 1;
	setp.lt.s32 	%p120, %r1156, %r566;
	@%p120 bra 	BB3_124;

BB3_125:
	mov.u32 	%r576, 1;
	sub.s32 	%r577, %r576, %r98;
	add.s32 	%r578, %r577, %r1156;
	shl.b32 	%r579, %r578, 3;
	add.s32 	%r581, %r48, %r579;
	st.local.u64 	[%r581], %rl1383;
	ld.local.u64 	%rl1384, [%r48+24];
	ld.local.u64 	%rl1385, [%r48+16];
	and.b32  	%r582, %r97, 63;
	setp.eq.s32 	%p121, %r582, 0;
	@%p121 bra 	BB3_127;

	and.b64  	%rl749, %rl99, 63;
	cvt.u32.u64 	%r583, %rl749;
	shl.b64 	%rl750, %rl1384, %r583;
	neg.s32 	%r584, %r97;
	and.b32  	%r585, %r584, 63;
	shr.u64 	%rl751, %rl1385, %r585;
	or.b64  	%rl1384, %rl751, %rl750;
	shl.b64 	%rl752, %rl1385, %r583;
	ld.local.u64 	%rl753, [%r48+8];
	shr.u64 	%rl754, %rl753, %r585;
	or.b64  	%rl1385, %rl754, %rl752;

BB3_127:
	shr.u64 	%rl755, %rl1384, 62;
	cvt.u32.u64 	%r586, %rl755;
	shr.u64 	%rl756, %rl1385, 62;
	shl.b64 	%rl757, %rl1384, 2;
	or.b64  	%rl1390, %rl756, %rl757;
	shl.b64 	%rl111, %rl1385, 2;
	setp.ne.s64 	%p122, %rl111, 0;
	selp.u64 	%rl758, 1, 0, %p122;
	or.b64  	%rl759, %rl758, %rl1390;
	setp.gt.u64 	%p123, %rl759, -9223372036854775808;
	selp.u32 	%r587, 1, 0, %p123;
	add.s32 	%r588, %r587, %r586;
	neg.s32 	%r589, %r588;
	setp.lt.s64 	%p124, %rl97, 0;
	selp.b32 	%r1158, %r589, %r588, %p124;
	@%p123 bra 	BB3_129;

	mov.u64 	%rl1389, %rl111;
	bra.uni 	BB3_130;

BB3_129:
	not.b64 	%rl760, %rl1390;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p125, %rl111, 0;
	selp.u64 	%rl761, 1, 0, %p125;
	add.s64 	%rl1390, %rl761, %rl760;
	xor.b64  	%rl1386, %rl1386, -9223372036854775808;
	mov.u64 	%rl1389, %rl112;

BB3_130:
	mov.u64 	%rl1388, %rl1389;
	setp.gt.s64 	%p126, %rl1390, 0;
	@%p126 bra 	BB3_132;

	mov.u32 	%r1157, 0;
	bra.uni 	BB3_134;

BB3_132:
	mov.u32 	%r1157, 0;

BB3_133:
	shr.u64 	%rl762, %rl1388, 63;
	shl.b64 	%rl763, %rl1390, 1;
	or.b64  	%rl1390, %rl762, %rl763;
	shl.b64 	%rl1388, %rl1388, 1;
	add.s32 	%r1157, %r1157, -1;
	setp.gt.s64 	%p127, %rl1390, 0;
	@%p127 bra 	BB3_133;

BB3_134:
	mul.lo.s64 	%rl1392, %rl1390, -3958705157555305931;
	mov.u64 	%rl766, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl764, %rl1390, %rl766;
	// inline asm
	setp.gt.s64 	%p128, %rl764, 0;
	mov.u64 	%rl1391, %rl764;
	@%p128 bra 	BB3_135;
	bra.uni 	BB3_136;

BB3_135:
	shl.b64 	%rl767, %rl764, 1;
	shr.u64 	%rl768, %rl1392, 63;
	or.b64  	%rl1391, %rl767, %rl768;
	mul.lo.s64 	%rl1392, %rl1390, -7917410315110611862;
	add.s32 	%r1157, %r1157, -1;

BB3_136:
	setp.ne.s64 	%p129, %rl1392, 0;
	selp.u64 	%rl769, 1, 0, %p129;
	add.s64 	%rl770, %rl769, %rl1391;
	add.s32 	%r592, %r1157, 1022;
	cvt.u64.u32 	%rl771, %r592;
	shl.b64 	%rl772, %rl771, 52;
	shr.u64 	%rl773, %rl770, 11;
	shr.u64 	%rl774, %rl770, 10;
	and.b64  	%rl775, %rl774, 1;
	add.s64 	%rl776, %rl772, %rl773;
	add.s64 	%rl777, %rl776, %rl775;
	or.b64  	%rl778, %rl777, %rl1386;
	mov.b64 	 %fd2634, %rl778;

BB3_137:
	and.b32  	%r593, %r1158, 1;
	setp.eq.s32 	%p130, %r593, 0;
	mul.rn.f64 	%fd106, %fd2634, %fd2634;
	@%p130 bra 	BB3_139;

	mov.f64 	%fd948, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd950, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd947, %fd948, %fd106, %fd950;
	// inline asm
	mov.f64 	%fd954, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd951, %fd947, %fd106, %fd954;
	// inline asm
	mov.f64 	%fd958, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd955, %fd951, %fd106, %fd958;
	// inline asm
	mov.f64 	%fd962, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd959, %fd955, %fd106, %fd962;
	// inline asm
	mov.f64 	%fd966, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd963, %fd959, %fd106, %fd966;
	// inline asm
	mov.f64 	%fd970, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd967, %fd963, %fd106, %fd970;
	// inline asm
	mov.f64 	%fd974, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd971, %fd967, %fd106, %fd974;
	// inline asm
	mov.f64 	%fd2635, %fd971;
	bra.uni 	BB3_140;

BB3_139:
	mov.f64 	%fd976, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd978, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd975, %fd976, %fd106, %fd978;
	// inline asm
	mov.f64 	%fd982, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd979, %fd975, %fd106, %fd982;
	// inline asm
	mov.f64 	%fd986, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd983, %fd979, %fd106, %fd986;
	// inline asm
	mov.f64 	%fd990, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd987, %fd983, %fd106, %fd990;
	// inline asm
	mov.f64 	%fd994, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd991, %fd987, %fd106, %fd994;
	// inline asm
	mul.rn.f64 	%fd996, %fd991, %fd106;
	// inline asm
	fma.rn.f64 	%fd995, %fd996, %fd2634, %fd2634;
	// inline asm
	mov.f64 	%fd2635, %fd995;

BB3_140:
	and.b32  	%r594, %r1158, 2;
	setp.eq.s32 	%p131, %r594, 0;
	neg.f64 	%fd999, %fd2635;
	selp.f64 	%fd2636, %fd2635, %fd999, %p131;
	bra.uni 	BB3_142;

BB3_141:
	mov.f64 	%fd1000, 0d0000000000000000;
	mul.rn.f64 	%fd2636, %fd92, %fd1000;

BB3_142:
	neg.f64 	%fd1001, %fd2636;
	mov.f64 	%fd2713, %fd2633;
	mov.f64 	%fd2714, %fd1001;
	ld.param.u32 	%r1135, [DIT10C2C_param_3];
	setp.eq.s32 	%p132, %r1135, 0;
	@%p132 bra 	BB3_143;
	bra.uni 	BB3_144;

BB3_143:
	mov.f64 	%fd2713, %fd2633;
	mov.f64 	%fd2714, %fd2636;

BB3_144:
	mul.f64 	%fd1003, %fd53, %fd2713;
	neg.f64 	%fd1005, %fd54;
	fma.rn.f64 	%fd1006, %fd1005, %fd2714, %fd1003;
	mul.f64 	%fd1007, %fd54, %fd2713;
	fma.rn.f64 	%fd1008, %fd53, %fd2714, %fd1007;
	mul.f64 	%fd1009, %fd69, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd113, %fd1009, %fd70;
	setp.eq.f64 	%p3, %fd113, 0d7FF0000000000000;
	setp.eq.f64 	%p4, %fd113, 0dFFF0000000000000;
	or.pred  	%p133, %p3, %p4;
	@%p133 bra 	BB3_167;

	// inline asm
	abs.f64 	%fd1010, %fd113;
	// inline asm
	setp.gt.f64 	%p134, %fd1010, 0d41E0000000000000;
	@%p134 bra 	BB3_147;

	mov.f64 	%fd1025, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1012, %fd113, %fd1025;
	// inline asm
	cvt.rni.s32.f64 	%r595, %fd1012;
	// inline asm
	cvt.rn.f64.s32 	%fd1026, %r595;
	neg.f64 	%fd1022, %fd1026;
	mov.f64 	%fd1015, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1013, %fd1022, %fd1015, %fd113;
	// inline asm
	mov.f64 	%fd1019, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1017, %fd1022, %fd1019, %fd1013;
	// inline asm
	mov.f64 	%fd1023, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1021, %fd1022, %fd1023, %fd1017;
	// inline asm
	mov.u32 	%r1161, %r595;
	mov.f64 	%fd2637, %fd1021;
	bra.uni 	BB3_163;

BB3_147:
	mov.b64 	 %rl129, %fd113;
	and.b64  	%rl1396, %rl129, -9223372036854775808;
	shr.u64 	%rl131, %rl129, 52;
	and.b64  	%rl779, %rl131, 2047;
	add.s64 	%rl780, %rl779, 4294966272;
	cvt.u32.u64 	%r113, %rl780;
	shl.b64 	%rl781, %rl129, 11;
	or.b64  	%rl132, %rl781, -9223372036854775808;
	shr.u32 	%r599, %r113, 6;
	mov.u32 	%r600, 16;
	sub.s32 	%r114, %r600, %r599;
	mov.u32 	%r601, 15;
	sub.s32 	%r1159, %r601, %r599;
	mov.u32 	%r602, 19;
	sub.s32 	%r116, %r602, %r599;
	mov.u32 	%r597, 18;
	// inline asm
	min.s32 	%r596, %r597, %r116;
	// inline asm
	setp.lt.s32 	%p135, %r1159, %r596;
	@%p135 bra 	BB3_149;

	mov.u64 	%rl1393, 0;
	bra.uni 	BB3_151;

BB3_149:
	mov.u32 	%r603, 1;
	sub.s32 	%r117, %r603, %r114;
	mov.u64 	%rl1393, 0;

BB3_150:
	.pragma "nounroll";
	shl.b32 	%r607, %r1159, 3;
	mov.u32 	%r608, __internal_i2opi_d;
	add.s32 	%r609, %r608, %r607;
	ld.const.u64 	%rl785, [%r609];
	mul.lo.s64 	%rl787, %rl785, %rl132;
	// inline asm
	mul.hi.u64 	%rl784, %rl785, %rl132;
	// inline asm
	mad.lo.s64 	%rl788, %rl785, %rl132, %rl1393;
	setp.lt.u64 	%p136, %rl788, %rl787;
	selp.u64 	%rl789, 1, 0, %p136;
	add.s64 	%rl1393, %rl789, %rl784;
	add.s32 	%r610, %r117, %r1159;
	shl.b32 	%r611, %r610, 3;
	add.s32 	%r613, %r48, %r611;
	st.local.u64 	[%r613], %rl788;
	// inline asm
	min.s32 	%r604, %r597, %r116;
	// inline asm
	add.s32 	%r1159, %r1159, 1;
	setp.lt.s32 	%p137, %r1159, %r604;
	@%p137 bra 	BB3_150;

BB3_151:
	mov.u32 	%r614, 1;
	sub.s32 	%r615, %r614, %r114;
	add.s32 	%r616, %r615, %r1159;
	shl.b32 	%r617, %r616, 3;
	add.s32 	%r619, %r48, %r617;
	st.local.u64 	[%r619], %rl1393;
	ld.local.u64 	%rl1394, [%r48+24];
	ld.local.u64 	%rl1395, [%r48+16];
	and.b32  	%r620, %r113, 63;
	setp.eq.s32 	%p138, %r620, 0;
	@%p138 bra 	BB3_153;

	and.b64  	%rl790, %rl131, 63;
	cvt.u32.u64 	%r621, %rl790;
	shl.b64 	%rl791, %rl1394, %r621;
	neg.s32 	%r622, %r113;
	and.b32  	%r623, %r622, 63;
	shr.u64 	%rl792, %rl1395, %r623;
	or.b64  	%rl1394, %rl792, %rl791;
	shl.b64 	%rl793, %rl1395, %r621;
	ld.local.u64 	%rl794, [%r48+8];
	shr.u64 	%rl795, %rl794, %r623;
	or.b64  	%rl1395, %rl795, %rl793;

BB3_153:
	shr.u64 	%rl796, %rl1394, 62;
	cvt.u32.u64 	%r624, %rl796;
	shr.u64 	%rl797, %rl1395, 62;
	shl.b64 	%rl798, %rl1394, 2;
	or.b64  	%rl1400, %rl797, %rl798;
	shl.b64 	%rl143, %rl1395, 2;
	setp.ne.s64 	%p139, %rl143, 0;
	selp.u64 	%rl799, 1, 0, %p139;
	or.b64  	%rl800, %rl799, %rl1400;
	setp.gt.u64 	%p140, %rl800, -9223372036854775808;
	selp.u32 	%r625, 1, 0, %p140;
	add.s32 	%r626, %r625, %r624;
	neg.s32 	%r627, %r626;
	setp.lt.s64 	%p141, %rl129, 0;
	selp.b32 	%r1161, %r627, %r626, %p141;
	@%p140 bra 	BB3_155;

	mov.u64 	%rl1399, %rl143;
	bra.uni 	BB3_156;

BB3_155:
	not.b64 	%rl801, %rl1400;
	neg.s64 	%rl144, %rl143;
	setp.eq.s64 	%p142, %rl143, 0;
	selp.u64 	%rl802, 1, 0, %p142;
	add.s64 	%rl1400, %rl802, %rl801;
	xor.b64  	%rl1396, %rl1396, -9223372036854775808;
	mov.u64 	%rl1399, %rl144;

BB3_156:
	mov.u64 	%rl1398, %rl1399;
	setp.gt.s64 	%p143, %rl1400, 0;
	@%p143 bra 	BB3_158;

	mov.u32 	%r1160, 0;
	bra.uni 	BB3_160;

BB3_158:
	mov.u32 	%r1160, 0;

BB3_159:
	shr.u64 	%rl803, %rl1398, 63;
	shl.b64 	%rl804, %rl1400, 1;
	or.b64  	%rl1400, %rl803, %rl804;
	shl.b64 	%rl1398, %rl1398, 1;
	add.s32 	%r1160, %r1160, -1;
	setp.gt.s64 	%p144, %rl1400, 0;
	@%p144 bra 	BB3_159;

BB3_160:
	mul.lo.s64 	%rl1402, %rl1400, -3958705157555305931;
	mov.u64 	%rl807, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl805, %rl1400, %rl807;
	// inline asm
	setp.gt.s64 	%p145, %rl805, 0;
	mov.u64 	%rl1401, %rl805;
	@%p145 bra 	BB3_161;
	bra.uni 	BB3_162;

BB3_161:
	shl.b64 	%rl808, %rl805, 1;
	shr.u64 	%rl809, %rl1402, 63;
	or.b64  	%rl1401, %rl808, %rl809;
	mul.lo.s64 	%rl1402, %rl1400, -7917410315110611862;
	add.s32 	%r1160, %r1160, -1;

BB3_162:
	setp.ne.s64 	%p146, %rl1402, 0;
	selp.u64 	%rl810, 1, 0, %p146;
	add.s64 	%rl811, %rl810, %rl1401;
	add.s32 	%r630, %r1160, 1022;
	cvt.u64.u32 	%rl812, %r630;
	shl.b64 	%rl813, %rl812, 52;
	shr.u64 	%rl814, %rl811, 11;
	shr.u64 	%rl815, %rl811, 10;
	and.b64  	%rl816, %rl815, 1;
	add.s64 	%rl817, %rl813, %rl814;
	add.s64 	%rl818, %rl817, %rl816;
	or.b64  	%rl819, %rl818, %rl1396;
	mov.b64 	 %fd2637, %rl819;

BB3_163:
	add.s32 	%r128, %r1161, 1;
	and.b32  	%r631, %r128, 1;
	setp.eq.s32 	%p147, %r631, 0;
	mul.rn.f64 	%fd117, %fd2637, %fd2637;
	@%p147 bra 	BB3_165;

	mov.f64 	%fd1028, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1030, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1027, %fd1028, %fd117, %fd1030;
	// inline asm
	mov.f64 	%fd1034, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1031, %fd1027, %fd117, %fd1034;
	// inline asm
	mov.f64 	%fd1038, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1035, %fd1031, %fd117, %fd1038;
	// inline asm
	mov.f64 	%fd1042, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1039, %fd1035, %fd117, %fd1042;
	// inline asm
	mov.f64 	%fd1046, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1043, %fd1039, %fd117, %fd1046;
	// inline asm
	mov.f64 	%fd1050, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1047, %fd1043, %fd117, %fd1050;
	// inline asm
	mov.f64 	%fd1054, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1051, %fd1047, %fd117, %fd1054;
	// inline asm
	mov.f64 	%fd2638, %fd1051;
	bra.uni 	BB3_166;

BB3_165:
	mov.f64 	%fd1056, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1058, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1055, %fd1056, %fd117, %fd1058;
	// inline asm
	mov.f64 	%fd1062, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1059, %fd1055, %fd117, %fd1062;
	// inline asm
	mov.f64 	%fd1066, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1063, %fd1059, %fd117, %fd1066;
	// inline asm
	mov.f64 	%fd1070, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1067, %fd1063, %fd117, %fd1070;
	// inline asm
	mov.f64 	%fd1074, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1071, %fd1067, %fd117, %fd1074;
	// inline asm
	mul.rn.f64 	%fd1076, %fd1071, %fd117;
	// inline asm
	fma.rn.f64 	%fd1075, %fd1076, %fd2637, %fd2637;
	// inline asm
	mov.f64 	%fd2638, %fd1075;

BB3_166:
	and.b32  	%r632, %r128, 2;
	setp.eq.s32 	%p148, %r632, 0;
	neg.f64 	%fd1079, %fd2638;
	selp.f64 	%fd2639, %fd2638, %fd1079, %p148;
	bra.uni 	BB3_168;

BB3_167:
	mov.f64 	%fd2639, 0dFFF8000000000000;

BB3_168:
	setp.eq.f64 	%p149, %fd113, 0d0000000000000000;
	or.pred  	%p150, %p4, %p149;
	or.pred  	%p151, %p3, %p150;
	@%p151 bra 	BB3_191;

	// inline asm
	abs.f64 	%fd1081, %fd113;
	// inline asm
	setp.gt.f64 	%p152, %fd1081, 0d41E0000000000000;
	@%p152 bra 	BB3_171;

	mov.f64 	%fd1096, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1083, %fd113, %fd1096;
	// inline asm
	cvt.rni.s32.f64 	%r633, %fd1083;
	// inline asm
	cvt.rn.f64.s32 	%fd1097, %r633;
	neg.f64 	%fd1093, %fd1097;
	mov.f64 	%fd1086, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1084, %fd1093, %fd1086, %fd113;
	// inline asm
	mov.f64 	%fd1090, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1088, %fd1093, %fd1090, %fd1084;
	// inline asm
	mov.f64 	%fd1094, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1092, %fd1093, %fd1094, %fd1088;
	// inline asm
	mov.u32 	%r1164, %r633;
	mov.f64 	%fd2640, %fd1092;
	bra.uni 	BB3_187;

BB3_171:
	mov.b64 	 %rl161, %fd113;
	and.b64  	%rl1406, %rl161, -9223372036854775808;
	shr.u64 	%rl163, %rl161, 52;
	and.b64  	%rl820, %rl163, 2047;
	add.s64 	%rl821, %rl820, 4294966272;
	cvt.u32.u64 	%r130, %rl821;
	shl.b64 	%rl822, %rl161, 11;
	or.b64  	%rl164, %rl822, -9223372036854775808;
	shr.u32 	%r637, %r130, 6;
	mov.u32 	%r638, 16;
	sub.s32 	%r131, %r638, %r637;
	mov.u32 	%r639, 15;
	sub.s32 	%r1162, %r639, %r637;
	mov.u32 	%r640, 19;
	sub.s32 	%r133, %r640, %r637;
	mov.u32 	%r635, 18;
	// inline asm
	min.s32 	%r634, %r635, %r133;
	// inline asm
	setp.lt.s32 	%p153, %r1162, %r634;
	@%p153 bra 	BB3_173;

	mov.u64 	%rl1403, 0;
	bra.uni 	BB3_175;

BB3_173:
	mov.u32 	%r641, 1;
	sub.s32 	%r134, %r641, %r131;
	mov.u64 	%rl1403, 0;

BB3_174:
	.pragma "nounroll";
	shl.b32 	%r645, %r1162, 3;
	mov.u32 	%r646, __internal_i2opi_d;
	add.s32 	%r647, %r646, %r645;
	ld.const.u64 	%rl826, [%r647];
	mul.lo.s64 	%rl828, %rl826, %rl164;
	// inline asm
	mul.hi.u64 	%rl825, %rl826, %rl164;
	// inline asm
	mad.lo.s64 	%rl829, %rl826, %rl164, %rl1403;
	setp.lt.u64 	%p154, %rl829, %rl828;
	selp.u64 	%rl830, 1, 0, %p154;
	add.s64 	%rl1403, %rl830, %rl825;
	add.s32 	%r648, %r134, %r1162;
	shl.b32 	%r649, %r648, 3;
	add.s32 	%r651, %r48, %r649;
	st.local.u64 	[%r651], %rl829;
	// inline asm
	min.s32 	%r642, %r635, %r133;
	// inline asm
	add.s32 	%r1162, %r1162, 1;
	setp.lt.s32 	%p155, %r1162, %r642;
	@%p155 bra 	BB3_174;

BB3_175:
	mov.u32 	%r652, 1;
	sub.s32 	%r653, %r652, %r131;
	add.s32 	%r654, %r653, %r1162;
	shl.b32 	%r655, %r654, 3;
	add.s32 	%r657, %r48, %r655;
	st.local.u64 	[%r657], %rl1403;
	ld.local.u64 	%rl1404, [%r48+24];
	ld.local.u64 	%rl1405, [%r48+16];
	and.b32  	%r658, %r130, 63;
	setp.eq.s32 	%p156, %r658, 0;
	@%p156 bra 	BB3_177;

	and.b64  	%rl831, %rl163, 63;
	cvt.u32.u64 	%r659, %rl831;
	shl.b64 	%rl832, %rl1404, %r659;
	neg.s32 	%r660, %r130;
	and.b32  	%r661, %r660, 63;
	shr.u64 	%rl833, %rl1405, %r661;
	or.b64  	%rl1404, %rl833, %rl832;
	shl.b64 	%rl834, %rl1405, %r659;
	ld.local.u64 	%rl835, [%r48+8];
	shr.u64 	%rl836, %rl835, %r661;
	or.b64  	%rl1405, %rl836, %rl834;

BB3_177:
	shr.u64 	%rl837, %rl1404, 62;
	cvt.u32.u64 	%r662, %rl837;
	shr.u64 	%rl838, %rl1405, 62;
	shl.b64 	%rl839, %rl1404, 2;
	or.b64  	%rl1410, %rl838, %rl839;
	shl.b64 	%rl175, %rl1405, 2;
	setp.ne.s64 	%p157, %rl175, 0;
	selp.u64 	%rl840, 1, 0, %p157;
	or.b64  	%rl841, %rl840, %rl1410;
	setp.gt.u64 	%p158, %rl841, -9223372036854775808;
	selp.u32 	%r663, 1, 0, %p158;
	add.s32 	%r664, %r663, %r662;
	neg.s32 	%r665, %r664;
	setp.lt.s64 	%p159, %rl161, 0;
	selp.b32 	%r1164, %r665, %r664, %p159;
	@%p158 bra 	BB3_179;

	mov.u64 	%rl1409, %rl175;
	bra.uni 	BB3_180;

BB3_179:
	not.b64 	%rl842, %rl1410;
	neg.s64 	%rl176, %rl175;
	setp.eq.s64 	%p160, %rl175, 0;
	selp.u64 	%rl843, 1, 0, %p160;
	add.s64 	%rl1410, %rl843, %rl842;
	xor.b64  	%rl1406, %rl1406, -9223372036854775808;
	mov.u64 	%rl1409, %rl176;

BB3_180:
	mov.u64 	%rl1408, %rl1409;
	setp.gt.s64 	%p161, %rl1410, 0;
	@%p161 bra 	BB3_182;

	mov.u32 	%r1163, 0;
	bra.uni 	BB3_184;

BB3_182:
	mov.u32 	%r1163, 0;

BB3_183:
	shr.u64 	%rl844, %rl1408, 63;
	shl.b64 	%rl845, %rl1410, 1;
	or.b64  	%rl1410, %rl844, %rl845;
	shl.b64 	%rl1408, %rl1408, 1;
	add.s32 	%r1163, %r1163, -1;
	setp.gt.s64 	%p162, %rl1410, 0;
	@%p162 bra 	BB3_183;

BB3_184:
	mul.lo.s64 	%rl1412, %rl1410, -3958705157555305931;
	mov.u64 	%rl848, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl846, %rl1410, %rl848;
	// inline asm
	setp.gt.s64 	%p163, %rl846, 0;
	mov.u64 	%rl1411, %rl846;
	@%p163 bra 	BB3_185;
	bra.uni 	BB3_186;

BB3_185:
	shl.b64 	%rl849, %rl846, 1;
	shr.u64 	%rl850, %rl1412, 63;
	or.b64  	%rl1411, %rl849, %rl850;
	mul.lo.s64 	%rl1412, %rl1410, -7917410315110611862;
	add.s32 	%r1163, %r1163, -1;

BB3_186:
	setp.ne.s64 	%p164, %rl1412, 0;
	selp.u64 	%rl851, 1, 0, %p164;
	add.s64 	%rl852, %rl851, %rl1411;
	add.s32 	%r668, %r1163, 1022;
	cvt.u64.u32 	%rl853, %r668;
	shl.b64 	%rl854, %rl853, 52;
	shr.u64 	%rl855, %rl852, 11;
	shr.u64 	%rl856, %rl852, 10;
	and.b64  	%rl857, %rl856, 1;
	add.s64 	%rl858, %rl854, %rl855;
	add.s64 	%rl859, %rl858, %rl857;
	or.b64  	%rl860, %rl859, %rl1406;
	mov.b64 	 %fd2640, %rl860;

BB3_187:
	and.b32  	%r669, %r1164, 1;
	setp.eq.s32 	%p165, %r669, 0;
	mul.rn.f64 	%fd127, %fd2640, %fd2640;
	@%p165 bra 	BB3_189;

	mov.f64 	%fd1099, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1101, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1098, %fd1099, %fd127, %fd1101;
	// inline asm
	mov.f64 	%fd1105, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1102, %fd1098, %fd127, %fd1105;
	// inline asm
	mov.f64 	%fd1109, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1106, %fd1102, %fd127, %fd1109;
	// inline asm
	mov.f64 	%fd1113, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1110, %fd1106, %fd127, %fd1113;
	// inline asm
	mov.f64 	%fd1117, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1114, %fd1110, %fd127, %fd1117;
	// inline asm
	mov.f64 	%fd1121, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1118, %fd1114, %fd127, %fd1121;
	// inline asm
	mov.f64 	%fd1125, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1122, %fd1118, %fd127, %fd1125;
	// inline asm
	mov.f64 	%fd2641, %fd1122;
	bra.uni 	BB3_190;

BB3_189:
	mov.f64 	%fd1127, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1129, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1126, %fd1127, %fd127, %fd1129;
	// inline asm
	mov.f64 	%fd1133, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1130, %fd1126, %fd127, %fd1133;
	// inline asm
	mov.f64 	%fd1137, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1134, %fd1130, %fd127, %fd1137;
	// inline asm
	mov.f64 	%fd1141, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1138, %fd1134, %fd127, %fd1141;
	// inline asm
	mov.f64 	%fd1145, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1142, %fd1138, %fd127, %fd1145;
	// inline asm
	mul.rn.f64 	%fd1147, %fd1142, %fd127;
	// inline asm
	fma.rn.f64 	%fd1146, %fd1147, %fd2640, %fd2640;
	// inline asm
	mov.f64 	%fd2641, %fd1146;

BB3_190:
	and.b32  	%r670, %r1164, 2;
	setp.eq.s32 	%p166, %r670, 0;
	neg.f64 	%fd1150, %fd2641;
	selp.f64 	%fd2642, %fd2641, %fd1150, %p166;
	bra.uni 	BB3_192;

BB3_191:
	mov.f64 	%fd1151, 0d0000000000000000;
	mul.rn.f64 	%fd2642, %fd113, %fd1151;

BB3_192:
	neg.f64 	%fd1152, %fd2642;
	mov.f64 	%fd2711, %fd2639;
	mov.f64 	%fd2712, %fd1152;
	ld.param.u32 	%r1134, [DIT10C2C_param_3];
	setp.eq.s32 	%p167, %r1134, 0;
	@%p167 bra 	BB3_193;
	bra.uni 	BB3_194;

BB3_193:
	mov.f64 	%fd2711, %fd2639;
	mov.f64 	%fd2712, %fd2642;

BB3_194:
	mul.f64 	%fd1154, %fd55, %fd2711;
	neg.f64 	%fd1156, %fd56;
	fma.rn.f64 	%fd1157, %fd1156, %fd2712, %fd1154;
	mul.f64 	%fd1158, %fd56, %fd2711;
	fma.rn.f64 	%fd1159, %fd55, %fd2712, %fd1158;
	mul.f64 	%fd1160, %fd69, 0d403921FB54442D18;
	div.rn.f64 	%fd134, %fd1160, %fd70;
	setp.eq.f64 	%p5, %fd134, 0d7FF0000000000000;
	setp.eq.f64 	%p6, %fd134, 0dFFF0000000000000;
	or.pred  	%p168, %p5, %p6;
	mov.f64 	%fd2679, %fd705;
	mov.f64 	%fd2680, %fd706;
	mov.f64 	%fd2681, %fd855;
	mov.f64 	%fd2682, %fd857;
	mov.f64 	%fd2683, %fd1006;
	mov.f64 	%fd2684, %fd1008;
	mov.f64 	%fd2685, %fd1157;
	mov.f64 	%fd2686, %fd1159;
	@%p168 bra 	BB3_217;

	// inline asm
	abs.f64 	%fd1161, %fd134;
	// inline asm
	setp.gt.f64 	%p169, %fd1161, 0d41E0000000000000;
	@%p169 bra 	BB3_197;

	mov.f64 	%fd1176, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1163, %fd134, %fd1176;
	// inline asm
	cvt.rni.s32.f64 	%r671, %fd1163;
	// inline asm
	cvt.rn.f64.s32 	%fd1177, %r671;
	neg.f64 	%fd1173, %fd1177;
	mov.f64 	%fd1166, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1164, %fd1173, %fd1166, %fd134;
	// inline asm
	mov.f64 	%fd1170, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1168, %fd1173, %fd1170, %fd1164;
	// inline asm
	mov.f64 	%fd1174, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1172, %fd1173, %fd1174, %fd1168;
	// inline asm
	mov.u32 	%r1167, %r671;
	mov.f64 	%fd2643, %fd1172;
	bra.uni 	BB3_213;

BB3_197:
	mov.b64 	 %rl193, %fd134;
	and.b64  	%rl1416, %rl193, -9223372036854775808;
	shr.u64 	%rl195, %rl193, 52;
	and.b64  	%rl861, %rl195, 2047;
	add.s64 	%rl862, %rl861, 4294966272;
	cvt.u32.u64 	%r146, %rl862;
	shl.b64 	%rl863, %rl193, 11;
	or.b64  	%rl196, %rl863, -9223372036854775808;
	shr.u32 	%r675, %r146, 6;
	mov.u32 	%r676, 16;
	sub.s32 	%r147, %r676, %r675;
	mov.u32 	%r677, 15;
	sub.s32 	%r1165, %r677, %r675;
	mov.u32 	%r678, 19;
	sub.s32 	%r149, %r678, %r675;
	mov.u32 	%r673, 18;
	// inline asm
	min.s32 	%r672, %r673, %r149;
	// inline asm
	setp.lt.s32 	%p170, %r1165, %r672;
	@%p170 bra 	BB3_199;

	mov.u64 	%rl1413, 0;
	bra.uni 	BB3_201;

BB3_199:
	mov.u32 	%r679, 1;
	sub.s32 	%r150, %r679, %r147;
	mov.u64 	%rl1413, 0;

BB3_200:
	.pragma "nounroll";
	shl.b32 	%r683, %r1165, 3;
	mov.u32 	%r684, __internal_i2opi_d;
	add.s32 	%r685, %r684, %r683;
	ld.const.u64 	%rl867, [%r685];
	mul.lo.s64 	%rl869, %rl867, %rl196;
	// inline asm
	mul.hi.u64 	%rl866, %rl867, %rl196;
	// inline asm
	mad.lo.s64 	%rl870, %rl867, %rl196, %rl1413;
	setp.lt.u64 	%p171, %rl870, %rl869;
	selp.u64 	%rl871, 1, 0, %p171;
	add.s64 	%rl1413, %rl871, %rl866;
	add.s32 	%r686, %r150, %r1165;
	shl.b32 	%r687, %r686, 3;
	add.s32 	%r689, %r48, %r687;
	st.local.u64 	[%r689], %rl870;
	// inline asm
	min.s32 	%r680, %r673, %r149;
	// inline asm
	add.s32 	%r1165, %r1165, 1;
	setp.lt.s32 	%p172, %r1165, %r680;
	@%p172 bra 	BB3_200;

BB3_201:
	mov.u32 	%r690, 1;
	sub.s32 	%r691, %r690, %r147;
	add.s32 	%r692, %r691, %r1165;
	shl.b32 	%r693, %r692, 3;
	add.s32 	%r695, %r48, %r693;
	st.local.u64 	[%r695], %rl1413;
	ld.local.u64 	%rl1414, [%r48+24];
	ld.local.u64 	%rl1415, [%r48+16];
	and.b32  	%r696, %r146, 63;
	setp.eq.s32 	%p173, %r696, 0;
	@%p173 bra 	BB3_203;

	and.b64  	%rl872, %rl195, 63;
	cvt.u32.u64 	%r697, %rl872;
	shl.b64 	%rl873, %rl1414, %r697;
	neg.s32 	%r698, %r146;
	and.b32  	%r699, %r698, 63;
	shr.u64 	%rl874, %rl1415, %r699;
	or.b64  	%rl1414, %rl874, %rl873;
	shl.b64 	%rl875, %rl1415, %r697;
	ld.local.u64 	%rl876, [%r48+8];
	shr.u64 	%rl877, %rl876, %r699;
	or.b64  	%rl1415, %rl877, %rl875;

BB3_203:
	shr.u64 	%rl878, %rl1414, 62;
	cvt.u32.u64 	%r700, %rl878;
	shr.u64 	%rl879, %rl1415, 62;
	shl.b64 	%rl880, %rl1414, 2;
	or.b64  	%rl1420, %rl879, %rl880;
	shl.b64 	%rl207, %rl1415, 2;
	setp.ne.s64 	%p174, %rl207, 0;
	selp.u64 	%rl881, 1, 0, %p174;
	or.b64  	%rl882, %rl881, %rl1420;
	setp.gt.u64 	%p175, %rl882, -9223372036854775808;
	selp.u32 	%r701, 1, 0, %p175;
	add.s32 	%r702, %r701, %r700;
	neg.s32 	%r703, %r702;
	setp.lt.s64 	%p176, %rl193, 0;
	selp.b32 	%r1167, %r703, %r702, %p176;
	@%p175 bra 	BB3_205;

	mov.u64 	%rl1419, %rl207;
	bra.uni 	BB3_206;

BB3_205:
	not.b64 	%rl883, %rl1420;
	neg.s64 	%rl208, %rl207;
	setp.eq.s64 	%p177, %rl207, 0;
	selp.u64 	%rl884, 1, 0, %p177;
	add.s64 	%rl1420, %rl884, %rl883;
	xor.b64  	%rl1416, %rl1416, -9223372036854775808;
	mov.u64 	%rl1419, %rl208;

BB3_206:
	mov.u64 	%rl1418, %rl1419;
	setp.gt.s64 	%p178, %rl1420, 0;
	@%p178 bra 	BB3_208;

	mov.u32 	%r1166, 0;
	bra.uni 	BB3_210;

BB3_208:
	mov.u32 	%r1166, 0;

BB3_209:
	shr.u64 	%rl885, %rl1418, 63;
	shl.b64 	%rl886, %rl1420, 1;
	or.b64  	%rl1420, %rl885, %rl886;
	shl.b64 	%rl1418, %rl1418, 1;
	add.s32 	%r1166, %r1166, -1;
	setp.gt.s64 	%p179, %rl1420, 0;
	@%p179 bra 	BB3_209;

BB3_210:
	mul.lo.s64 	%rl1422, %rl1420, -3958705157555305931;
	mov.u64 	%rl889, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl887, %rl1420, %rl889;
	// inline asm
	setp.gt.s64 	%p180, %rl887, 0;
	mov.u64 	%rl1421, %rl887;
	@%p180 bra 	BB3_211;
	bra.uni 	BB3_212;

BB3_211:
	shl.b64 	%rl890, %rl887, 1;
	shr.u64 	%rl891, %rl1422, 63;
	or.b64  	%rl1421, %rl890, %rl891;
	mul.lo.s64 	%rl1422, %rl1420, -7917410315110611862;
	add.s32 	%r1166, %r1166, -1;

BB3_212:
	setp.ne.s64 	%p181, %rl1422, 0;
	selp.u64 	%rl892, 1, 0, %p181;
	add.s64 	%rl893, %rl892, %rl1421;
	add.s32 	%r706, %r1166, 1022;
	cvt.u64.u32 	%rl894, %r706;
	shl.b64 	%rl895, %rl894, 52;
	shr.u64 	%rl896, %rl893, 11;
	shr.u64 	%rl897, %rl893, 10;
	and.b64  	%rl898, %rl897, 1;
	add.s64 	%rl899, %rl895, %rl896;
	add.s64 	%rl900, %rl899, %rl898;
	or.b64  	%rl901, %rl900, %rl1416;
	mov.b64 	 %fd2643, %rl901;

BB3_213:
	add.s32 	%r161, %r1167, 1;
	and.b32  	%r707, %r161, 1;
	setp.eq.s32 	%p182, %r707, 0;
	mul.rn.f64 	%fd138, %fd2643, %fd2643;
	@%p182 bra 	BB3_215;

	mov.f64 	%fd1179, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1181, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1178, %fd1179, %fd138, %fd1181;
	// inline asm
	mov.f64 	%fd1185, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1182, %fd1178, %fd138, %fd1185;
	// inline asm
	mov.f64 	%fd1189, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1186, %fd1182, %fd138, %fd1189;
	// inline asm
	mov.f64 	%fd1193, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1190, %fd1186, %fd138, %fd1193;
	// inline asm
	mov.f64 	%fd1197, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1194, %fd1190, %fd138, %fd1197;
	// inline asm
	mov.f64 	%fd1201, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1198, %fd1194, %fd138, %fd1201;
	// inline asm
	mov.f64 	%fd1205, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1202, %fd1198, %fd138, %fd1205;
	// inline asm
	mov.f64 	%fd2644, %fd1202;
	bra.uni 	BB3_216;

BB3_215:
	mov.f64 	%fd1207, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1209, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1206, %fd1207, %fd138, %fd1209;
	// inline asm
	mov.f64 	%fd1213, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1210, %fd1206, %fd138, %fd1213;
	// inline asm
	mov.f64 	%fd1217, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1214, %fd1210, %fd138, %fd1217;
	// inline asm
	mov.f64 	%fd1221, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1218, %fd1214, %fd138, %fd1221;
	// inline asm
	mov.f64 	%fd1225, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1222, %fd1218, %fd138, %fd1225;
	// inline asm
	mul.rn.f64 	%fd1227, %fd1222, %fd138;
	// inline asm
	fma.rn.f64 	%fd1226, %fd1227, %fd2643, %fd2643;
	// inline asm
	mov.f64 	%fd2644, %fd1226;

BB3_216:
	and.b32  	%r708, %r161, 2;
	setp.eq.s32 	%p183, %r708, 0;
	neg.f64 	%fd1230, %fd2644;
	selp.f64 	%fd2645, %fd2644, %fd1230, %p183;
	bra.uni 	BB3_218;

BB3_217:
	mov.f64 	%fd2645, 0dFFF8000000000000;

BB3_218:
	setp.eq.f64 	%p184, %fd134, 0d0000000000000000;
	or.pred  	%p185, %p6, %p184;
	or.pred  	%p186, %p5, %p185;
	@%p186 bra 	BB3_241;

	// inline asm
	abs.f64 	%fd1232, %fd134;
	// inline asm
	setp.gt.f64 	%p187, %fd1232, 0d41E0000000000000;
	@%p187 bra 	BB3_221;

	mov.f64 	%fd1247, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1234, %fd134, %fd1247;
	// inline asm
	cvt.rni.s32.f64 	%r709, %fd1234;
	// inline asm
	cvt.rn.f64.s32 	%fd1248, %r709;
	neg.f64 	%fd1244, %fd1248;
	mov.f64 	%fd1237, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1235, %fd1244, %fd1237, %fd134;
	// inline asm
	mov.f64 	%fd1241, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1239, %fd1244, %fd1241, %fd1235;
	// inline asm
	mov.f64 	%fd1245, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1243, %fd1244, %fd1245, %fd1239;
	// inline asm
	mov.u32 	%r1170, %r709;
	mov.f64 	%fd2646, %fd1243;
	bra.uni 	BB3_237;

BB3_221:
	mov.b64 	 %rl225, %fd134;
	and.b64  	%rl1426, %rl225, -9223372036854775808;
	shr.u64 	%rl227, %rl225, 52;
	and.b64  	%rl902, %rl227, 2047;
	add.s64 	%rl903, %rl902, 4294966272;
	cvt.u32.u64 	%r163, %rl903;
	shl.b64 	%rl904, %rl225, 11;
	or.b64  	%rl228, %rl904, -9223372036854775808;
	shr.u32 	%r713, %r163, 6;
	mov.u32 	%r714, 16;
	sub.s32 	%r164, %r714, %r713;
	mov.u32 	%r715, 15;
	sub.s32 	%r1168, %r715, %r713;
	mov.u32 	%r716, 19;
	sub.s32 	%r166, %r716, %r713;
	mov.u32 	%r711, 18;
	// inline asm
	min.s32 	%r710, %r711, %r166;
	// inline asm
	setp.lt.s32 	%p188, %r1168, %r710;
	@%p188 bra 	BB3_223;

	mov.u64 	%rl1423, 0;
	bra.uni 	BB3_225;

BB3_223:
	mov.u32 	%r717, 1;
	sub.s32 	%r167, %r717, %r164;
	mov.u64 	%rl1423, 0;

BB3_224:
	.pragma "nounroll";
	shl.b32 	%r721, %r1168, 3;
	mov.u32 	%r722, __internal_i2opi_d;
	add.s32 	%r723, %r722, %r721;
	ld.const.u64 	%rl908, [%r723];
	mul.lo.s64 	%rl910, %rl908, %rl228;
	// inline asm
	mul.hi.u64 	%rl907, %rl908, %rl228;
	// inline asm
	mad.lo.s64 	%rl911, %rl908, %rl228, %rl1423;
	setp.lt.u64 	%p189, %rl911, %rl910;
	selp.u64 	%rl912, 1, 0, %p189;
	add.s64 	%rl1423, %rl912, %rl907;
	add.s32 	%r724, %r167, %r1168;
	shl.b32 	%r725, %r724, 3;
	add.s32 	%r727, %r48, %r725;
	st.local.u64 	[%r727], %rl911;
	// inline asm
	min.s32 	%r718, %r711, %r166;
	// inline asm
	add.s32 	%r1168, %r1168, 1;
	setp.lt.s32 	%p190, %r1168, %r718;
	@%p190 bra 	BB3_224;

BB3_225:
	mov.u32 	%r728, 1;
	sub.s32 	%r729, %r728, %r164;
	add.s32 	%r730, %r729, %r1168;
	shl.b32 	%r731, %r730, 3;
	add.s32 	%r733, %r48, %r731;
	st.local.u64 	[%r733], %rl1423;
	ld.local.u64 	%rl1424, [%r48+24];
	ld.local.u64 	%rl1425, [%r48+16];
	and.b32  	%r734, %r163, 63;
	setp.eq.s32 	%p191, %r734, 0;
	@%p191 bra 	BB3_227;

	and.b64  	%rl913, %rl227, 63;
	cvt.u32.u64 	%r735, %rl913;
	shl.b64 	%rl914, %rl1424, %r735;
	neg.s32 	%r736, %r163;
	and.b32  	%r737, %r736, 63;
	shr.u64 	%rl915, %rl1425, %r737;
	or.b64  	%rl1424, %rl915, %rl914;
	shl.b64 	%rl916, %rl1425, %r735;
	ld.local.u64 	%rl917, [%r48+8];
	shr.u64 	%rl918, %rl917, %r737;
	or.b64  	%rl1425, %rl918, %rl916;

BB3_227:
	shr.u64 	%rl919, %rl1424, 62;
	cvt.u32.u64 	%r738, %rl919;
	shr.u64 	%rl920, %rl1425, 62;
	shl.b64 	%rl921, %rl1424, 2;
	or.b64  	%rl1430, %rl920, %rl921;
	shl.b64 	%rl239, %rl1425, 2;
	setp.ne.s64 	%p192, %rl239, 0;
	selp.u64 	%rl922, 1, 0, %p192;
	or.b64  	%rl923, %rl922, %rl1430;
	setp.gt.u64 	%p193, %rl923, -9223372036854775808;
	selp.u32 	%r739, 1, 0, %p193;
	add.s32 	%r740, %r739, %r738;
	neg.s32 	%r741, %r740;
	setp.lt.s64 	%p194, %rl225, 0;
	selp.b32 	%r1170, %r741, %r740, %p194;
	@%p193 bra 	BB3_229;

	mov.u64 	%rl1429, %rl239;
	bra.uni 	BB3_230;

BB3_229:
	not.b64 	%rl924, %rl1430;
	neg.s64 	%rl240, %rl239;
	setp.eq.s64 	%p195, %rl239, 0;
	selp.u64 	%rl925, 1, 0, %p195;
	add.s64 	%rl1430, %rl925, %rl924;
	xor.b64  	%rl1426, %rl1426, -9223372036854775808;
	mov.u64 	%rl1429, %rl240;

BB3_230:
	mov.u64 	%rl1428, %rl1429;
	setp.gt.s64 	%p196, %rl1430, 0;
	@%p196 bra 	BB3_232;

	mov.u32 	%r1169, 0;
	bra.uni 	BB3_234;

BB3_232:
	mov.u32 	%r1169, 0;

BB3_233:
	shr.u64 	%rl926, %rl1428, 63;
	shl.b64 	%rl927, %rl1430, 1;
	or.b64  	%rl1430, %rl926, %rl927;
	shl.b64 	%rl1428, %rl1428, 1;
	add.s32 	%r1169, %r1169, -1;
	setp.gt.s64 	%p197, %rl1430, 0;
	@%p197 bra 	BB3_233;

BB3_234:
	mul.lo.s64 	%rl1432, %rl1430, -3958705157555305931;
	mov.u64 	%rl930, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl928, %rl1430, %rl930;
	// inline asm
	setp.gt.s64 	%p198, %rl928, 0;
	mov.u64 	%rl1431, %rl928;
	@%p198 bra 	BB3_235;
	bra.uni 	BB3_236;

BB3_235:
	shl.b64 	%rl931, %rl928, 1;
	shr.u64 	%rl932, %rl1432, 63;
	or.b64  	%rl1431, %rl931, %rl932;
	mul.lo.s64 	%rl1432, %rl1430, -7917410315110611862;
	add.s32 	%r1169, %r1169, -1;

BB3_236:
	setp.ne.s64 	%p199, %rl1432, 0;
	selp.u64 	%rl933, 1, 0, %p199;
	add.s64 	%rl934, %rl933, %rl1431;
	add.s32 	%r744, %r1169, 1022;
	cvt.u64.u32 	%rl935, %r744;
	shl.b64 	%rl936, %rl935, 52;
	shr.u64 	%rl937, %rl934, 11;
	shr.u64 	%rl938, %rl934, 10;
	and.b64  	%rl939, %rl938, 1;
	add.s64 	%rl940, %rl936, %rl937;
	add.s64 	%rl941, %rl940, %rl939;
	or.b64  	%rl942, %rl941, %rl1426;
	mov.b64 	 %fd2646, %rl942;

BB3_237:
	and.b32  	%r745, %r1170, 1;
	setp.eq.s32 	%p200, %r745, 0;
	mul.rn.f64 	%fd148, %fd2646, %fd2646;
	@%p200 bra 	BB3_239;

	mov.f64 	%fd1250, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1252, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1249, %fd1250, %fd148, %fd1252;
	// inline asm
	mov.f64 	%fd1256, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1253, %fd1249, %fd148, %fd1256;
	// inline asm
	mov.f64 	%fd1260, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1257, %fd1253, %fd148, %fd1260;
	// inline asm
	mov.f64 	%fd1264, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1261, %fd1257, %fd148, %fd1264;
	// inline asm
	mov.f64 	%fd1268, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1265, %fd1261, %fd148, %fd1268;
	// inline asm
	mov.f64 	%fd1272, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1269, %fd1265, %fd148, %fd1272;
	// inline asm
	mov.f64 	%fd1276, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1273, %fd1269, %fd148, %fd1276;
	// inline asm
	mov.f64 	%fd2647, %fd1273;
	bra.uni 	BB3_240;

BB3_239:
	mov.f64 	%fd1278, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1280, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1277, %fd1278, %fd148, %fd1280;
	// inline asm
	mov.f64 	%fd1284, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1281, %fd1277, %fd148, %fd1284;
	// inline asm
	mov.f64 	%fd1288, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1285, %fd1281, %fd148, %fd1288;
	// inline asm
	mov.f64 	%fd1292, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1289, %fd1285, %fd148, %fd1292;
	// inline asm
	mov.f64 	%fd1296, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1293, %fd1289, %fd148, %fd1296;
	// inline asm
	mul.rn.f64 	%fd1298, %fd1293, %fd148;
	// inline asm
	fma.rn.f64 	%fd1297, %fd1298, %fd2646, %fd2646;
	// inline asm
	mov.f64 	%fd2647, %fd1297;

BB3_240:
	and.b32  	%r746, %r1170, 2;
	setp.eq.s32 	%p201, %r746, 0;
	neg.f64 	%fd1301, %fd2647;
	selp.f64 	%fd2648, %fd2647, %fd1301, %p201;
	bra.uni 	BB3_242;

BB3_241:
	mov.f64 	%fd1302, 0d0000000000000000;
	mul.rn.f64 	%fd2648, %fd134, %fd1302;

BB3_242:
	neg.f64 	%fd1303, %fd2648;
	mov.f64 	%fd2709, %fd2645;
	mov.f64 	%fd2710, %fd1303;
	ld.param.u32 	%r1133, [DIT10C2C_param_3];
	setp.eq.s32 	%p202, %r1133, 0;
	@%p202 bra 	BB3_243;
	bra.uni 	BB3_244;

BB3_243:
	mov.f64 	%fd2709, %fd2645;
	mov.f64 	%fd2710, %fd2648;

BB3_244:
	mul.f64 	%fd1305, %fd57, %fd2709;
	neg.f64 	%fd1307, %fd58;
	fma.rn.f64 	%fd1308, %fd1307, %fd2710, %fd1305;
	mul.f64 	%fd1309, %fd58, %fd2709;
	fma.rn.f64 	%fd1310, %fd57, %fd2710, %fd1309;
	mov.f64 	%fd2687, %fd1308;
	mov.f64 	%fd2688, %fd1310;
	mul.f64 	%fd1311, %fd69, 0d403F6A7A2955385E;
	div.rn.f64 	%fd155, %fd1311, %fd70;
	setp.eq.f64 	%p7, %fd155, 0d7FF0000000000000;
	setp.eq.f64 	%p8, %fd155, 0dFFF0000000000000;
	or.pred  	%p203, %p7, %p8;
	@%p203 bra 	BB3_267;

	// inline asm
	abs.f64 	%fd1312, %fd155;
	// inline asm
	setp.gt.f64 	%p204, %fd1312, 0d41E0000000000000;
	@%p204 bra 	BB3_247;

	mov.f64 	%fd1327, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1314, %fd155, %fd1327;
	// inline asm
	cvt.rni.s32.f64 	%r747, %fd1314;
	// inline asm
	cvt.rn.f64.s32 	%fd1328, %r747;
	neg.f64 	%fd1324, %fd1328;
	mov.f64 	%fd1317, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1315, %fd1324, %fd1317, %fd155;
	// inline asm
	mov.f64 	%fd1321, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1319, %fd1324, %fd1321, %fd1315;
	// inline asm
	mov.f64 	%fd1325, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1323, %fd1324, %fd1325, %fd1319;
	// inline asm
	mov.u32 	%r1173, %r747;
	mov.f64 	%fd2649, %fd1323;
	bra.uni 	BB3_263;

BB3_247:
	mov.b64 	 %rl257, %fd155;
	and.b64  	%rl1436, %rl257, -9223372036854775808;
	shr.u64 	%rl259, %rl257, 52;
	and.b64  	%rl943, %rl259, 2047;
	add.s64 	%rl944, %rl943, 4294966272;
	cvt.u32.u64 	%r179, %rl944;
	shl.b64 	%rl945, %rl257, 11;
	or.b64  	%rl260, %rl945, -9223372036854775808;
	shr.u32 	%r751, %r179, 6;
	mov.u32 	%r752, 16;
	sub.s32 	%r180, %r752, %r751;
	mov.u32 	%r753, 15;
	sub.s32 	%r1171, %r753, %r751;
	mov.u32 	%r754, 19;
	sub.s32 	%r182, %r754, %r751;
	mov.u32 	%r749, 18;
	// inline asm
	min.s32 	%r748, %r749, %r182;
	// inline asm
	setp.lt.s32 	%p205, %r1171, %r748;
	@%p205 bra 	BB3_249;

	mov.u64 	%rl1433, 0;
	bra.uni 	BB3_251;

BB3_249:
	mov.u32 	%r755, 1;
	sub.s32 	%r183, %r755, %r180;
	mov.u64 	%rl1433, 0;

BB3_250:
	.pragma "nounroll";
	shl.b32 	%r759, %r1171, 3;
	mov.u32 	%r760, __internal_i2opi_d;
	add.s32 	%r761, %r760, %r759;
	ld.const.u64 	%rl949, [%r761];
	mul.lo.s64 	%rl951, %rl949, %rl260;
	// inline asm
	mul.hi.u64 	%rl948, %rl949, %rl260;
	// inline asm
	mad.lo.s64 	%rl952, %rl949, %rl260, %rl1433;
	setp.lt.u64 	%p206, %rl952, %rl951;
	selp.u64 	%rl953, 1, 0, %p206;
	add.s64 	%rl1433, %rl953, %rl948;
	add.s32 	%r762, %r183, %r1171;
	shl.b32 	%r763, %r762, 3;
	add.s32 	%r765, %r48, %r763;
	st.local.u64 	[%r765], %rl952;
	// inline asm
	min.s32 	%r756, %r749, %r182;
	// inline asm
	add.s32 	%r1171, %r1171, 1;
	setp.lt.s32 	%p207, %r1171, %r756;
	@%p207 bra 	BB3_250;

BB3_251:
	mov.u32 	%r766, 1;
	sub.s32 	%r767, %r766, %r180;
	add.s32 	%r768, %r767, %r1171;
	shl.b32 	%r769, %r768, 3;
	add.s32 	%r771, %r48, %r769;
	st.local.u64 	[%r771], %rl1433;
	ld.local.u64 	%rl1434, [%r48+24];
	ld.local.u64 	%rl1435, [%r48+16];
	and.b32  	%r772, %r179, 63;
	setp.eq.s32 	%p208, %r772, 0;
	@%p208 bra 	BB3_253;

	and.b64  	%rl954, %rl259, 63;
	cvt.u32.u64 	%r773, %rl954;
	shl.b64 	%rl955, %rl1434, %r773;
	neg.s32 	%r774, %r179;
	and.b32  	%r775, %r774, 63;
	shr.u64 	%rl956, %rl1435, %r775;
	or.b64  	%rl1434, %rl956, %rl955;
	shl.b64 	%rl957, %rl1435, %r773;
	ld.local.u64 	%rl958, [%r48+8];
	shr.u64 	%rl959, %rl958, %r775;
	or.b64  	%rl1435, %rl959, %rl957;

BB3_253:
	shr.u64 	%rl960, %rl1434, 62;
	cvt.u32.u64 	%r776, %rl960;
	shr.u64 	%rl961, %rl1435, 62;
	shl.b64 	%rl962, %rl1434, 2;
	or.b64  	%rl1440, %rl961, %rl962;
	shl.b64 	%rl271, %rl1435, 2;
	setp.ne.s64 	%p209, %rl271, 0;
	selp.u64 	%rl963, 1, 0, %p209;
	or.b64  	%rl964, %rl963, %rl1440;
	setp.gt.u64 	%p210, %rl964, -9223372036854775808;
	selp.u32 	%r777, 1, 0, %p210;
	add.s32 	%r778, %r777, %r776;
	neg.s32 	%r779, %r778;
	setp.lt.s64 	%p211, %rl257, 0;
	selp.b32 	%r1173, %r779, %r778, %p211;
	@%p210 bra 	BB3_255;

	mov.u64 	%rl1439, %rl271;
	bra.uni 	BB3_256;

BB3_255:
	not.b64 	%rl965, %rl1440;
	neg.s64 	%rl272, %rl271;
	setp.eq.s64 	%p212, %rl271, 0;
	selp.u64 	%rl966, 1, 0, %p212;
	add.s64 	%rl1440, %rl966, %rl965;
	xor.b64  	%rl1436, %rl1436, -9223372036854775808;
	mov.u64 	%rl1439, %rl272;

BB3_256:
	mov.u64 	%rl1438, %rl1439;
	setp.gt.s64 	%p213, %rl1440, 0;
	@%p213 bra 	BB3_258;

	mov.u32 	%r1172, 0;
	bra.uni 	BB3_260;

BB3_258:
	mov.u32 	%r1172, 0;

BB3_259:
	shr.u64 	%rl967, %rl1438, 63;
	shl.b64 	%rl968, %rl1440, 1;
	or.b64  	%rl1440, %rl967, %rl968;
	shl.b64 	%rl1438, %rl1438, 1;
	add.s32 	%r1172, %r1172, -1;
	setp.gt.s64 	%p214, %rl1440, 0;
	@%p214 bra 	BB3_259;

BB3_260:
	mul.lo.s64 	%rl1442, %rl1440, -3958705157555305931;
	mov.u64 	%rl971, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl969, %rl1440, %rl971;
	// inline asm
	setp.gt.s64 	%p215, %rl969, 0;
	mov.u64 	%rl1441, %rl969;
	@%p215 bra 	BB3_261;
	bra.uni 	BB3_262;

BB3_261:
	shl.b64 	%rl972, %rl969, 1;
	shr.u64 	%rl973, %rl1442, 63;
	or.b64  	%rl1441, %rl972, %rl973;
	mul.lo.s64 	%rl1442, %rl1440, -7917410315110611862;
	add.s32 	%r1172, %r1172, -1;

BB3_262:
	setp.ne.s64 	%p216, %rl1442, 0;
	selp.u64 	%rl974, 1, 0, %p216;
	add.s64 	%rl975, %rl974, %rl1441;
	add.s32 	%r782, %r1172, 1022;
	cvt.u64.u32 	%rl976, %r782;
	shl.b64 	%rl977, %rl976, 52;
	shr.u64 	%rl978, %rl975, 11;
	shr.u64 	%rl979, %rl975, 10;
	and.b64  	%rl980, %rl979, 1;
	add.s64 	%rl981, %rl977, %rl978;
	add.s64 	%rl982, %rl981, %rl980;
	or.b64  	%rl983, %rl982, %rl1436;
	mov.b64 	 %fd2649, %rl983;

BB3_263:
	add.s32 	%r194, %r1173, 1;
	and.b32  	%r783, %r194, 1;
	setp.eq.s32 	%p217, %r783, 0;
	mul.rn.f64 	%fd159, %fd2649, %fd2649;
	@%p217 bra 	BB3_265;

	mov.f64 	%fd1330, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1332, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1329, %fd1330, %fd159, %fd1332;
	// inline asm
	mov.f64 	%fd1336, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1333, %fd1329, %fd159, %fd1336;
	// inline asm
	mov.f64 	%fd1340, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1337, %fd1333, %fd159, %fd1340;
	// inline asm
	mov.f64 	%fd1344, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1341, %fd1337, %fd159, %fd1344;
	// inline asm
	mov.f64 	%fd1348, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1345, %fd1341, %fd159, %fd1348;
	// inline asm
	mov.f64 	%fd1352, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1349, %fd1345, %fd159, %fd1352;
	// inline asm
	mov.f64 	%fd1356, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1353, %fd1349, %fd159, %fd1356;
	// inline asm
	mov.f64 	%fd2650, %fd1353;
	bra.uni 	BB3_266;

BB3_265:
	mov.f64 	%fd1358, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1360, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1357, %fd1358, %fd159, %fd1360;
	// inline asm
	mov.f64 	%fd1364, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1361, %fd1357, %fd159, %fd1364;
	// inline asm
	mov.f64 	%fd1368, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1365, %fd1361, %fd159, %fd1368;
	// inline asm
	mov.f64 	%fd1372, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1369, %fd1365, %fd159, %fd1372;
	// inline asm
	mov.f64 	%fd1376, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1373, %fd1369, %fd159, %fd1376;
	// inline asm
	mul.rn.f64 	%fd1378, %fd1373, %fd159;
	// inline asm
	fma.rn.f64 	%fd1377, %fd1378, %fd2649, %fd2649;
	// inline asm
	mov.f64 	%fd2650, %fd1377;

BB3_266:
	and.b32  	%r784, %r194, 2;
	setp.eq.s32 	%p218, %r784, 0;
	neg.f64 	%fd1381, %fd2650;
	selp.f64 	%fd2651, %fd2650, %fd1381, %p218;
	bra.uni 	BB3_268;

BB3_267:
	mov.f64 	%fd2651, 0dFFF8000000000000;

BB3_268:
	setp.eq.f64 	%p219, %fd155, 0d0000000000000000;
	or.pred  	%p220, %p8, %p219;
	or.pred  	%p221, %p7, %p220;
	@%p221 bra 	BB3_291;

	// inline asm
	abs.f64 	%fd1383, %fd155;
	// inline asm
	setp.gt.f64 	%p222, %fd1383, 0d41E0000000000000;
	@%p222 bra 	BB3_271;

	mov.f64 	%fd1398, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1385, %fd155, %fd1398;
	// inline asm
	cvt.rni.s32.f64 	%r785, %fd1385;
	// inline asm
	cvt.rn.f64.s32 	%fd1399, %r785;
	neg.f64 	%fd1395, %fd1399;
	mov.f64 	%fd1388, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1386, %fd1395, %fd1388, %fd155;
	// inline asm
	mov.f64 	%fd1392, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1390, %fd1395, %fd1392, %fd1386;
	// inline asm
	mov.f64 	%fd1396, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1394, %fd1395, %fd1396, %fd1390;
	// inline asm
	mov.u32 	%r1176, %r785;
	mov.f64 	%fd2652, %fd1394;
	bra.uni 	BB3_287;

BB3_271:
	mov.b64 	 %rl289, %fd155;
	and.b64  	%rl1446, %rl289, -9223372036854775808;
	shr.u64 	%rl291, %rl289, 52;
	and.b64  	%rl984, %rl291, 2047;
	add.s64 	%rl985, %rl984, 4294966272;
	cvt.u32.u64 	%r196, %rl985;
	shl.b64 	%rl986, %rl289, 11;
	or.b64  	%rl292, %rl986, -9223372036854775808;
	shr.u32 	%r789, %r196, 6;
	mov.u32 	%r790, 16;
	sub.s32 	%r197, %r790, %r789;
	mov.u32 	%r791, 15;
	sub.s32 	%r1174, %r791, %r789;
	mov.u32 	%r792, 19;
	sub.s32 	%r199, %r792, %r789;
	mov.u32 	%r787, 18;
	// inline asm
	min.s32 	%r786, %r787, %r199;
	// inline asm
	setp.lt.s32 	%p223, %r1174, %r786;
	@%p223 bra 	BB3_273;

	mov.u64 	%rl1443, 0;
	bra.uni 	BB3_275;

BB3_273:
	mov.u32 	%r793, 1;
	sub.s32 	%r200, %r793, %r197;
	mov.u64 	%rl1443, 0;

BB3_274:
	.pragma "nounroll";
	shl.b32 	%r797, %r1174, 3;
	mov.u32 	%r798, __internal_i2opi_d;
	add.s32 	%r799, %r798, %r797;
	ld.const.u64 	%rl990, [%r799];
	mul.lo.s64 	%rl992, %rl990, %rl292;
	// inline asm
	mul.hi.u64 	%rl989, %rl990, %rl292;
	// inline asm
	mad.lo.s64 	%rl993, %rl990, %rl292, %rl1443;
	setp.lt.u64 	%p224, %rl993, %rl992;
	selp.u64 	%rl994, 1, 0, %p224;
	add.s64 	%rl1443, %rl994, %rl989;
	add.s32 	%r800, %r200, %r1174;
	shl.b32 	%r801, %r800, 3;
	add.s32 	%r803, %r48, %r801;
	st.local.u64 	[%r803], %rl993;
	// inline asm
	min.s32 	%r794, %r787, %r199;
	// inline asm
	add.s32 	%r1174, %r1174, 1;
	setp.lt.s32 	%p225, %r1174, %r794;
	@%p225 bra 	BB3_274;

BB3_275:
	mov.u32 	%r804, 1;
	sub.s32 	%r805, %r804, %r197;
	add.s32 	%r806, %r805, %r1174;
	shl.b32 	%r807, %r806, 3;
	add.s32 	%r809, %r48, %r807;
	st.local.u64 	[%r809], %rl1443;
	ld.local.u64 	%rl1444, [%r48+24];
	ld.local.u64 	%rl1445, [%r48+16];
	and.b32  	%r810, %r196, 63;
	setp.eq.s32 	%p226, %r810, 0;
	@%p226 bra 	BB3_277;

	and.b64  	%rl995, %rl291, 63;
	cvt.u32.u64 	%r811, %rl995;
	shl.b64 	%rl996, %rl1444, %r811;
	neg.s32 	%r812, %r196;
	and.b32  	%r813, %r812, 63;
	shr.u64 	%rl997, %rl1445, %r813;
	or.b64  	%rl1444, %rl997, %rl996;
	shl.b64 	%rl998, %rl1445, %r811;
	ld.local.u64 	%rl999, [%r48+8];
	shr.u64 	%rl1000, %rl999, %r813;
	or.b64  	%rl1445, %rl1000, %rl998;

BB3_277:
	shr.u64 	%rl1001, %rl1444, 62;
	cvt.u32.u64 	%r814, %rl1001;
	shr.u64 	%rl1002, %rl1445, 62;
	shl.b64 	%rl1003, %rl1444, 2;
	or.b64  	%rl1450, %rl1002, %rl1003;
	shl.b64 	%rl303, %rl1445, 2;
	setp.ne.s64 	%p227, %rl303, 0;
	selp.u64 	%rl1004, 1, 0, %p227;
	or.b64  	%rl1005, %rl1004, %rl1450;
	setp.gt.u64 	%p228, %rl1005, -9223372036854775808;
	selp.u32 	%r815, 1, 0, %p228;
	add.s32 	%r816, %r815, %r814;
	neg.s32 	%r817, %r816;
	setp.lt.s64 	%p229, %rl289, 0;
	selp.b32 	%r1176, %r817, %r816, %p229;
	@%p228 bra 	BB3_279;

	mov.u64 	%rl1449, %rl303;
	bra.uni 	BB3_280;

BB3_279:
	not.b64 	%rl1006, %rl1450;
	neg.s64 	%rl304, %rl303;
	setp.eq.s64 	%p230, %rl303, 0;
	selp.u64 	%rl1007, 1, 0, %p230;
	add.s64 	%rl1450, %rl1007, %rl1006;
	xor.b64  	%rl1446, %rl1446, -9223372036854775808;
	mov.u64 	%rl1449, %rl304;

BB3_280:
	mov.u64 	%rl1448, %rl1449;
	setp.gt.s64 	%p231, %rl1450, 0;
	@%p231 bra 	BB3_282;

	mov.u32 	%r1175, 0;
	bra.uni 	BB3_284;

BB3_282:
	mov.u32 	%r1175, 0;

BB3_283:
	shr.u64 	%rl1008, %rl1448, 63;
	shl.b64 	%rl1009, %rl1450, 1;
	or.b64  	%rl1450, %rl1008, %rl1009;
	shl.b64 	%rl1448, %rl1448, 1;
	add.s32 	%r1175, %r1175, -1;
	setp.gt.s64 	%p232, %rl1450, 0;
	@%p232 bra 	BB3_283;

BB3_284:
	mul.lo.s64 	%rl1452, %rl1450, -3958705157555305931;
	mov.u64 	%rl1012, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1010, %rl1450, %rl1012;
	// inline asm
	setp.gt.s64 	%p233, %rl1010, 0;
	mov.u64 	%rl1451, %rl1010;
	@%p233 bra 	BB3_285;
	bra.uni 	BB3_286;

BB3_285:
	shl.b64 	%rl1013, %rl1010, 1;
	shr.u64 	%rl1014, %rl1452, 63;
	or.b64  	%rl1451, %rl1013, %rl1014;
	mul.lo.s64 	%rl1452, %rl1450, -7917410315110611862;
	add.s32 	%r1175, %r1175, -1;

BB3_286:
	setp.ne.s64 	%p234, %rl1452, 0;
	selp.u64 	%rl1015, 1, 0, %p234;
	add.s64 	%rl1016, %rl1015, %rl1451;
	add.s32 	%r820, %r1175, 1022;
	cvt.u64.u32 	%rl1017, %r820;
	shl.b64 	%rl1018, %rl1017, 52;
	shr.u64 	%rl1019, %rl1016, 11;
	shr.u64 	%rl1020, %rl1016, 10;
	and.b64  	%rl1021, %rl1020, 1;
	add.s64 	%rl1022, %rl1018, %rl1019;
	add.s64 	%rl1023, %rl1022, %rl1021;
	or.b64  	%rl1024, %rl1023, %rl1446;
	mov.b64 	 %fd2652, %rl1024;

BB3_287:
	and.b32  	%r821, %r1176, 1;
	setp.eq.s32 	%p235, %r821, 0;
	mul.rn.f64 	%fd169, %fd2652, %fd2652;
	@%p235 bra 	BB3_289;

	mov.f64 	%fd1401, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1403, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1400, %fd1401, %fd169, %fd1403;
	// inline asm
	mov.f64 	%fd1407, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1404, %fd1400, %fd169, %fd1407;
	// inline asm
	mov.f64 	%fd1411, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1408, %fd1404, %fd169, %fd1411;
	// inline asm
	mov.f64 	%fd1415, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1412, %fd1408, %fd169, %fd1415;
	// inline asm
	mov.f64 	%fd1419, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1416, %fd1412, %fd169, %fd1419;
	// inline asm
	mov.f64 	%fd1423, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1420, %fd1416, %fd169, %fd1423;
	// inline asm
	mov.f64 	%fd1427, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1424, %fd1420, %fd169, %fd1427;
	// inline asm
	mov.f64 	%fd2653, %fd1424;
	bra.uni 	BB3_290;

BB3_289:
	mov.f64 	%fd1429, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1431, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1428, %fd1429, %fd169, %fd1431;
	// inline asm
	mov.f64 	%fd1435, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1432, %fd1428, %fd169, %fd1435;
	// inline asm
	mov.f64 	%fd1439, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1436, %fd1432, %fd169, %fd1439;
	// inline asm
	mov.f64 	%fd1443, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1440, %fd1436, %fd169, %fd1443;
	// inline asm
	mov.f64 	%fd1447, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1444, %fd1440, %fd169, %fd1447;
	// inline asm
	mul.rn.f64 	%fd1449, %fd1444, %fd169;
	// inline asm
	fma.rn.f64 	%fd1448, %fd1449, %fd2652, %fd2652;
	// inline asm
	mov.f64 	%fd2653, %fd1448;

BB3_290:
	and.b32  	%r822, %r1176, 2;
	setp.eq.s32 	%p236, %r822, 0;
	neg.f64 	%fd1452, %fd2653;
	selp.f64 	%fd2654, %fd2653, %fd1452, %p236;
	bra.uni 	BB3_292;

BB3_291:
	mov.f64 	%fd1453, 0d0000000000000000;
	mul.rn.f64 	%fd2654, %fd155, %fd1453;

BB3_292:
	neg.f64 	%fd1454, %fd2654;
	mov.f64 	%fd2707, %fd2651;
	mov.f64 	%fd2708, %fd1454;
	ld.param.u32 	%r1132, [DIT10C2C_param_3];
	setp.eq.s32 	%p237, %r1132, 0;
	@%p237 bra 	BB3_293;
	bra.uni 	BB3_294;

BB3_293:
	mov.f64 	%fd2707, %fd2651;
	mov.f64 	%fd2708, %fd2654;

BB3_294:
	mul.f64 	%fd1456, %fd59, %fd2707;
	neg.f64 	%fd1458, %fd60;
	fma.rn.f64 	%fd1459, %fd1458, %fd2708, %fd1456;
	mul.f64 	%fd1460, %fd60, %fd2707;
	fma.rn.f64 	%fd1461, %fd59, %fd2708, %fd1460;
	mov.f64 	%fd2689, %fd1459;
	mov.f64 	%fd2690, %fd1461;
	mul.f64 	%fd1462, %fd69, 0d4042D97C7F3321D2;
	div.rn.f64 	%fd176, %fd1462, %fd70;
	setp.eq.f64 	%p9, %fd176, 0d7FF0000000000000;
	setp.eq.f64 	%p10, %fd176, 0dFFF0000000000000;
	or.pred  	%p238, %p9, %p10;
	@%p238 bra 	BB3_317;

	// inline asm
	abs.f64 	%fd1463, %fd176;
	// inline asm
	setp.gt.f64 	%p239, %fd1463, 0d41E0000000000000;
	@%p239 bra 	BB3_297;

	mov.f64 	%fd1478, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1465, %fd176, %fd1478;
	// inline asm
	cvt.rni.s32.f64 	%r823, %fd1465;
	// inline asm
	cvt.rn.f64.s32 	%fd1479, %r823;
	neg.f64 	%fd1475, %fd1479;
	mov.f64 	%fd1468, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1466, %fd1475, %fd1468, %fd176;
	// inline asm
	mov.f64 	%fd1472, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1470, %fd1475, %fd1472, %fd1466;
	// inline asm
	mov.f64 	%fd1476, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1474, %fd1475, %fd1476, %fd1470;
	// inline asm
	mov.u32 	%r1179, %r823;
	mov.f64 	%fd2655, %fd1474;
	bra.uni 	BB3_313;

BB3_297:
	mov.b64 	 %rl321, %fd176;
	and.b64  	%rl1456, %rl321, -9223372036854775808;
	shr.u64 	%rl323, %rl321, 52;
	and.b64  	%rl1025, %rl323, 2047;
	add.s64 	%rl1026, %rl1025, 4294966272;
	cvt.u32.u64 	%r212, %rl1026;
	shl.b64 	%rl1027, %rl321, 11;
	or.b64  	%rl324, %rl1027, -9223372036854775808;
	shr.u32 	%r827, %r212, 6;
	mov.u32 	%r828, 16;
	sub.s32 	%r213, %r828, %r827;
	mov.u32 	%r829, 15;
	sub.s32 	%r1177, %r829, %r827;
	mov.u32 	%r830, 19;
	sub.s32 	%r215, %r830, %r827;
	mov.u32 	%r825, 18;
	// inline asm
	min.s32 	%r824, %r825, %r215;
	// inline asm
	setp.lt.s32 	%p240, %r1177, %r824;
	@%p240 bra 	BB3_299;

	mov.u64 	%rl1453, 0;
	bra.uni 	BB3_301;

BB3_299:
	mov.u32 	%r831, 1;
	sub.s32 	%r216, %r831, %r213;
	mov.u64 	%rl1453, 0;

BB3_300:
	.pragma "nounroll";
	shl.b32 	%r835, %r1177, 3;
	mov.u32 	%r836, __internal_i2opi_d;
	add.s32 	%r837, %r836, %r835;
	ld.const.u64 	%rl1031, [%r837];
	mul.lo.s64 	%rl1033, %rl1031, %rl324;
	// inline asm
	mul.hi.u64 	%rl1030, %rl1031, %rl324;
	// inline asm
	mad.lo.s64 	%rl1034, %rl1031, %rl324, %rl1453;
	setp.lt.u64 	%p241, %rl1034, %rl1033;
	selp.u64 	%rl1035, 1, 0, %p241;
	add.s64 	%rl1453, %rl1035, %rl1030;
	add.s32 	%r838, %r216, %r1177;
	shl.b32 	%r839, %r838, 3;
	add.s32 	%r841, %r48, %r839;
	st.local.u64 	[%r841], %rl1034;
	// inline asm
	min.s32 	%r832, %r825, %r215;
	// inline asm
	add.s32 	%r1177, %r1177, 1;
	setp.lt.s32 	%p242, %r1177, %r832;
	@%p242 bra 	BB3_300;

BB3_301:
	mov.u32 	%r842, 1;
	sub.s32 	%r843, %r842, %r213;
	add.s32 	%r844, %r843, %r1177;
	shl.b32 	%r845, %r844, 3;
	add.s32 	%r847, %r48, %r845;
	st.local.u64 	[%r847], %rl1453;
	ld.local.u64 	%rl1454, [%r48+24];
	ld.local.u64 	%rl1455, [%r48+16];
	and.b32  	%r848, %r212, 63;
	setp.eq.s32 	%p243, %r848, 0;
	@%p243 bra 	BB3_303;

	and.b64  	%rl1036, %rl323, 63;
	cvt.u32.u64 	%r849, %rl1036;
	shl.b64 	%rl1037, %rl1454, %r849;
	neg.s32 	%r850, %r212;
	and.b32  	%r851, %r850, 63;
	shr.u64 	%rl1038, %rl1455, %r851;
	or.b64  	%rl1454, %rl1038, %rl1037;
	shl.b64 	%rl1039, %rl1455, %r849;
	ld.local.u64 	%rl1040, [%r48+8];
	shr.u64 	%rl1041, %rl1040, %r851;
	or.b64  	%rl1455, %rl1041, %rl1039;

BB3_303:
	shr.u64 	%rl1042, %rl1454, 62;
	cvt.u32.u64 	%r852, %rl1042;
	shr.u64 	%rl1043, %rl1455, 62;
	shl.b64 	%rl1044, %rl1454, 2;
	or.b64  	%rl1460, %rl1043, %rl1044;
	shl.b64 	%rl335, %rl1455, 2;
	setp.ne.s64 	%p244, %rl335, 0;
	selp.u64 	%rl1045, 1, 0, %p244;
	or.b64  	%rl1046, %rl1045, %rl1460;
	setp.gt.u64 	%p245, %rl1046, -9223372036854775808;
	selp.u32 	%r853, 1, 0, %p245;
	add.s32 	%r854, %r853, %r852;
	neg.s32 	%r855, %r854;
	setp.lt.s64 	%p246, %rl321, 0;
	selp.b32 	%r1179, %r855, %r854, %p246;
	@%p245 bra 	BB3_305;

	mov.u64 	%rl1459, %rl335;
	bra.uni 	BB3_306;

BB3_305:
	not.b64 	%rl1047, %rl1460;
	neg.s64 	%rl336, %rl335;
	setp.eq.s64 	%p247, %rl335, 0;
	selp.u64 	%rl1048, 1, 0, %p247;
	add.s64 	%rl1460, %rl1048, %rl1047;
	xor.b64  	%rl1456, %rl1456, -9223372036854775808;
	mov.u64 	%rl1459, %rl336;

BB3_306:
	mov.u64 	%rl1458, %rl1459;
	setp.gt.s64 	%p248, %rl1460, 0;
	@%p248 bra 	BB3_308;

	mov.u32 	%r1178, 0;
	bra.uni 	BB3_310;

BB3_308:
	mov.u32 	%r1178, 0;

BB3_309:
	shr.u64 	%rl1049, %rl1458, 63;
	shl.b64 	%rl1050, %rl1460, 1;
	or.b64  	%rl1460, %rl1049, %rl1050;
	shl.b64 	%rl1458, %rl1458, 1;
	add.s32 	%r1178, %r1178, -1;
	setp.gt.s64 	%p249, %rl1460, 0;
	@%p249 bra 	BB3_309;

BB3_310:
	mul.lo.s64 	%rl1462, %rl1460, -3958705157555305931;
	mov.u64 	%rl1053, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1051, %rl1460, %rl1053;
	// inline asm
	setp.gt.s64 	%p250, %rl1051, 0;
	mov.u64 	%rl1461, %rl1051;
	@%p250 bra 	BB3_311;
	bra.uni 	BB3_312;

BB3_311:
	shl.b64 	%rl1054, %rl1051, 1;
	shr.u64 	%rl1055, %rl1462, 63;
	or.b64  	%rl1461, %rl1054, %rl1055;
	mul.lo.s64 	%rl1462, %rl1460, -7917410315110611862;
	add.s32 	%r1178, %r1178, -1;

BB3_312:
	setp.ne.s64 	%p251, %rl1462, 0;
	selp.u64 	%rl1056, 1, 0, %p251;
	add.s64 	%rl1057, %rl1056, %rl1461;
	add.s32 	%r858, %r1178, 1022;
	cvt.u64.u32 	%rl1058, %r858;
	shl.b64 	%rl1059, %rl1058, 52;
	shr.u64 	%rl1060, %rl1057, 11;
	shr.u64 	%rl1061, %rl1057, 10;
	and.b64  	%rl1062, %rl1061, 1;
	add.s64 	%rl1063, %rl1059, %rl1060;
	add.s64 	%rl1064, %rl1063, %rl1062;
	or.b64  	%rl1065, %rl1064, %rl1456;
	mov.b64 	 %fd2655, %rl1065;

BB3_313:
	add.s32 	%r227, %r1179, 1;
	and.b32  	%r859, %r227, 1;
	setp.eq.s32 	%p252, %r859, 0;
	mul.rn.f64 	%fd180, %fd2655, %fd2655;
	@%p252 bra 	BB3_315;

	mov.f64 	%fd1481, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1483, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1480, %fd1481, %fd180, %fd1483;
	// inline asm
	mov.f64 	%fd1487, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1484, %fd1480, %fd180, %fd1487;
	// inline asm
	mov.f64 	%fd1491, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1488, %fd1484, %fd180, %fd1491;
	// inline asm
	mov.f64 	%fd1495, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1492, %fd1488, %fd180, %fd1495;
	// inline asm
	mov.f64 	%fd1499, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1496, %fd1492, %fd180, %fd1499;
	// inline asm
	mov.f64 	%fd1503, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1500, %fd1496, %fd180, %fd1503;
	// inline asm
	mov.f64 	%fd1507, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1504, %fd1500, %fd180, %fd1507;
	// inline asm
	mov.f64 	%fd2656, %fd1504;
	bra.uni 	BB3_316;

BB3_315:
	mov.f64 	%fd1509, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1511, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1508, %fd1509, %fd180, %fd1511;
	// inline asm
	mov.f64 	%fd1515, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1512, %fd1508, %fd180, %fd1515;
	// inline asm
	mov.f64 	%fd1519, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1516, %fd1512, %fd180, %fd1519;
	// inline asm
	mov.f64 	%fd1523, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1520, %fd1516, %fd180, %fd1523;
	// inline asm
	mov.f64 	%fd1527, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1524, %fd1520, %fd180, %fd1527;
	// inline asm
	mul.rn.f64 	%fd1529, %fd1524, %fd180;
	// inline asm
	fma.rn.f64 	%fd1528, %fd1529, %fd2655, %fd2655;
	// inline asm
	mov.f64 	%fd2656, %fd1528;

BB3_316:
	and.b32  	%r860, %r227, 2;
	setp.eq.s32 	%p253, %r860, 0;
	neg.f64 	%fd1532, %fd2656;
	selp.f64 	%fd2657, %fd2656, %fd1532, %p253;
	bra.uni 	BB3_318;

BB3_317:
	mov.f64 	%fd2657, 0dFFF8000000000000;

BB3_318:
	setp.eq.f64 	%p254, %fd176, 0d0000000000000000;
	or.pred  	%p255, %p10, %p254;
	or.pred  	%p256, %p9, %p255;
	@%p256 bra 	BB3_341;

	// inline asm
	abs.f64 	%fd1534, %fd176;
	// inline asm
	setp.gt.f64 	%p257, %fd1534, 0d41E0000000000000;
	@%p257 bra 	BB3_321;

	mov.f64 	%fd1549, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1536, %fd176, %fd1549;
	// inline asm
	cvt.rni.s32.f64 	%r861, %fd1536;
	// inline asm
	cvt.rn.f64.s32 	%fd1550, %r861;
	neg.f64 	%fd1546, %fd1550;
	mov.f64 	%fd1539, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1537, %fd1546, %fd1539, %fd176;
	// inline asm
	mov.f64 	%fd1543, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1541, %fd1546, %fd1543, %fd1537;
	// inline asm
	mov.f64 	%fd1547, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1545, %fd1546, %fd1547, %fd1541;
	// inline asm
	mov.u32 	%r1182, %r861;
	mov.f64 	%fd2658, %fd1545;
	bra.uni 	BB3_337;

BB3_321:
	mov.b64 	 %rl353, %fd176;
	and.b64  	%rl1466, %rl353, -9223372036854775808;
	shr.u64 	%rl355, %rl353, 52;
	and.b64  	%rl1066, %rl355, 2047;
	add.s64 	%rl1067, %rl1066, 4294966272;
	cvt.u32.u64 	%r229, %rl1067;
	shl.b64 	%rl1068, %rl353, 11;
	or.b64  	%rl356, %rl1068, -9223372036854775808;
	shr.u32 	%r865, %r229, 6;
	mov.u32 	%r866, 16;
	sub.s32 	%r230, %r866, %r865;
	mov.u32 	%r867, 15;
	sub.s32 	%r1180, %r867, %r865;
	mov.u32 	%r868, 19;
	sub.s32 	%r232, %r868, %r865;
	mov.u32 	%r863, 18;
	// inline asm
	min.s32 	%r862, %r863, %r232;
	// inline asm
	setp.lt.s32 	%p258, %r1180, %r862;
	@%p258 bra 	BB3_323;

	mov.u64 	%rl1463, 0;
	bra.uni 	BB3_325;

BB3_323:
	mov.u32 	%r869, 1;
	sub.s32 	%r233, %r869, %r230;
	mov.u64 	%rl1463, 0;

BB3_324:
	.pragma "nounroll";
	shl.b32 	%r873, %r1180, 3;
	mov.u32 	%r874, __internal_i2opi_d;
	add.s32 	%r875, %r874, %r873;
	ld.const.u64 	%rl1072, [%r875];
	mul.lo.s64 	%rl1074, %rl1072, %rl356;
	// inline asm
	mul.hi.u64 	%rl1071, %rl1072, %rl356;
	// inline asm
	mad.lo.s64 	%rl1075, %rl1072, %rl356, %rl1463;
	setp.lt.u64 	%p259, %rl1075, %rl1074;
	selp.u64 	%rl1076, 1, 0, %p259;
	add.s64 	%rl1463, %rl1076, %rl1071;
	add.s32 	%r876, %r233, %r1180;
	shl.b32 	%r877, %r876, 3;
	add.s32 	%r879, %r48, %r877;
	st.local.u64 	[%r879], %rl1075;
	// inline asm
	min.s32 	%r870, %r863, %r232;
	// inline asm
	add.s32 	%r1180, %r1180, 1;
	setp.lt.s32 	%p260, %r1180, %r870;
	@%p260 bra 	BB3_324;

BB3_325:
	mov.u32 	%r880, 1;
	sub.s32 	%r881, %r880, %r230;
	add.s32 	%r882, %r881, %r1180;
	shl.b32 	%r883, %r882, 3;
	add.s32 	%r885, %r48, %r883;
	st.local.u64 	[%r885], %rl1463;
	ld.local.u64 	%rl1464, [%r48+24];
	ld.local.u64 	%rl1465, [%r48+16];
	and.b32  	%r886, %r229, 63;
	setp.eq.s32 	%p261, %r886, 0;
	@%p261 bra 	BB3_327;

	and.b64  	%rl1077, %rl355, 63;
	cvt.u32.u64 	%r887, %rl1077;
	shl.b64 	%rl1078, %rl1464, %r887;
	neg.s32 	%r888, %r229;
	and.b32  	%r889, %r888, 63;
	shr.u64 	%rl1079, %rl1465, %r889;
	or.b64  	%rl1464, %rl1079, %rl1078;
	shl.b64 	%rl1080, %rl1465, %r887;
	ld.local.u64 	%rl1081, [%r48+8];
	shr.u64 	%rl1082, %rl1081, %r889;
	or.b64  	%rl1465, %rl1082, %rl1080;

BB3_327:
	shr.u64 	%rl1083, %rl1464, 62;
	cvt.u32.u64 	%r890, %rl1083;
	shr.u64 	%rl1084, %rl1465, 62;
	shl.b64 	%rl1085, %rl1464, 2;
	or.b64  	%rl1470, %rl1084, %rl1085;
	shl.b64 	%rl367, %rl1465, 2;
	setp.ne.s64 	%p262, %rl367, 0;
	selp.u64 	%rl1086, 1, 0, %p262;
	or.b64  	%rl1087, %rl1086, %rl1470;
	setp.gt.u64 	%p263, %rl1087, -9223372036854775808;
	selp.u32 	%r891, 1, 0, %p263;
	add.s32 	%r892, %r891, %r890;
	neg.s32 	%r893, %r892;
	setp.lt.s64 	%p264, %rl353, 0;
	selp.b32 	%r1182, %r893, %r892, %p264;
	@%p263 bra 	BB3_329;

	mov.u64 	%rl1469, %rl367;
	bra.uni 	BB3_330;

BB3_329:
	not.b64 	%rl1088, %rl1470;
	neg.s64 	%rl368, %rl367;
	setp.eq.s64 	%p265, %rl367, 0;
	selp.u64 	%rl1089, 1, 0, %p265;
	add.s64 	%rl1470, %rl1089, %rl1088;
	xor.b64  	%rl1466, %rl1466, -9223372036854775808;
	mov.u64 	%rl1469, %rl368;

BB3_330:
	mov.u64 	%rl1468, %rl1469;
	setp.gt.s64 	%p266, %rl1470, 0;
	@%p266 bra 	BB3_332;

	mov.u32 	%r1181, 0;
	bra.uni 	BB3_334;

BB3_332:
	mov.u32 	%r1181, 0;

BB3_333:
	shr.u64 	%rl1090, %rl1468, 63;
	shl.b64 	%rl1091, %rl1470, 1;
	or.b64  	%rl1470, %rl1090, %rl1091;
	shl.b64 	%rl1468, %rl1468, 1;
	add.s32 	%r1181, %r1181, -1;
	setp.gt.s64 	%p267, %rl1470, 0;
	@%p267 bra 	BB3_333;

BB3_334:
	mul.lo.s64 	%rl1472, %rl1470, -3958705157555305931;
	mov.u64 	%rl1094, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1092, %rl1470, %rl1094;
	// inline asm
	setp.gt.s64 	%p268, %rl1092, 0;
	mov.u64 	%rl1471, %rl1092;
	@%p268 bra 	BB3_335;
	bra.uni 	BB3_336;

BB3_335:
	shl.b64 	%rl1095, %rl1092, 1;
	shr.u64 	%rl1096, %rl1472, 63;
	or.b64  	%rl1471, %rl1095, %rl1096;
	mul.lo.s64 	%rl1472, %rl1470, -7917410315110611862;
	add.s32 	%r1181, %r1181, -1;

BB3_336:
	setp.ne.s64 	%p269, %rl1472, 0;
	selp.u64 	%rl1097, 1, 0, %p269;
	add.s64 	%rl1098, %rl1097, %rl1471;
	add.s32 	%r896, %r1181, 1022;
	cvt.u64.u32 	%rl1099, %r896;
	shl.b64 	%rl1100, %rl1099, 52;
	shr.u64 	%rl1101, %rl1098, 11;
	shr.u64 	%rl1102, %rl1098, 10;
	and.b64  	%rl1103, %rl1102, 1;
	add.s64 	%rl1104, %rl1100, %rl1101;
	add.s64 	%rl1105, %rl1104, %rl1103;
	or.b64  	%rl1106, %rl1105, %rl1466;
	mov.b64 	 %fd2658, %rl1106;

BB3_337:
	and.b32  	%r897, %r1182, 1;
	setp.eq.s32 	%p270, %r897, 0;
	mul.rn.f64 	%fd190, %fd2658, %fd2658;
	@%p270 bra 	BB3_339;

	mov.f64 	%fd1552, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1554, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1551, %fd1552, %fd190, %fd1554;
	// inline asm
	mov.f64 	%fd1558, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1555, %fd1551, %fd190, %fd1558;
	// inline asm
	mov.f64 	%fd1562, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1559, %fd1555, %fd190, %fd1562;
	// inline asm
	mov.f64 	%fd1566, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1563, %fd1559, %fd190, %fd1566;
	// inline asm
	mov.f64 	%fd1570, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1567, %fd1563, %fd190, %fd1570;
	// inline asm
	mov.f64 	%fd1574, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1571, %fd1567, %fd190, %fd1574;
	// inline asm
	mov.f64 	%fd1578, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1575, %fd1571, %fd190, %fd1578;
	// inline asm
	mov.f64 	%fd2659, %fd1575;
	bra.uni 	BB3_340;

BB3_339:
	mov.f64 	%fd1580, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1582, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1579, %fd1580, %fd190, %fd1582;
	// inline asm
	mov.f64 	%fd1586, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1583, %fd1579, %fd190, %fd1586;
	// inline asm
	mov.f64 	%fd1590, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1587, %fd1583, %fd190, %fd1590;
	// inline asm
	mov.f64 	%fd1594, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1591, %fd1587, %fd190, %fd1594;
	// inline asm
	mov.f64 	%fd1598, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1595, %fd1591, %fd190, %fd1598;
	// inline asm
	mul.rn.f64 	%fd1600, %fd1595, %fd190;
	// inline asm
	fma.rn.f64 	%fd1599, %fd1600, %fd2658, %fd2658;
	// inline asm
	mov.f64 	%fd2659, %fd1599;

BB3_340:
	and.b32  	%r898, %r1182, 2;
	setp.eq.s32 	%p271, %r898, 0;
	neg.f64 	%fd1603, %fd2659;
	selp.f64 	%fd2660, %fd2659, %fd1603, %p271;
	bra.uni 	BB3_342;

BB3_341:
	mov.f64 	%fd1604, 0d0000000000000000;
	mul.rn.f64 	%fd2660, %fd176, %fd1604;

BB3_342:
	neg.f64 	%fd1605, %fd2660;
	mov.f64 	%fd2705, %fd2657;
	mov.f64 	%fd2706, %fd1605;
	ld.param.u32 	%r1131, [DIT10C2C_param_3];
	setp.eq.s32 	%p272, %r1131, 0;
	@%p272 bra 	BB3_343;
	bra.uni 	BB3_344;

BB3_343:
	mov.f64 	%fd2705, %fd2657;
	mov.f64 	%fd2706, %fd2660;

BB3_344:
	mul.f64 	%fd1607, %fd61, %fd2705;
	neg.f64 	%fd1609, %fd62;
	fma.rn.f64 	%fd1610, %fd1609, %fd2706, %fd1607;
	mul.f64 	%fd1611, %fd62, %fd2705;
	fma.rn.f64 	%fd1612, %fd61, %fd2706, %fd1611;
	mov.f64 	%fd2691, %fd1610;
	mov.f64 	%fd2692, %fd1612;
	mul.f64 	%fd1613, %fd69, 0d4045FDBBE9BBA775;
	div.rn.f64 	%fd197, %fd1613, %fd70;
	setp.eq.f64 	%p11, %fd197, 0d7FF0000000000000;
	setp.eq.f64 	%p12, %fd197, 0dFFF0000000000000;
	or.pred  	%p273, %p11, %p12;
	@%p273 bra 	BB3_367;

	// inline asm
	abs.f64 	%fd1614, %fd197;
	// inline asm
	setp.gt.f64 	%p274, %fd1614, 0d41E0000000000000;
	@%p274 bra 	BB3_347;

	mov.f64 	%fd1629, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1616, %fd197, %fd1629;
	// inline asm
	cvt.rni.s32.f64 	%r899, %fd1616;
	// inline asm
	cvt.rn.f64.s32 	%fd1630, %r899;
	neg.f64 	%fd1626, %fd1630;
	mov.f64 	%fd1619, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1617, %fd1626, %fd1619, %fd197;
	// inline asm
	mov.f64 	%fd1623, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1621, %fd1626, %fd1623, %fd1617;
	// inline asm
	mov.f64 	%fd1627, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1625, %fd1626, %fd1627, %fd1621;
	// inline asm
	mov.u32 	%r1185, %r899;
	mov.f64 	%fd2661, %fd1625;
	bra.uni 	BB3_363;

BB3_347:
	mov.b64 	 %rl385, %fd197;
	and.b64  	%rl1476, %rl385, -9223372036854775808;
	shr.u64 	%rl387, %rl385, 52;
	and.b64  	%rl1107, %rl387, 2047;
	add.s64 	%rl1108, %rl1107, 4294966272;
	cvt.u32.u64 	%r245, %rl1108;
	shl.b64 	%rl1109, %rl385, 11;
	or.b64  	%rl388, %rl1109, -9223372036854775808;
	shr.u32 	%r903, %r245, 6;
	mov.u32 	%r904, 16;
	sub.s32 	%r246, %r904, %r903;
	mov.u32 	%r905, 15;
	sub.s32 	%r1183, %r905, %r903;
	mov.u32 	%r906, 19;
	sub.s32 	%r248, %r906, %r903;
	mov.u32 	%r901, 18;
	// inline asm
	min.s32 	%r900, %r901, %r248;
	// inline asm
	setp.lt.s32 	%p275, %r1183, %r900;
	@%p275 bra 	BB3_349;

	mov.u64 	%rl1473, 0;
	bra.uni 	BB3_351;

BB3_349:
	mov.u32 	%r907, 1;
	sub.s32 	%r249, %r907, %r246;
	mov.u64 	%rl1473, 0;

BB3_350:
	.pragma "nounroll";
	shl.b32 	%r911, %r1183, 3;
	mov.u32 	%r912, __internal_i2opi_d;
	add.s32 	%r913, %r912, %r911;
	ld.const.u64 	%rl1113, [%r913];
	mul.lo.s64 	%rl1115, %rl1113, %rl388;
	// inline asm
	mul.hi.u64 	%rl1112, %rl1113, %rl388;
	// inline asm
	mad.lo.s64 	%rl1116, %rl1113, %rl388, %rl1473;
	setp.lt.u64 	%p276, %rl1116, %rl1115;
	selp.u64 	%rl1117, 1, 0, %p276;
	add.s64 	%rl1473, %rl1117, %rl1112;
	add.s32 	%r914, %r249, %r1183;
	shl.b32 	%r915, %r914, 3;
	add.s32 	%r917, %r48, %r915;
	st.local.u64 	[%r917], %rl1116;
	// inline asm
	min.s32 	%r908, %r901, %r248;
	// inline asm
	add.s32 	%r1183, %r1183, 1;
	setp.lt.s32 	%p277, %r1183, %r908;
	@%p277 bra 	BB3_350;

BB3_351:
	mov.u32 	%r918, 1;
	sub.s32 	%r919, %r918, %r246;
	add.s32 	%r920, %r919, %r1183;
	shl.b32 	%r921, %r920, 3;
	add.s32 	%r923, %r48, %r921;
	st.local.u64 	[%r923], %rl1473;
	ld.local.u64 	%rl1474, [%r48+24];
	ld.local.u64 	%rl1475, [%r48+16];
	and.b32  	%r924, %r245, 63;
	setp.eq.s32 	%p278, %r924, 0;
	@%p278 bra 	BB3_353;

	and.b64  	%rl1118, %rl387, 63;
	cvt.u32.u64 	%r925, %rl1118;
	shl.b64 	%rl1119, %rl1474, %r925;
	neg.s32 	%r926, %r245;
	and.b32  	%r927, %r926, 63;
	shr.u64 	%rl1120, %rl1475, %r927;
	or.b64  	%rl1474, %rl1120, %rl1119;
	shl.b64 	%rl1121, %rl1475, %r925;
	ld.local.u64 	%rl1122, [%r48+8];
	shr.u64 	%rl1123, %rl1122, %r927;
	or.b64  	%rl1475, %rl1123, %rl1121;

BB3_353:
	shr.u64 	%rl1124, %rl1474, 62;
	cvt.u32.u64 	%r928, %rl1124;
	shr.u64 	%rl1125, %rl1475, 62;
	shl.b64 	%rl1126, %rl1474, 2;
	or.b64  	%rl1480, %rl1125, %rl1126;
	shl.b64 	%rl399, %rl1475, 2;
	setp.ne.s64 	%p279, %rl399, 0;
	selp.u64 	%rl1127, 1, 0, %p279;
	or.b64  	%rl1128, %rl1127, %rl1480;
	setp.gt.u64 	%p280, %rl1128, -9223372036854775808;
	selp.u32 	%r929, 1, 0, %p280;
	add.s32 	%r930, %r929, %r928;
	neg.s32 	%r931, %r930;
	setp.lt.s64 	%p281, %rl385, 0;
	selp.b32 	%r1185, %r931, %r930, %p281;
	@%p280 bra 	BB3_355;

	mov.u64 	%rl1479, %rl399;
	bra.uni 	BB3_356;

BB3_355:
	not.b64 	%rl1129, %rl1480;
	neg.s64 	%rl400, %rl399;
	setp.eq.s64 	%p282, %rl399, 0;
	selp.u64 	%rl1130, 1, 0, %p282;
	add.s64 	%rl1480, %rl1130, %rl1129;
	xor.b64  	%rl1476, %rl1476, -9223372036854775808;
	mov.u64 	%rl1479, %rl400;

BB3_356:
	mov.u64 	%rl1478, %rl1479;
	setp.gt.s64 	%p283, %rl1480, 0;
	@%p283 bra 	BB3_358;

	mov.u32 	%r1184, 0;
	bra.uni 	BB3_360;

BB3_358:
	mov.u32 	%r1184, 0;

BB3_359:
	shr.u64 	%rl1131, %rl1478, 63;
	shl.b64 	%rl1132, %rl1480, 1;
	or.b64  	%rl1480, %rl1131, %rl1132;
	shl.b64 	%rl1478, %rl1478, 1;
	add.s32 	%r1184, %r1184, -1;
	setp.gt.s64 	%p284, %rl1480, 0;
	@%p284 bra 	BB3_359;

BB3_360:
	mul.lo.s64 	%rl1482, %rl1480, -3958705157555305931;
	mov.u64 	%rl1135, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1133, %rl1480, %rl1135;
	// inline asm
	setp.gt.s64 	%p285, %rl1133, 0;
	mov.u64 	%rl1481, %rl1133;
	@%p285 bra 	BB3_361;
	bra.uni 	BB3_362;

BB3_361:
	shl.b64 	%rl1136, %rl1133, 1;
	shr.u64 	%rl1137, %rl1482, 63;
	or.b64  	%rl1481, %rl1136, %rl1137;
	mul.lo.s64 	%rl1482, %rl1480, -7917410315110611862;
	add.s32 	%r1184, %r1184, -1;

BB3_362:
	setp.ne.s64 	%p286, %rl1482, 0;
	selp.u64 	%rl1138, 1, 0, %p286;
	add.s64 	%rl1139, %rl1138, %rl1481;
	add.s32 	%r934, %r1184, 1022;
	cvt.u64.u32 	%rl1140, %r934;
	shl.b64 	%rl1141, %rl1140, 52;
	shr.u64 	%rl1142, %rl1139, 11;
	shr.u64 	%rl1143, %rl1139, 10;
	and.b64  	%rl1144, %rl1143, 1;
	add.s64 	%rl1145, %rl1141, %rl1142;
	add.s64 	%rl1146, %rl1145, %rl1144;
	or.b64  	%rl1147, %rl1146, %rl1476;
	mov.b64 	 %fd2661, %rl1147;

BB3_363:
	add.s32 	%r260, %r1185, 1;
	and.b32  	%r935, %r260, 1;
	setp.eq.s32 	%p287, %r935, 0;
	mul.rn.f64 	%fd201, %fd2661, %fd2661;
	@%p287 bra 	BB3_365;

	mov.f64 	%fd1632, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1634, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1631, %fd1632, %fd201, %fd1634;
	// inline asm
	mov.f64 	%fd1638, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1635, %fd1631, %fd201, %fd1638;
	// inline asm
	mov.f64 	%fd1642, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1639, %fd1635, %fd201, %fd1642;
	// inline asm
	mov.f64 	%fd1646, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1643, %fd1639, %fd201, %fd1646;
	// inline asm
	mov.f64 	%fd1650, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1647, %fd1643, %fd201, %fd1650;
	// inline asm
	mov.f64 	%fd1654, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1651, %fd1647, %fd201, %fd1654;
	// inline asm
	mov.f64 	%fd1658, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1655, %fd1651, %fd201, %fd1658;
	// inline asm
	mov.f64 	%fd2662, %fd1655;
	bra.uni 	BB3_366;

BB3_365:
	mov.f64 	%fd1660, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1662, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1659, %fd1660, %fd201, %fd1662;
	// inline asm
	mov.f64 	%fd1666, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1663, %fd1659, %fd201, %fd1666;
	// inline asm
	mov.f64 	%fd1670, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1667, %fd1663, %fd201, %fd1670;
	// inline asm
	mov.f64 	%fd1674, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1671, %fd1667, %fd201, %fd1674;
	// inline asm
	mov.f64 	%fd1678, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1675, %fd1671, %fd201, %fd1678;
	// inline asm
	mul.rn.f64 	%fd1680, %fd1675, %fd201;
	// inline asm
	fma.rn.f64 	%fd1679, %fd1680, %fd2661, %fd2661;
	// inline asm
	mov.f64 	%fd2662, %fd1679;

BB3_366:
	and.b32  	%r936, %r260, 2;
	setp.eq.s32 	%p288, %r936, 0;
	neg.f64 	%fd1683, %fd2662;
	selp.f64 	%fd2663, %fd2662, %fd1683, %p288;
	bra.uni 	BB3_368;

BB3_367:
	mov.f64 	%fd2663, 0dFFF8000000000000;

BB3_368:
	setp.eq.f64 	%p289, %fd197, 0d0000000000000000;
	or.pred  	%p290, %p12, %p289;
	or.pred  	%p291, %p11, %p290;
	@%p291 bra 	BB3_391;

	// inline asm
	abs.f64 	%fd1685, %fd197;
	// inline asm
	setp.gt.f64 	%p292, %fd1685, 0d41E0000000000000;
	@%p292 bra 	BB3_371;

	mov.f64 	%fd1700, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1687, %fd197, %fd1700;
	// inline asm
	cvt.rni.s32.f64 	%r937, %fd1687;
	// inline asm
	cvt.rn.f64.s32 	%fd1701, %r937;
	neg.f64 	%fd1697, %fd1701;
	mov.f64 	%fd1690, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1688, %fd1697, %fd1690, %fd197;
	// inline asm
	mov.f64 	%fd1694, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1692, %fd1697, %fd1694, %fd1688;
	// inline asm
	mov.f64 	%fd1698, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1696, %fd1697, %fd1698, %fd1692;
	// inline asm
	mov.u32 	%r1188, %r937;
	mov.f64 	%fd2664, %fd1696;
	bra.uni 	BB3_387;

BB3_371:
	mov.b64 	 %rl417, %fd197;
	and.b64  	%rl1486, %rl417, -9223372036854775808;
	shr.u64 	%rl419, %rl417, 52;
	and.b64  	%rl1148, %rl419, 2047;
	add.s64 	%rl1149, %rl1148, 4294966272;
	cvt.u32.u64 	%r262, %rl1149;
	shl.b64 	%rl1150, %rl417, 11;
	or.b64  	%rl420, %rl1150, -9223372036854775808;
	shr.u32 	%r941, %r262, 6;
	mov.u32 	%r942, 16;
	sub.s32 	%r263, %r942, %r941;
	mov.u32 	%r943, 15;
	sub.s32 	%r1186, %r943, %r941;
	mov.u32 	%r944, 19;
	sub.s32 	%r265, %r944, %r941;
	mov.u32 	%r939, 18;
	// inline asm
	min.s32 	%r938, %r939, %r265;
	// inline asm
	setp.lt.s32 	%p293, %r1186, %r938;
	@%p293 bra 	BB3_373;

	mov.u64 	%rl1483, 0;
	bra.uni 	BB3_375;

BB3_373:
	mov.u32 	%r945, 1;
	sub.s32 	%r266, %r945, %r263;
	mov.u64 	%rl1483, 0;

BB3_374:
	.pragma "nounroll";
	shl.b32 	%r949, %r1186, 3;
	mov.u32 	%r950, __internal_i2opi_d;
	add.s32 	%r951, %r950, %r949;
	ld.const.u64 	%rl1154, [%r951];
	mul.lo.s64 	%rl1156, %rl1154, %rl420;
	// inline asm
	mul.hi.u64 	%rl1153, %rl1154, %rl420;
	// inline asm
	mad.lo.s64 	%rl1157, %rl1154, %rl420, %rl1483;
	setp.lt.u64 	%p294, %rl1157, %rl1156;
	selp.u64 	%rl1158, 1, 0, %p294;
	add.s64 	%rl1483, %rl1158, %rl1153;
	add.s32 	%r952, %r266, %r1186;
	shl.b32 	%r953, %r952, 3;
	add.s32 	%r955, %r48, %r953;
	st.local.u64 	[%r955], %rl1157;
	// inline asm
	min.s32 	%r946, %r939, %r265;
	// inline asm
	add.s32 	%r1186, %r1186, 1;
	setp.lt.s32 	%p295, %r1186, %r946;
	@%p295 bra 	BB3_374;

BB3_375:
	mov.u32 	%r956, 1;
	sub.s32 	%r957, %r956, %r263;
	add.s32 	%r958, %r957, %r1186;
	shl.b32 	%r959, %r958, 3;
	add.s32 	%r961, %r48, %r959;
	st.local.u64 	[%r961], %rl1483;
	ld.local.u64 	%rl1484, [%r48+24];
	ld.local.u64 	%rl1485, [%r48+16];
	and.b32  	%r962, %r262, 63;
	setp.eq.s32 	%p296, %r962, 0;
	@%p296 bra 	BB3_377;

	and.b64  	%rl1159, %rl419, 63;
	cvt.u32.u64 	%r963, %rl1159;
	shl.b64 	%rl1160, %rl1484, %r963;
	neg.s32 	%r964, %r262;
	and.b32  	%r965, %r964, 63;
	shr.u64 	%rl1161, %rl1485, %r965;
	or.b64  	%rl1484, %rl1161, %rl1160;
	shl.b64 	%rl1162, %rl1485, %r963;
	ld.local.u64 	%rl1163, [%r48+8];
	shr.u64 	%rl1164, %rl1163, %r965;
	or.b64  	%rl1485, %rl1164, %rl1162;

BB3_377:
	shr.u64 	%rl1165, %rl1484, 62;
	cvt.u32.u64 	%r966, %rl1165;
	shr.u64 	%rl1166, %rl1485, 62;
	shl.b64 	%rl1167, %rl1484, 2;
	or.b64  	%rl1490, %rl1166, %rl1167;
	shl.b64 	%rl431, %rl1485, 2;
	setp.ne.s64 	%p297, %rl431, 0;
	selp.u64 	%rl1168, 1, 0, %p297;
	or.b64  	%rl1169, %rl1168, %rl1490;
	setp.gt.u64 	%p298, %rl1169, -9223372036854775808;
	selp.u32 	%r967, 1, 0, %p298;
	add.s32 	%r968, %r967, %r966;
	neg.s32 	%r969, %r968;
	setp.lt.s64 	%p299, %rl417, 0;
	selp.b32 	%r1188, %r969, %r968, %p299;
	@%p298 bra 	BB3_379;

	mov.u64 	%rl1489, %rl431;
	bra.uni 	BB3_380;

BB3_379:
	not.b64 	%rl1170, %rl1490;
	neg.s64 	%rl432, %rl431;
	setp.eq.s64 	%p300, %rl431, 0;
	selp.u64 	%rl1171, 1, 0, %p300;
	add.s64 	%rl1490, %rl1171, %rl1170;
	xor.b64  	%rl1486, %rl1486, -9223372036854775808;
	mov.u64 	%rl1489, %rl432;

BB3_380:
	mov.u64 	%rl1488, %rl1489;
	setp.gt.s64 	%p301, %rl1490, 0;
	@%p301 bra 	BB3_382;

	mov.u32 	%r1187, 0;
	bra.uni 	BB3_384;

BB3_382:
	mov.u32 	%r1187, 0;

BB3_383:
	shr.u64 	%rl1172, %rl1488, 63;
	shl.b64 	%rl1173, %rl1490, 1;
	or.b64  	%rl1490, %rl1172, %rl1173;
	shl.b64 	%rl1488, %rl1488, 1;
	add.s32 	%r1187, %r1187, -1;
	setp.gt.s64 	%p302, %rl1490, 0;
	@%p302 bra 	BB3_383;

BB3_384:
	mul.lo.s64 	%rl1492, %rl1490, -3958705157555305931;
	mov.u64 	%rl1176, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1174, %rl1490, %rl1176;
	// inline asm
	setp.gt.s64 	%p303, %rl1174, 0;
	mov.u64 	%rl1491, %rl1174;
	@%p303 bra 	BB3_385;
	bra.uni 	BB3_386;

BB3_385:
	shl.b64 	%rl1177, %rl1174, 1;
	shr.u64 	%rl1178, %rl1492, 63;
	or.b64  	%rl1491, %rl1177, %rl1178;
	mul.lo.s64 	%rl1492, %rl1490, -7917410315110611862;
	add.s32 	%r1187, %r1187, -1;

BB3_386:
	setp.ne.s64 	%p304, %rl1492, 0;
	selp.u64 	%rl1179, 1, 0, %p304;
	add.s64 	%rl1180, %rl1179, %rl1491;
	add.s32 	%r972, %r1187, 1022;
	cvt.u64.u32 	%rl1181, %r972;
	shl.b64 	%rl1182, %rl1181, 52;
	shr.u64 	%rl1183, %rl1180, 11;
	shr.u64 	%rl1184, %rl1180, 10;
	and.b64  	%rl1185, %rl1184, 1;
	add.s64 	%rl1186, %rl1182, %rl1183;
	add.s64 	%rl1187, %rl1186, %rl1185;
	or.b64  	%rl1188, %rl1187, %rl1486;
	mov.b64 	 %fd2664, %rl1188;

BB3_387:
	and.b32  	%r973, %r1188, 1;
	setp.eq.s32 	%p305, %r973, 0;
	mul.rn.f64 	%fd211, %fd2664, %fd2664;
	@%p305 bra 	BB3_389;

	mov.f64 	%fd1703, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1705, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1702, %fd1703, %fd211, %fd1705;
	// inline asm
	mov.f64 	%fd1709, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1706, %fd1702, %fd211, %fd1709;
	// inline asm
	mov.f64 	%fd1713, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1710, %fd1706, %fd211, %fd1713;
	// inline asm
	mov.f64 	%fd1717, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1714, %fd1710, %fd211, %fd1717;
	// inline asm
	mov.f64 	%fd1721, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1718, %fd1714, %fd211, %fd1721;
	// inline asm
	mov.f64 	%fd1725, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1722, %fd1718, %fd211, %fd1725;
	// inline asm
	mov.f64 	%fd1729, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1726, %fd1722, %fd211, %fd1729;
	// inline asm
	mov.f64 	%fd2665, %fd1726;
	bra.uni 	BB3_390;

BB3_389:
	mov.f64 	%fd1731, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1733, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1730, %fd1731, %fd211, %fd1733;
	// inline asm
	mov.f64 	%fd1737, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1734, %fd1730, %fd211, %fd1737;
	// inline asm
	mov.f64 	%fd1741, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1738, %fd1734, %fd211, %fd1741;
	// inline asm
	mov.f64 	%fd1745, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1742, %fd1738, %fd211, %fd1745;
	// inline asm
	mov.f64 	%fd1749, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1746, %fd1742, %fd211, %fd1749;
	// inline asm
	mul.rn.f64 	%fd1751, %fd1746, %fd211;
	// inline asm
	fma.rn.f64 	%fd1750, %fd1751, %fd2664, %fd2664;
	// inline asm
	mov.f64 	%fd2665, %fd1750;

BB3_390:
	and.b32  	%r974, %r1188, 2;
	setp.eq.s32 	%p306, %r974, 0;
	neg.f64 	%fd1754, %fd2665;
	selp.f64 	%fd2666, %fd2665, %fd1754, %p306;
	bra.uni 	BB3_392;

BB3_391:
	mov.f64 	%fd1755, 0d0000000000000000;
	mul.rn.f64 	%fd2666, %fd197, %fd1755;

BB3_392:
	neg.f64 	%fd1756, %fd2666;
	mov.f64 	%fd2703, %fd2663;
	mov.f64 	%fd2704, %fd1756;
	ld.param.u32 	%r1130, [DIT10C2C_param_3];
	setp.eq.s32 	%p307, %r1130, 0;
	@%p307 bra 	BB3_393;
	bra.uni 	BB3_394;

BB3_393:
	mov.f64 	%fd2703, %fd2663;
	mov.f64 	%fd2704, %fd2666;

BB3_394:
	mul.f64 	%fd1758, %fd63, %fd2703;
	neg.f64 	%fd1760, %fd64;
	fma.rn.f64 	%fd1761, %fd1760, %fd2704, %fd1758;
	mul.f64 	%fd1762, %fd64, %fd2703;
	fma.rn.f64 	%fd1763, %fd63, %fd2704, %fd1762;
	mov.f64 	%fd2693, %fd1761;
	mov.f64 	%fd2694, %fd1763;
	mul.f64 	%fd1764, %fd69, 0d404921FB54442D18;
	div.rn.f64 	%fd218, %fd1764, %fd70;
	setp.eq.f64 	%p13, %fd218, 0d7FF0000000000000;
	setp.eq.f64 	%p14, %fd218, 0dFFF0000000000000;
	or.pred  	%p308, %p13, %p14;
	@%p308 bra 	BB3_417;

	// inline asm
	abs.f64 	%fd1765, %fd218;
	// inline asm
	setp.gt.f64 	%p309, %fd1765, 0d41E0000000000000;
	@%p309 bra 	BB3_397;

	mov.f64 	%fd1780, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1767, %fd218, %fd1780;
	// inline asm
	cvt.rni.s32.f64 	%r975, %fd1767;
	// inline asm
	cvt.rn.f64.s32 	%fd1781, %r975;
	neg.f64 	%fd1777, %fd1781;
	mov.f64 	%fd1770, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1768, %fd1777, %fd1770, %fd218;
	// inline asm
	mov.f64 	%fd1774, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1772, %fd1777, %fd1774, %fd1768;
	// inline asm
	mov.f64 	%fd1778, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1776, %fd1777, %fd1778, %fd1772;
	// inline asm
	mov.u32 	%r1191, %r975;
	mov.f64 	%fd2667, %fd1776;
	bra.uni 	BB3_413;

BB3_397:
	mov.b64 	 %rl449, %fd218;
	and.b64  	%rl1496, %rl449, -9223372036854775808;
	shr.u64 	%rl451, %rl449, 52;
	and.b64  	%rl1189, %rl451, 2047;
	add.s64 	%rl1190, %rl1189, 4294966272;
	cvt.u32.u64 	%r278, %rl1190;
	shl.b64 	%rl1191, %rl449, 11;
	or.b64  	%rl452, %rl1191, -9223372036854775808;
	shr.u32 	%r979, %r278, 6;
	mov.u32 	%r980, 16;
	sub.s32 	%r279, %r980, %r979;
	mov.u32 	%r981, 15;
	sub.s32 	%r1189, %r981, %r979;
	mov.u32 	%r982, 19;
	sub.s32 	%r281, %r982, %r979;
	mov.u32 	%r977, 18;
	// inline asm
	min.s32 	%r976, %r977, %r281;
	// inline asm
	setp.lt.s32 	%p310, %r1189, %r976;
	@%p310 bra 	BB3_399;

	mov.u64 	%rl1493, 0;
	bra.uni 	BB3_401;

BB3_399:
	mov.u32 	%r983, 1;
	sub.s32 	%r282, %r983, %r279;
	mov.u64 	%rl1493, 0;

BB3_400:
	.pragma "nounroll";
	shl.b32 	%r987, %r1189, 3;
	mov.u32 	%r988, __internal_i2opi_d;
	add.s32 	%r989, %r988, %r987;
	ld.const.u64 	%rl1195, [%r989];
	mul.lo.s64 	%rl1197, %rl1195, %rl452;
	// inline asm
	mul.hi.u64 	%rl1194, %rl1195, %rl452;
	// inline asm
	mad.lo.s64 	%rl1198, %rl1195, %rl452, %rl1493;
	setp.lt.u64 	%p311, %rl1198, %rl1197;
	selp.u64 	%rl1199, 1, 0, %p311;
	add.s64 	%rl1493, %rl1199, %rl1194;
	add.s32 	%r990, %r282, %r1189;
	shl.b32 	%r991, %r990, 3;
	add.s32 	%r993, %r48, %r991;
	st.local.u64 	[%r993], %rl1198;
	// inline asm
	min.s32 	%r984, %r977, %r281;
	// inline asm
	add.s32 	%r1189, %r1189, 1;
	setp.lt.s32 	%p312, %r1189, %r984;
	@%p312 bra 	BB3_400;

BB3_401:
	mov.u32 	%r994, 1;
	sub.s32 	%r995, %r994, %r279;
	add.s32 	%r996, %r995, %r1189;
	shl.b32 	%r997, %r996, 3;
	add.s32 	%r999, %r48, %r997;
	st.local.u64 	[%r999], %rl1493;
	ld.local.u64 	%rl1494, [%r48+24];
	ld.local.u64 	%rl1495, [%r48+16];
	and.b32  	%r1000, %r278, 63;
	setp.eq.s32 	%p313, %r1000, 0;
	@%p313 bra 	BB3_403;

	and.b64  	%rl1200, %rl451, 63;
	cvt.u32.u64 	%r1001, %rl1200;
	shl.b64 	%rl1201, %rl1494, %r1001;
	neg.s32 	%r1002, %r278;
	and.b32  	%r1003, %r1002, 63;
	shr.u64 	%rl1202, %rl1495, %r1003;
	or.b64  	%rl1494, %rl1202, %rl1201;
	shl.b64 	%rl1203, %rl1495, %r1001;
	ld.local.u64 	%rl1204, [%r48+8];
	shr.u64 	%rl1205, %rl1204, %r1003;
	or.b64  	%rl1495, %rl1205, %rl1203;

BB3_403:
	shr.u64 	%rl1206, %rl1494, 62;
	cvt.u32.u64 	%r1004, %rl1206;
	shr.u64 	%rl1207, %rl1495, 62;
	shl.b64 	%rl1208, %rl1494, 2;
	or.b64  	%rl1500, %rl1207, %rl1208;
	shl.b64 	%rl463, %rl1495, 2;
	setp.ne.s64 	%p314, %rl463, 0;
	selp.u64 	%rl1209, 1, 0, %p314;
	or.b64  	%rl1210, %rl1209, %rl1500;
	setp.gt.u64 	%p315, %rl1210, -9223372036854775808;
	selp.u32 	%r1005, 1, 0, %p315;
	add.s32 	%r1006, %r1005, %r1004;
	neg.s32 	%r1007, %r1006;
	setp.lt.s64 	%p316, %rl449, 0;
	selp.b32 	%r1191, %r1007, %r1006, %p316;
	@%p315 bra 	BB3_405;

	mov.u64 	%rl1499, %rl463;
	bra.uni 	BB3_406;

BB3_405:
	not.b64 	%rl1211, %rl1500;
	neg.s64 	%rl464, %rl463;
	setp.eq.s64 	%p317, %rl463, 0;
	selp.u64 	%rl1212, 1, 0, %p317;
	add.s64 	%rl1500, %rl1212, %rl1211;
	xor.b64  	%rl1496, %rl1496, -9223372036854775808;
	mov.u64 	%rl1499, %rl464;

BB3_406:
	mov.u64 	%rl1498, %rl1499;
	setp.gt.s64 	%p318, %rl1500, 0;
	@%p318 bra 	BB3_408;

	mov.u32 	%r1190, 0;
	bra.uni 	BB3_410;

BB3_408:
	mov.u32 	%r1190, 0;

BB3_409:
	shr.u64 	%rl1213, %rl1498, 63;
	shl.b64 	%rl1214, %rl1500, 1;
	or.b64  	%rl1500, %rl1213, %rl1214;
	shl.b64 	%rl1498, %rl1498, 1;
	add.s32 	%r1190, %r1190, -1;
	setp.gt.s64 	%p319, %rl1500, 0;
	@%p319 bra 	BB3_409;

BB3_410:
	mul.lo.s64 	%rl1502, %rl1500, -3958705157555305931;
	mov.u64 	%rl1217, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1215, %rl1500, %rl1217;
	// inline asm
	setp.gt.s64 	%p320, %rl1215, 0;
	mov.u64 	%rl1501, %rl1215;
	@%p320 bra 	BB3_411;
	bra.uni 	BB3_412;

BB3_411:
	shl.b64 	%rl1218, %rl1215, 1;
	shr.u64 	%rl1219, %rl1502, 63;
	or.b64  	%rl1501, %rl1218, %rl1219;
	mul.lo.s64 	%rl1502, %rl1500, -7917410315110611862;
	add.s32 	%r1190, %r1190, -1;

BB3_412:
	setp.ne.s64 	%p321, %rl1502, 0;
	selp.u64 	%rl1220, 1, 0, %p321;
	add.s64 	%rl1221, %rl1220, %rl1501;
	add.s32 	%r1010, %r1190, 1022;
	cvt.u64.u32 	%rl1222, %r1010;
	shl.b64 	%rl1223, %rl1222, 52;
	shr.u64 	%rl1224, %rl1221, 11;
	shr.u64 	%rl1225, %rl1221, 10;
	and.b64  	%rl1226, %rl1225, 1;
	add.s64 	%rl1227, %rl1223, %rl1224;
	add.s64 	%rl1228, %rl1227, %rl1226;
	or.b64  	%rl1229, %rl1228, %rl1496;
	mov.b64 	 %fd2667, %rl1229;

BB3_413:
	add.s32 	%r293, %r1191, 1;
	and.b32  	%r1011, %r293, 1;
	setp.eq.s32 	%p322, %r1011, 0;
	mul.rn.f64 	%fd222, %fd2667, %fd2667;
	@%p322 bra 	BB3_415;

	mov.f64 	%fd1783, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1785, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1782, %fd1783, %fd222, %fd1785;
	// inline asm
	mov.f64 	%fd1789, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1786, %fd1782, %fd222, %fd1789;
	// inline asm
	mov.f64 	%fd1793, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1790, %fd1786, %fd222, %fd1793;
	// inline asm
	mov.f64 	%fd1797, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1794, %fd1790, %fd222, %fd1797;
	// inline asm
	mov.f64 	%fd1801, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1798, %fd1794, %fd222, %fd1801;
	// inline asm
	mov.f64 	%fd1805, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1802, %fd1798, %fd222, %fd1805;
	// inline asm
	mov.f64 	%fd1809, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1806, %fd1802, %fd222, %fd1809;
	// inline asm
	mov.f64 	%fd2668, %fd1806;
	bra.uni 	BB3_416;

BB3_415:
	mov.f64 	%fd1811, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1813, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1810, %fd1811, %fd222, %fd1813;
	// inline asm
	mov.f64 	%fd1817, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1814, %fd1810, %fd222, %fd1817;
	// inline asm
	mov.f64 	%fd1821, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1818, %fd1814, %fd222, %fd1821;
	// inline asm
	mov.f64 	%fd1825, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1822, %fd1818, %fd222, %fd1825;
	// inline asm
	mov.f64 	%fd1829, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1826, %fd1822, %fd222, %fd1829;
	// inline asm
	mul.rn.f64 	%fd1831, %fd1826, %fd222;
	// inline asm
	fma.rn.f64 	%fd1830, %fd1831, %fd2667, %fd2667;
	// inline asm
	mov.f64 	%fd2668, %fd1830;

BB3_416:
	and.b32  	%r1012, %r293, 2;
	setp.eq.s32 	%p323, %r1012, 0;
	neg.f64 	%fd1834, %fd2668;
	selp.f64 	%fd2669, %fd2668, %fd1834, %p323;
	bra.uni 	BB3_418;

BB3_417:
	mov.f64 	%fd2669, 0dFFF8000000000000;

BB3_418:
	setp.eq.f64 	%p324, %fd218, 0d0000000000000000;
	or.pred  	%p325, %p14, %p324;
	or.pred  	%p326, %p13, %p325;
	@%p326 bra 	BB3_441;

	// inline asm
	abs.f64 	%fd1836, %fd218;
	// inline asm
	setp.gt.f64 	%p327, %fd1836, 0d41E0000000000000;
	@%p327 bra 	BB3_421;

	mov.f64 	%fd1851, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1838, %fd218, %fd1851;
	// inline asm
	cvt.rni.s32.f64 	%r1013, %fd1838;
	// inline asm
	cvt.rn.f64.s32 	%fd1852, %r1013;
	neg.f64 	%fd1848, %fd1852;
	mov.f64 	%fd1841, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1839, %fd1848, %fd1841, %fd218;
	// inline asm
	mov.f64 	%fd1845, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1843, %fd1848, %fd1845, %fd1839;
	// inline asm
	mov.f64 	%fd1849, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1847, %fd1848, %fd1849, %fd1843;
	// inline asm
	mov.u32 	%r1194, %r1013;
	mov.f64 	%fd2670, %fd1847;
	bra.uni 	BB3_437;

BB3_421:
	mov.b64 	 %rl481, %fd218;
	and.b64  	%rl1506, %rl481, -9223372036854775808;
	shr.u64 	%rl483, %rl481, 52;
	and.b64  	%rl1230, %rl483, 2047;
	add.s64 	%rl1231, %rl1230, 4294966272;
	cvt.u32.u64 	%r295, %rl1231;
	shl.b64 	%rl1232, %rl481, 11;
	or.b64  	%rl484, %rl1232, -9223372036854775808;
	shr.u32 	%r1017, %r295, 6;
	mov.u32 	%r1018, 16;
	sub.s32 	%r296, %r1018, %r1017;
	mov.u32 	%r1019, 15;
	sub.s32 	%r1192, %r1019, %r1017;
	mov.u32 	%r1020, 19;
	sub.s32 	%r298, %r1020, %r1017;
	mov.u32 	%r1015, 18;
	// inline asm
	min.s32 	%r1014, %r1015, %r298;
	// inline asm
	setp.lt.s32 	%p328, %r1192, %r1014;
	@%p328 bra 	BB3_423;

	mov.u64 	%rl1503, 0;
	bra.uni 	BB3_425;

BB3_423:
	mov.u32 	%r1021, 1;
	sub.s32 	%r299, %r1021, %r296;
	mov.u64 	%rl1503, 0;

BB3_424:
	.pragma "nounroll";
	shl.b32 	%r1025, %r1192, 3;
	mov.u32 	%r1026, __internal_i2opi_d;
	add.s32 	%r1027, %r1026, %r1025;
	ld.const.u64 	%rl1236, [%r1027];
	mul.lo.s64 	%rl1238, %rl1236, %rl484;
	// inline asm
	mul.hi.u64 	%rl1235, %rl1236, %rl484;
	// inline asm
	mad.lo.s64 	%rl1239, %rl1236, %rl484, %rl1503;
	setp.lt.u64 	%p329, %rl1239, %rl1238;
	selp.u64 	%rl1240, 1, 0, %p329;
	add.s64 	%rl1503, %rl1240, %rl1235;
	add.s32 	%r1028, %r299, %r1192;
	shl.b32 	%r1029, %r1028, 3;
	add.s32 	%r1031, %r48, %r1029;
	st.local.u64 	[%r1031], %rl1239;
	// inline asm
	min.s32 	%r1022, %r1015, %r298;
	// inline asm
	add.s32 	%r1192, %r1192, 1;
	setp.lt.s32 	%p330, %r1192, %r1022;
	@%p330 bra 	BB3_424;

BB3_425:
	mov.u32 	%r1032, 1;
	sub.s32 	%r1033, %r1032, %r296;
	add.s32 	%r1034, %r1033, %r1192;
	shl.b32 	%r1035, %r1034, 3;
	add.s32 	%r1037, %r48, %r1035;
	st.local.u64 	[%r1037], %rl1503;
	ld.local.u64 	%rl1504, [%r48+24];
	ld.local.u64 	%rl1505, [%r48+16];
	and.b32  	%r1038, %r295, 63;
	setp.eq.s32 	%p331, %r1038, 0;
	@%p331 bra 	BB3_427;

	and.b64  	%rl1241, %rl483, 63;
	cvt.u32.u64 	%r1039, %rl1241;
	shl.b64 	%rl1242, %rl1504, %r1039;
	neg.s32 	%r1040, %r295;
	and.b32  	%r1041, %r1040, 63;
	shr.u64 	%rl1243, %rl1505, %r1041;
	or.b64  	%rl1504, %rl1243, %rl1242;
	shl.b64 	%rl1244, %rl1505, %r1039;
	ld.local.u64 	%rl1245, [%r48+8];
	shr.u64 	%rl1246, %rl1245, %r1041;
	or.b64  	%rl1505, %rl1246, %rl1244;

BB3_427:
	shr.u64 	%rl1247, %rl1504, 62;
	cvt.u32.u64 	%r1042, %rl1247;
	shr.u64 	%rl1248, %rl1505, 62;
	shl.b64 	%rl1249, %rl1504, 2;
	or.b64  	%rl1510, %rl1248, %rl1249;
	shl.b64 	%rl495, %rl1505, 2;
	setp.ne.s64 	%p332, %rl495, 0;
	selp.u64 	%rl1250, 1, 0, %p332;
	or.b64  	%rl1251, %rl1250, %rl1510;
	setp.gt.u64 	%p333, %rl1251, -9223372036854775808;
	selp.u32 	%r1043, 1, 0, %p333;
	add.s32 	%r1044, %r1043, %r1042;
	neg.s32 	%r1045, %r1044;
	setp.lt.s64 	%p334, %rl481, 0;
	selp.b32 	%r1194, %r1045, %r1044, %p334;
	@%p333 bra 	BB3_429;

	mov.u64 	%rl1509, %rl495;
	bra.uni 	BB3_430;

BB3_429:
	not.b64 	%rl1252, %rl1510;
	neg.s64 	%rl496, %rl495;
	setp.eq.s64 	%p335, %rl495, 0;
	selp.u64 	%rl1253, 1, 0, %p335;
	add.s64 	%rl1510, %rl1253, %rl1252;
	xor.b64  	%rl1506, %rl1506, -9223372036854775808;
	mov.u64 	%rl1509, %rl496;

BB3_430:
	mov.u64 	%rl1508, %rl1509;
	setp.gt.s64 	%p336, %rl1510, 0;
	@%p336 bra 	BB3_432;

	mov.u32 	%r1193, 0;
	bra.uni 	BB3_434;

BB3_432:
	mov.u32 	%r1193, 0;

BB3_433:
	shr.u64 	%rl1254, %rl1508, 63;
	shl.b64 	%rl1255, %rl1510, 1;
	or.b64  	%rl1510, %rl1254, %rl1255;
	shl.b64 	%rl1508, %rl1508, 1;
	add.s32 	%r1193, %r1193, -1;
	setp.gt.s64 	%p337, %rl1510, 0;
	@%p337 bra 	BB3_433;

BB3_434:
	mul.lo.s64 	%rl1512, %rl1510, -3958705157555305931;
	mov.u64 	%rl1258, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1256, %rl1510, %rl1258;
	// inline asm
	setp.gt.s64 	%p338, %rl1256, 0;
	mov.u64 	%rl1511, %rl1256;
	@%p338 bra 	BB3_435;
	bra.uni 	BB3_436;

BB3_435:
	shl.b64 	%rl1259, %rl1256, 1;
	shr.u64 	%rl1260, %rl1512, 63;
	or.b64  	%rl1511, %rl1259, %rl1260;
	mul.lo.s64 	%rl1512, %rl1510, -7917410315110611862;
	add.s32 	%r1193, %r1193, -1;

BB3_436:
	setp.ne.s64 	%p339, %rl1512, 0;
	selp.u64 	%rl1261, 1, 0, %p339;
	add.s64 	%rl1262, %rl1261, %rl1511;
	add.s32 	%r1048, %r1193, 1022;
	cvt.u64.u32 	%rl1263, %r1048;
	shl.b64 	%rl1264, %rl1263, 52;
	shr.u64 	%rl1265, %rl1262, 11;
	shr.u64 	%rl1266, %rl1262, 10;
	and.b64  	%rl1267, %rl1266, 1;
	add.s64 	%rl1268, %rl1264, %rl1265;
	add.s64 	%rl1269, %rl1268, %rl1267;
	or.b64  	%rl1270, %rl1269, %rl1506;
	mov.b64 	 %fd2670, %rl1270;

BB3_437:
	and.b32  	%r1049, %r1194, 1;
	setp.eq.s32 	%p340, %r1049, 0;
	mul.rn.f64 	%fd232, %fd2670, %fd2670;
	@%p340 bra 	BB3_439;

	mov.f64 	%fd1854, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1856, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1853, %fd1854, %fd232, %fd1856;
	// inline asm
	mov.f64 	%fd1860, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1857, %fd1853, %fd232, %fd1860;
	// inline asm
	mov.f64 	%fd1864, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1861, %fd1857, %fd232, %fd1864;
	// inline asm
	mov.f64 	%fd1868, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1865, %fd1861, %fd232, %fd1868;
	// inline asm
	mov.f64 	%fd1872, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1869, %fd1865, %fd232, %fd1872;
	// inline asm
	mov.f64 	%fd1876, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1873, %fd1869, %fd232, %fd1876;
	// inline asm
	mov.f64 	%fd1880, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1877, %fd1873, %fd232, %fd1880;
	// inline asm
	mov.f64 	%fd2671, %fd1877;
	bra.uni 	BB3_440;

BB3_439:
	mov.f64 	%fd1882, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1884, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1881, %fd1882, %fd232, %fd1884;
	// inline asm
	mov.f64 	%fd1888, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1885, %fd1881, %fd232, %fd1888;
	// inline asm
	mov.f64 	%fd1892, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1889, %fd1885, %fd232, %fd1892;
	// inline asm
	mov.f64 	%fd1896, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1893, %fd1889, %fd232, %fd1896;
	// inline asm
	mov.f64 	%fd1900, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1897, %fd1893, %fd232, %fd1900;
	// inline asm
	mul.rn.f64 	%fd1902, %fd1897, %fd232;
	// inline asm
	fma.rn.f64 	%fd1901, %fd1902, %fd2670, %fd2670;
	// inline asm
	mov.f64 	%fd2671, %fd1901;

BB3_440:
	and.b32  	%r1050, %r1194, 2;
	setp.eq.s32 	%p341, %r1050, 0;
	neg.f64 	%fd1905, %fd2671;
	selp.f64 	%fd2672, %fd2671, %fd1905, %p341;
	bra.uni 	BB3_442;

BB3_441:
	mov.f64 	%fd1906, 0d0000000000000000;
	mul.rn.f64 	%fd2672, %fd218, %fd1906;

BB3_442:
	neg.f64 	%fd1907, %fd2672;
	mov.f64 	%fd2701, %fd2669;
	mov.f64 	%fd2702, %fd1907;
	ld.param.u32 	%r1129, [DIT10C2C_param_3];
	setp.eq.s32 	%p342, %r1129, 0;
	@%p342 bra 	BB3_443;
	bra.uni 	BB3_444;

BB3_443:
	mov.f64 	%fd2701, %fd2669;
	mov.f64 	%fd2702, %fd2672;

BB3_444:
	mul.f64 	%fd1909, %fd65, %fd2701;
	neg.f64 	%fd1911, %fd66;
	fma.rn.f64 	%fd1912, %fd1911, %fd2702, %fd1909;
	mul.f64 	%fd1913, %fd66, %fd2701;
	fma.rn.f64 	%fd1914, %fd65, %fd2702, %fd1913;
	mov.f64 	%fd2695, %fd1912;
	mov.f64 	%fd2696, %fd1914;
	mul.f64 	%fd1915, %fd69, 0d404C463ABECCB2BB;
	div.rn.f64 	%fd239, %fd1915, %fd70;
	setp.eq.f64 	%p15, %fd239, 0d7FF0000000000000;
	setp.eq.f64 	%p16, %fd239, 0dFFF0000000000000;
	or.pred  	%p343, %p15, %p16;
	@%p343 bra 	BB3_467;

	// inline asm
	abs.f64 	%fd1916, %fd239;
	// inline asm
	setp.gt.f64 	%p344, %fd1916, 0d41E0000000000000;
	@%p344 bra 	BB3_447;

	mov.f64 	%fd1931, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1918, %fd239, %fd1931;
	// inline asm
	cvt.rni.s32.f64 	%r1051, %fd1918;
	// inline asm
	cvt.rn.f64.s32 	%fd1932, %r1051;
	neg.f64 	%fd1928, %fd1932;
	mov.f64 	%fd1921, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1919, %fd1928, %fd1921, %fd239;
	// inline asm
	mov.f64 	%fd1925, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1923, %fd1928, %fd1925, %fd1919;
	// inline asm
	mov.f64 	%fd1929, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1927, %fd1928, %fd1929, %fd1923;
	// inline asm
	mov.u32 	%r1197, %r1051;
	mov.f64 	%fd2673, %fd1927;
	bra.uni 	BB3_463;

BB3_447:
	mov.b64 	 %rl513, %fd239;
	and.b64  	%rl1516, %rl513, -9223372036854775808;
	shr.u64 	%rl515, %rl513, 52;
	and.b64  	%rl1271, %rl515, 2047;
	add.s64 	%rl1272, %rl1271, 4294966272;
	cvt.u32.u64 	%r311, %rl1272;
	shl.b64 	%rl1273, %rl513, 11;
	or.b64  	%rl516, %rl1273, -9223372036854775808;
	shr.u32 	%r1055, %r311, 6;
	mov.u32 	%r1056, 16;
	sub.s32 	%r312, %r1056, %r1055;
	mov.u32 	%r1057, 15;
	sub.s32 	%r1195, %r1057, %r1055;
	mov.u32 	%r1058, 19;
	sub.s32 	%r314, %r1058, %r1055;
	mov.u32 	%r1053, 18;
	// inline asm
	min.s32 	%r1052, %r1053, %r314;
	// inline asm
	setp.lt.s32 	%p345, %r1195, %r1052;
	@%p345 bra 	BB3_449;

	mov.u64 	%rl1513, 0;
	bra.uni 	BB3_451;

BB3_449:
	mov.u32 	%r1059, 1;
	sub.s32 	%r315, %r1059, %r312;
	mov.u64 	%rl1513, 0;

BB3_450:
	.pragma "nounroll";
	shl.b32 	%r1063, %r1195, 3;
	mov.u32 	%r1064, __internal_i2opi_d;
	add.s32 	%r1065, %r1064, %r1063;
	ld.const.u64 	%rl1277, [%r1065];
	mul.lo.s64 	%rl1279, %rl1277, %rl516;
	// inline asm
	mul.hi.u64 	%rl1276, %rl1277, %rl516;
	// inline asm
	mad.lo.s64 	%rl1280, %rl1277, %rl516, %rl1513;
	setp.lt.u64 	%p346, %rl1280, %rl1279;
	selp.u64 	%rl1281, 1, 0, %p346;
	add.s64 	%rl1513, %rl1281, %rl1276;
	add.s32 	%r1066, %r315, %r1195;
	shl.b32 	%r1067, %r1066, 3;
	add.s32 	%r1069, %r48, %r1067;
	st.local.u64 	[%r1069], %rl1280;
	// inline asm
	min.s32 	%r1060, %r1053, %r314;
	// inline asm
	add.s32 	%r1195, %r1195, 1;
	setp.lt.s32 	%p347, %r1195, %r1060;
	@%p347 bra 	BB3_450;

BB3_451:
	mov.u32 	%r1070, 1;
	sub.s32 	%r1071, %r1070, %r312;
	add.s32 	%r1072, %r1071, %r1195;
	shl.b32 	%r1073, %r1072, 3;
	add.s32 	%r1075, %r48, %r1073;
	st.local.u64 	[%r1075], %rl1513;
	ld.local.u64 	%rl1514, [%r48+24];
	ld.local.u64 	%rl1515, [%r48+16];
	and.b32  	%r1076, %r311, 63;
	setp.eq.s32 	%p348, %r1076, 0;
	@%p348 bra 	BB3_453;

	and.b64  	%rl1282, %rl515, 63;
	cvt.u32.u64 	%r1077, %rl1282;
	shl.b64 	%rl1283, %rl1514, %r1077;
	neg.s32 	%r1078, %r311;
	and.b32  	%r1079, %r1078, 63;
	shr.u64 	%rl1284, %rl1515, %r1079;
	or.b64  	%rl1514, %rl1284, %rl1283;
	shl.b64 	%rl1285, %rl1515, %r1077;
	ld.local.u64 	%rl1286, [%r48+8];
	shr.u64 	%rl1287, %rl1286, %r1079;
	or.b64  	%rl1515, %rl1287, %rl1285;

BB3_453:
	shr.u64 	%rl1288, %rl1514, 62;
	cvt.u32.u64 	%r1080, %rl1288;
	shr.u64 	%rl1289, %rl1515, 62;
	shl.b64 	%rl1290, %rl1514, 2;
	or.b64  	%rl1520, %rl1289, %rl1290;
	shl.b64 	%rl527, %rl1515, 2;
	setp.ne.s64 	%p349, %rl527, 0;
	selp.u64 	%rl1291, 1, 0, %p349;
	or.b64  	%rl1292, %rl1291, %rl1520;
	setp.gt.u64 	%p350, %rl1292, -9223372036854775808;
	selp.u32 	%r1081, 1, 0, %p350;
	add.s32 	%r1082, %r1081, %r1080;
	neg.s32 	%r1083, %r1082;
	setp.lt.s64 	%p351, %rl513, 0;
	selp.b32 	%r1197, %r1083, %r1082, %p351;
	@%p350 bra 	BB3_455;

	mov.u64 	%rl1519, %rl527;
	bra.uni 	BB3_456;

BB3_455:
	not.b64 	%rl1293, %rl1520;
	neg.s64 	%rl528, %rl527;
	setp.eq.s64 	%p352, %rl527, 0;
	selp.u64 	%rl1294, 1, 0, %p352;
	add.s64 	%rl1520, %rl1294, %rl1293;
	xor.b64  	%rl1516, %rl1516, -9223372036854775808;
	mov.u64 	%rl1519, %rl528;

BB3_456:
	mov.u64 	%rl1518, %rl1519;
	setp.gt.s64 	%p353, %rl1520, 0;
	@%p353 bra 	BB3_458;

	mov.u32 	%r1196, 0;
	bra.uni 	BB3_460;

BB3_458:
	mov.u32 	%r1196, 0;

BB3_459:
	shr.u64 	%rl1295, %rl1518, 63;
	shl.b64 	%rl1296, %rl1520, 1;
	or.b64  	%rl1520, %rl1295, %rl1296;
	shl.b64 	%rl1518, %rl1518, 1;
	add.s32 	%r1196, %r1196, -1;
	setp.gt.s64 	%p354, %rl1520, 0;
	@%p354 bra 	BB3_459;

BB3_460:
	mul.lo.s64 	%rl1522, %rl1520, -3958705157555305931;
	mov.u64 	%rl1299, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1297, %rl1520, %rl1299;
	// inline asm
	setp.gt.s64 	%p355, %rl1297, 0;
	mov.u64 	%rl1521, %rl1297;
	@%p355 bra 	BB3_461;
	bra.uni 	BB3_462;

BB3_461:
	shl.b64 	%rl1300, %rl1297, 1;
	shr.u64 	%rl1301, %rl1522, 63;
	or.b64  	%rl1521, %rl1300, %rl1301;
	mul.lo.s64 	%rl1522, %rl1520, -7917410315110611862;
	add.s32 	%r1196, %r1196, -1;

BB3_462:
	setp.ne.s64 	%p356, %rl1522, 0;
	selp.u64 	%rl1302, 1, 0, %p356;
	add.s64 	%rl1303, %rl1302, %rl1521;
	add.s32 	%r1086, %r1196, 1022;
	cvt.u64.u32 	%rl1304, %r1086;
	shl.b64 	%rl1305, %rl1304, 52;
	shr.u64 	%rl1306, %rl1303, 11;
	shr.u64 	%rl1307, %rl1303, 10;
	and.b64  	%rl1308, %rl1307, 1;
	add.s64 	%rl1309, %rl1305, %rl1306;
	add.s64 	%rl1310, %rl1309, %rl1308;
	or.b64  	%rl1311, %rl1310, %rl1516;
	mov.b64 	 %fd2673, %rl1311;

BB3_463:
	add.s32 	%r326, %r1197, 1;
	and.b32  	%r1087, %r326, 1;
	setp.eq.s32 	%p357, %r1087, 0;
	mul.rn.f64 	%fd243, %fd2673, %fd2673;
	@%p357 bra 	BB3_465;

	mov.f64 	%fd1934, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd1936, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd1933, %fd1934, %fd243, %fd1936;
	// inline asm
	mov.f64 	%fd1940, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd1937, %fd1933, %fd243, %fd1940;
	// inline asm
	mov.f64 	%fd1944, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd1941, %fd1937, %fd243, %fd1944;
	// inline asm
	mov.f64 	%fd1948, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd1945, %fd1941, %fd243, %fd1948;
	// inline asm
	mov.f64 	%fd1952, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1949, %fd1945, %fd243, %fd1952;
	// inline asm
	mov.f64 	%fd1956, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1953, %fd1949, %fd243, %fd1956;
	// inline asm
	mov.f64 	%fd1960, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1957, %fd1953, %fd243, %fd1960;
	// inline asm
	mov.f64 	%fd2674, %fd1957;
	bra.uni 	BB3_466;

BB3_465:
	mov.f64 	%fd1962, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1964, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1961, %fd1962, %fd243, %fd1964;
	// inline asm
	mov.f64 	%fd1968, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1965, %fd1961, %fd243, %fd1968;
	// inline asm
	mov.f64 	%fd1972, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1969, %fd1965, %fd243, %fd1972;
	// inline asm
	mov.f64 	%fd1976, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1973, %fd1969, %fd243, %fd1976;
	// inline asm
	mov.f64 	%fd1980, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1977, %fd1973, %fd243, %fd1980;
	// inline asm
	mul.rn.f64 	%fd1982, %fd1977, %fd243;
	// inline asm
	fma.rn.f64 	%fd1981, %fd1982, %fd2673, %fd2673;
	// inline asm
	mov.f64 	%fd2674, %fd1981;

BB3_466:
	and.b32  	%r1088, %r326, 2;
	setp.eq.s32 	%p358, %r1088, 0;
	neg.f64 	%fd1985, %fd2674;
	selp.f64 	%fd2675, %fd2674, %fd1985, %p358;
	bra.uni 	BB3_468;

BB3_467:
	mov.f64 	%fd2675, 0dFFF8000000000000;

BB3_468:
	setp.eq.f64 	%p359, %fd239, 0d0000000000000000;
	or.pred  	%p360, %p16, %p359;
	or.pred  	%p361, %p15, %p360;
	@%p361 bra 	BB3_491;

	// inline asm
	abs.f64 	%fd1987, %fd239;
	// inline asm
	setp.gt.f64 	%p362, %fd1987, 0d41E0000000000000;
	@%p362 bra 	BB3_471;

	mov.f64 	%fd2002, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd1989, %fd239, %fd2002;
	// inline asm
	cvt.rni.s32.f64 	%r1089, %fd1989;
	// inline asm
	cvt.rn.f64.s32 	%fd2003, %r1089;
	neg.f64 	%fd1999, %fd2003;
	mov.f64 	%fd1992, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd1990, %fd1999, %fd1992, %fd239;
	// inline asm
	mov.f64 	%fd1996, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd1994, %fd1999, %fd1996, %fd1990;
	// inline asm
	mov.f64 	%fd2000, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd1998, %fd1999, %fd2000, %fd1994;
	// inline asm
	mov.u32 	%r1200, %r1089;
	mov.f64 	%fd2676, %fd1998;
	bra.uni 	BB3_487;

BB3_471:
	mov.b64 	 %rl545, %fd239;
	and.b64  	%rl1526, %rl545, -9223372036854775808;
	shr.u64 	%rl547, %rl545, 52;
	and.b64  	%rl1312, %rl547, 2047;
	add.s64 	%rl1313, %rl1312, 4294966272;
	cvt.u32.u64 	%r328, %rl1313;
	shl.b64 	%rl1314, %rl545, 11;
	or.b64  	%rl548, %rl1314, -9223372036854775808;
	shr.u32 	%r1093, %r328, 6;
	mov.u32 	%r1094, 16;
	sub.s32 	%r329, %r1094, %r1093;
	mov.u32 	%r1095, 15;
	sub.s32 	%r1198, %r1095, %r1093;
	mov.u32 	%r1096, 19;
	sub.s32 	%r331, %r1096, %r1093;
	mov.u32 	%r1091, 18;
	// inline asm
	min.s32 	%r1090, %r1091, %r331;
	// inline asm
	setp.lt.s32 	%p363, %r1198, %r1090;
	@%p363 bra 	BB3_473;

	mov.u64 	%rl1523, 0;
	bra.uni 	BB3_475;

BB3_473:
	mov.u32 	%r1097, 1;
	sub.s32 	%r332, %r1097, %r329;
	mov.u64 	%rl1523, 0;

BB3_474:
	.pragma "nounroll";
	shl.b32 	%r1101, %r1198, 3;
	mov.u32 	%r1102, __internal_i2opi_d;
	add.s32 	%r1103, %r1102, %r1101;
	ld.const.u64 	%rl1318, [%r1103];
	mul.lo.s64 	%rl1320, %rl1318, %rl548;
	// inline asm
	mul.hi.u64 	%rl1317, %rl1318, %rl548;
	// inline asm
	mad.lo.s64 	%rl1321, %rl1318, %rl548, %rl1523;
	setp.lt.u64 	%p364, %rl1321, %rl1320;
	selp.u64 	%rl1322, 1, 0, %p364;
	add.s64 	%rl1523, %rl1322, %rl1317;
	add.s32 	%r1104, %r332, %r1198;
	shl.b32 	%r1105, %r1104, 3;
	add.s32 	%r1107, %r48, %r1105;
	st.local.u64 	[%r1107], %rl1321;
	// inline asm
	min.s32 	%r1098, %r1091, %r331;
	// inline asm
	add.s32 	%r1198, %r1198, 1;
	setp.lt.s32 	%p365, %r1198, %r1098;
	@%p365 bra 	BB3_474;

BB3_475:
	mov.u32 	%r1108, 1;
	sub.s32 	%r1109, %r1108, %r329;
	add.s32 	%r1110, %r1109, %r1198;
	shl.b32 	%r1111, %r1110, 3;
	add.s32 	%r1113, %r48, %r1111;
	st.local.u64 	[%r1113], %rl1523;
	ld.local.u64 	%rl1524, [%r48+24];
	ld.local.u64 	%rl1525, [%r48+16];
	and.b32  	%r1114, %r328, 63;
	setp.eq.s32 	%p366, %r1114, 0;
	@%p366 bra 	BB3_477;

	and.b64  	%rl1323, %rl547, 63;
	cvt.u32.u64 	%r1115, %rl1323;
	shl.b64 	%rl1324, %rl1524, %r1115;
	neg.s32 	%r1116, %r328;
	and.b32  	%r1117, %r1116, 63;
	shr.u64 	%rl1325, %rl1525, %r1117;
	or.b64  	%rl1524, %rl1325, %rl1324;
	shl.b64 	%rl1326, %rl1525, %r1115;
	ld.local.u64 	%rl1327, [%r48+8];
	shr.u64 	%rl1328, %rl1327, %r1117;
	or.b64  	%rl1525, %rl1328, %rl1326;

BB3_477:
	shr.u64 	%rl1329, %rl1524, 62;
	cvt.u32.u64 	%r1118, %rl1329;
	shr.u64 	%rl1330, %rl1525, 62;
	shl.b64 	%rl1331, %rl1524, 2;
	or.b64  	%rl1530, %rl1330, %rl1331;
	shl.b64 	%rl559, %rl1525, 2;
	setp.ne.s64 	%p367, %rl559, 0;
	selp.u64 	%rl1332, 1, 0, %p367;
	or.b64  	%rl1333, %rl1332, %rl1530;
	setp.gt.u64 	%p368, %rl1333, -9223372036854775808;
	selp.u32 	%r1119, 1, 0, %p368;
	add.s32 	%r1120, %r1119, %r1118;
	neg.s32 	%r1121, %r1120;
	setp.lt.s64 	%p369, %rl545, 0;
	selp.b32 	%r1200, %r1121, %r1120, %p369;
	@%p368 bra 	BB3_479;

	mov.u64 	%rl1529, %rl559;
	bra.uni 	BB3_480;

BB3_479:
	not.b64 	%rl1334, %rl1530;
	neg.s64 	%rl560, %rl559;
	setp.eq.s64 	%p370, %rl559, 0;
	selp.u64 	%rl1335, 1, 0, %p370;
	add.s64 	%rl1530, %rl1335, %rl1334;
	xor.b64  	%rl1526, %rl1526, -9223372036854775808;
	mov.u64 	%rl1529, %rl560;

BB3_480:
	mov.u64 	%rl1528, %rl1529;
	setp.gt.s64 	%p371, %rl1530, 0;
	@%p371 bra 	BB3_482;

	mov.u32 	%r1199, 0;
	bra.uni 	BB3_484;

BB3_482:
	mov.u32 	%r1199, 0;

BB3_483:
	shr.u64 	%rl1336, %rl1528, 63;
	shl.b64 	%rl1337, %rl1530, 1;
	or.b64  	%rl1530, %rl1336, %rl1337;
	shl.b64 	%rl1528, %rl1528, 1;
	add.s32 	%r1199, %r1199, -1;
	setp.gt.s64 	%p372, %rl1530, 0;
	@%p372 bra 	BB3_483;

BB3_484:
	mul.lo.s64 	%rl1532, %rl1530, -3958705157555305931;
	mov.u64 	%rl1340, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl1338, %rl1530, %rl1340;
	// inline asm
	setp.gt.s64 	%p373, %rl1338, 0;
	mov.u64 	%rl1531, %rl1338;
	@%p373 bra 	BB3_485;
	bra.uni 	BB3_486;

BB3_485:
	shl.b64 	%rl1341, %rl1338, 1;
	shr.u64 	%rl1342, %rl1532, 63;
	or.b64  	%rl1531, %rl1341, %rl1342;
	mul.lo.s64 	%rl1532, %rl1530, -7917410315110611862;
	add.s32 	%r1199, %r1199, -1;

BB3_486:
	setp.ne.s64 	%p374, %rl1532, 0;
	selp.u64 	%rl1343, 1, 0, %p374;
	add.s64 	%rl1344, %rl1343, %rl1531;
	add.s32 	%r1124, %r1199, 1022;
	cvt.u64.u32 	%rl1345, %r1124;
	shl.b64 	%rl1346, %rl1345, 52;
	shr.u64 	%rl1347, %rl1344, 11;
	shr.u64 	%rl1348, %rl1344, 10;
	and.b64  	%rl1349, %rl1348, 1;
	add.s64 	%rl1350, %rl1346, %rl1347;
	add.s64 	%rl1351, %rl1350, %rl1349;
	or.b64  	%rl1352, %rl1351, %rl1526;
	mov.b64 	 %fd2676, %rl1352;

BB3_487:
	and.b32  	%r1125, %r1200, 1;
	setp.eq.s32 	%p375, %r1125, 0;
	mul.rn.f64 	%fd253, %fd2676, %fd2676;
	@%p375 bra 	BB3_489;

	mov.f64 	%fd2005, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd2007, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd2004, %fd2005, %fd253, %fd2007;
	// inline asm
	mov.f64 	%fd2011, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd2008, %fd2004, %fd253, %fd2011;
	// inline asm
	mov.f64 	%fd2015, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd2012, %fd2008, %fd253, %fd2015;
	// inline asm
	mov.f64 	%fd2019, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd2016, %fd2012, %fd253, %fd2019;
	// inline asm
	mov.f64 	%fd2023, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd2020, %fd2016, %fd253, %fd2023;
	// inline asm
	mov.f64 	%fd2027, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd2024, %fd2020, %fd253, %fd2027;
	// inline asm
	mov.f64 	%fd2031, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd2028, %fd2024, %fd253, %fd2031;
	// inline asm
	mov.f64 	%fd2677, %fd2028;
	bra.uni 	BB3_490;

BB3_489:
	mov.f64 	%fd2033, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd2035, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd2032, %fd2033, %fd253, %fd2035;
	// inline asm
	mov.f64 	%fd2039, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd2036, %fd2032, %fd253, %fd2039;
	// inline asm
	mov.f64 	%fd2043, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd2040, %fd2036, %fd253, %fd2043;
	// inline asm
	mov.f64 	%fd2047, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd2044, %fd2040, %fd253, %fd2047;
	// inline asm
	mov.f64 	%fd2051, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd2048, %fd2044, %fd253, %fd2051;
	// inline asm
	mul.rn.f64 	%fd2053, %fd2048, %fd253;
	// inline asm
	fma.rn.f64 	%fd2052, %fd2053, %fd2676, %fd2676;
	// inline asm
	mov.f64 	%fd2677, %fd2052;

BB3_490:
	and.b32  	%r1126, %r1200, 2;
	setp.eq.s32 	%p376, %r1126, 0;
	neg.f64 	%fd2056, %fd2677;
	selp.f64 	%fd2678, %fd2677, %fd2056, %p376;
	bra.uni 	BB3_492;

BB3_491:
	mov.f64 	%fd2057, 0d0000000000000000;
	mul.rn.f64 	%fd2678, %fd239, %fd2057;

BB3_492:
	neg.f64 	%fd2058, %fd2678;
	mov.f64 	%fd2699, %fd2675;
	mov.f64 	%fd2700, %fd2058;
	ld.param.u32 	%r1128, [DIT10C2C_param_3];
	setp.eq.s32 	%p377, %r1128, 0;
	@%p377 bra 	BB3_493;
	bra.uni 	BB3_494;

BB3_493:
	mov.f64 	%fd2699, %fd2675;
	mov.f64 	%fd2700, %fd2678;

BB3_494:
	mul.f64 	%fd2060, %fd67, %fd2699;
	neg.f64 	%fd2062, %fd68;
	fma.rn.f64 	%fd2063, %fd2062, %fd2700, %fd2060;
	mul.f64 	%fd2064, %fd68, %fd2699;
	fma.rn.f64 	%fd2065, %fd67, %fd2700, %fd2064;
	mov.f64 	%fd2697, %fd2063;
	mov.f64 	%fd2698, %fd2065;

BB3_495:
	add.f64 	%fd2068, %fd2679, %fd2681;
	add.f64 	%fd2070, %fd2068, %fd2683;
	add.f64 	%fd2072, %fd2070, %fd2685;
	add.f64 	%fd2074, %fd2072, %fd2687;
	add.f64 	%fd2076, %fd2074, %fd2689;
	add.f64 	%fd2078, %fd2076, %fd2691;
	add.f64 	%fd2080, %fd2078, %fd2693;
	add.f64 	%fd2082, %fd2080, %fd2695;
	add.f64 	%fd2084, %fd2082, %fd2697;
	st.global.f64 	[%r38], %fd2084;
	add.f64 	%fd2087, %fd2680, %fd2682;
	add.f64 	%fd2089, %fd2087, %fd2684;
	add.f64 	%fd2091, %fd2089, %fd2686;
	add.f64 	%fd2093, %fd2091, %fd2688;
	add.f64 	%fd2095, %fd2093, %fd2690;
	add.f64 	%fd2097, %fd2095, %fd2692;
	add.f64 	%fd2099, %fd2097, %fd2694;
	add.f64 	%fd2101, %fd2099, %fd2696;
	add.f64 	%fd2103, %fd2101, %fd2698;
	st.global.f64 	[%r38+8], %fd2103;
	mul.f64 	%fd2104, %fd2681, 0d3FE9E3779B97F4A4;
	mul.f64 	%fd2105, %fd2682, 0d3FE2CF2304755A5D;
	mov.f64 	%fd2106, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2107, %fd2681, 0d3FE9E3779B97F4A4, %fd2105;
	add.f64 	%fd2108, %fd2679, %fd2107;
	mul.f64 	%fd2109, %fd2683, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2110, %fd2684, 0d3FEE6F0E13445503;
	mov.f64 	%fd2111, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2112, %fd2683, 0d3FD3C6EF372FE948, %fd2110;
	add.f64 	%fd2113, %fd2108, %fd2112;
	mul.f64 	%fd2114, %fd2685, 0dBFD3C6EF372FE948;
	mul.f64 	%fd2115, %fd2686, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2116, %fd2685, 0dBFD3C6EF372FE948, %fd2115;
	add.f64 	%fd2117, %fd2113, %fd2116;
	mul.f64 	%fd2118, %fd2687, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2119, %fd2688, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2120, %fd2687, 0dBFE9E3779B97F4A4, %fd2119;
	add.f64 	%fd2121, %fd2117, %fd2120;
	sub.f64 	%fd2122, %fd2121, %fd2689;
	mul.f64 	%fd2123, %fd2691, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2124, %fd2692, 0d3FE2CF2304755A5D;
	neg.f64 	%fd2125, %fd2692;
	fma.rn.f64 	%fd2126, %fd2125, %fd2106, %fd2123;
	add.f64 	%fd2127, %fd2122, %fd2126;
	mul.f64 	%fd2128, %fd2693, 0dBFD3C6EF372FE948;
	mul.f64 	%fd2129, %fd2694, 0d3FEE6F0E13445503;
	neg.f64 	%fd2130, %fd2694;
	fma.rn.f64 	%fd2131, %fd2130, %fd2111, %fd2128;
	add.f64 	%fd2132, %fd2127, %fd2131;
	mul.f64 	%fd2133, %fd2695, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2134, %fd2696, 0d3FEE6F0E13445503;
	neg.f64 	%fd2135, %fd2696;
	fma.rn.f64 	%fd2136, %fd2135, %fd2111, %fd2133;
	add.f64 	%fd2137, %fd2132, %fd2136;
	mul.f64 	%fd2138, %fd2697, 0d3FE9E3779B97F4A4;
	mul.f64 	%fd2139, %fd2698, 0d3FE2CF2304755A5D;
	neg.f64 	%fd2140, %fd2698;
	fma.rn.f64 	%fd2141, %fd2140, %fd2106, %fd2138;
	add.f64 	%fd2142, %fd2137, %fd2141;
	st.global.f64 	[%r39], %fd2142;
	mul.f64 	%fd2143, %fd2682, 0d3FE9E3779B97F4A4;
	mul.f64 	%fd2144, %fd2681, 0d3FE2CF2304755A5D;
	neg.f64 	%fd2145, %fd2681;
	fma.rn.f64 	%fd2146, %fd2145, %fd2106, %fd2143;
	add.f64 	%fd2147, %fd2680, %fd2146;
	mul.f64 	%fd2148, %fd2684, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2149, %fd2683, 0d3FEE6F0E13445503;
	neg.f64 	%fd2150, %fd2683;
	fma.rn.f64 	%fd2151, %fd2150, %fd2111, %fd2148;
	add.f64 	%fd2152, %fd2147, %fd2151;
	mul.f64 	%fd2153, %fd2686, 0dBFD3C6EF372FE948;
	mul.f64 	%fd2154, %fd2685, 0d3FEE6F0E13445503;
	neg.f64 	%fd2155, %fd2685;
	fma.rn.f64 	%fd2156, %fd2155, %fd2111, %fd2153;
	add.f64 	%fd2157, %fd2152, %fd2156;
	mul.f64 	%fd2158, %fd2688, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2159, %fd2687, 0d3FE2CF2304755A5D;
	neg.f64 	%fd2160, %fd2687;
	fma.rn.f64 	%fd2161, %fd2160, %fd2106, %fd2158;
	add.f64 	%fd2162, %fd2157, %fd2161;
	sub.f64 	%fd2163, %fd2162, %fd2690;
	mul.f64 	%fd2164, %fd2692, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2165, %fd2691, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2166, %fd2692, 0dBFE9E3779B97F4A4, %fd2165;
	add.f64 	%fd2167, %fd2163, %fd2166;
	mul.f64 	%fd2168, %fd2694, 0dBFD3C6EF372FE948;
	mul.f64 	%fd2169, %fd2693, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2170, %fd2694, 0dBFD3C6EF372FE948, %fd2169;
	add.f64 	%fd2171, %fd2167, %fd2170;
	mul.f64 	%fd2172, %fd2696, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2173, %fd2695, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2174, %fd2696, 0d3FD3C6EF372FE948, %fd2173;
	add.f64 	%fd2175, %fd2171, %fd2174;
	mul.f64 	%fd2176, %fd2698, 0d3FE9E3779B97F4A4;
	mul.f64 	%fd2177, %fd2697, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2178, %fd2698, 0d3FE9E3779B97F4A4, %fd2177;
	add.f64 	%fd2179, %fd2175, %fd2178;
	st.global.f64 	[%r39+8], %fd2179;
	mul.f64 	%fd2180, %fd2681, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2181, %fd2682, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2182, %fd2681, 0d3FD3C6EF372FE948, %fd2181;
	add.f64 	%fd2183, %fd2679, %fd2182;
	mul.f64 	%fd2184, %fd2683, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2185, %fd2684, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2186, %fd2683, 0dBFE9E3779B97F4A4, %fd2185;
	add.f64 	%fd2187, %fd2183, %fd2186;
	mul.f64 	%fd2188, %fd2685, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2189, %fd2686, 0d3FE2CF2304755A5D;
	neg.f64 	%fd2190, %fd2686;
	fma.rn.f64 	%fd2191, %fd2190, %fd2106, %fd2188;
	add.f64 	%fd2192, %fd2187, %fd2191;
	mul.f64 	%fd2193, %fd2687, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2194, %fd2688, 0d3FEE6F0E13445503;
	neg.f64 	%fd2195, %fd2688;
	fma.rn.f64 	%fd2196, %fd2195, %fd2111, %fd2193;
	add.f64 	%fd2197, %fd2192, %fd2196;
	add.f64 	%fd2198, %fd2197, %fd2689;
	mul.f64 	%fd2199, %fd2691, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2200, %fd2692, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2201, %fd2691, 0d3FD3C6EF372FE948, %fd2200;
	add.f64 	%fd2202, %fd2198, %fd2201;
	mul.f64 	%fd2203, %fd2693, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2204, %fd2694, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2205, %fd2693, 0dBFE9E3779B97F4A4, %fd2204;
	add.f64 	%fd2206, %fd2202, %fd2205;
	mul.f64 	%fd2207, %fd2695, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2208, %fd2696, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2209, %fd2135, %fd2106, %fd2207;
	add.f64 	%fd2210, %fd2206, %fd2209;
	mul.f64 	%fd2211, %fd2697, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2212, %fd2698, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2213, %fd2140, %fd2111, %fd2211;
	add.f64 	%fd2214, %fd2210, %fd2213;
	st.global.f64 	[%r40], %fd2214;
	mul.f64 	%fd2215, %fd2682, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2216, %fd2681, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2217, %fd2145, %fd2111, %fd2215;
	add.f64 	%fd2218, %fd2680, %fd2217;
	mul.f64 	%fd2219, %fd2684, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2220, %fd2683, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2221, %fd2150, %fd2106, %fd2219;
	add.f64 	%fd2222, %fd2218, %fd2221;
	mul.f64 	%fd2223, %fd2686, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2224, %fd2685, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2225, %fd2686, 0dBFE9E3779B97F4A4, %fd2224;
	add.f64 	%fd2226, %fd2222, %fd2225;
	mul.f64 	%fd2227, %fd2688, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2228, %fd2687, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2229, %fd2688, 0d3FD3C6EF372FE948, %fd2228;
	add.f64 	%fd2230, %fd2226, %fd2229;
	add.f64 	%fd2231, %fd2230, %fd2690;
	mul.f64 	%fd2232, %fd2692, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2233, %fd2691, 0d3FEE6F0E13445503;
	neg.f64 	%fd2234, %fd2691;
	fma.rn.f64 	%fd2235, %fd2234, %fd2111, %fd2232;
	add.f64 	%fd2236, %fd2231, %fd2235;
	mul.f64 	%fd2237, %fd2694, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2238, %fd2693, 0d3FE2CF2304755A5D;
	neg.f64 	%fd2239, %fd2693;
	fma.rn.f64 	%fd2240, %fd2239, %fd2106, %fd2237;
	add.f64 	%fd2241, %fd2236, %fd2240;
	mul.f64 	%fd2242, %fd2696, 0dBFE9E3779B97F4A4;
	mul.f64 	%fd2243, %fd2695, 0d3FE2CF2304755A5D;
	fma.rn.f64 	%fd2244, %fd2696, 0dBFE9E3779B97F4A4, %fd2243;
	add.f64 	%fd2245, %fd2241, %fd2244;
	mul.f64 	%fd2246, %fd2698, 0d3FD3C6EF372FE948;
	mul.f64 	%fd2247, %fd2697, 0d3FEE6F0E13445503;
	fma.rn.f64 	%fd2248, %fd2698, 0d3FD3C6EF372FE948, %fd2247;
	add.f64 	%fd2249, %fd2245, %fd2248;
	st.global.f64 	[%r40+8], %fd2249;
	mul.f64 	%fd2250, %fd2681, 0dBFD3C6EF372FE948;
	fma.rn.f64 	%fd2251, %fd2681, 0dBFD3C6EF372FE948, %fd2181;
	add.f64 	%fd2252, %fd2679, %fd2251;
	neg.f64 	%fd2253, %fd2684;
	fma.rn.f64 	%fd2254, %fd2253, %fd2106, %fd2184;
	add.f64 	%fd2255, %fd2252, %fd2254;
	mul.f64 	%fd2256, %fd2685, 0d3FE9E3779B97F4A4;
	fma.rn.f64 	%fd2257, %fd2190, %fd2106, %fd2256;
	add.f64 	%fd2258, %fd2255, %fd2257;
	fma.rn.f64 	%fd2259, %fd2687, 0d3FD3C6EF372FE948, %fd2194;
	add.f64 	%fd2260, %fd2258, %fd2259;
	sub.f64 	%fd2261, %fd2260, %fd2689;
	fma.rn.f64 	%fd2262, %fd2125, %fd2111, %fd2199;
	add.f64 	%fd2263, %fd2261, %fd2262;
	mul.f64 	%fd2264, %fd2693, 0d3FE9E3779B97F4A4;
	fma.rn.f64 	%fd2265, %fd2693, 0d3FE9E3779B97F4A4, %fd2204;
	add.f64 	%fd2266, %fd2263, %fd2265;
	fma.rn.f64 	%fd2267, %fd2695, 0dBFE9E3779B97F4A4, %fd2208;
	add.f64 	%fd2268, %fd2266, %fd2267;
	mul.f64 	%fd2269, %fd2697, 0dBFD3C6EF372FE948;
	fma.rn.f64 	%fd2270, %fd2140, %fd2111, %fd2269;
	add.f64 	%fd2271, %fd2268, %fd2270;
	st.global.f64 	[%r41], %fd2271;
	mul.f64 	%fd2272, %fd2682, 0dBFD3C6EF372FE948;
	fma.rn.f64 	%fd2273, %fd2145, %fd2111, %fd2272;
	add.f64 	%fd2274, %fd2680, %fd2273;
	fma.rn.f64 	%fd2275, %fd2684, 0dBFE9E3779B97F4A4, %fd2220;
	add.f64 	%fd2276, %fd2274, %fd2275;
	mul.f64 	%fd2277, %fd2686, 0d3FE9E3779B97F4A4;
	fma.rn.f64 	%fd2278, %fd2686, 0d3FE9E3779B97F4A4, %fd2224;
	add.f64 	%fd2279, %fd2276, %fd2278;
	fma.rn.f64 	%fd2280, %fd2160, %fd2111, %fd2227;
	add.f64 	%fd2281, %fd2279, %fd2280;
	sub.f64 	%fd2282, %fd2281, %fd2690;
	fma.rn.f64 	%fd2283, %fd2692, 0d3FD3C6EF372FE948, %fd2233;
	add.f64 	%fd2284, %fd2282, %fd2283;
	mul.f64 	%fd2285, %fd2694, 0d3FE9E3779B97F4A4;
	fma.rn.f64 	%fd2286, %fd2239, %fd2106, %fd2285;
	add.f64 	%fd2287, %fd2284, %fd2286;
	neg.f64 	%fd2288, %fd2695;
	fma.rn.f64 	%fd2289, %fd2288, %fd2106, %fd2242;
	add.f64 	%fd2290, %fd2287, %fd2289;
	mul.f64 	%fd2291, %fd2698, 0dBFD3C6EF372FE948;
	fma.rn.f64 	%fd2292, %fd2698, 0dBFD3C6EF372FE948, %fd2247;
	add.f64 	%fd2293, %fd2290, %fd2292;
	st.global.f64 	[%r41+8], %fd2293;
	mul.f64 	%fd2294, %fd2681, 0dBFE9E3779B97F4A4;
	fma.rn.f64 	%fd2295, %fd2681, 0dBFE9E3779B97F4A4, %fd2105;
	add.f64 	%fd2296, %fd2679, %fd2295;
	fma.rn.f64 	%fd2297, %fd2253, %fd2111, %fd2109;
	add.f64 	%fd2298, %fd2296, %fd2297;
	mul.f64 	%fd2299, %fd2685, 0d3FD3C6EF372FE948;
	fma.rn.f64 	%fd2300, %fd2685, 0d3FD3C6EF372FE948, %fd2115;
	add.f64 	%fd2301, %fd2298, %fd2300;
	fma.rn.f64 	%fd2302, %fd2195, %fd2106, %fd2118;
	add.f64 	%fd2303, %fd2301, %fd2302;
	add.f64 	%fd2304, %fd2303, %fd2689;
	fma.rn.f64 	%fd2305, %fd2691, 0dBFE9E3779B97F4A4, %fd2124;
	add.f64 	%fd2306, %fd2304, %fd2305;
	mul.f64 	%fd2307, %fd2693, 0d3FD3C6EF372FE948;
	fma.rn.f64 	%fd2308, %fd2130, %fd2111, %fd2307;
	add.f64 	%fd2309, %fd2306, %fd2308;
	fma.rn.f64 	%fd2310, %fd2695, 0d3FD3C6EF372FE948, %fd2134;
	add.f64 	%fd2311, %fd2309, %fd2310;
	mul.f64 	%fd2312, %fd2697, 0dBFE9E3779B97F4A4;
	fma.rn.f64 	%fd2313, %fd2140, %fd2106, %fd2312;
	add.f64 	%fd2314, %fd2311, %fd2313;
	st.global.f64 	[%r42], %fd2314;
	mul.f64 	%fd2315, %fd2682, 0dBFE9E3779B97F4A4;
	fma.rn.f64 	%fd2316, %fd2145, %fd2106, %fd2315;
	add.f64 	%fd2317, %fd2680, %fd2316;
	fma.rn.f64 	%fd2318, %fd2684, 0d3FD3C6EF372FE948, %fd2149;
	add.f64 	%fd2319, %fd2317, %fd2318;
	mul.f64 	%fd2320, %fd2686, 0d3FD3C6EF372FE948;
	fma.rn.f64 	%fd2321, %fd2155, %fd2111, %fd2320;
	add.f64 	%fd2322, %fd2319, %fd2321;
	fma.rn.f64 	%fd2323, %fd2688, 0dBFE9E3779B97F4A4, %fd2159;
	add.f64 	%fd2324, %fd2322, %fd2323;
	add.f64 	%fd2325, %fd2324, %fd2690;
	fma.rn.f64 	%fd2326, %fd2234, %fd2106, %fd2164;
	add.f64 	%fd2327, %fd2325, %fd2326;
	mul.f64 	%fd2328, %fd2694, 0d3FD3C6EF372FE948;
	fma.rn.f64 	%fd2329, %fd2694, 0d3FD3C6EF372FE948, %fd2169;
	add.f64 	%fd2330, %fd2327, %fd2329;
	fma.rn.f64 	%fd2331, %fd2288, %fd2111, %fd2172;
	add.f64 	%fd2332, %fd2330, %fd2331;
	mul.f64 	%fd2333, %fd2698, 0dBFE9E3779B97F4A4;
	fma.rn.f64 	%fd2334, %fd2698, 0dBFE9E3779B97F4A4, %fd2177;
	add.f64 	%fd2335, %fd2332, %fd2334;
	st.global.f64 	[%r42+8], %fd2335;
	sub.f64 	%fd2336, %fd2679, %fd2681;
	add.f64 	%fd2337, %fd2336, %fd2683;
	sub.f64 	%fd2338, %fd2337, %fd2685;
	add.f64 	%fd2339, %fd2338, %fd2687;
	sub.f64 	%fd2340, %fd2339, %fd2689;
	add.f64 	%fd2341, %fd2340, %fd2691;
	sub.f64 	%fd2342, %fd2341, %fd2693;
	add.f64 	%fd2343, %fd2342, %fd2695;
	sub.f64 	%fd2344, %fd2343, %fd2697;
	st.global.f64 	[%r43], %fd2344;
	sub.f64 	%fd2345, %fd2680, %fd2682;
	add.f64 	%fd2346, %fd2345, %fd2684;
	sub.f64 	%fd2347, %fd2346, %fd2686;
	add.f64 	%fd2348, %fd2347, %fd2688;
	sub.f64 	%fd2349, %fd2348, %fd2690;
	add.f64 	%fd2350, %fd2349, %fd2692;
	sub.f64 	%fd2351, %fd2350, %fd2694;
	add.f64 	%fd2352, %fd2351, %fd2696;
	sub.f64 	%fd2353, %fd2352, %fd2698;
	st.global.f64 	[%r43+8], %fd2353;
	neg.f64 	%fd2354, %fd2682;
	fma.rn.f64 	%fd2355, %fd2354, %fd2106, %fd2294;
	add.f64 	%fd2356, %fd2679, %fd2355;
	add.f64 	%fd2357, %fd2356, %fd2112;
	fma.rn.f64 	%fd2358, %fd2190, %fd2111, %fd2299;
	add.f64 	%fd2359, %fd2357, %fd2358;
	add.f64 	%fd2360, %fd2359, %fd2120;
	add.f64 	%fd2361, %fd2360, %fd2689;
	add.f64 	%fd2362, %fd2361, %fd2126;
	fma.rn.f64 	%fd2363, %fd2693, 0d3FD3C6EF372FE948, %fd2129;
	add.f64 	%fd2364, %fd2362, %fd2363;
	add.f64 	%fd2365, %fd2364, %fd2136;
	fma.rn.f64 	%fd2366, %fd2697, 0dBFE9E3779B97F4A4, %fd2139;
	add.f64 	%fd2367, %fd2365, %fd2366;
	st.global.f64 	[%r44], %fd2367;
	fma.rn.f64 	%fd2368, %fd2682, 0dBFE9E3779B97F4A4, %fd2144;
	add.f64 	%fd2369, %fd2680, %fd2368;
	add.f64 	%fd2370, %fd2369, %fd2151;
	fma.rn.f64 	%fd2371, %fd2686, 0d3FD3C6EF372FE948, %fd2154;
	add.f64 	%fd2372, %fd2370, %fd2371;
	add.f64 	%fd2373, %fd2372, %fd2161;
	add.f64 	%fd2374, %fd2373, %fd2690;
	add.f64 	%fd2375, %fd2374, %fd2166;
	fma.rn.f64 	%fd2376, %fd2239, %fd2111, %fd2328;
	add.f64 	%fd2377, %fd2375, %fd2376;
	add.f64 	%fd2378, %fd2377, %fd2174;
	neg.f64 	%fd2379, %fd2697;
	fma.rn.f64 	%fd2380, %fd2379, %fd2106, %fd2333;
	add.f64 	%fd2381, %fd2378, %fd2380;
	st.global.f64 	[%r44+8], %fd2381;
	fma.rn.f64 	%fd2382, %fd2354, %fd2111, %fd2250;
	add.f64 	%fd2383, %fd2679, %fd2382;
	add.f64 	%fd2384, %fd2383, %fd2186;
	fma.rn.f64 	%fd2385, %fd2685, 0d3FE9E3779B97F4A4, %fd2189;
	add.f64 	%fd2386, %fd2384, %fd2385;
	add.f64 	%fd2387, %fd2386, %fd2196;
	sub.f64 	%fd2388, %fd2387, %fd2689;
	add.f64 	%fd2389, %fd2388, %fd2201;
	fma.rn.f64 	%fd2390, %fd2130, %fd2106, %fd2264;
	add.f64 	%fd2391, %fd2389, %fd2390;
	add.f64 	%fd2392, %fd2391, %fd2209;
	fma.rn.f64 	%fd2393, %fd2697, 0dBFD3C6EF372FE948, %fd2212;
	add.f64 	%fd2394, %fd2392, %fd2393;
	st.global.f64 	[%r45], %fd2394;
	fma.rn.f64 	%fd2395, %fd2682, 0dBFD3C6EF372FE948, %fd2216;
	add.f64 	%fd2396, %fd2680, %fd2395;
	add.f64 	%fd2397, %fd2396, %fd2221;
	fma.rn.f64 	%fd2398, %fd2155, %fd2106, %fd2277;
	add.f64 	%fd2399, %fd2397, %fd2398;
	add.f64 	%fd2400, %fd2399, %fd2229;
	sub.f64 	%fd2401, %fd2400, %fd2690;
	add.f64 	%fd2402, %fd2401, %fd2235;
	fma.rn.f64 	%fd2403, %fd2694, 0d3FE9E3779B97F4A4, %fd2238;
	add.f64 	%fd2404, %fd2402, %fd2403;
	add.f64 	%fd2405, %fd2404, %fd2244;
	fma.rn.f64 	%fd2406, %fd2379, %fd2111, %fd2291;
	add.f64 	%fd2407, %fd2405, %fd2406;
	st.global.f64 	[%r45+8], %fd2407;
	fma.rn.f64 	%fd2408, %fd2354, %fd2111, %fd2180;
	add.f64 	%fd2409, %fd2679, %fd2408;
	add.f64 	%fd2410, %fd2409, %fd2254;
	fma.rn.f64 	%fd2411, %fd2685, 0dBFE9E3779B97F4A4, %fd2189;
	add.f64 	%fd2412, %fd2410, %fd2411;
	add.f64 	%fd2413, %fd2412, %fd2259;
	add.f64 	%fd2414, %fd2413, %fd2689;
	add.f64 	%fd2415, %fd2414, %fd2262;
	fma.rn.f64 	%fd2416, %fd2130, %fd2106, %fd2203;
	add.f64 	%fd2417, %fd2415, %fd2416;
	add.f64 	%fd2418, %fd2417, %fd2267;
	fma.rn.f64 	%fd2419, %fd2697, 0d3FD3C6EF372FE948, %fd2212;
	add.f64 	%fd2420, %fd2418, %fd2419;
	st.global.f64 	[%r46], %fd2420;
	fma.rn.f64 	%fd2421, %fd2682, 0d3FD3C6EF372FE948, %fd2216;
	add.f64 	%fd2422, %fd2680, %fd2421;
	add.f64 	%fd2423, %fd2422, %fd2275;
	fma.rn.f64 	%fd2424, %fd2155, %fd2106, %fd2223;
	add.f64 	%fd2425, %fd2423, %fd2424;
	add.f64 	%fd2426, %fd2425, %fd2280;
	add.f64 	%fd2427, %fd2426, %fd2690;
	add.f64 	%fd2428, %fd2427, %fd2283;
	fma.rn.f64 	%fd2429, %fd2694, 0dBFE9E3779B97F4A4, %fd2238;
	add.f64 	%fd2430, %fd2428, %fd2429;
	add.f64 	%fd2431, %fd2430, %fd2289;
	fma.rn.f64 	%fd2432, %fd2379, %fd2111, %fd2246;
	add.f64 	%fd2433, %fd2431, %fd2432;
	st.global.f64 	[%r46+8], %fd2433;
	fma.rn.f64 	%fd2434, %fd2354, %fd2106, %fd2104;
	add.f64 	%fd2435, %fd2679, %fd2434;
	add.f64 	%fd2436, %fd2435, %fd2297;
	fma.rn.f64 	%fd2437, %fd2190, %fd2111, %fd2114;
	add.f64 	%fd2438, %fd2436, %fd2437;
	add.f64 	%fd2439, %fd2438, %fd2302;
	sub.f64 	%fd2440, %fd2439, %fd2689;
	add.f64 	%fd2441, %fd2440, %fd2305;
	fma.rn.f64 	%fd2442, %fd2693, 0dBFD3C6EF372FE948, %fd2129;
	add.f64 	%fd2443, %fd2441, %fd2442;
	add.f64 	%fd2444, %fd2443, %fd2310;
	fma.rn.f64 	%fd2445, %fd2697, 0d3FE9E3779B97F4A4, %fd2139;
	add.f64 	%fd2446, %fd2444, %fd2445;
	st.global.f64 	[%r47], %fd2446;
	fma.rn.f64 	%fd2447, %fd2682, 0d3FE9E3779B97F4A4, %fd2144;
	add.f64 	%fd2448, %fd2680, %fd2447;
	add.f64 	%fd2449, %fd2448, %fd2318;
	fma.rn.f64 	%fd2450, %fd2686, 0dBFD3C6EF372FE948, %fd2154;
	add.f64 	%fd2451, %fd2449, %fd2450;
	add.f64 	%fd2452, %fd2451, %fd2323;
	sub.f64 	%fd2453, %fd2452, %fd2690;
	add.f64 	%fd2454, %fd2453, %fd2326;
	fma.rn.f64 	%fd2455, %fd2239, %fd2111, %fd2168;
	add.f64 	%fd2456, %fd2454, %fd2455;
	add.f64 	%fd2457, %fd2456, %fd2331;
	fma.rn.f64 	%fd2458, %fd2379, %fd2106, %fd2176;
	add.f64 	%fd2459, %fd2457, %fd2458;
	st.global.f64 	[%r47+8], %fd2459;
	ret;
}

.entry DIT8C2C(
	.param .u32 .ptr .global .align 8 DIT8C2C_param_0,
	.param .u32 .ptr .global .align 16 DIT8C2C_param_1,
	.param .u32 DIT8C2C_param_2,
	.param .u32 DIT8C2C_param_3,
	.param .u32 DIT8C2C_param_4,
	.param .u32 DIT8C2C_param_5
)
{
	.reg .f64 	%fd<267>;
	.reg .pred 	%p<5>;
	.reg .s32 	%r<11>;


	ld.param.u32 	%r1, [DIT8C2C_param_0];
	ld.param.u32 	%r2, [DIT8C2C_param_4];
	ld.global.f64 	%fd15, [%r1];
	ld.global.f64 	%fd16, [%r1+8];
	add.f64 	%fd17, %fd15, %fd15;
	add.f64 	%fd18, %fd16, %fd16;
	sub.f64 	%fd5, %fd15, %fd15;
	sub.f64 	%fd6, %fd16, %fd16;
	setp.eq.s32 	%p1, %r2, 1;
	mov.f64 	%fd257, %fd5;
	mov.f64 	%fd258, %fd6;
	mov.f64 	%fd265, %fd5;
	mov.f64 	%fd266, %fd6;
	mov.f64 	%fd255, %fd17;
	mov.f64 	%fd256, %fd18;
	mov.f64 	%fd263, %fd17;
	mov.f64 	%fd264, %fd18;
	mov.f64 	%fd261, %fd5;
	mov.f64 	%fd262, %fd6;
	mov.f64 	%fd259, %fd17;
	mov.f64 	%fd260, %fd18;
	mov.f64 	%fd253, %fd5;
	mov.f64 	%fd254, %fd6;
	mov.f64 	%fd251, %fd17;
	mov.f64 	%fd252, %fd18;
	@%p1 bra 	BB4_3;

	ld.param.u32 	%r10, [DIT8C2C_param_4];
	setp.ne.s32 	%p2, %r10, 0;
	@%p2 bra 	BB4_4;

	sub.f64 	%fd19, %fd5, %fd6;
	mul.f64 	%fd20, %fd19, 0d3FE6A09E667F3BCD;
	add.f64 	%fd21, %fd6, %fd5;
	mul.f64 	%fd22, %fd21, 0dBFE6A09E667F3BCD;
	mul.f64 	%fd24, %fd21, 0d3FE6A09E667F3BCD;
	neg.f64 	%fd27, %fd6;
	mov.f64 	%fd251, %fd17;
	mov.f64 	%fd252, %fd18;
	mov.f64 	%fd253, %fd5;
	mov.f64 	%fd254, %fd6;
	mov.f64 	%fd255, %fd17;
	mov.f64 	%fd256, %fd18;
	mov.f64 	%fd259, %fd17;
	mov.f64 	%fd260, %fd18;
	mov.f64 	%fd257, %fd27;
	mov.f64 	%fd258, %fd5;
	mov.f64 	%fd263, %fd17;
	mov.f64 	%fd264, %fd18;
	mov.f64 	%fd261, %fd20;
	mov.f64 	%fd262, %fd24;
	mov.f64 	%fd265, %fd22;
	mov.f64 	%fd266, %fd20;
	bra.uni 	BB4_4;

BB4_3:
	add.f64 	%fd28, %fd5, %fd6;
	mul.f64 	%fd29, %fd28, 0d3FE6A09E667F3BCD;
	sub.f64 	%fd30, %fd6, %fd5;
	mul.f64 	%fd31, %fd30, 0d3FE6A09E667F3BCD;
	add.f64 	%fd34, %fd6, %fd5;
	mul.f64 	%fd35, %fd34, 0dBFE6A09E667F3BCD;
	neg.f64 	%fd36, %fd5;
	mov.f64 	%fd251, %fd17;
	mov.f64 	%fd252, %fd18;
	mov.f64 	%fd253, %fd5;
	mov.f64 	%fd254, %fd6;
	mov.f64 	%fd255, %fd17;
	mov.f64 	%fd256, %fd18;
	mov.f64 	%fd259, %fd17;
	mov.f64 	%fd260, %fd18;
	mov.f64 	%fd257, %fd6;
	mov.f64 	%fd258, %fd36;
	mov.f64 	%fd263, %fd17;
	mov.f64 	%fd264, %fd18;
	mov.f64 	%fd261, %fd29;
	mov.f64 	%fd262, %fd31;
	mov.f64 	%fd265, %fd31;
	mov.f64 	%fd266, %fd35;

BB4_4:
	add.f64 	%fd39, %fd251, %fd255;
	add.f64 	%fd42, %fd252, %fd256;
	add.f64 	%fd45, %fd253, %fd257;
	add.f64 	%fd48, %fd254, %fd258;
	sub.f64 	%fd49, %fd251, %fd255;
	sub.f64 	%fd50, %fd252, %fd256;
	sub.f64 	%fd51, %fd253, %fd257;
	sub.f64 	%fd52, %fd254, %fd258;
	add.f64 	%fd53, %fd259, %fd263;
	add.f64 	%fd54, %fd260, %fd264;
	add.f64 	%fd55, %fd261, %fd265;
	add.f64 	%fd56, %fd262, %fd266;
	ld.param.u32 	%r9, [DIT8C2C_param_4];
	setp.eq.s32 	%p3, %r9, 1;
	mov.f64 	%fd93, %fd49;
	mov.f64 	%fd94, %fd50;
	mov.f64 	%fd77, %fd39;
	mov.f64 	%fd78, %fd42;
	mov.f64 	%fd249, %fd15;
	mov.f64 	%fd250, %fd16;
	mov.f64 	%fd247, %fd51;
	mov.f64 	%fd248, %fd52;
	mov.f64 	%fd85, %fd45;
	mov.f64 	%fd86, %fd48;
	mov.f64 	%fd109, %fd53;
	mov.f64 	%fd110, %fd54;
	mov.f64 	%fd117, %fd55;
	mov.f64 	%fd118, %fd56;
	@%p3 bra 	BB4_7;

	ld.param.u32 	%r8, [DIT8C2C_param_4];
	setp.ne.s32 	%p4, %r8, 0;
	@%p4 bra 	BB4_8;

	sub.f64 	%fd57, %fd264, %fd260;
	sub.f64 	%fd58, %fd259, %fd263;
	sub.f64 	%fd59, %fd266, %fd262;
	sub.f64 	%fd60, %fd261, %fd265;
	mov.f64 	%fd247, %fd51;
	mov.f64 	%fd248, %fd52;
	mov.f64 	%fd121, %fd57;
	mov.f64 	%fd122, %fd58;
	mov.f64 	%fd249, %fd59;
	mov.f64 	%fd250, %fd60;
	bra.uni 	BB4_8;

BB4_7:
	sub.f64 	%fd61, %fd260, %fd264;
	sub.f64 	%fd62, %fd263, %fd259;
	sub.f64 	%fd63, %fd262, %fd266;
	sub.f64 	%fd64, %fd265, %fd261;
	mov.f64 	%fd247, %fd51;
	mov.f64 	%fd248, %fd52;
	mov.f64 	%fd123, %fd61;
	mov.f64 	%fd124, %fd62;
	mov.f64 	%fd249, %fd63;
	mov.f64 	%fd250, %fd64;

BB4_8:
	sub.f64 	%fd67, %fd247, %fd249;
	ld.param.u32 	%r7, [DIT8C2C_param_0];
	st.global.f64 	[%r7], %fd67;
	sub.f64 	%fd70, %fd248, %fd250;
	st.global.f64 	[%r7+8], %fd70;
	ret;
}

.entry DIT2C2CM(
	.param .u32 .ptr .global .align 8 DIT2C2CM_param_0,
	.param .u32 DIT2C2CM_param_1,
	.param .u32 DIT2C2CM_param_2,
	.param .u32 DIT2C2CM_param_3,
	.param .u32 DIT2C2CM_param_4,
	.param .u32 DIT2C2CM_param_5
)
{
	.local .align 8 .b8 	__local_depot5[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<200>;
	.reg .pred 	%p<76>;
	.reg .s32 	%r<240>;
	.reg .s64 	%rl<167>;


	mov.u32 	%SP, __local_depot5;
	ld.param.u32 	%r82, [DIT2C2CM_param_3];
	// inline asm
	mov.u32 	%r74, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r75, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r76, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r77, %tid.x;
	// inline asm
	add.s32 	%r83, %r77, %r74;
	mad.lo.s32 	%r5, %r76, %r75, %r83;
	// inline asm
	mov.u32 	%r78, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r79, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r80, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r81, %tid.y;
	// inline asm
	add.s32 	%r84, %r81, %r78;
	mad.lo.s32 	%r6, %r80, %r79, %r84;
	mul.hi.s32 	%r85, %r82, 780903145;
	shr.u32 	%r86, %r85, 31;
	shr.s32 	%r87, %r85, 1;
	add.s32 	%r7, %r87, %r86;
	mul.lo.s32 	%r88, %r7, 11;
	sub.s32 	%r8, %r82, %r88;
	setp.gt.s32 	%p9, %r82, 10;
	@%p9 bra 	BB5_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB5_23;

BB5_2:
	mov.f32 	%f1, 0f41300000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40000000;
	add.f32 	%f2, %f47, 0f41300000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r227, 1;
	mov.u32 	%r226, 0;

BB5_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p10, %p3, %p3;
	@%p10 bra 	BB5_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mul.rn.f32 	%f56, %f47, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p11, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p12, %f1, %f52;
	and.pred  	%p4, %p11, %p12;
	setp.eq.f32 	%p13, %f48, 0f00000000;
	@%p13 bra 	BB5_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r11, %f58;
	shr.u32 	%r91, %r11, 23;
	and.b32  	%r92, %r91, 255;
	add.s32 	%r228, %r92, -127;
	setp.eq.s32 	%p14, %r92, 0;
	mov.f32 	%f280, %f58;
	@%p14 bra 	BB5_6;
	bra.uni 	BB5_7;

BB5_6:
	and.b32  	%r93, %r11, -2139095041;
	or.b32  	%r94, %r93, 1065353216;
	mov.b32 	 %f60, %r94;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r95, %f61;
	shr.u32 	%r96, %r95, 23;
	and.b32  	%r97, %r96, 255;
	add.s32 	%r228, %r97, -253;
	and.b32  	%r98, %r95, -2139095041;
	or.b32  	%r99, %r98, 1065353216;
	mov.b32 	 %f280, %r99;

BB5_7:
	mov.b32 	 %r100, %f280;
	and.b32  	%r101, %r100, -2139095041;
	or.b32  	%r102, %r101, 1065353216;
	mov.b32 	 %f281, %r102;
	setp.gt.f32 	%p15, %f281, 0f3FB504F3;
	@%p15 bra 	BB5_8;
	bra.uni 	BB5_9;

BB5_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r228, %r228, 1;

BB5_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f47, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r103, %f73;
	and.b32  	%r104, %r103, -4096;
	mov.b32 	 %f81, %r104;
	mov.b32 	 %r105, %f64;
	and.b32  	%r106, %r105, -4096;
	mov.b32 	 %f82, %r106;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f47, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r228;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p16, %f69, 0f77F684DF;
	@%p16 bra 	BB5_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB5_12;

BB5_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB5_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r17, %f131;
	setp.eq.s32 	%p17, %r17, 1118925336;
	@%p17 bra 	BB5_13;
	bra.uni 	BB5_14;

BB5_13:
	add.s32 	%r107, %r17, -1;
	mov.b32 	 %f133, %r107;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB5_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p18, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p18;
	setp.gt.f32 	%p19, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p19;
	setp.neu.f32 	%p20, %f19, %f3;
	@%p20 bra 	BB5_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB5_21;

BB5_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB5_21;

BB5_17:
	@%p3 bra 	BB5_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB5_21;

BB5_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB5_21;

BB5_20:
	mov.f32 	%f283, %f7;

BB5_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r227;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r227, %f157;
	add.s32 	%r226, %r226, 1;
	setp.lt.s32 	%p21, %r226, %r7;
	@%p21 bra 	BB5_3;

	cvt.rn.f32.s32 	%f284, %r227;

BB5_23:
	mov.f32 	%f159, 0f40000000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r8;
	setp.eq.f32 	%p22, %f287, 0f00000000;
	@%p22 bra 	BB5_45;

	setp.nan.f32 	%p23, %f287, %f287;
	@%p23 bra 	BB5_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p24, %f287, 0f7F800000;
	setp.eq.f32 	%p25, %f287, 0fFF800000;
	or.pred  	%p26, %p24, %p25;
	@%p26 bra 	BB5_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p29, %f158, 0f00000000;
	@%p29 bra 	BB5_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r20, %f168;
	shr.u32 	%r108, %r20, 23;
	and.b32  	%r109, %r108, 255;
	add.s32 	%r229, %r109, -127;
	setp.eq.s32 	%p30, %r109, 0;
	mov.f32 	%f285, %f168;
	@%p30 bra 	BB5_28;
	bra.uni 	BB5_29;

BB5_28:
	and.b32  	%r110, %r20, -2139095041;
	or.b32  	%r111, %r110, 1065353216;
	mov.b32 	 %f170, %r111;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r112, %f171;
	shr.u32 	%r113, %r112, 23;
	and.b32  	%r114, %r113, 255;
	add.s32 	%r229, %r114, -253;
	and.b32  	%r115, %r112, -2139095041;
	or.b32  	%r116, %r115, 1065353216;
	mov.b32 	 %f285, %r116;

BB5_29:
	mov.b32 	 %r117, %f285;
	and.b32  	%r118, %r117, -2139095041;
	or.b32  	%r119, %r118, 1065353216;
	mov.b32 	 %f286, %r119;
	setp.gt.f32 	%p31, %f286, 0f3FB504F3;
	@%p31 bra 	BB5_30;
	bra.uni 	BB5_31;

BB5_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r229, %r229, 1;

BB5_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mul.rn.f32 	%f183, %f159, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r120, %f183;
	and.b32  	%r121, %r120, -4096;
	mov.b32 	 %f191, %r121;
	mov.b32 	 %r122, %f174;
	and.b32  	%r123, %r122, -4096;
	mov.b32 	 %f192, %r123;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f159, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r229;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p32, %f179, 0f77F684DF;
	@%p32 bra 	BB5_32;
	bra.uni 	BB5_33;

BB5_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB5_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r26, %f241;
	setp.eq.s32 	%p33, %r26, 1118925336;
	@%p33 bra 	BB5_34;
	bra.uni 	BB5_35;

BB5_34:
	add.s32 	%r124, %r26, -1;
	mov.b32 	 %f243, %r124;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB5_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p34, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p34;
	setp.gt.f32 	%p35, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p35;
	setp.neu.f32 	%p36, %f39, %f27;
	@%p36 bra 	BB5_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB5_46;

BB5_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB5_46;

BB5_38:
	setp.lt.f32 	%p37, %f287, 0f00000000;
	@%p37 bra 	BB5_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB5_46;

BB5_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB5_46;

BB5_41:
	setp.lt.f32 	%p38, %f158, 0f3F800000;
	mov.b32 	 %r125, %f287;
	setp.lt.s32 	%p6, %r125, 0;
	@%p38 bra 	BB5_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB5_46;

BB5_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB5_46;

BB5_44:
	add.f32 	%f288, %f287, 0f40000000;
	bra.uni 	BB5_46;

BB5_45:
	mov.f32 	%f288, 0f3F800000;

BB5_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r27, %f267;
	shr.u32 	%r126, %r27, 31;
	add.s32 	%r127, %r27, %r126;
	shr.s32 	%r28, %r127, 1;
	ld.param.u32 	%r225, [DIT2C2CM_param_5];
	setp.eq.s32 	%p39, %r225, 0;
	@%p39 bra 	BB5_49;

	ld.param.u32 	%r224, [DIT2C2CM_param_5];
	setp.ne.s32 	%p40, %r224, 1;
	@%p40 bra 	BB5_50;

	div.s32 	%r230, %r6, %r28;
	rem.s32 	%r231, %r6, %r28;
	ld.param.u32 	%r233, [DIT2C2CM_param_1];
	mov.u32 	%r232, %r5;
	bra.uni 	BB5_51;

BB5_49:
	ld.param.u32 	%r222, [DIT2C2CM_param_1];
	mul.lo.s32 	%r31, %r6, %r222;
	div.s32 	%r230, %r5, %r28;
	rem.s32 	%r231, %r5, %r28;
	mov.u32 	%r233, 1;
	mov.u32 	%r232, %r31;
	bra.uni 	BB5_51;

BB5_50:
	mov.u32 	%r233, 1;
	mov.u32 	%r232, 0;
	mov.u32 	%r231, %r133;
	mov.u32 	%r230, %r134;

BB5_51:
	mad.lo.s32 	%r135, %r230, %r27, %r231;
	mad.lo.s32 	%r136, %r233, %r135, %r232;
	shl.b32 	%r38, %r136, 1;
	mad.lo.s32 	%r137, %r230, %r27, %r28;
	add.s32 	%r138, %r137, %r231;
	mad.lo.s32 	%r139, %r233, %r138, %r232;
	shl.b32 	%r39, %r139, 1;
	cvt.rn.f64.s32 	%fd22, %r231;
	mul.f64 	%fd23, %fd22, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd24, %r27;
	div.rn.f64 	%fd1, %fd23, %fd24;
	setp.eq.f64 	%p7, %fd1, 0d7FF0000000000000;
	setp.eq.f64 	%p8, %fd1, 0dFFF0000000000000;
	or.pred  	%p41, %p7, %p8;
	add.u32 	%r40, %SP, 0;
	@%p41 bra 	BB5_74;

	// inline asm
	abs.f64 	%fd25, %fd1;
	// inline asm
	setp.gt.f64 	%p42, %fd25, 0d41E0000000000000;
	@%p42 bra 	BB5_54;

	mov.f64 	%fd40, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd27, %fd1, %fd40;
	// inline asm
	cvt.rni.s32.f64 	%r140, %fd27;
	// inline asm
	cvt.rn.f64.s32 	%fd41, %r140;
	neg.f64 	%fd37, %fd41;
	mov.f64 	%fd30, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd28, %fd37, %fd30, %fd1;
	// inline asm
	mov.f64 	%fd34, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd32, %fd37, %fd34, %fd28;
	// inline asm
	mov.f64 	%fd38, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd36, %fd37, %fd38, %fd32;
	// inline asm
	mov.u32 	%r236, %r140;
	mov.f64 	%fd192, %fd36;
	bra.uni 	BB5_70;

BB5_54:
	mov.b64 	 %rl1, %fd1;
	and.b64  	%rl150, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl65, %rl3, 2047;
	add.s64 	%rl66, %rl65, 4294966272;
	cvt.u32.u64 	%r42, %rl66;
	shl.b64 	%rl67, %rl1, 11;
	or.b64  	%rl4, %rl67, -9223372036854775808;
	shr.u32 	%r144, %r42, 6;
	mov.u32 	%r145, 16;
	sub.s32 	%r43, %r145, %r144;
	mov.u32 	%r146, 15;
	sub.s32 	%r234, %r146, %r144;
	mov.u32 	%r147, 19;
	sub.s32 	%r45, %r147, %r144;
	mov.u32 	%r142, 18;
	// inline asm
	min.s32 	%r141, %r142, %r45;
	// inline asm
	setp.lt.s32 	%p43, %r234, %r141;
	@%p43 bra 	BB5_56;

	mov.u64 	%rl147, 0;
	bra.uni 	BB5_58;

BB5_56:
	mov.u32 	%r148, 1;
	sub.s32 	%r46, %r148, %r43;
	mov.u64 	%rl147, 0;

BB5_57:
	.pragma "nounroll";
	shl.b32 	%r152, %r234, 3;
	mov.u32 	%r153, __internal_i2opi_d;
	add.s32 	%r154, %r153, %r152;
	ld.const.u64 	%rl71, [%r154];
	mul.lo.s64 	%rl73, %rl71, %rl4;
	// inline asm
	mul.hi.u64 	%rl70, %rl71, %rl4;
	// inline asm
	mad.lo.s64 	%rl74, %rl71, %rl4, %rl147;
	setp.lt.u64 	%p44, %rl74, %rl73;
	selp.u64 	%rl75, 1, 0, %p44;
	add.s64 	%rl147, %rl75, %rl70;
	add.s32 	%r155, %r46, %r234;
	shl.b32 	%r156, %r155, 3;
	add.s32 	%r158, %r40, %r156;
	st.local.u64 	[%r158], %rl74;
	// inline asm
	min.s32 	%r149, %r142, %r45;
	// inline asm
	add.s32 	%r234, %r234, 1;
	setp.lt.s32 	%p45, %r234, %r149;
	@%p45 bra 	BB5_57;

BB5_58:
	mov.u32 	%r159, 1;
	sub.s32 	%r160, %r159, %r43;
	add.s32 	%r161, %r160, %r234;
	shl.b32 	%r162, %r161, 3;
	add.s32 	%r164, %r40, %r162;
	st.local.u64 	[%r164], %rl147;
	ld.local.u64 	%rl148, [%r40+24];
	ld.local.u64 	%rl149, [%r40+16];
	and.b32  	%r165, %r42, 63;
	setp.eq.s32 	%p46, %r165, 0;
	@%p46 bra 	BB5_60;

	and.b64  	%rl76, %rl3, 63;
	cvt.u32.u64 	%r166, %rl76;
	shl.b64 	%rl77, %rl148, %r166;
	neg.s32 	%r167, %r42;
	and.b32  	%r168, %r167, 63;
	shr.u64 	%rl78, %rl149, %r168;
	or.b64  	%rl148, %rl78, %rl77;
	shl.b64 	%rl79, %rl149, %r166;
	ld.local.u64 	%rl80, [%r40+8];
	shr.u64 	%rl81, %rl80, %r168;
	or.b64  	%rl149, %rl81, %rl79;

BB5_60:
	shr.u64 	%rl82, %rl148, 62;
	cvt.u32.u64 	%r169, %rl82;
	shr.u64 	%rl83, %rl149, 62;
	shl.b64 	%rl84, %rl148, 2;
	or.b64  	%rl154, %rl83, %rl84;
	shl.b64 	%rl15, %rl149, 2;
	setp.ne.s64 	%p47, %rl15, 0;
	selp.u64 	%rl85, 1, 0, %p47;
	or.b64  	%rl86, %rl85, %rl154;
	setp.gt.u64 	%p48, %rl86, -9223372036854775808;
	selp.u32 	%r170, 1, 0, %p48;
	add.s32 	%r171, %r170, %r169;
	neg.s32 	%r172, %r171;
	setp.lt.s64 	%p49, %rl1, 0;
	selp.b32 	%r236, %r172, %r171, %p49;
	@%p48 bra 	BB5_62;

	mov.u64 	%rl153, %rl15;
	bra.uni 	BB5_63;

BB5_62:
	not.b64 	%rl87, %rl154;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p50, %rl15, 0;
	selp.u64 	%rl88, 1, 0, %p50;
	add.s64 	%rl154, %rl88, %rl87;
	xor.b64  	%rl150, %rl150, -9223372036854775808;
	mov.u64 	%rl153, %rl16;

BB5_63:
	mov.u64 	%rl152, %rl153;
	setp.gt.s64 	%p51, %rl154, 0;
	@%p51 bra 	BB5_65;

	mov.u32 	%r235, 0;
	bra.uni 	BB5_67;

BB5_65:
	mov.u32 	%r235, 0;

BB5_66:
	shr.u64 	%rl89, %rl152, 63;
	shl.b64 	%rl90, %rl154, 1;
	or.b64  	%rl154, %rl89, %rl90;
	shl.b64 	%rl152, %rl152, 1;
	add.s32 	%r235, %r235, -1;
	setp.gt.s64 	%p52, %rl154, 0;
	@%p52 bra 	BB5_66;

BB5_67:
	mul.lo.s64 	%rl156, %rl154, -3958705157555305931;
	mov.u64 	%rl93, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl91, %rl154, %rl93;
	// inline asm
	setp.gt.s64 	%p53, %rl91, 0;
	mov.u64 	%rl155, %rl91;
	@%p53 bra 	BB5_68;
	bra.uni 	BB5_69;

BB5_68:
	shl.b64 	%rl94, %rl91, 1;
	shr.u64 	%rl95, %rl156, 63;
	or.b64  	%rl155, %rl94, %rl95;
	mul.lo.s64 	%rl156, %rl154, -7917410315110611862;
	add.s32 	%r235, %r235, -1;

BB5_69:
	setp.ne.s64 	%p54, %rl156, 0;
	selp.u64 	%rl96, 1, 0, %p54;
	add.s64 	%rl97, %rl96, %rl155;
	add.s32 	%r175, %r235, 1022;
	cvt.u64.u32 	%rl98, %r175;
	shl.b64 	%rl99, %rl98, 52;
	shr.u64 	%rl100, %rl97, 11;
	shr.u64 	%rl101, %rl97, 10;
	and.b64  	%rl102, %rl101, 1;
	add.s64 	%rl103, %rl99, %rl100;
	add.s64 	%rl104, %rl103, %rl102;
	or.b64  	%rl105, %rl104, %rl150;
	mov.b64 	 %fd192, %rl105;

BB5_70:
	add.s32 	%r57, %r236, 1;
	and.b32  	%r176, %r57, 1;
	setp.eq.s32 	%p55, %r176, 0;
	mul.rn.f64 	%fd5, %fd192, %fd192;
	@%p55 bra 	BB5_72;

	mov.f64 	%fd43, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd45, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd42, %fd43, %fd5, %fd45;
	// inline asm
	mov.f64 	%fd49, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd46, %fd42, %fd5, %fd49;
	// inline asm
	mov.f64 	%fd53, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd50, %fd46, %fd5, %fd53;
	// inline asm
	mov.f64 	%fd57, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd54, %fd50, %fd5, %fd57;
	// inline asm
	mov.f64 	%fd61, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd58, %fd54, %fd5, %fd61;
	// inline asm
	mov.f64 	%fd65, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd62, %fd58, %fd5, %fd65;
	// inline asm
	mov.f64 	%fd69, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd66, %fd62, %fd5, %fd69;
	// inline asm
	mov.f64 	%fd193, %fd66;
	bra.uni 	BB5_73;

BB5_72:
	mov.f64 	%fd71, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd73, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd70, %fd71, %fd5, %fd73;
	// inline asm
	mov.f64 	%fd77, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd74, %fd70, %fd5, %fd77;
	// inline asm
	mov.f64 	%fd81, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd78, %fd74, %fd5, %fd81;
	// inline asm
	mov.f64 	%fd85, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd82, %fd78, %fd5, %fd85;
	// inline asm
	mov.f64 	%fd89, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd86, %fd82, %fd5, %fd89;
	// inline asm
	mul.rn.f64 	%fd91, %fd86, %fd5;
	// inline asm
	fma.rn.f64 	%fd90, %fd91, %fd192, %fd192;
	// inline asm
	mov.f64 	%fd193, %fd90;

BB5_73:
	and.b32  	%r177, %r57, 2;
	setp.eq.s32 	%p56, %r177, 0;
	neg.f64 	%fd94, %fd193;
	selp.f64 	%fd194, %fd193, %fd94, %p56;
	bra.uni 	BB5_75;

BB5_74:
	mov.f64 	%fd194, 0dFFF8000000000000;

BB5_75:
	setp.eq.f64 	%p57, %fd1, 0d0000000000000000;
	or.pred  	%p58, %p8, %p57;
	or.pred  	%p59, %p7, %p58;
	@%p59 bra 	BB5_98;

	// inline asm
	abs.f64 	%fd96, %fd1;
	// inline asm
	setp.gt.f64 	%p60, %fd96, 0d41E0000000000000;
	@%p60 bra 	BB5_78;

	mov.f64 	%fd111, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd98, %fd1, %fd111;
	// inline asm
	cvt.rni.s32.f64 	%r178, %fd98;
	// inline asm
	cvt.rn.f64.s32 	%fd112, %r178;
	neg.f64 	%fd108, %fd112;
	mov.f64 	%fd101, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd99, %fd108, %fd101, %fd1;
	// inline asm
	mov.f64 	%fd105, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd103, %fd108, %fd105, %fd99;
	// inline asm
	mov.f64 	%fd109, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd107, %fd108, %fd109, %fd103;
	// inline asm
	mov.u32 	%r239, %r178;
	mov.f64 	%fd195, %fd107;
	bra.uni 	BB5_94;

BB5_78:
	mov.b64 	 %rl33, %fd1;
	and.b64  	%rl160, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl106, %rl35, 2047;
	add.s64 	%rl107, %rl106, 4294966272;
	cvt.u32.u64 	%r59, %rl107;
	shl.b64 	%rl108, %rl33, 11;
	or.b64  	%rl36, %rl108, -9223372036854775808;
	shr.u32 	%r182, %r59, 6;
	mov.u32 	%r183, 16;
	sub.s32 	%r60, %r183, %r182;
	mov.u32 	%r184, 15;
	sub.s32 	%r237, %r184, %r182;
	mov.u32 	%r185, 19;
	sub.s32 	%r62, %r185, %r182;
	mov.u32 	%r180, 18;
	// inline asm
	min.s32 	%r179, %r180, %r62;
	// inline asm
	setp.lt.s32 	%p61, %r237, %r179;
	@%p61 bra 	BB5_80;

	mov.u64 	%rl157, 0;
	bra.uni 	BB5_82;

BB5_80:
	mov.u32 	%r186, 1;
	sub.s32 	%r63, %r186, %r60;
	mov.u64 	%rl157, 0;

BB5_81:
	.pragma "nounroll";
	shl.b32 	%r190, %r237, 3;
	mov.u32 	%r191, __internal_i2opi_d;
	add.s32 	%r192, %r191, %r190;
	ld.const.u64 	%rl112, [%r192];
	mul.lo.s64 	%rl114, %rl112, %rl36;
	// inline asm
	mul.hi.u64 	%rl111, %rl112, %rl36;
	// inline asm
	mad.lo.s64 	%rl115, %rl112, %rl36, %rl157;
	setp.lt.u64 	%p62, %rl115, %rl114;
	selp.u64 	%rl116, 1, 0, %p62;
	add.s64 	%rl157, %rl116, %rl111;
	add.s32 	%r193, %r63, %r237;
	shl.b32 	%r194, %r193, 3;
	add.s32 	%r196, %r40, %r194;
	st.local.u64 	[%r196], %rl115;
	// inline asm
	min.s32 	%r187, %r180, %r62;
	// inline asm
	add.s32 	%r237, %r237, 1;
	setp.lt.s32 	%p63, %r237, %r187;
	@%p63 bra 	BB5_81;

BB5_82:
	mov.u32 	%r197, 1;
	sub.s32 	%r198, %r197, %r60;
	add.s32 	%r199, %r198, %r237;
	shl.b32 	%r200, %r199, 3;
	add.s32 	%r202, %r40, %r200;
	st.local.u64 	[%r202], %rl157;
	ld.local.u64 	%rl158, [%r40+24];
	ld.local.u64 	%rl159, [%r40+16];
	and.b32  	%r203, %r59, 63;
	setp.eq.s32 	%p64, %r203, 0;
	@%p64 bra 	BB5_84;

	and.b64  	%rl117, %rl35, 63;
	cvt.u32.u64 	%r204, %rl117;
	shl.b64 	%rl118, %rl158, %r204;
	neg.s32 	%r205, %r59;
	and.b32  	%r206, %r205, 63;
	shr.u64 	%rl119, %rl159, %r206;
	or.b64  	%rl158, %rl119, %rl118;
	shl.b64 	%rl120, %rl159, %r204;
	ld.local.u64 	%rl121, [%r40+8];
	shr.u64 	%rl122, %rl121, %r206;
	or.b64  	%rl159, %rl122, %rl120;

BB5_84:
	shr.u64 	%rl123, %rl158, 62;
	cvt.u32.u64 	%r207, %rl123;
	shr.u64 	%rl124, %rl159, 62;
	shl.b64 	%rl125, %rl158, 2;
	or.b64  	%rl164, %rl124, %rl125;
	shl.b64 	%rl47, %rl159, 2;
	setp.ne.s64 	%p65, %rl47, 0;
	selp.u64 	%rl126, 1, 0, %p65;
	or.b64  	%rl127, %rl126, %rl164;
	setp.gt.u64 	%p66, %rl127, -9223372036854775808;
	selp.u32 	%r208, 1, 0, %p66;
	add.s32 	%r209, %r208, %r207;
	neg.s32 	%r210, %r209;
	setp.lt.s64 	%p67, %rl33, 0;
	selp.b32 	%r239, %r210, %r209, %p67;
	@%p66 bra 	BB5_86;

	mov.u64 	%rl163, %rl47;
	bra.uni 	BB5_87;

BB5_86:
	not.b64 	%rl128, %rl164;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p68, %rl47, 0;
	selp.u64 	%rl129, 1, 0, %p68;
	add.s64 	%rl164, %rl129, %rl128;
	xor.b64  	%rl160, %rl160, -9223372036854775808;
	mov.u64 	%rl163, %rl48;

BB5_87:
	mov.u64 	%rl162, %rl163;
	setp.gt.s64 	%p69, %rl164, 0;
	@%p69 bra 	BB5_89;

	mov.u32 	%r238, 0;
	bra.uni 	BB5_91;

BB5_89:
	mov.u32 	%r238, 0;

BB5_90:
	shr.u64 	%rl130, %rl162, 63;
	shl.b64 	%rl131, %rl164, 1;
	or.b64  	%rl164, %rl130, %rl131;
	shl.b64 	%rl162, %rl162, 1;
	add.s32 	%r238, %r238, -1;
	setp.gt.s64 	%p70, %rl164, 0;
	@%p70 bra 	BB5_90;

BB5_91:
	mul.lo.s64 	%rl166, %rl164, -3958705157555305931;
	mov.u64 	%rl134, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl132, %rl164, %rl134;
	// inline asm
	setp.gt.s64 	%p71, %rl132, 0;
	mov.u64 	%rl165, %rl132;
	@%p71 bra 	BB5_92;
	bra.uni 	BB5_93;

BB5_92:
	shl.b64 	%rl135, %rl132, 1;
	shr.u64 	%rl136, %rl166, 63;
	or.b64  	%rl165, %rl135, %rl136;
	mul.lo.s64 	%rl166, %rl164, -7917410315110611862;
	add.s32 	%r238, %r238, -1;

BB5_93:
	setp.ne.s64 	%p72, %rl166, 0;
	selp.u64 	%rl137, 1, 0, %p72;
	add.s64 	%rl138, %rl137, %rl165;
	add.s32 	%r213, %r238, 1022;
	cvt.u64.u32 	%rl139, %r213;
	shl.b64 	%rl140, %rl139, 52;
	shr.u64 	%rl141, %rl138, 11;
	shr.u64 	%rl142, %rl138, 10;
	and.b64  	%rl143, %rl142, 1;
	add.s64 	%rl144, %rl140, %rl141;
	add.s64 	%rl145, %rl144, %rl143;
	or.b64  	%rl146, %rl145, %rl160;
	mov.b64 	 %fd195, %rl146;

BB5_94:
	and.b32  	%r214, %r239, 1;
	setp.eq.s32 	%p73, %r214, 0;
	mul.rn.f64 	%fd15, %fd195, %fd195;
	@%p73 bra 	BB5_96;

	mov.f64 	%fd114, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd116, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd113, %fd114, %fd15, %fd116;
	// inline asm
	mov.f64 	%fd120, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd117, %fd113, %fd15, %fd120;
	// inline asm
	mov.f64 	%fd124, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd121, %fd117, %fd15, %fd124;
	// inline asm
	mov.f64 	%fd128, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd125, %fd121, %fd15, %fd128;
	// inline asm
	mov.f64 	%fd132, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd129, %fd125, %fd15, %fd132;
	// inline asm
	mov.f64 	%fd136, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd133, %fd129, %fd15, %fd136;
	// inline asm
	mov.f64 	%fd140, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd137, %fd133, %fd15, %fd140;
	// inline asm
	mov.f64 	%fd196, %fd137;
	bra.uni 	BB5_97;

BB5_96:
	mov.f64 	%fd142, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd144, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd141, %fd142, %fd15, %fd144;
	// inline asm
	mov.f64 	%fd148, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd145, %fd141, %fd15, %fd148;
	// inline asm
	mov.f64 	%fd152, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd149, %fd145, %fd15, %fd152;
	// inline asm
	mov.f64 	%fd156, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd153, %fd149, %fd15, %fd156;
	// inline asm
	mov.f64 	%fd160, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd157, %fd153, %fd15, %fd160;
	// inline asm
	mul.rn.f64 	%fd162, %fd157, %fd15;
	// inline asm
	fma.rn.f64 	%fd161, %fd162, %fd195, %fd195;
	// inline asm
	mov.f64 	%fd196, %fd161;

BB5_97:
	and.b32  	%r215, %r239, 2;
	setp.eq.s32 	%p74, %r215, 0;
	neg.f64 	%fd165, %fd196;
	selp.f64 	%fd197, %fd196, %fd165, %p74;
	bra.uni 	BB5_99;

BB5_98:
	mov.f64 	%fd166, 0d0000000000000000;
	mul.rn.f64 	%fd197, %fd1, %fd166;

BB5_99:
	mov.f64 	%fd198, %fd194;
	mov.f64 	%fd199, %fd197;
	ld.param.u32 	%r223, [DIT2C2CM_param_4];
	setp.eq.s32 	%p75, %r223, 0;
	@%p75 bra 	BB5_100;
	bra.uni 	BB5_101;

BB5_100:
	neg.f64 	%fd167, %fd197;
	mov.f64 	%fd198, %fd194;
	mov.f64 	%fd199, %fd167;

BB5_101:
	shl.b32 	%r216, %r38, 3;
	ld.param.u32 	%r220, [DIT2C2CM_param_0];
	add.s32 	%r217, %r220, %r216;
	shl.b32 	%r218, %r39, 3;
	add.s32 	%r219, %r220, %r218;
	ld.global.f64 	%fd169, [%r219];
	ld.global.f64 	%fd170, [%r217];
	fma.rn.f64 	%fd171, %fd169, %fd198, %fd170;
	ld.global.f64 	%fd173, [%r219+8];
	neg.f64 	%fd174, %fd173;
	fma.rn.f64 	%fd175, %fd174, %fd199, %fd171;
	ld.global.f64 	%fd176, [%r217+8];
	fma.rn.f64 	%fd177, %fd173, %fd198, %fd176;
	fma.rn.f64 	%fd178, %fd169, %fd199, %fd177;
	neg.f64 	%fd179, %fd169;
	fma.rn.f64 	%fd180, %fd179, %fd198, %fd170;
	fma.rn.f64 	%fd181, %fd173, %fd199, %fd180;
	fma.rn.f64 	%fd182, %fd174, %fd198, %fd176;
	fma.rn.f64 	%fd183, %fd179, %fd199, %fd182;
	st.global.f64 	[%r217], %fd175;
	st.global.f64 	[%r217+8], %fd178;
	st.global.f64 	[%r219], %fd181;
	st.global.f64 	[%r219+8], %fd183;
	ret;
}

.entry DIT7C2C(
	.param .u32 .ptr .global .align 8 DIT7C2C_param_0,
	.param .u32 DIT7C2C_param_1,
	.param .u32 DIT7C2C_param_2,
	.param .u32 DIT7C2C_param_3
)
{
	.local .align 8 .b8 	__local_depot6[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<1480>;
	.reg .pred 	%p<258>;
	.reg .s32 	%r<786>;
	.reg .s64 	%rl<1006>;


	mov.u32 	%SP, __local_depot6;
	ld.param.u32 	%r236, [DIT7C2C_param_2];
	// inline asm
	mov.u32 	%r232, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r233, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r234, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r235, %tid.x;
	// inline asm
	add.s32 	%r237, %r235, %r232;
	mad.lo.s32 	%r3, %r234, %r233, %r237;
	mul.hi.s32 	%r238, %r236, 780903145;
	shr.u32 	%r239, %r238, 31;
	shr.s32 	%r240, %r238, 1;
	add.s32 	%r4, %r240, %r239;
	mul.lo.s32 	%r241, %r4, 11;
	sub.s32 	%r5, %r236, %r241;
	setp.gt.s32 	%p17, %r236, 10;
	@%p17 bra 	BB6_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB6_23;

BB6_2:
	mov.f32 	%f1, 0f41300000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40E00000;
	add.f32 	%f2, %f47, 0f41300000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r747, 0;
	mov.u32 	%r746, 1;

BB6_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p18, %p3, %p3;
	@%p18 bra 	BB6_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mov.f32 	%f55, 0f40000000;
	mul.rn.f32 	%f56, %f55, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p19, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p20, %f1, %f52;
	and.pred  	%p4, %p19, %p20;
	setp.eq.f32 	%p21, %f48, 0f00000000;
	@%p21 bra 	BB6_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r8, %f58;
	shr.u32 	%r244, %r8, 23;
	and.b32  	%r245, %r244, 255;
	add.s32 	%r748, %r245, -127;
	setp.eq.s32 	%p22, %r245, 0;
	mov.f32 	%f280, %f58;
	@%p22 bra 	BB6_6;
	bra.uni 	BB6_7;

BB6_6:
	and.b32  	%r246, %r8, -2139095041;
	or.b32  	%r247, %r246, 1065353216;
	mov.b32 	 %f60, %r247;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r248, %f61;
	shr.u32 	%r249, %r248, 23;
	and.b32  	%r250, %r249, 255;
	add.s32 	%r748, %r250, -253;
	and.b32  	%r251, %r248, -2139095041;
	or.b32  	%r252, %r251, 1065353216;
	mov.b32 	 %f280, %r252;

BB6_7:
	mov.b32 	 %r253, %f280;
	and.b32  	%r254, %r253, -2139095041;
	or.b32  	%r255, %r254, 1065353216;
	mov.b32 	 %f281, %r255;
	setp.gt.f32 	%p23, %f281, 0f3FB504F3;
	@%p23 bra 	BB6_8;
	bra.uni 	BB6_9;

BB6_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r748, %r748, 1;

BB6_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f55, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r256, %f73;
	and.b32  	%r257, %r256, -4096;
	mov.b32 	 %f81, %r257;
	mov.b32 	 %r258, %f64;
	and.b32  	%r259, %r258, -4096;
	mov.b32 	 %f82, %r259;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f55, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r748;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p24, %f69, 0f77F684DF;
	@%p24 bra 	BB6_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB6_12;

BB6_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB6_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r14, %f131;
	setp.eq.s32 	%p25, %r14, 1118925336;
	@%p25 bra 	BB6_13;
	bra.uni 	BB6_14;

BB6_13:
	add.s32 	%r260, %r14, -1;
	mov.b32 	 %f133, %r260;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB6_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p26, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p26;
	setp.gt.f32 	%p27, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p27;
	setp.neu.f32 	%p28, %f19, %f3;
	@%p28 bra 	BB6_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB6_21;

BB6_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB6_21;

BB6_17:
	@%p3 bra 	BB6_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB6_21;

BB6_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB6_21;

BB6_20:
	mov.f32 	%f283, %f7;

BB6_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r746;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r746, %f157;
	add.s32 	%r747, %r747, 1;
	setp.lt.s32 	%p29, %r747, %r4;
	@%p29 bra 	BB6_3;

	cvt.rn.f32.s32 	%f284, %r746;

BB6_23:
	mov.f32 	%f159, 0f40E00000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r5;
	setp.eq.f32 	%p30, %f287, 0f00000000;
	@%p30 bra 	BB6_45;

	setp.nan.f32 	%p31, %f287, %f287;
	@%p31 bra 	BB6_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p32, %f287, 0f7F800000;
	setp.eq.f32 	%p33, %f287, 0fFF800000;
	or.pred  	%p34, %p32, %p33;
	@%p34 bra 	BB6_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p37, %f158, 0f00000000;
	@%p37 bra 	BB6_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r17, %f168;
	shr.u32 	%r261, %r17, 23;
	and.b32  	%r262, %r261, 255;
	add.s32 	%r749, %r262, -127;
	setp.eq.s32 	%p38, %r262, 0;
	mov.f32 	%f285, %f168;
	@%p38 bra 	BB6_28;
	bra.uni 	BB6_29;

BB6_28:
	and.b32  	%r263, %r17, -2139095041;
	or.b32  	%r264, %r263, 1065353216;
	mov.b32 	 %f170, %r264;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r265, %f171;
	shr.u32 	%r266, %r265, 23;
	and.b32  	%r267, %r266, 255;
	add.s32 	%r749, %r267, -253;
	and.b32  	%r268, %r265, -2139095041;
	or.b32  	%r269, %r268, 1065353216;
	mov.b32 	 %f285, %r269;

BB6_29:
	mov.b32 	 %r270, %f285;
	and.b32  	%r271, %r270, -2139095041;
	or.b32  	%r272, %r271, 1065353216;
	mov.b32 	 %f286, %r272;
	setp.gt.f32 	%p39, %f286, 0f3FB504F3;
	@%p39 bra 	BB6_30;
	bra.uni 	BB6_31;

BB6_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r749, %r749, 1;

BB6_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mov.f32 	%f182, 0f40000000;
	mul.rn.f32 	%f183, %f182, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r273, %f183;
	and.b32  	%r274, %r273, -4096;
	mov.b32 	 %f191, %r274;
	mov.b32 	 %r275, %f174;
	and.b32  	%r276, %r275, -4096;
	mov.b32 	 %f192, %r276;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f182, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r749;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p40, %f179, 0f77F684DF;
	@%p40 bra 	BB6_32;
	bra.uni 	BB6_33;

BB6_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB6_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r23, %f241;
	setp.eq.s32 	%p41, %r23, 1118925336;
	@%p41 bra 	BB6_34;
	bra.uni 	BB6_35;

BB6_34:
	add.s32 	%r277, %r23, -1;
	mov.b32 	 %f243, %r277;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB6_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p42, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p42;
	setp.gt.f32 	%p43, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p43;
	setp.neu.f32 	%p44, %f39, %f27;
	@%p44 bra 	BB6_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB6_46;

BB6_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB6_46;

BB6_38:
	setp.lt.f32 	%p45, %f287, 0f00000000;
	@%p45 bra 	BB6_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB6_46;

BB6_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB6_46;

BB6_41:
	setp.lt.f32 	%p46, %f158, 0f3F800000;
	mov.b32 	 %r278, %f287;
	setp.lt.s32 	%p6, %r278, 0;
	@%p46 bra 	BB6_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB6_46;

BB6_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB6_46;

BB6_44:
	add.f32 	%f288, %f287, 0f40E00000;
	bra.uni 	BB6_46;

BB6_45:
	mov.f32 	%f288, 0f3F800000;

BB6_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r24, %f267;
	mul.hi.s32 	%r279, %r24, -1840700269;
	add.s32 	%r280, %r279, %r24;
	shr.u32 	%r281, %r280, 31;
	shr.s32 	%r282, %r280, 2;
	add.s32 	%r283, %r282, %r281;
	div.s32 	%r284, %r3, %r283;
	rem.s32 	%r25, %r3, %r283;
	mad.lo.s32 	%r285, %r284, %r24, %r25;
	shl.b32 	%r286, %r285, 4;
	ld.param.u32 	%r744, [DIT7C2C_param_0];
	add.s32 	%r26, %r744, %r286;
	ld.global.f64 	%fd141, [%r26];
	ld.global.f64 	%fd142, [%r26+8];
	shl.b32 	%r287, %r283, 4;
	add.s32 	%r27, %r26, %r287;
	ld.global.f64 	%fd1, [%r27];
	ld.global.f64 	%fd2, [%r27+8];
	add.s32 	%r28, %r27, %r287;
	ld.global.f64 	%fd3, [%r28];
	ld.global.f64 	%fd4, [%r28+8];
	add.s32 	%r29, %r28, %r287;
	ld.global.f64 	%fd5, [%r29];
	ld.global.f64 	%fd6, [%r29+8];
	add.s32 	%r30, %r29, %r287;
	ld.global.f64 	%fd7, [%r30];
	ld.global.f64 	%fd8, [%r30+8];
	mov.f64 	%fd1462, %fd7;
	mov.f64 	%fd1463, %fd8;
	add.s32 	%r31, %r30, %r287;
	ld.global.f64 	%fd9, [%r31];
	ld.global.f64 	%fd10, [%r31+8];
	mov.f64 	%fd1464, %fd9;
	mov.f64 	%fd1465, %fd10;
	add.s32 	%r32, %r31, %r287;
	ld.global.f64 	%fd11, [%r32];
	ld.global.f64 	%fd12, [%r32+8];
	mov.f64 	%fd1466, %fd11;
	mov.f64 	%fd1467, %fd12;
	setp.eq.s32 	%p47, %r25, 0;
	mov.f64 	%fd1454, %fd141;
	mov.f64 	%fd1455, %fd142;
	mov.f64 	%fd1456, %fd1;
	mov.f64 	%fd1457, %fd2;
	mov.f64 	%fd1458, %fd3;
	mov.f64 	%fd1459, %fd4;
	mov.f64 	%fd1460, %fd5;
	mov.f64 	%fd1461, %fd6;
	@%p47 bra 	BB6_348;

	cvt.rn.f64.s32 	%fd13, %r25;
	mul.f64 	%fd143, %fd13, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd14, %r24;
	div.rn.f64 	%fd15, %fd143, %fd14;
	setp.eq.f64 	%p48, %fd15, 0d7FF0000000000000;
	setp.eq.f64 	%p49, %fd15, 0dFFF0000000000000;
	or.pred  	%p50, %p48, %p49;
	add.u32 	%r33, %SP, 0;
	@%p50 bra 	BB6_70;

	// inline asm
	abs.f64 	%fd144, %fd15;
	// inline asm
	setp.gt.f64 	%p51, %fd144, 0d41E0000000000000;
	@%p51 bra 	BB6_50;

	mov.f64 	%fd159, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd146, %fd15, %fd159;
	// inline asm
	cvt.rni.s32.f64 	%r288, %fd146;
	// inline asm
	cvt.rn.f64.s32 	%fd160, %r288;
	neg.f64 	%fd156, %fd160;
	mov.f64 	%fd149, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd147, %fd156, %fd149, %fd15;
	// inline asm
	mov.f64 	%fd153, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd151, %fd156, %fd153, %fd147;
	// inline asm
	mov.f64 	%fd157, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd155, %fd156, %fd157, %fd151;
	// inline asm
	mov.u32 	%r752, %r288;
	mov.f64 	%fd1418, %fd155;
	bra.uni 	BB6_66;

BB6_50:
	mov.b64 	 %rl381, %fd15;
	shr.u64 	%rl382, %rl381, 52;
	and.b64  	%rl383, %rl382, 2047;
	add.s64 	%rl384, %rl383, 4294966272;
	cvt.u32.u64 	%r35, %rl384;
	shl.b64 	%rl385, %rl381, 11;
	or.b64  	%rl1, %rl385, -9223372036854775808;
	shr.u32 	%r292, %r35, 6;
	mov.u32 	%r293, 16;
	sub.s32 	%r36, %r293, %r292;
	mov.u32 	%r294, 15;
	sub.s32 	%r750, %r294, %r292;
	mov.u32 	%r295, 19;
	sub.s32 	%r38, %r295, %r292;
	mov.u32 	%r290, 18;
	// inline asm
	min.s32 	%r289, %r290, %r38;
	// inline asm
	setp.lt.s32 	%p52, %r750, %r289;
	@%p52 bra 	BB6_52;

	mov.u64 	%rl886, 0;
	bra.uni 	BB6_54;

BB6_52:
	mov.u32 	%r296, 1;
	sub.s32 	%r39, %r296, %r36;
	mov.u64 	%rl886, 0;

BB6_53:
	.pragma "nounroll";
	shl.b32 	%r300, %r750, 3;
	mov.u32 	%r301, __internal_i2opi_d;
	add.s32 	%r302, %r301, %r300;
	ld.const.u64 	%rl389, [%r302];
	mul.lo.s64 	%rl391, %rl389, %rl1;
	// inline asm
	mul.hi.u64 	%rl388, %rl389, %rl1;
	// inline asm
	mad.lo.s64 	%rl392, %rl389, %rl1, %rl886;
	setp.lt.u64 	%p53, %rl392, %rl391;
	selp.u64 	%rl393, 1, 0, %p53;
	add.s64 	%rl886, %rl393, %rl388;
	add.s32 	%r303, %r39, %r750;
	shl.b32 	%r304, %r303, 3;
	add.s32 	%r306, %r33, %r304;
	st.local.u64 	[%r306], %rl392;
	// inline asm
	min.s32 	%r297, %r290, %r38;
	// inline asm
	add.s32 	%r750, %r750, 1;
	setp.lt.s32 	%p54, %r750, %r297;
	@%p54 bra 	BB6_53;

BB6_54:
	mov.u32 	%r307, 1;
	sub.s32 	%r308, %r307, %r36;
	add.s32 	%r309, %r308, %r750;
	shl.b32 	%r310, %r309, 3;
	add.s32 	%r312, %r33, %r310;
	st.local.u64 	[%r312], %rl886;
	ld.local.u64 	%rl887, [%r33+24];
	ld.local.u64 	%rl888, [%r33+16];
	and.b32  	%r313, %r35, 63;
	setp.eq.s32 	%p55, %r313, 0;
	@%p55 bra 	BB6_56;

	and.b64  	%rl396, %rl382, 63;
	cvt.u32.u64 	%r314, %rl396;
	shl.b64 	%rl397, %rl887, %r314;
	neg.s32 	%r315, %r35;
	and.b32  	%r316, %r315, 63;
	shr.u64 	%rl398, %rl888, %r316;
	or.b64  	%rl887, %rl398, %rl397;
	shl.b64 	%rl399, %rl888, %r314;
	ld.local.u64 	%rl400, [%r33+8];
	shr.u64 	%rl401, %rl400, %r316;
	or.b64  	%rl888, %rl401, %rl399;

BB6_56:
	shr.u64 	%rl402, %rl887, 62;
	cvt.u32.u64 	%r317, %rl402;
	shr.u64 	%rl403, %rl888, 62;
	shl.b64 	%rl404, %rl887, 2;
	or.b64  	%rl893, %rl403, %rl404;
	shl.b64 	%rl12, %rl888, 2;
	setp.ne.s64 	%p56, %rl12, 0;
	selp.u64 	%rl405, 1, 0, %p56;
	or.b64  	%rl406, %rl405, %rl893;
	setp.gt.u64 	%p57, %rl406, -9223372036854775808;
	selp.u32 	%r318, 1, 0, %p57;
	add.s32 	%r319, %r318, %r317;
	setp.lt.s64 	%p58, %rl381, 0;
	neg.s32 	%r320, %r319;
	selp.b32 	%r752, %r320, %r319, %p58;
	@%p57 bra 	BB6_58;

	and.b64  	%rl889, %rl381, -9223372036854775808;
	mov.u64 	%rl892, %rl12;
	bra.uni 	BB6_59;

BB6_58:
	not.b64 	%rl409, %rl893;
	neg.s64 	%rl14, %rl12;
	setp.eq.s64 	%p59, %rl12, 0;
	selp.u64 	%rl410, 1, 0, %p59;
	add.s64 	%rl893, %rl410, %rl409;
	and.b64  	%rl412, %rl381, -9223372036854775808;
	xor.b64  	%rl889, %rl412, -9223372036854775808;
	mov.u64 	%rl892, %rl14;

BB6_59:
	mov.u64 	%rl891, %rl892;
	setp.gt.s64 	%p60, %rl893, 0;
	@%p60 bra 	BB6_61;

	mov.u32 	%r751, 0;
	bra.uni 	BB6_63;

BB6_61:
	mov.u32 	%r751, 0;

BB6_62:
	shr.u64 	%rl413, %rl891, 63;
	shl.b64 	%rl414, %rl893, 1;
	or.b64  	%rl893, %rl413, %rl414;
	shl.b64 	%rl891, %rl891, 1;
	add.s32 	%r751, %r751, -1;
	setp.gt.s64 	%p61, %rl893, 0;
	@%p61 bra 	BB6_62;

BB6_63:
	mul.lo.s64 	%rl895, %rl893, -3958705157555305931;
	mov.u64 	%rl417, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl415, %rl893, %rl417;
	// inline asm
	setp.gt.s64 	%p62, %rl415, 0;
	mov.u64 	%rl894, %rl415;
	@%p62 bra 	BB6_64;
	bra.uni 	BB6_65;

BB6_64:
	shl.b64 	%rl418, %rl415, 1;
	shr.u64 	%rl419, %rl895, 63;
	or.b64  	%rl894, %rl418, %rl419;
	mul.lo.s64 	%rl895, %rl893, -7917410315110611862;
	add.s32 	%r751, %r751, -1;

BB6_65:
	setp.ne.s64 	%p63, %rl895, 0;
	selp.u64 	%rl420, 1, 0, %p63;
	add.s64 	%rl421, %rl420, %rl894;
	add.s32 	%r323, %r751, 1022;
	cvt.u64.u32 	%rl422, %r323;
	shl.b64 	%rl423, %rl422, 52;
	shr.u64 	%rl424, %rl421, 11;
	shr.u64 	%rl425, %rl421, 10;
	and.b64  	%rl426, %rl425, 1;
	add.s64 	%rl427, %rl423, %rl424;
	add.s64 	%rl428, %rl427, %rl426;
	or.b64  	%rl429, %rl428, %rl889;
	mov.b64 	 %fd1418, %rl429;

BB6_66:
	add.s32 	%r50, %r752, 1;
	and.b32  	%r324, %r50, 1;
	setp.eq.s32 	%p64, %r324, 0;
	mul.rn.f64 	%fd19, %fd1418, %fd1418;
	@%p64 bra 	BB6_68;

	mov.f64 	%fd162, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd164, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd161, %fd162, %fd19, %fd164;
	// inline asm
	mov.f64 	%fd168, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd165, %fd161, %fd19, %fd168;
	// inline asm
	mov.f64 	%fd172, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd169, %fd165, %fd19, %fd172;
	// inline asm
	mov.f64 	%fd176, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd173, %fd169, %fd19, %fd176;
	// inline asm
	mov.f64 	%fd180, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd177, %fd173, %fd19, %fd180;
	// inline asm
	mov.f64 	%fd184, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd181, %fd177, %fd19, %fd184;
	// inline asm
	mov.f64 	%fd188, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd185, %fd181, %fd19, %fd188;
	// inline asm
	mov.f64 	%fd1419, %fd185;
	bra.uni 	BB6_69;

BB6_68:
	mov.f64 	%fd190, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd192, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd189, %fd190, %fd19, %fd192;
	// inline asm
	mov.f64 	%fd196, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd193, %fd189, %fd19, %fd196;
	// inline asm
	mov.f64 	%fd200, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd197, %fd193, %fd19, %fd200;
	// inline asm
	mov.f64 	%fd204, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd201, %fd197, %fd19, %fd204;
	// inline asm
	mov.f64 	%fd208, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd205, %fd201, %fd19, %fd208;
	// inline asm
	mul.rn.f64 	%fd210, %fd205, %fd19;
	// inline asm
	fma.rn.f64 	%fd209, %fd210, %fd1418, %fd1418;
	// inline asm
	mov.f64 	%fd1419, %fd209;

BB6_69:
	and.b32  	%r325, %r50, 2;
	setp.eq.s32 	%p65, %r325, 0;
	neg.f64 	%fd213, %fd1419;
	selp.f64 	%fd1420, %fd1419, %fd213, %p65;
	bra.uni 	BB6_71;

BB6_70:
	mov.f64 	%fd1420, 0dFFF8000000000000;

BB6_71:
	setp.eq.f64 	%p67, %fd15, 0d0000000000000000;
	or.pred  	%p68, %p49, %p67;
	or.pred  	%p70, %p48, %p68;
	@%p70 bra 	BB6_94;

	// inline asm
	abs.f64 	%fd215, %fd15;
	// inline asm
	setp.gt.f64 	%p71, %fd215, 0d41E0000000000000;
	@%p71 bra 	BB6_74;

	mov.f64 	%fd230, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd217, %fd15, %fd230;
	// inline asm
	cvt.rni.s32.f64 	%r326, %fd217;
	// inline asm
	cvt.rn.f64.s32 	%fd231, %r326;
	neg.f64 	%fd227, %fd231;
	mov.f64 	%fd220, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd218, %fd227, %fd220, %fd15;
	// inline asm
	mov.f64 	%fd224, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd222, %fd227, %fd224, %fd218;
	// inline asm
	mov.f64 	%fd228, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd226, %fd227, %fd228, %fd222;
	// inline asm
	mov.u32 	%r755, %r326;
	mov.f64 	%fd1421, %fd226;
	bra.uni 	BB6_90;

BB6_74:
	mov.b64 	 %rl31, %fd15;
	and.b64  	%rl899, %rl31, -9223372036854775808;
	shr.u64 	%rl33, %rl31, 52;
	and.b64  	%rl430, %rl33, 2047;
	add.s64 	%rl431, %rl430, 4294966272;
	cvt.u32.u64 	%r52, %rl431;
	shl.b64 	%rl432, %rl31, 11;
	or.b64  	%rl34, %rl432, -9223372036854775808;
	shr.u32 	%r330, %r52, 6;
	mov.u32 	%r331, 16;
	sub.s32 	%r53, %r331, %r330;
	mov.u32 	%r332, 15;
	sub.s32 	%r753, %r332, %r330;
	mov.u32 	%r333, 19;
	sub.s32 	%r55, %r333, %r330;
	mov.u32 	%r328, 18;
	// inline asm
	min.s32 	%r327, %r328, %r55;
	// inline asm
	setp.lt.s32 	%p72, %r753, %r327;
	@%p72 bra 	BB6_76;

	mov.u64 	%rl896, 0;
	bra.uni 	BB6_78;

BB6_76:
	mov.u32 	%r334, 1;
	sub.s32 	%r56, %r334, %r53;
	mov.u64 	%rl896, 0;

BB6_77:
	.pragma "nounroll";
	shl.b32 	%r338, %r753, 3;
	mov.u32 	%r339, __internal_i2opi_d;
	add.s32 	%r340, %r339, %r338;
	ld.const.u64 	%rl436, [%r340];
	mul.lo.s64 	%rl438, %rl436, %rl34;
	// inline asm
	mul.hi.u64 	%rl435, %rl436, %rl34;
	// inline asm
	mad.lo.s64 	%rl439, %rl436, %rl34, %rl896;
	setp.lt.u64 	%p73, %rl439, %rl438;
	selp.u64 	%rl440, 1, 0, %p73;
	add.s64 	%rl896, %rl440, %rl435;
	add.s32 	%r341, %r56, %r753;
	shl.b32 	%r342, %r341, 3;
	add.s32 	%r344, %r33, %r342;
	st.local.u64 	[%r344], %rl439;
	// inline asm
	min.s32 	%r335, %r328, %r55;
	// inline asm
	add.s32 	%r753, %r753, 1;
	setp.lt.s32 	%p74, %r753, %r335;
	@%p74 bra 	BB6_77;

BB6_78:
	mov.u32 	%r345, 1;
	sub.s32 	%r346, %r345, %r53;
	add.s32 	%r347, %r346, %r753;
	shl.b32 	%r348, %r347, 3;
	add.s32 	%r350, %r33, %r348;
	st.local.u64 	[%r350], %rl896;
	ld.local.u64 	%rl897, [%r33+24];
	ld.local.u64 	%rl898, [%r33+16];
	and.b32  	%r351, %r52, 63;
	setp.eq.s32 	%p75, %r351, 0;
	@%p75 bra 	BB6_80;

	and.b64  	%rl441, %rl33, 63;
	cvt.u32.u64 	%r352, %rl441;
	shl.b64 	%rl442, %rl897, %r352;
	neg.s32 	%r353, %r52;
	and.b32  	%r354, %r353, 63;
	shr.u64 	%rl443, %rl898, %r354;
	or.b64  	%rl897, %rl443, %rl442;
	shl.b64 	%rl444, %rl898, %r352;
	ld.local.u64 	%rl445, [%r33+8];
	shr.u64 	%rl446, %rl445, %r354;
	or.b64  	%rl898, %rl446, %rl444;

BB6_80:
	shr.u64 	%rl447, %rl897, 62;
	cvt.u32.u64 	%r355, %rl447;
	shr.u64 	%rl448, %rl898, 62;
	shl.b64 	%rl449, %rl897, 2;
	or.b64  	%rl903, %rl448, %rl449;
	shl.b64 	%rl45, %rl898, 2;
	setp.ne.s64 	%p76, %rl45, 0;
	selp.u64 	%rl450, 1, 0, %p76;
	or.b64  	%rl451, %rl450, %rl903;
	setp.gt.u64 	%p77, %rl451, -9223372036854775808;
	selp.u32 	%r356, 1, 0, %p77;
	add.s32 	%r357, %r356, %r355;
	neg.s32 	%r358, %r357;
	setp.lt.s64 	%p78, %rl31, 0;
	selp.b32 	%r755, %r358, %r357, %p78;
	@%p77 bra 	BB6_82;

	mov.u64 	%rl902, %rl45;
	bra.uni 	BB6_83;

BB6_82:
	not.b64 	%rl452, %rl903;
	neg.s64 	%rl46, %rl45;
	setp.eq.s64 	%p79, %rl45, 0;
	selp.u64 	%rl453, 1, 0, %p79;
	add.s64 	%rl903, %rl453, %rl452;
	xor.b64  	%rl899, %rl899, -9223372036854775808;
	mov.u64 	%rl902, %rl46;

BB6_83:
	mov.u64 	%rl901, %rl902;
	setp.gt.s64 	%p80, %rl903, 0;
	@%p80 bra 	BB6_85;

	mov.u32 	%r754, 0;
	bra.uni 	BB6_87;

BB6_85:
	mov.u32 	%r754, 0;

BB6_86:
	shr.u64 	%rl454, %rl901, 63;
	shl.b64 	%rl455, %rl903, 1;
	or.b64  	%rl903, %rl454, %rl455;
	shl.b64 	%rl901, %rl901, 1;
	add.s32 	%r754, %r754, -1;
	setp.gt.s64 	%p81, %rl903, 0;
	@%p81 bra 	BB6_86;

BB6_87:
	mul.lo.s64 	%rl905, %rl903, -3958705157555305931;
	mov.u64 	%rl458, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl456, %rl903, %rl458;
	// inline asm
	setp.gt.s64 	%p82, %rl456, 0;
	mov.u64 	%rl904, %rl456;
	@%p82 bra 	BB6_88;
	bra.uni 	BB6_89;

BB6_88:
	shl.b64 	%rl459, %rl456, 1;
	shr.u64 	%rl460, %rl905, 63;
	or.b64  	%rl904, %rl459, %rl460;
	mul.lo.s64 	%rl905, %rl903, -7917410315110611862;
	add.s32 	%r754, %r754, -1;

BB6_89:
	setp.ne.s64 	%p83, %rl905, 0;
	selp.u64 	%rl461, 1, 0, %p83;
	add.s64 	%rl462, %rl461, %rl904;
	add.s32 	%r361, %r754, 1022;
	cvt.u64.u32 	%rl463, %r361;
	shl.b64 	%rl464, %rl463, 52;
	shr.u64 	%rl465, %rl462, 11;
	shr.u64 	%rl466, %rl462, 10;
	and.b64  	%rl467, %rl466, 1;
	add.s64 	%rl468, %rl464, %rl465;
	add.s64 	%rl469, %rl468, %rl467;
	or.b64  	%rl470, %rl469, %rl899;
	mov.b64 	 %fd1421, %rl470;

BB6_90:
	and.b32  	%r362, %r755, 1;
	setp.eq.s32 	%p84, %r362, 0;
	mul.rn.f64 	%fd29, %fd1421, %fd1421;
	@%p84 bra 	BB6_92;

	mov.f64 	%fd233, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd235, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd232, %fd233, %fd29, %fd235;
	// inline asm
	mov.f64 	%fd239, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd236, %fd232, %fd29, %fd239;
	// inline asm
	mov.f64 	%fd243, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd240, %fd236, %fd29, %fd243;
	// inline asm
	mov.f64 	%fd247, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd244, %fd240, %fd29, %fd247;
	// inline asm
	mov.f64 	%fd251, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd248, %fd244, %fd29, %fd251;
	// inline asm
	mov.f64 	%fd255, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd252, %fd248, %fd29, %fd255;
	// inline asm
	mov.f64 	%fd259, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd256, %fd252, %fd29, %fd259;
	// inline asm
	mov.f64 	%fd1422, %fd256;
	bra.uni 	BB6_93;

BB6_92:
	mov.f64 	%fd261, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd263, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd260, %fd261, %fd29, %fd263;
	// inline asm
	mov.f64 	%fd267, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd264, %fd260, %fd29, %fd267;
	// inline asm
	mov.f64 	%fd271, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd268, %fd264, %fd29, %fd271;
	// inline asm
	mov.f64 	%fd275, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd272, %fd268, %fd29, %fd275;
	// inline asm
	mov.f64 	%fd279, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd276, %fd272, %fd29, %fd279;
	// inline asm
	mul.rn.f64 	%fd281, %fd276, %fd29;
	// inline asm
	fma.rn.f64 	%fd280, %fd281, %fd1421, %fd1421;
	// inline asm
	mov.f64 	%fd1422, %fd280;

BB6_93:
	and.b32  	%r363, %r755, 2;
	setp.eq.s32 	%p85, %r363, 0;
	neg.f64 	%fd284, %fd1422;
	selp.f64 	%fd1423, %fd1422, %fd284, %p85;
	bra.uni 	BB6_95;

BB6_94:
	mov.f64 	%fd285, 0d0000000000000000;
	mul.rn.f64 	%fd1423, %fd15, %fd285;

BB6_95:
	neg.f64 	%fd286, %fd1423;
	mov.f64 	%fd1478, %fd1420;
	mov.f64 	%fd1479, %fd286;
	ld.param.u32 	%r745, [DIT7C2C_param_3];
	setp.eq.s32 	%p7, %r745, 0;
	@%p7 bra 	BB6_96;
	bra.uni 	BB6_97;

BB6_96:
	mov.f64 	%fd1478, %fd1420;
	mov.f64 	%fd1479, %fd1423;

BB6_97:
	mul.f64 	%fd288, %fd1, %fd1478;
	neg.f64 	%fd290, %fd2;
	fma.rn.f64 	%fd291, %fd290, %fd1479, %fd288;
	mul.f64 	%fd292, %fd2, %fd1478;
	fma.rn.f64 	%fd293, %fd1, %fd1479, %fd292;
	mul.f64 	%fd294, %fd13, 0d402921FB54442D18;
	div.rn.f64 	%fd36, %fd294, %fd14;
	setp.eq.f64 	%p8, %fd36, 0d7FF0000000000000;
	setp.eq.f64 	%p86, %fd36, 0dFFF0000000000000;
	or.pred  	%p87, %p8, %p86;
	@%p87 bra 	BB6_120;

	// inline asm
	abs.f64 	%fd295, %fd36;
	// inline asm
	setp.gt.f64 	%p88, %fd295, 0d41E0000000000000;
	@%p88 bra 	BB6_100;

	mov.f64 	%fd310, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd297, %fd36, %fd310;
	// inline asm
	cvt.rni.s32.f64 	%r364, %fd297;
	// inline asm
	cvt.rn.f64.s32 	%fd311, %r364;
	neg.f64 	%fd307, %fd311;
	mov.f64 	%fd300, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd298, %fd307, %fd300, %fd36;
	// inline asm
	mov.f64 	%fd304, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd302, %fd307, %fd304, %fd298;
	// inline asm
	mov.f64 	%fd308, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd306, %fd307, %fd308, %fd302;
	// inline asm
	mov.u32 	%r758, %r364;
	mov.f64 	%fd1424, %fd306;
	bra.uni 	BB6_116;

BB6_100:
	mov.b64 	 %rl471, %fd36;
	and.b64  	%rl909, %rl471, -9223372036854775808;
	shr.u64 	%rl472, %rl471, 52;
	and.b64  	%rl473, %rl472, 2047;
	add.s64 	%rl474, %rl473, 4294966272;
	cvt.u32.u64 	%r68, %rl474;
	shl.b64 	%rl475, %rl471, 11;
	or.b64  	%rl64, %rl475, -9223372036854775808;
	shr.u32 	%r368, %r68, 6;
	mov.u32 	%r369, 16;
	sub.s32 	%r69, %r369, %r368;
	mov.u32 	%r370, 15;
	sub.s32 	%r756, %r370, %r368;
	mov.u32 	%r371, 19;
	sub.s32 	%r71, %r371, %r368;
	mov.u32 	%r366, 18;
	// inline asm
	min.s32 	%r365, %r366, %r71;
	// inline asm
	setp.lt.s32 	%p89, %r756, %r365;
	@%p89 bra 	BB6_102;

	mov.u64 	%rl906, 0;
	bra.uni 	BB6_104;

BB6_102:
	mov.u32 	%r372, 1;
	sub.s32 	%r72, %r372, %r69;
	mov.u64 	%rl906, 0;

BB6_103:
	.pragma "nounroll";
	shl.b32 	%r376, %r756, 3;
	mov.u32 	%r377, __internal_i2opi_d;
	add.s32 	%r378, %r377, %r376;
	ld.const.u64 	%rl479, [%r378];
	mul.lo.s64 	%rl481, %rl479, %rl64;
	// inline asm
	mul.hi.u64 	%rl478, %rl479, %rl64;
	// inline asm
	mad.lo.s64 	%rl482, %rl479, %rl64, %rl906;
	setp.lt.u64 	%p90, %rl482, %rl481;
	selp.u64 	%rl483, 1, 0, %p90;
	add.s64 	%rl906, %rl483, %rl478;
	add.s32 	%r379, %r72, %r756;
	shl.b32 	%r380, %r379, 3;
	add.s32 	%r382, %r33, %r380;
	st.local.u64 	[%r382], %rl482;
	// inline asm
	min.s32 	%r373, %r366, %r71;
	// inline asm
	add.s32 	%r756, %r756, 1;
	setp.lt.s32 	%p91, %r756, %r373;
	@%p91 bra 	BB6_103;

BB6_104:
	mov.u32 	%r383, 1;
	sub.s32 	%r384, %r383, %r69;
	add.s32 	%r385, %r384, %r756;
	shl.b32 	%r386, %r385, 3;
	add.s32 	%r388, %r33, %r386;
	st.local.u64 	[%r388], %rl906;
	ld.local.u64 	%rl907, [%r33+24];
	ld.local.u64 	%rl908, [%r33+16];
	and.b32  	%r389, %r68, 63;
	setp.eq.s32 	%p92, %r389, 0;
	@%p92 bra 	BB6_106;

	and.b64  	%rl486, %rl472, 63;
	cvt.u32.u64 	%r390, %rl486;
	shl.b64 	%rl487, %rl907, %r390;
	neg.s32 	%r391, %r68;
	and.b32  	%r392, %r391, 63;
	shr.u64 	%rl488, %rl908, %r392;
	or.b64  	%rl907, %rl488, %rl487;
	shl.b64 	%rl489, %rl908, %r390;
	ld.local.u64 	%rl490, [%r33+8];
	shr.u64 	%rl491, %rl490, %r392;
	or.b64  	%rl908, %rl491, %rl489;

BB6_106:
	shr.u64 	%rl492, %rl907, 62;
	cvt.u32.u64 	%r393, %rl492;
	shr.u64 	%rl493, %rl908, 62;
	shl.b64 	%rl494, %rl907, 2;
	or.b64  	%rl913, %rl493, %rl494;
	shl.b64 	%rl75, %rl908, 2;
	setp.ne.s64 	%p93, %rl75, 0;
	selp.u64 	%rl495, 1, 0, %p93;
	or.b64  	%rl496, %rl495, %rl913;
	setp.gt.u64 	%p94, %rl496, -9223372036854775808;
	selp.u32 	%r394, 1, 0, %p94;
	add.s32 	%r395, %r394, %r393;
	setp.lt.s64 	%p95, %rl471, 0;
	neg.s32 	%r396, %r395;
	selp.b32 	%r758, %r396, %r395, %p95;
	@%p94 bra 	BB6_108;

	mov.u64 	%rl912, %rl75;
	bra.uni 	BB6_109;

BB6_108:
	not.b64 	%rl498, %rl913;
	neg.s64 	%rl76, %rl75;
	setp.eq.s64 	%p96, %rl75, 0;
	selp.u64 	%rl499, 1, 0, %p96;
	add.s64 	%rl913, %rl499, %rl498;
	xor.b64  	%rl909, %rl909, -9223372036854775808;
	mov.u64 	%rl912, %rl76;

BB6_109:
	mov.u64 	%rl911, %rl912;
	setp.gt.s64 	%p97, %rl913, 0;
	@%p97 bra 	BB6_111;

	mov.u32 	%r757, 0;
	bra.uni 	BB6_113;

BB6_111:
	mov.u32 	%r757, 0;

BB6_112:
	shr.u64 	%rl500, %rl911, 63;
	shl.b64 	%rl501, %rl913, 1;
	or.b64  	%rl913, %rl500, %rl501;
	shl.b64 	%rl911, %rl911, 1;
	add.s32 	%r757, %r757, -1;
	setp.gt.s64 	%p98, %rl913, 0;
	@%p98 bra 	BB6_112;

BB6_113:
	mul.lo.s64 	%rl915, %rl913, -3958705157555305931;
	mov.u64 	%rl504, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl502, %rl913, %rl504;
	// inline asm
	setp.gt.s64 	%p99, %rl502, 0;
	mov.u64 	%rl914, %rl502;
	@%p99 bra 	BB6_114;
	bra.uni 	BB6_115;

BB6_114:
	shl.b64 	%rl505, %rl502, 1;
	shr.u64 	%rl506, %rl915, 63;
	or.b64  	%rl914, %rl505, %rl506;
	mul.lo.s64 	%rl915, %rl913, -7917410315110611862;
	add.s32 	%r757, %r757, -1;

BB6_115:
	setp.ne.s64 	%p100, %rl915, 0;
	selp.u64 	%rl507, 1, 0, %p100;
	add.s64 	%rl508, %rl507, %rl914;
	add.s32 	%r399, %r757, 1022;
	cvt.u64.u32 	%rl509, %r399;
	shl.b64 	%rl510, %rl509, 52;
	shr.u64 	%rl511, %rl508, 11;
	shr.u64 	%rl512, %rl508, 10;
	and.b64  	%rl513, %rl512, 1;
	add.s64 	%rl514, %rl510, %rl511;
	add.s64 	%rl515, %rl514, %rl513;
	or.b64  	%rl516, %rl515, %rl909;
	mov.b64 	 %fd1424, %rl516;

BB6_116:
	add.s32 	%r83, %r758, 1;
	and.b32  	%r400, %r83, 1;
	setp.eq.s32 	%p101, %r400, 0;
	mul.rn.f64 	%fd40, %fd1424, %fd1424;
	@%p101 bra 	BB6_118;

	mov.f64 	%fd313, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd315, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd312, %fd313, %fd40, %fd315;
	// inline asm
	mov.f64 	%fd319, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd316, %fd312, %fd40, %fd319;
	// inline asm
	mov.f64 	%fd323, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd320, %fd316, %fd40, %fd323;
	// inline asm
	mov.f64 	%fd327, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd324, %fd320, %fd40, %fd327;
	// inline asm
	mov.f64 	%fd331, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd328, %fd324, %fd40, %fd331;
	// inline asm
	mov.f64 	%fd335, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd332, %fd328, %fd40, %fd335;
	// inline asm
	mov.f64 	%fd339, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd336, %fd332, %fd40, %fd339;
	// inline asm
	mov.f64 	%fd1425, %fd336;
	bra.uni 	BB6_119;

BB6_118:
	mov.f64 	%fd341, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd343, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd340, %fd341, %fd40, %fd343;
	// inline asm
	mov.f64 	%fd347, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd344, %fd340, %fd40, %fd347;
	// inline asm
	mov.f64 	%fd351, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd348, %fd344, %fd40, %fd351;
	// inline asm
	mov.f64 	%fd355, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd352, %fd348, %fd40, %fd355;
	// inline asm
	mov.f64 	%fd359, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd356, %fd352, %fd40, %fd359;
	// inline asm
	mul.rn.f64 	%fd361, %fd356, %fd40;
	// inline asm
	fma.rn.f64 	%fd360, %fd361, %fd1424, %fd1424;
	// inline asm
	mov.f64 	%fd1425, %fd360;

BB6_119:
	and.b32  	%r401, %r83, 2;
	setp.eq.s32 	%p102, %r401, 0;
	neg.f64 	%fd364, %fd1425;
	selp.f64 	%fd1426, %fd1425, %fd364, %p102;
	bra.uni 	BB6_121;

BB6_120:
	mov.f64 	%fd1426, 0dFFF8000000000000;

BB6_121:
	setp.eq.f64 	%p104, %fd36, 0d0000000000000000;
	or.pred  	%p105, %p86, %p104;
	or.pred  	%p106, %p8, %p105;
	@%p106 bra 	BB6_144;

	// inline asm
	abs.f64 	%fd366, %fd36;
	// inline asm
	setp.gt.f64 	%p107, %fd366, 0d41E0000000000000;
	@%p107 bra 	BB6_124;

	mov.f64 	%fd381, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd368, %fd36, %fd381;
	// inline asm
	cvt.rni.s32.f64 	%r402, %fd368;
	// inline asm
	cvt.rn.f64.s32 	%fd382, %r402;
	neg.f64 	%fd378, %fd382;
	mov.f64 	%fd371, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd369, %fd378, %fd371, %fd36;
	// inline asm
	mov.f64 	%fd375, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd373, %fd378, %fd375, %fd369;
	// inline asm
	mov.f64 	%fd379, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd377, %fd378, %fd379, %fd373;
	// inline asm
	mov.u32 	%r761, %r402;
	mov.f64 	%fd1427, %fd377;
	bra.uni 	BB6_140;

BB6_124:
	mov.b64 	 %rl93, %fd36;
	and.b64  	%rl919, %rl93, -9223372036854775808;
	shr.u64 	%rl95, %rl93, 52;
	and.b64  	%rl517, %rl95, 2047;
	add.s64 	%rl518, %rl517, 4294966272;
	cvt.u32.u64 	%r85, %rl518;
	shl.b64 	%rl519, %rl93, 11;
	or.b64  	%rl96, %rl519, -9223372036854775808;
	shr.u32 	%r406, %r85, 6;
	mov.u32 	%r407, 16;
	sub.s32 	%r86, %r407, %r406;
	mov.u32 	%r408, 15;
	sub.s32 	%r759, %r408, %r406;
	mov.u32 	%r409, 19;
	sub.s32 	%r88, %r409, %r406;
	mov.u32 	%r404, 18;
	// inline asm
	min.s32 	%r403, %r404, %r88;
	// inline asm
	setp.lt.s32 	%p108, %r759, %r403;
	@%p108 bra 	BB6_126;

	mov.u64 	%rl916, 0;
	bra.uni 	BB6_128;

BB6_126:
	mov.u32 	%r410, 1;
	sub.s32 	%r89, %r410, %r86;
	mov.u64 	%rl916, 0;

BB6_127:
	.pragma "nounroll";
	shl.b32 	%r414, %r759, 3;
	mov.u32 	%r415, __internal_i2opi_d;
	add.s32 	%r416, %r415, %r414;
	ld.const.u64 	%rl523, [%r416];
	mul.lo.s64 	%rl525, %rl523, %rl96;
	// inline asm
	mul.hi.u64 	%rl522, %rl523, %rl96;
	// inline asm
	mad.lo.s64 	%rl526, %rl523, %rl96, %rl916;
	setp.lt.u64 	%p109, %rl526, %rl525;
	selp.u64 	%rl527, 1, 0, %p109;
	add.s64 	%rl916, %rl527, %rl522;
	add.s32 	%r417, %r89, %r759;
	shl.b32 	%r418, %r417, 3;
	add.s32 	%r420, %r33, %r418;
	st.local.u64 	[%r420], %rl526;
	// inline asm
	min.s32 	%r411, %r404, %r88;
	// inline asm
	add.s32 	%r759, %r759, 1;
	setp.lt.s32 	%p110, %r759, %r411;
	@%p110 bra 	BB6_127;

BB6_128:
	mov.u32 	%r421, 1;
	sub.s32 	%r422, %r421, %r86;
	add.s32 	%r423, %r422, %r759;
	shl.b32 	%r424, %r423, 3;
	add.s32 	%r426, %r33, %r424;
	st.local.u64 	[%r426], %rl916;
	ld.local.u64 	%rl917, [%r33+24];
	ld.local.u64 	%rl918, [%r33+16];
	and.b32  	%r427, %r85, 63;
	setp.eq.s32 	%p111, %r427, 0;
	@%p111 bra 	BB6_130;

	and.b64  	%rl528, %rl95, 63;
	cvt.u32.u64 	%r428, %rl528;
	shl.b64 	%rl529, %rl917, %r428;
	neg.s32 	%r429, %r85;
	and.b32  	%r430, %r429, 63;
	shr.u64 	%rl530, %rl918, %r430;
	or.b64  	%rl917, %rl530, %rl529;
	shl.b64 	%rl531, %rl918, %r428;
	ld.local.u64 	%rl532, [%r33+8];
	shr.u64 	%rl533, %rl532, %r430;
	or.b64  	%rl918, %rl533, %rl531;

BB6_130:
	shr.u64 	%rl534, %rl917, 62;
	cvt.u32.u64 	%r431, %rl534;
	shr.u64 	%rl535, %rl918, 62;
	shl.b64 	%rl536, %rl917, 2;
	or.b64  	%rl923, %rl535, %rl536;
	shl.b64 	%rl107, %rl918, 2;
	setp.ne.s64 	%p112, %rl107, 0;
	selp.u64 	%rl537, 1, 0, %p112;
	or.b64  	%rl538, %rl537, %rl923;
	setp.gt.u64 	%p113, %rl538, -9223372036854775808;
	selp.u32 	%r432, 1, 0, %p113;
	add.s32 	%r433, %r432, %r431;
	neg.s32 	%r434, %r433;
	setp.lt.s64 	%p114, %rl93, 0;
	selp.b32 	%r761, %r434, %r433, %p114;
	@%p113 bra 	BB6_132;

	mov.u64 	%rl922, %rl107;
	bra.uni 	BB6_133;

BB6_132:
	not.b64 	%rl539, %rl923;
	neg.s64 	%rl108, %rl107;
	setp.eq.s64 	%p115, %rl107, 0;
	selp.u64 	%rl540, 1, 0, %p115;
	add.s64 	%rl923, %rl540, %rl539;
	xor.b64  	%rl919, %rl919, -9223372036854775808;
	mov.u64 	%rl922, %rl108;

BB6_133:
	mov.u64 	%rl921, %rl922;
	setp.gt.s64 	%p116, %rl923, 0;
	@%p116 bra 	BB6_135;

	mov.u32 	%r760, 0;
	bra.uni 	BB6_137;

BB6_135:
	mov.u32 	%r760, 0;

BB6_136:
	shr.u64 	%rl541, %rl921, 63;
	shl.b64 	%rl542, %rl923, 1;
	or.b64  	%rl923, %rl541, %rl542;
	shl.b64 	%rl921, %rl921, 1;
	add.s32 	%r760, %r760, -1;
	setp.gt.s64 	%p117, %rl923, 0;
	@%p117 bra 	BB6_136;

BB6_137:
	mul.lo.s64 	%rl925, %rl923, -3958705157555305931;
	mov.u64 	%rl545, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl543, %rl923, %rl545;
	// inline asm
	setp.gt.s64 	%p118, %rl543, 0;
	mov.u64 	%rl924, %rl543;
	@%p118 bra 	BB6_138;
	bra.uni 	BB6_139;

BB6_138:
	shl.b64 	%rl546, %rl543, 1;
	shr.u64 	%rl547, %rl925, 63;
	or.b64  	%rl924, %rl546, %rl547;
	mul.lo.s64 	%rl925, %rl923, -7917410315110611862;
	add.s32 	%r760, %r760, -1;

BB6_139:
	setp.ne.s64 	%p119, %rl925, 0;
	selp.u64 	%rl548, 1, 0, %p119;
	add.s64 	%rl549, %rl548, %rl924;
	add.s32 	%r437, %r760, 1022;
	cvt.u64.u32 	%rl550, %r437;
	shl.b64 	%rl551, %rl550, 52;
	shr.u64 	%rl552, %rl549, 11;
	shr.u64 	%rl553, %rl549, 10;
	and.b64  	%rl554, %rl553, 1;
	add.s64 	%rl555, %rl551, %rl552;
	add.s64 	%rl556, %rl555, %rl554;
	or.b64  	%rl557, %rl556, %rl919;
	mov.b64 	 %fd1427, %rl557;

BB6_140:
	and.b32  	%r438, %r761, 1;
	setp.eq.s32 	%p120, %r438, 0;
	mul.rn.f64 	%fd50, %fd1427, %fd1427;
	@%p120 bra 	BB6_142;

	mov.f64 	%fd384, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd386, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd383, %fd384, %fd50, %fd386;
	// inline asm
	mov.f64 	%fd390, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd387, %fd383, %fd50, %fd390;
	// inline asm
	mov.f64 	%fd394, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd391, %fd387, %fd50, %fd394;
	// inline asm
	mov.f64 	%fd398, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd395, %fd391, %fd50, %fd398;
	// inline asm
	mov.f64 	%fd402, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd399, %fd395, %fd50, %fd402;
	// inline asm
	mov.f64 	%fd406, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd403, %fd399, %fd50, %fd406;
	// inline asm
	mov.f64 	%fd410, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd407, %fd403, %fd50, %fd410;
	// inline asm
	mov.f64 	%fd1428, %fd407;
	bra.uni 	BB6_143;

BB6_142:
	mov.f64 	%fd412, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd414, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd411, %fd412, %fd50, %fd414;
	// inline asm
	mov.f64 	%fd418, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd415, %fd411, %fd50, %fd418;
	// inline asm
	mov.f64 	%fd422, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd419, %fd415, %fd50, %fd422;
	// inline asm
	mov.f64 	%fd426, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd423, %fd419, %fd50, %fd426;
	// inline asm
	mov.f64 	%fd430, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd427, %fd423, %fd50, %fd430;
	// inline asm
	mul.rn.f64 	%fd432, %fd427, %fd50;
	// inline asm
	fma.rn.f64 	%fd431, %fd432, %fd1427, %fd1427;
	// inline asm
	mov.f64 	%fd1428, %fd431;

BB6_143:
	and.b32  	%r439, %r761, 2;
	setp.eq.s32 	%p121, %r439, 0;
	neg.f64 	%fd435, %fd1428;
	selp.f64 	%fd1429, %fd1428, %fd435, %p121;
	bra.uni 	BB6_145;

BB6_144:
	mov.f64 	%fd436, 0d0000000000000000;
	mul.rn.f64 	%fd1429, %fd36, %fd436;

BB6_145:
	neg.f64 	%fd437, %fd1429;
	mov.f64 	%fd1476, %fd1426;
	mov.f64 	%fd1477, %fd437;
	@%p7 bra 	BB6_146;
	bra.uni 	BB6_147;

BB6_146:
	mov.f64 	%fd1476, %fd1426;
	mov.f64 	%fd1477, %fd1429;

BB6_147:
	mul.f64 	%fd439, %fd3, %fd1476;
	neg.f64 	%fd441, %fd4;
	fma.rn.f64 	%fd442, %fd441, %fd1477, %fd439;
	mul.f64 	%fd443, %fd4, %fd1476;
	fma.rn.f64 	%fd444, %fd3, %fd1477, %fd443;
	mul.f64 	%fd445, %fd13, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd57, %fd445, %fd14;
	setp.eq.f64 	%p9, %fd57, 0d7FF0000000000000;
	setp.eq.f64 	%p10, %fd57, 0dFFF0000000000000;
	or.pred  	%p122, %p9, %p10;
	@%p122 bra 	BB6_170;

	// inline asm
	abs.f64 	%fd446, %fd57;
	// inline asm
	setp.gt.f64 	%p123, %fd446, 0d41E0000000000000;
	@%p123 bra 	BB6_150;

	mov.f64 	%fd461, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd448, %fd57, %fd461;
	// inline asm
	cvt.rni.s32.f64 	%r440, %fd448;
	// inline asm
	cvt.rn.f64.s32 	%fd462, %r440;
	neg.f64 	%fd458, %fd462;
	mov.f64 	%fd451, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd449, %fd458, %fd451, %fd57;
	// inline asm
	mov.f64 	%fd455, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd453, %fd458, %fd455, %fd449;
	// inline asm
	mov.f64 	%fd459, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd457, %fd458, %fd459, %fd453;
	// inline asm
	mov.u32 	%r764, %r440;
	mov.f64 	%fd1430, %fd457;
	bra.uni 	BB6_166;

BB6_150:
	mov.b64 	 %rl125, %fd57;
	and.b64  	%rl929, %rl125, -9223372036854775808;
	shr.u64 	%rl127, %rl125, 52;
	and.b64  	%rl558, %rl127, 2047;
	add.s64 	%rl559, %rl558, 4294966272;
	cvt.u32.u64 	%r101, %rl559;
	shl.b64 	%rl560, %rl125, 11;
	or.b64  	%rl128, %rl560, -9223372036854775808;
	shr.u32 	%r444, %r101, 6;
	mov.u32 	%r445, 16;
	sub.s32 	%r102, %r445, %r444;
	mov.u32 	%r446, 15;
	sub.s32 	%r762, %r446, %r444;
	mov.u32 	%r447, 19;
	sub.s32 	%r104, %r447, %r444;
	mov.u32 	%r442, 18;
	// inline asm
	min.s32 	%r441, %r442, %r104;
	// inline asm
	setp.lt.s32 	%p124, %r762, %r441;
	@%p124 bra 	BB6_152;

	mov.u64 	%rl926, 0;
	bra.uni 	BB6_154;

BB6_152:
	mov.u32 	%r448, 1;
	sub.s32 	%r105, %r448, %r102;
	mov.u64 	%rl926, 0;

BB6_153:
	.pragma "nounroll";
	shl.b32 	%r452, %r762, 3;
	mov.u32 	%r453, __internal_i2opi_d;
	add.s32 	%r454, %r453, %r452;
	ld.const.u64 	%rl564, [%r454];
	mul.lo.s64 	%rl566, %rl564, %rl128;
	// inline asm
	mul.hi.u64 	%rl563, %rl564, %rl128;
	// inline asm
	mad.lo.s64 	%rl567, %rl564, %rl128, %rl926;
	setp.lt.u64 	%p125, %rl567, %rl566;
	selp.u64 	%rl568, 1, 0, %p125;
	add.s64 	%rl926, %rl568, %rl563;
	add.s32 	%r455, %r105, %r762;
	shl.b32 	%r456, %r455, 3;
	add.s32 	%r458, %r33, %r456;
	st.local.u64 	[%r458], %rl567;
	// inline asm
	min.s32 	%r449, %r442, %r104;
	// inline asm
	add.s32 	%r762, %r762, 1;
	setp.lt.s32 	%p126, %r762, %r449;
	@%p126 bra 	BB6_153;

BB6_154:
	mov.u32 	%r459, 1;
	sub.s32 	%r460, %r459, %r102;
	add.s32 	%r461, %r460, %r762;
	shl.b32 	%r462, %r461, 3;
	add.s32 	%r464, %r33, %r462;
	st.local.u64 	[%r464], %rl926;
	ld.local.u64 	%rl927, [%r33+24];
	ld.local.u64 	%rl928, [%r33+16];
	and.b32  	%r465, %r101, 63;
	setp.eq.s32 	%p127, %r465, 0;
	@%p127 bra 	BB6_156;

	and.b64  	%rl569, %rl127, 63;
	cvt.u32.u64 	%r466, %rl569;
	shl.b64 	%rl570, %rl927, %r466;
	neg.s32 	%r467, %r101;
	and.b32  	%r468, %r467, 63;
	shr.u64 	%rl571, %rl928, %r468;
	or.b64  	%rl927, %rl571, %rl570;
	shl.b64 	%rl572, %rl928, %r466;
	ld.local.u64 	%rl573, [%r33+8];
	shr.u64 	%rl574, %rl573, %r468;
	or.b64  	%rl928, %rl574, %rl572;

BB6_156:
	shr.u64 	%rl575, %rl927, 62;
	cvt.u32.u64 	%r469, %rl575;
	shr.u64 	%rl576, %rl928, 62;
	shl.b64 	%rl577, %rl927, 2;
	or.b64  	%rl933, %rl576, %rl577;
	shl.b64 	%rl139, %rl928, 2;
	setp.ne.s64 	%p128, %rl139, 0;
	selp.u64 	%rl578, 1, 0, %p128;
	or.b64  	%rl579, %rl578, %rl933;
	setp.gt.u64 	%p129, %rl579, -9223372036854775808;
	selp.u32 	%r470, 1, 0, %p129;
	add.s32 	%r471, %r470, %r469;
	neg.s32 	%r472, %r471;
	setp.lt.s64 	%p130, %rl125, 0;
	selp.b32 	%r764, %r472, %r471, %p130;
	@%p129 bra 	BB6_158;

	mov.u64 	%rl932, %rl139;
	bra.uni 	BB6_159;

BB6_158:
	not.b64 	%rl580, %rl933;
	neg.s64 	%rl140, %rl139;
	setp.eq.s64 	%p131, %rl139, 0;
	selp.u64 	%rl581, 1, 0, %p131;
	add.s64 	%rl933, %rl581, %rl580;
	xor.b64  	%rl929, %rl929, -9223372036854775808;
	mov.u64 	%rl932, %rl140;

BB6_159:
	mov.u64 	%rl931, %rl932;
	setp.gt.s64 	%p132, %rl933, 0;
	@%p132 bra 	BB6_161;

	mov.u32 	%r763, 0;
	bra.uni 	BB6_163;

BB6_161:
	mov.u32 	%r763, 0;

BB6_162:
	shr.u64 	%rl582, %rl931, 63;
	shl.b64 	%rl583, %rl933, 1;
	or.b64  	%rl933, %rl582, %rl583;
	shl.b64 	%rl931, %rl931, 1;
	add.s32 	%r763, %r763, -1;
	setp.gt.s64 	%p133, %rl933, 0;
	@%p133 bra 	BB6_162;

BB6_163:
	mul.lo.s64 	%rl935, %rl933, -3958705157555305931;
	mov.u64 	%rl586, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl584, %rl933, %rl586;
	// inline asm
	setp.gt.s64 	%p134, %rl584, 0;
	mov.u64 	%rl934, %rl584;
	@%p134 bra 	BB6_164;
	bra.uni 	BB6_165;

BB6_164:
	shl.b64 	%rl587, %rl584, 1;
	shr.u64 	%rl588, %rl935, 63;
	or.b64  	%rl934, %rl587, %rl588;
	mul.lo.s64 	%rl935, %rl933, -7917410315110611862;
	add.s32 	%r763, %r763, -1;

BB6_165:
	setp.ne.s64 	%p135, %rl935, 0;
	selp.u64 	%rl589, 1, 0, %p135;
	add.s64 	%rl590, %rl589, %rl934;
	add.s32 	%r475, %r763, 1022;
	cvt.u64.u32 	%rl591, %r475;
	shl.b64 	%rl592, %rl591, 52;
	shr.u64 	%rl593, %rl590, 11;
	shr.u64 	%rl594, %rl590, 10;
	and.b64  	%rl595, %rl594, 1;
	add.s64 	%rl596, %rl592, %rl593;
	add.s64 	%rl597, %rl596, %rl595;
	or.b64  	%rl598, %rl597, %rl929;
	mov.b64 	 %fd1430, %rl598;

BB6_166:
	add.s32 	%r116, %r764, 1;
	and.b32  	%r476, %r116, 1;
	setp.eq.s32 	%p136, %r476, 0;
	mul.rn.f64 	%fd61, %fd1430, %fd1430;
	@%p136 bra 	BB6_168;

	mov.f64 	%fd464, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd466, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd463, %fd464, %fd61, %fd466;
	// inline asm
	mov.f64 	%fd470, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd467, %fd463, %fd61, %fd470;
	// inline asm
	mov.f64 	%fd474, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd471, %fd467, %fd61, %fd474;
	// inline asm
	mov.f64 	%fd478, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd475, %fd471, %fd61, %fd478;
	// inline asm
	mov.f64 	%fd482, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd479, %fd475, %fd61, %fd482;
	// inline asm
	mov.f64 	%fd486, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd483, %fd479, %fd61, %fd486;
	// inline asm
	mov.f64 	%fd490, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd487, %fd483, %fd61, %fd490;
	// inline asm
	mov.f64 	%fd1431, %fd487;
	bra.uni 	BB6_169;

BB6_168:
	mov.f64 	%fd492, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd494, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd491, %fd492, %fd61, %fd494;
	// inline asm
	mov.f64 	%fd498, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd495, %fd491, %fd61, %fd498;
	// inline asm
	mov.f64 	%fd502, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd499, %fd495, %fd61, %fd502;
	// inline asm
	mov.f64 	%fd506, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd503, %fd499, %fd61, %fd506;
	// inline asm
	mov.f64 	%fd510, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd507, %fd503, %fd61, %fd510;
	// inline asm
	mul.rn.f64 	%fd512, %fd507, %fd61;
	// inline asm
	fma.rn.f64 	%fd511, %fd512, %fd1430, %fd1430;
	// inline asm
	mov.f64 	%fd1431, %fd511;

BB6_169:
	and.b32  	%r477, %r116, 2;
	setp.eq.s32 	%p137, %r477, 0;
	neg.f64 	%fd515, %fd1431;
	selp.f64 	%fd1432, %fd1431, %fd515, %p137;
	bra.uni 	BB6_171;

BB6_170:
	mov.f64 	%fd1432, 0dFFF8000000000000;

BB6_171:
	setp.eq.f64 	%p138, %fd57, 0d0000000000000000;
	or.pred  	%p139, %p10, %p138;
	or.pred  	%p140, %p9, %p139;
	@%p140 bra 	BB6_194;

	// inline asm
	abs.f64 	%fd517, %fd57;
	// inline asm
	setp.gt.f64 	%p141, %fd517, 0d41E0000000000000;
	@%p141 bra 	BB6_174;

	mov.f64 	%fd532, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd519, %fd57, %fd532;
	// inline asm
	cvt.rni.s32.f64 	%r478, %fd519;
	// inline asm
	cvt.rn.f64.s32 	%fd533, %r478;
	neg.f64 	%fd529, %fd533;
	mov.f64 	%fd522, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd520, %fd529, %fd522, %fd57;
	// inline asm
	mov.f64 	%fd526, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd524, %fd529, %fd526, %fd520;
	// inline asm
	mov.f64 	%fd530, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd528, %fd529, %fd530, %fd524;
	// inline asm
	mov.u32 	%r767, %r478;
	mov.f64 	%fd1433, %fd528;
	bra.uni 	BB6_190;

BB6_174:
	mov.b64 	 %rl157, %fd57;
	and.b64  	%rl939, %rl157, -9223372036854775808;
	shr.u64 	%rl159, %rl157, 52;
	and.b64  	%rl599, %rl159, 2047;
	add.s64 	%rl600, %rl599, 4294966272;
	cvt.u32.u64 	%r118, %rl600;
	shl.b64 	%rl601, %rl157, 11;
	or.b64  	%rl160, %rl601, -9223372036854775808;
	shr.u32 	%r482, %r118, 6;
	mov.u32 	%r483, 16;
	sub.s32 	%r119, %r483, %r482;
	mov.u32 	%r484, 15;
	sub.s32 	%r765, %r484, %r482;
	mov.u32 	%r485, 19;
	sub.s32 	%r121, %r485, %r482;
	mov.u32 	%r480, 18;
	// inline asm
	min.s32 	%r479, %r480, %r121;
	// inline asm
	setp.lt.s32 	%p142, %r765, %r479;
	@%p142 bra 	BB6_176;

	mov.u64 	%rl936, 0;
	bra.uni 	BB6_178;

BB6_176:
	mov.u32 	%r486, 1;
	sub.s32 	%r122, %r486, %r119;
	mov.u64 	%rl936, 0;

BB6_177:
	.pragma "nounroll";
	shl.b32 	%r490, %r765, 3;
	mov.u32 	%r491, __internal_i2opi_d;
	add.s32 	%r492, %r491, %r490;
	ld.const.u64 	%rl605, [%r492];
	mul.lo.s64 	%rl607, %rl605, %rl160;
	// inline asm
	mul.hi.u64 	%rl604, %rl605, %rl160;
	// inline asm
	mad.lo.s64 	%rl608, %rl605, %rl160, %rl936;
	setp.lt.u64 	%p143, %rl608, %rl607;
	selp.u64 	%rl609, 1, 0, %p143;
	add.s64 	%rl936, %rl609, %rl604;
	add.s32 	%r493, %r122, %r765;
	shl.b32 	%r494, %r493, 3;
	add.s32 	%r496, %r33, %r494;
	st.local.u64 	[%r496], %rl608;
	// inline asm
	min.s32 	%r487, %r480, %r121;
	// inline asm
	add.s32 	%r765, %r765, 1;
	setp.lt.s32 	%p144, %r765, %r487;
	@%p144 bra 	BB6_177;

BB6_178:
	mov.u32 	%r497, 1;
	sub.s32 	%r498, %r497, %r119;
	add.s32 	%r499, %r498, %r765;
	shl.b32 	%r500, %r499, 3;
	add.s32 	%r502, %r33, %r500;
	st.local.u64 	[%r502], %rl936;
	ld.local.u64 	%rl937, [%r33+24];
	ld.local.u64 	%rl938, [%r33+16];
	and.b32  	%r503, %r118, 63;
	setp.eq.s32 	%p145, %r503, 0;
	@%p145 bra 	BB6_180;

	and.b64  	%rl610, %rl159, 63;
	cvt.u32.u64 	%r504, %rl610;
	shl.b64 	%rl611, %rl937, %r504;
	neg.s32 	%r505, %r118;
	and.b32  	%r506, %r505, 63;
	shr.u64 	%rl612, %rl938, %r506;
	or.b64  	%rl937, %rl612, %rl611;
	shl.b64 	%rl613, %rl938, %r504;
	ld.local.u64 	%rl614, [%r33+8];
	shr.u64 	%rl615, %rl614, %r506;
	or.b64  	%rl938, %rl615, %rl613;

BB6_180:
	shr.u64 	%rl616, %rl937, 62;
	cvt.u32.u64 	%r507, %rl616;
	shr.u64 	%rl617, %rl938, 62;
	shl.b64 	%rl618, %rl937, 2;
	or.b64  	%rl943, %rl617, %rl618;
	shl.b64 	%rl171, %rl938, 2;
	setp.ne.s64 	%p146, %rl171, 0;
	selp.u64 	%rl619, 1, 0, %p146;
	or.b64  	%rl620, %rl619, %rl943;
	setp.gt.u64 	%p147, %rl620, -9223372036854775808;
	selp.u32 	%r508, 1, 0, %p147;
	add.s32 	%r509, %r508, %r507;
	neg.s32 	%r510, %r509;
	setp.lt.s64 	%p148, %rl157, 0;
	selp.b32 	%r767, %r510, %r509, %p148;
	@%p147 bra 	BB6_182;

	mov.u64 	%rl942, %rl171;
	bra.uni 	BB6_183;

BB6_182:
	not.b64 	%rl621, %rl943;
	neg.s64 	%rl172, %rl171;
	setp.eq.s64 	%p149, %rl171, 0;
	selp.u64 	%rl622, 1, 0, %p149;
	add.s64 	%rl943, %rl622, %rl621;
	xor.b64  	%rl939, %rl939, -9223372036854775808;
	mov.u64 	%rl942, %rl172;

BB6_183:
	mov.u64 	%rl941, %rl942;
	setp.gt.s64 	%p150, %rl943, 0;
	@%p150 bra 	BB6_185;

	mov.u32 	%r766, 0;
	bra.uni 	BB6_187;

BB6_185:
	mov.u32 	%r766, 0;

BB6_186:
	shr.u64 	%rl623, %rl941, 63;
	shl.b64 	%rl624, %rl943, 1;
	or.b64  	%rl943, %rl623, %rl624;
	shl.b64 	%rl941, %rl941, 1;
	add.s32 	%r766, %r766, -1;
	setp.gt.s64 	%p151, %rl943, 0;
	@%p151 bra 	BB6_186;

BB6_187:
	mul.lo.s64 	%rl945, %rl943, -3958705157555305931;
	mov.u64 	%rl627, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl625, %rl943, %rl627;
	// inline asm
	setp.gt.s64 	%p152, %rl625, 0;
	mov.u64 	%rl944, %rl625;
	@%p152 bra 	BB6_188;
	bra.uni 	BB6_189;

BB6_188:
	shl.b64 	%rl628, %rl625, 1;
	shr.u64 	%rl629, %rl945, 63;
	or.b64  	%rl944, %rl628, %rl629;
	mul.lo.s64 	%rl945, %rl943, -7917410315110611862;
	add.s32 	%r766, %r766, -1;

BB6_189:
	setp.ne.s64 	%p153, %rl945, 0;
	selp.u64 	%rl630, 1, 0, %p153;
	add.s64 	%rl631, %rl630, %rl944;
	add.s32 	%r513, %r766, 1022;
	cvt.u64.u32 	%rl632, %r513;
	shl.b64 	%rl633, %rl632, 52;
	shr.u64 	%rl634, %rl631, 11;
	shr.u64 	%rl635, %rl631, 10;
	and.b64  	%rl636, %rl635, 1;
	add.s64 	%rl637, %rl633, %rl634;
	add.s64 	%rl638, %rl637, %rl636;
	or.b64  	%rl639, %rl638, %rl939;
	mov.b64 	 %fd1433, %rl639;

BB6_190:
	and.b32  	%r514, %r767, 1;
	setp.eq.s32 	%p154, %r514, 0;
	mul.rn.f64 	%fd71, %fd1433, %fd1433;
	@%p154 bra 	BB6_192;

	mov.f64 	%fd535, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd537, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd534, %fd535, %fd71, %fd537;
	// inline asm
	mov.f64 	%fd541, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd538, %fd534, %fd71, %fd541;
	// inline asm
	mov.f64 	%fd545, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd542, %fd538, %fd71, %fd545;
	// inline asm
	mov.f64 	%fd549, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd546, %fd542, %fd71, %fd549;
	// inline asm
	mov.f64 	%fd553, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd550, %fd546, %fd71, %fd553;
	// inline asm
	mov.f64 	%fd557, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd554, %fd550, %fd71, %fd557;
	// inline asm
	mov.f64 	%fd561, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd558, %fd554, %fd71, %fd561;
	// inline asm
	mov.f64 	%fd1434, %fd558;
	bra.uni 	BB6_193;

BB6_192:
	mov.f64 	%fd563, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd565, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd562, %fd563, %fd71, %fd565;
	// inline asm
	mov.f64 	%fd569, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd566, %fd562, %fd71, %fd569;
	// inline asm
	mov.f64 	%fd573, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd570, %fd566, %fd71, %fd573;
	// inline asm
	mov.f64 	%fd577, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd574, %fd570, %fd71, %fd577;
	// inline asm
	mov.f64 	%fd581, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd578, %fd574, %fd71, %fd581;
	// inline asm
	mul.rn.f64 	%fd583, %fd578, %fd71;
	// inline asm
	fma.rn.f64 	%fd582, %fd583, %fd1433, %fd1433;
	// inline asm
	mov.f64 	%fd1434, %fd582;

BB6_193:
	and.b32  	%r515, %r767, 2;
	setp.eq.s32 	%p155, %r515, 0;
	neg.f64 	%fd586, %fd1434;
	selp.f64 	%fd1435, %fd1434, %fd586, %p155;
	bra.uni 	BB6_195;

BB6_194:
	mov.f64 	%fd587, 0d0000000000000000;
	mul.rn.f64 	%fd1435, %fd57, %fd587;

BB6_195:
	neg.f64 	%fd588, %fd1435;
	mov.f64 	%fd1474, %fd1432;
	mov.f64 	%fd1475, %fd588;
	@%p7 bra 	BB6_196;
	bra.uni 	BB6_197;

BB6_196:
	mov.f64 	%fd1474, %fd1432;
	mov.f64 	%fd1475, %fd1435;

BB6_197:
	mul.f64 	%fd590, %fd5, %fd1474;
	neg.f64 	%fd592, %fd6;
	fma.rn.f64 	%fd593, %fd592, %fd1475, %fd590;
	mul.f64 	%fd594, %fd6, %fd1474;
	fma.rn.f64 	%fd595, %fd5, %fd1475, %fd594;
	mul.f64 	%fd596, %fd13, 0d403921FB54442D18;
	div.rn.f64 	%fd78, %fd596, %fd14;
	setp.eq.f64 	%p11, %fd78, 0d7FF0000000000000;
	setp.eq.f64 	%p12, %fd78, 0dFFF0000000000000;
	or.pred  	%p156, %p11, %p12;
	mov.f64 	%fd1454, %fd141;
	mov.f64 	%fd1455, %fd142;
	mov.f64 	%fd1456, %fd291;
	mov.f64 	%fd1457, %fd293;
	mov.f64 	%fd1458, %fd442;
	mov.f64 	%fd1459, %fd444;
	mov.f64 	%fd1460, %fd593;
	mov.f64 	%fd1461, %fd595;
	@%p156 bra 	BB6_220;

	// inline asm
	abs.f64 	%fd597, %fd78;
	// inline asm
	setp.gt.f64 	%p157, %fd597, 0d41E0000000000000;
	@%p157 bra 	BB6_200;

	mov.f64 	%fd612, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd599, %fd78, %fd612;
	// inline asm
	cvt.rni.s32.f64 	%r516, %fd599;
	// inline asm
	cvt.rn.f64.s32 	%fd613, %r516;
	neg.f64 	%fd609, %fd613;
	mov.f64 	%fd602, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd600, %fd609, %fd602, %fd78;
	// inline asm
	mov.f64 	%fd606, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd604, %fd609, %fd606, %fd600;
	// inline asm
	mov.f64 	%fd610, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd608, %fd609, %fd610, %fd604;
	// inline asm
	mov.u32 	%r770, %r516;
	mov.f64 	%fd1436, %fd608;
	bra.uni 	BB6_216;

BB6_200:
	mov.b64 	 %rl189, %fd78;
	and.b64  	%rl949, %rl189, -9223372036854775808;
	shr.u64 	%rl191, %rl189, 52;
	and.b64  	%rl640, %rl191, 2047;
	add.s64 	%rl641, %rl640, 4294966272;
	cvt.u32.u64 	%r134, %rl641;
	shl.b64 	%rl642, %rl189, 11;
	or.b64  	%rl192, %rl642, -9223372036854775808;
	shr.u32 	%r520, %r134, 6;
	mov.u32 	%r521, 16;
	sub.s32 	%r135, %r521, %r520;
	mov.u32 	%r522, 15;
	sub.s32 	%r768, %r522, %r520;
	mov.u32 	%r523, 19;
	sub.s32 	%r137, %r523, %r520;
	mov.u32 	%r518, 18;
	// inline asm
	min.s32 	%r517, %r518, %r137;
	// inline asm
	setp.lt.s32 	%p158, %r768, %r517;
	@%p158 bra 	BB6_202;

	mov.u64 	%rl946, 0;
	bra.uni 	BB6_204;

BB6_202:
	mov.u32 	%r524, 1;
	sub.s32 	%r138, %r524, %r135;
	mov.u64 	%rl946, 0;

BB6_203:
	.pragma "nounroll";
	shl.b32 	%r528, %r768, 3;
	mov.u32 	%r529, __internal_i2opi_d;
	add.s32 	%r530, %r529, %r528;
	ld.const.u64 	%rl646, [%r530];
	mul.lo.s64 	%rl648, %rl646, %rl192;
	// inline asm
	mul.hi.u64 	%rl645, %rl646, %rl192;
	// inline asm
	mad.lo.s64 	%rl649, %rl646, %rl192, %rl946;
	setp.lt.u64 	%p159, %rl649, %rl648;
	selp.u64 	%rl650, 1, 0, %p159;
	add.s64 	%rl946, %rl650, %rl645;
	add.s32 	%r531, %r138, %r768;
	shl.b32 	%r532, %r531, 3;
	add.s32 	%r534, %r33, %r532;
	st.local.u64 	[%r534], %rl649;
	// inline asm
	min.s32 	%r525, %r518, %r137;
	// inline asm
	add.s32 	%r768, %r768, 1;
	setp.lt.s32 	%p160, %r768, %r525;
	@%p160 bra 	BB6_203;

BB6_204:
	mov.u32 	%r535, 1;
	sub.s32 	%r536, %r535, %r135;
	add.s32 	%r537, %r536, %r768;
	shl.b32 	%r538, %r537, 3;
	add.s32 	%r540, %r33, %r538;
	st.local.u64 	[%r540], %rl946;
	ld.local.u64 	%rl947, [%r33+24];
	ld.local.u64 	%rl948, [%r33+16];
	and.b32  	%r541, %r134, 63;
	setp.eq.s32 	%p161, %r541, 0;
	@%p161 bra 	BB6_206;

	and.b64  	%rl651, %rl191, 63;
	cvt.u32.u64 	%r542, %rl651;
	shl.b64 	%rl652, %rl947, %r542;
	neg.s32 	%r543, %r134;
	and.b32  	%r544, %r543, 63;
	shr.u64 	%rl653, %rl948, %r544;
	or.b64  	%rl947, %rl653, %rl652;
	shl.b64 	%rl654, %rl948, %r542;
	ld.local.u64 	%rl655, [%r33+8];
	shr.u64 	%rl656, %rl655, %r544;
	or.b64  	%rl948, %rl656, %rl654;

BB6_206:
	shr.u64 	%rl657, %rl947, 62;
	cvt.u32.u64 	%r545, %rl657;
	shr.u64 	%rl658, %rl948, 62;
	shl.b64 	%rl659, %rl947, 2;
	or.b64  	%rl953, %rl658, %rl659;
	shl.b64 	%rl203, %rl948, 2;
	setp.ne.s64 	%p162, %rl203, 0;
	selp.u64 	%rl660, 1, 0, %p162;
	or.b64  	%rl661, %rl660, %rl953;
	setp.gt.u64 	%p163, %rl661, -9223372036854775808;
	selp.u32 	%r546, 1, 0, %p163;
	add.s32 	%r547, %r546, %r545;
	neg.s32 	%r548, %r547;
	setp.lt.s64 	%p164, %rl189, 0;
	selp.b32 	%r770, %r548, %r547, %p164;
	@%p163 bra 	BB6_208;

	mov.u64 	%rl952, %rl203;
	bra.uni 	BB6_209;

BB6_208:
	not.b64 	%rl662, %rl953;
	neg.s64 	%rl204, %rl203;
	setp.eq.s64 	%p165, %rl203, 0;
	selp.u64 	%rl663, 1, 0, %p165;
	add.s64 	%rl953, %rl663, %rl662;
	xor.b64  	%rl949, %rl949, -9223372036854775808;
	mov.u64 	%rl952, %rl204;

BB6_209:
	mov.u64 	%rl951, %rl952;
	setp.gt.s64 	%p166, %rl953, 0;
	@%p166 bra 	BB6_211;

	mov.u32 	%r769, 0;
	bra.uni 	BB6_213;

BB6_211:
	mov.u32 	%r769, 0;

BB6_212:
	shr.u64 	%rl664, %rl951, 63;
	shl.b64 	%rl665, %rl953, 1;
	or.b64  	%rl953, %rl664, %rl665;
	shl.b64 	%rl951, %rl951, 1;
	add.s32 	%r769, %r769, -1;
	setp.gt.s64 	%p167, %rl953, 0;
	@%p167 bra 	BB6_212;

BB6_213:
	mul.lo.s64 	%rl955, %rl953, -3958705157555305931;
	mov.u64 	%rl668, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl666, %rl953, %rl668;
	// inline asm
	setp.gt.s64 	%p168, %rl666, 0;
	mov.u64 	%rl954, %rl666;
	@%p168 bra 	BB6_214;
	bra.uni 	BB6_215;

BB6_214:
	shl.b64 	%rl669, %rl666, 1;
	shr.u64 	%rl670, %rl955, 63;
	or.b64  	%rl954, %rl669, %rl670;
	mul.lo.s64 	%rl955, %rl953, -7917410315110611862;
	add.s32 	%r769, %r769, -1;

BB6_215:
	setp.ne.s64 	%p169, %rl955, 0;
	selp.u64 	%rl671, 1, 0, %p169;
	add.s64 	%rl672, %rl671, %rl954;
	add.s32 	%r551, %r769, 1022;
	cvt.u64.u32 	%rl673, %r551;
	shl.b64 	%rl674, %rl673, 52;
	shr.u64 	%rl675, %rl672, 11;
	shr.u64 	%rl676, %rl672, 10;
	and.b64  	%rl677, %rl676, 1;
	add.s64 	%rl678, %rl674, %rl675;
	add.s64 	%rl679, %rl678, %rl677;
	or.b64  	%rl680, %rl679, %rl949;
	mov.b64 	 %fd1436, %rl680;

BB6_216:
	add.s32 	%r149, %r770, 1;
	and.b32  	%r552, %r149, 1;
	setp.eq.s32 	%p170, %r552, 0;
	mul.rn.f64 	%fd82, %fd1436, %fd1436;
	@%p170 bra 	BB6_218;

	mov.f64 	%fd615, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd617, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd614, %fd615, %fd82, %fd617;
	// inline asm
	mov.f64 	%fd621, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd618, %fd614, %fd82, %fd621;
	// inline asm
	mov.f64 	%fd625, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd622, %fd618, %fd82, %fd625;
	// inline asm
	mov.f64 	%fd629, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd626, %fd622, %fd82, %fd629;
	// inline asm
	mov.f64 	%fd633, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd630, %fd626, %fd82, %fd633;
	// inline asm
	mov.f64 	%fd637, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd634, %fd630, %fd82, %fd637;
	// inline asm
	mov.f64 	%fd641, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd638, %fd634, %fd82, %fd641;
	// inline asm
	mov.f64 	%fd1437, %fd638;
	bra.uni 	BB6_219;

BB6_218:
	mov.f64 	%fd643, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd645, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd642, %fd643, %fd82, %fd645;
	// inline asm
	mov.f64 	%fd649, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd646, %fd642, %fd82, %fd649;
	// inline asm
	mov.f64 	%fd653, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd650, %fd646, %fd82, %fd653;
	// inline asm
	mov.f64 	%fd657, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd654, %fd650, %fd82, %fd657;
	// inline asm
	mov.f64 	%fd661, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd658, %fd654, %fd82, %fd661;
	// inline asm
	mul.rn.f64 	%fd663, %fd658, %fd82;
	// inline asm
	fma.rn.f64 	%fd662, %fd663, %fd1436, %fd1436;
	// inline asm
	mov.f64 	%fd1437, %fd662;

BB6_219:
	and.b32  	%r553, %r149, 2;
	setp.eq.s32 	%p171, %r553, 0;
	neg.f64 	%fd666, %fd1437;
	selp.f64 	%fd1438, %fd1437, %fd666, %p171;
	bra.uni 	BB6_221;

BB6_220:
	mov.f64 	%fd1438, 0dFFF8000000000000;

BB6_221:
	setp.eq.f64 	%p172, %fd78, 0d0000000000000000;
	or.pred  	%p173, %p12, %p172;
	or.pred  	%p174, %p11, %p173;
	@%p174 bra 	BB6_244;

	// inline asm
	abs.f64 	%fd668, %fd78;
	// inline asm
	setp.gt.f64 	%p175, %fd668, 0d41E0000000000000;
	@%p175 bra 	BB6_224;

	mov.f64 	%fd683, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd670, %fd78, %fd683;
	// inline asm
	cvt.rni.s32.f64 	%r554, %fd670;
	// inline asm
	cvt.rn.f64.s32 	%fd684, %r554;
	neg.f64 	%fd680, %fd684;
	mov.f64 	%fd673, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd671, %fd680, %fd673, %fd78;
	// inline asm
	mov.f64 	%fd677, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd675, %fd680, %fd677, %fd671;
	// inline asm
	mov.f64 	%fd681, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd679, %fd680, %fd681, %fd675;
	// inline asm
	mov.u32 	%r773, %r554;
	mov.f64 	%fd1439, %fd679;
	bra.uni 	BB6_240;

BB6_224:
	mov.b64 	 %rl221, %fd78;
	and.b64  	%rl959, %rl221, -9223372036854775808;
	shr.u64 	%rl223, %rl221, 52;
	and.b64  	%rl681, %rl223, 2047;
	add.s64 	%rl682, %rl681, 4294966272;
	cvt.u32.u64 	%r151, %rl682;
	shl.b64 	%rl683, %rl221, 11;
	or.b64  	%rl224, %rl683, -9223372036854775808;
	shr.u32 	%r558, %r151, 6;
	mov.u32 	%r559, 16;
	sub.s32 	%r152, %r559, %r558;
	mov.u32 	%r560, 15;
	sub.s32 	%r771, %r560, %r558;
	mov.u32 	%r561, 19;
	sub.s32 	%r154, %r561, %r558;
	mov.u32 	%r556, 18;
	// inline asm
	min.s32 	%r555, %r556, %r154;
	// inline asm
	setp.lt.s32 	%p176, %r771, %r555;
	@%p176 bra 	BB6_226;

	mov.u64 	%rl956, 0;
	bra.uni 	BB6_228;

BB6_226:
	mov.u32 	%r562, 1;
	sub.s32 	%r155, %r562, %r152;
	mov.u64 	%rl956, 0;

BB6_227:
	.pragma "nounroll";
	shl.b32 	%r566, %r771, 3;
	mov.u32 	%r567, __internal_i2opi_d;
	add.s32 	%r568, %r567, %r566;
	ld.const.u64 	%rl687, [%r568];
	mul.lo.s64 	%rl689, %rl687, %rl224;
	// inline asm
	mul.hi.u64 	%rl686, %rl687, %rl224;
	// inline asm
	mad.lo.s64 	%rl690, %rl687, %rl224, %rl956;
	setp.lt.u64 	%p177, %rl690, %rl689;
	selp.u64 	%rl691, 1, 0, %p177;
	add.s64 	%rl956, %rl691, %rl686;
	add.s32 	%r569, %r155, %r771;
	shl.b32 	%r570, %r569, 3;
	add.s32 	%r572, %r33, %r570;
	st.local.u64 	[%r572], %rl690;
	// inline asm
	min.s32 	%r563, %r556, %r154;
	// inline asm
	add.s32 	%r771, %r771, 1;
	setp.lt.s32 	%p178, %r771, %r563;
	@%p178 bra 	BB6_227;

BB6_228:
	mov.u32 	%r573, 1;
	sub.s32 	%r574, %r573, %r152;
	add.s32 	%r575, %r574, %r771;
	shl.b32 	%r576, %r575, 3;
	add.s32 	%r578, %r33, %r576;
	st.local.u64 	[%r578], %rl956;
	ld.local.u64 	%rl957, [%r33+24];
	ld.local.u64 	%rl958, [%r33+16];
	and.b32  	%r579, %r151, 63;
	setp.eq.s32 	%p179, %r579, 0;
	@%p179 bra 	BB6_230;

	and.b64  	%rl692, %rl223, 63;
	cvt.u32.u64 	%r580, %rl692;
	shl.b64 	%rl693, %rl957, %r580;
	neg.s32 	%r581, %r151;
	and.b32  	%r582, %r581, 63;
	shr.u64 	%rl694, %rl958, %r582;
	or.b64  	%rl957, %rl694, %rl693;
	shl.b64 	%rl695, %rl958, %r580;
	ld.local.u64 	%rl696, [%r33+8];
	shr.u64 	%rl697, %rl696, %r582;
	or.b64  	%rl958, %rl697, %rl695;

BB6_230:
	shr.u64 	%rl698, %rl957, 62;
	cvt.u32.u64 	%r583, %rl698;
	shr.u64 	%rl699, %rl958, 62;
	shl.b64 	%rl700, %rl957, 2;
	or.b64  	%rl963, %rl699, %rl700;
	shl.b64 	%rl235, %rl958, 2;
	setp.ne.s64 	%p180, %rl235, 0;
	selp.u64 	%rl701, 1, 0, %p180;
	or.b64  	%rl702, %rl701, %rl963;
	setp.gt.u64 	%p181, %rl702, -9223372036854775808;
	selp.u32 	%r584, 1, 0, %p181;
	add.s32 	%r585, %r584, %r583;
	neg.s32 	%r586, %r585;
	setp.lt.s64 	%p182, %rl221, 0;
	selp.b32 	%r773, %r586, %r585, %p182;
	@%p181 bra 	BB6_232;

	mov.u64 	%rl962, %rl235;
	bra.uni 	BB6_233;

BB6_232:
	not.b64 	%rl703, %rl963;
	neg.s64 	%rl236, %rl235;
	setp.eq.s64 	%p183, %rl235, 0;
	selp.u64 	%rl704, 1, 0, %p183;
	add.s64 	%rl963, %rl704, %rl703;
	xor.b64  	%rl959, %rl959, -9223372036854775808;
	mov.u64 	%rl962, %rl236;

BB6_233:
	mov.u64 	%rl961, %rl962;
	setp.gt.s64 	%p184, %rl963, 0;
	@%p184 bra 	BB6_235;

	mov.u32 	%r772, 0;
	bra.uni 	BB6_237;

BB6_235:
	mov.u32 	%r772, 0;

BB6_236:
	shr.u64 	%rl705, %rl961, 63;
	shl.b64 	%rl706, %rl963, 1;
	or.b64  	%rl963, %rl705, %rl706;
	shl.b64 	%rl961, %rl961, 1;
	add.s32 	%r772, %r772, -1;
	setp.gt.s64 	%p185, %rl963, 0;
	@%p185 bra 	BB6_236;

BB6_237:
	mul.lo.s64 	%rl965, %rl963, -3958705157555305931;
	mov.u64 	%rl709, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl707, %rl963, %rl709;
	// inline asm
	setp.gt.s64 	%p186, %rl707, 0;
	mov.u64 	%rl964, %rl707;
	@%p186 bra 	BB6_238;
	bra.uni 	BB6_239;

BB6_238:
	shl.b64 	%rl710, %rl707, 1;
	shr.u64 	%rl711, %rl965, 63;
	or.b64  	%rl964, %rl710, %rl711;
	mul.lo.s64 	%rl965, %rl963, -7917410315110611862;
	add.s32 	%r772, %r772, -1;

BB6_239:
	setp.ne.s64 	%p187, %rl965, 0;
	selp.u64 	%rl712, 1, 0, %p187;
	add.s64 	%rl713, %rl712, %rl964;
	add.s32 	%r589, %r772, 1022;
	cvt.u64.u32 	%rl714, %r589;
	shl.b64 	%rl715, %rl714, 52;
	shr.u64 	%rl716, %rl713, 11;
	shr.u64 	%rl717, %rl713, 10;
	and.b64  	%rl718, %rl717, 1;
	add.s64 	%rl719, %rl715, %rl716;
	add.s64 	%rl720, %rl719, %rl718;
	or.b64  	%rl721, %rl720, %rl959;
	mov.b64 	 %fd1439, %rl721;

BB6_240:
	and.b32  	%r590, %r773, 1;
	setp.eq.s32 	%p188, %r590, 0;
	mul.rn.f64 	%fd92, %fd1439, %fd1439;
	@%p188 bra 	BB6_242;

	mov.f64 	%fd686, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd688, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd685, %fd686, %fd92, %fd688;
	// inline asm
	mov.f64 	%fd692, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd689, %fd685, %fd92, %fd692;
	// inline asm
	mov.f64 	%fd696, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd693, %fd689, %fd92, %fd696;
	// inline asm
	mov.f64 	%fd700, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd697, %fd693, %fd92, %fd700;
	// inline asm
	mov.f64 	%fd704, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd701, %fd697, %fd92, %fd704;
	// inline asm
	mov.f64 	%fd708, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd705, %fd701, %fd92, %fd708;
	// inline asm
	mov.f64 	%fd712, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd709, %fd705, %fd92, %fd712;
	// inline asm
	mov.f64 	%fd1440, %fd709;
	bra.uni 	BB6_243;

BB6_242:
	mov.f64 	%fd714, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd716, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd713, %fd714, %fd92, %fd716;
	// inline asm
	mov.f64 	%fd720, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd717, %fd713, %fd92, %fd720;
	// inline asm
	mov.f64 	%fd724, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd721, %fd717, %fd92, %fd724;
	// inline asm
	mov.f64 	%fd728, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd725, %fd721, %fd92, %fd728;
	// inline asm
	mov.f64 	%fd732, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd729, %fd725, %fd92, %fd732;
	// inline asm
	mul.rn.f64 	%fd734, %fd729, %fd92;
	// inline asm
	fma.rn.f64 	%fd733, %fd734, %fd1439, %fd1439;
	// inline asm
	mov.f64 	%fd1440, %fd733;

BB6_243:
	and.b32  	%r591, %r773, 2;
	setp.eq.s32 	%p189, %r591, 0;
	neg.f64 	%fd737, %fd1440;
	selp.f64 	%fd1441, %fd1440, %fd737, %p189;
	bra.uni 	BB6_245;

BB6_244:
	mov.f64 	%fd738, 0d0000000000000000;
	mul.rn.f64 	%fd1441, %fd78, %fd738;

BB6_245:
	neg.f64 	%fd739, %fd1441;
	mov.f64 	%fd1472, %fd1438;
	mov.f64 	%fd1473, %fd739;
	@%p7 bra 	BB6_246;
	bra.uni 	BB6_247;

BB6_246:
	mov.f64 	%fd1472, %fd1438;
	mov.f64 	%fd1473, %fd1441;

BB6_247:
	mul.f64 	%fd741, %fd7, %fd1472;
	neg.f64 	%fd743, %fd8;
	fma.rn.f64 	%fd744, %fd743, %fd1473, %fd741;
	mul.f64 	%fd745, %fd8, %fd1472;
	fma.rn.f64 	%fd746, %fd7, %fd1473, %fd745;
	mov.f64 	%fd1462, %fd744;
	mov.f64 	%fd1463, %fd746;
	mul.f64 	%fd747, %fd13, 0d403F6A7A2955385E;
	div.rn.f64 	%fd99, %fd747, %fd14;
	setp.eq.f64 	%p13, %fd99, 0d7FF0000000000000;
	setp.eq.f64 	%p14, %fd99, 0dFFF0000000000000;
	or.pred  	%p190, %p13, %p14;
	@%p190 bra 	BB6_270;

	// inline asm
	abs.f64 	%fd748, %fd99;
	// inline asm
	setp.gt.f64 	%p191, %fd748, 0d41E0000000000000;
	@%p191 bra 	BB6_250;

	mov.f64 	%fd763, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd750, %fd99, %fd763;
	// inline asm
	cvt.rni.s32.f64 	%r592, %fd750;
	// inline asm
	cvt.rn.f64.s32 	%fd764, %r592;
	neg.f64 	%fd760, %fd764;
	mov.f64 	%fd753, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd751, %fd760, %fd753, %fd99;
	// inline asm
	mov.f64 	%fd757, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd755, %fd760, %fd757, %fd751;
	// inline asm
	mov.f64 	%fd761, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd759, %fd760, %fd761, %fd755;
	// inline asm
	mov.u32 	%r776, %r592;
	mov.f64 	%fd1442, %fd759;
	bra.uni 	BB6_266;

BB6_250:
	mov.b64 	 %rl253, %fd99;
	and.b64  	%rl969, %rl253, -9223372036854775808;
	shr.u64 	%rl255, %rl253, 52;
	and.b64  	%rl722, %rl255, 2047;
	add.s64 	%rl723, %rl722, 4294966272;
	cvt.u32.u64 	%r167, %rl723;
	shl.b64 	%rl724, %rl253, 11;
	or.b64  	%rl256, %rl724, -9223372036854775808;
	shr.u32 	%r596, %r167, 6;
	mov.u32 	%r597, 16;
	sub.s32 	%r168, %r597, %r596;
	mov.u32 	%r598, 15;
	sub.s32 	%r774, %r598, %r596;
	mov.u32 	%r599, 19;
	sub.s32 	%r170, %r599, %r596;
	mov.u32 	%r594, 18;
	// inline asm
	min.s32 	%r593, %r594, %r170;
	// inline asm
	setp.lt.s32 	%p192, %r774, %r593;
	@%p192 bra 	BB6_252;

	mov.u64 	%rl966, 0;
	bra.uni 	BB6_254;

BB6_252:
	mov.u32 	%r600, 1;
	sub.s32 	%r171, %r600, %r168;
	mov.u64 	%rl966, 0;

BB6_253:
	.pragma "nounroll";
	shl.b32 	%r604, %r774, 3;
	mov.u32 	%r605, __internal_i2opi_d;
	add.s32 	%r606, %r605, %r604;
	ld.const.u64 	%rl728, [%r606];
	mul.lo.s64 	%rl730, %rl728, %rl256;
	// inline asm
	mul.hi.u64 	%rl727, %rl728, %rl256;
	// inline asm
	mad.lo.s64 	%rl731, %rl728, %rl256, %rl966;
	setp.lt.u64 	%p193, %rl731, %rl730;
	selp.u64 	%rl732, 1, 0, %p193;
	add.s64 	%rl966, %rl732, %rl727;
	add.s32 	%r607, %r171, %r774;
	shl.b32 	%r608, %r607, 3;
	add.s32 	%r610, %r33, %r608;
	st.local.u64 	[%r610], %rl731;
	// inline asm
	min.s32 	%r601, %r594, %r170;
	// inline asm
	add.s32 	%r774, %r774, 1;
	setp.lt.s32 	%p194, %r774, %r601;
	@%p194 bra 	BB6_253;

BB6_254:
	mov.u32 	%r611, 1;
	sub.s32 	%r612, %r611, %r168;
	add.s32 	%r613, %r612, %r774;
	shl.b32 	%r614, %r613, 3;
	add.s32 	%r616, %r33, %r614;
	st.local.u64 	[%r616], %rl966;
	ld.local.u64 	%rl967, [%r33+24];
	ld.local.u64 	%rl968, [%r33+16];
	and.b32  	%r617, %r167, 63;
	setp.eq.s32 	%p195, %r617, 0;
	@%p195 bra 	BB6_256;

	and.b64  	%rl733, %rl255, 63;
	cvt.u32.u64 	%r618, %rl733;
	shl.b64 	%rl734, %rl967, %r618;
	neg.s32 	%r619, %r167;
	and.b32  	%r620, %r619, 63;
	shr.u64 	%rl735, %rl968, %r620;
	or.b64  	%rl967, %rl735, %rl734;
	shl.b64 	%rl736, %rl968, %r618;
	ld.local.u64 	%rl737, [%r33+8];
	shr.u64 	%rl738, %rl737, %r620;
	or.b64  	%rl968, %rl738, %rl736;

BB6_256:
	shr.u64 	%rl739, %rl967, 62;
	cvt.u32.u64 	%r621, %rl739;
	shr.u64 	%rl740, %rl968, 62;
	shl.b64 	%rl741, %rl967, 2;
	or.b64  	%rl973, %rl740, %rl741;
	shl.b64 	%rl267, %rl968, 2;
	setp.ne.s64 	%p196, %rl267, 0;
	selp.u64 	%rl742, 1, 0, %p196;
	or.b64  	%rl743, %rl742, %rl973;
	setp.gt.u64 	%p197, %rl743, -9223372036854775808;
	selp.u32 	%r622, 1, 0, %p197;
	add.s32 	%r623, %r622, %r621;
	neg.s32 	%r624, %r623;
	setp.lt.s64 	%p198, %rl253, 0;
	selp.b32 	%r776, %r624, %r623, %p198;
	@%p197 bra 	BB6_258;

	mov.u64 	%rl972, %rl267;
	bra.uni 	BB6_259;

BB6_258:
	not.b64 	%rl744, %rl973;
	neg.s64 	%rl268, %rl267;
	setp.eq.s64 	%p199, %rl267, 0;
	selp.u64 	%rl745, 1, 0, %p199;
	add.s64 	%rl973, %rl745, %rl744;
	xor.b64  	%rl969, %rl969, -9223372036854775808;
	mov.u64 	%rl972, %rl268;

BB6_259:
	mov.u64 	%rl971, %rl972;
	setp.gt.s64 	%p200, %rl973, 0;
	@%p200 bra 	BB6_261;

	mov.u32 	%r775, 0;
	bra.uni 	BB6_263;

BB6_261:
	mov.u32 	%r775, 0;

BB6_262:
	shr.u64 	%rl746, %rl971, 63;
	shl.b64 	%rl747, %rl973, 1;
	or.b64  	%rl973, %rl746, %rl747;
	shl.b64 	%rl971, %rl971, 1;
	add.s32 	%r775, %r775, -1;
	setp.gt.s64 	%p201, %rl973, 0;
	@%p201 bra 	BB6_262;

BB6_263:
	mul.lo.s64 	%rl975, %rl973, -3958705157555305931;
	mov.u64 	%rl750, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl748, %rl973, %rl750;
	// inline asm
	setp.gt.s64 	%p202, %rl748, 0;
	mov.u64 	%rl974, %rl748;
	@%p202 bra 	BB6_264;
	bra.uni 	BB6_265;

BB6_264:
	shl.b64 	%rl751, %rl748, 1;
	shr.u64 	%rl752, %rl975, 63;
	or.b64  	%rl974, %rl751, %rl752;
	mul.lo.s64 	%rl975, %rl973, -7917410315110611862;
	add.s32 	%r775, %r775, -1;

BB6_265:
	setp.ne.s64 	%p203, %rl975, 0;
	selp.u64 	%rl753, 1, 0, %p203;
	add.s64 	%rl754, %rl753, %rl974;
	add.s32 	%r627, %r775, 1022;
	cvt.u64.u32 	%rl755, %r627;
	shl.b64 	%rl756, %rl755, 52;
	shr.u64 	%rl757, %rl754, 11;
	shr.u64 	%rl758, %rl754, 10;
	and.b64  	%rl759, %rl758, 1;
	add.s64 	%rl760, %rl756, %rl757;
	add.s64 	%rl761, %rl760, %rl759;
	or.b64  	%rl762, %rl761, %rl969;
	mov.b64 	 %fd1442, %rl762;

BB6_266:
	add.s32 	%r182, %r776, 1;
	and.b32  	%r628, %r182, 1;
	setp.eq.s32 	%p204, %r628, 0;
	mul.rn.f64 	%fd103, %fd1442, %fd1442;
	@%p204 bra 	BB6_268;

	mov.f64 	%fd766, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd768, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd765, %fd766, %fd103, %fd768;
	// inline asm
	mov.f64 	%fd772, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd769, %fd765, %fd103, %fd772;
	// inline asm
	mov.f64 	%fd776, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd773, %fd769, %fd103, %fd776;
	// inline asm
	mov.f64 	%fd780, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd777, %fd773, %fd103, %fd780;
	// inline asm
	mov.f64 	%fd784, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd781, %fd777, %fd103, %fd784;
	// inline asm
	mov.f64 	%fd788, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd785, %fd781, %fd103, %fd788;
	// inline asm
	mov.f64 	%fd792, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd789, %fd785, %fd103, %fd792;
	// inline asm
	mov.f64 	%fd1443, %fd789;
	bra.uni 	BB6_269;

BB6_268:
	mov.f64 	%fd794, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd796, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd793, %fd794, %fd103, %fd796;
	// inline asm
	mov.f64 	%fd800, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd797, %fd793, %fd103, %fd800;
	// inline asm
	mov.f64 	%fd804, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd801, %fd797, %fd103, %fd804;
	// inline asm
	mov.f64 	%fd808, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd805, %fd801, %fd103, %fd808;
	// inline asm
	mov.f64 	%fd812, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd809, %fd805, %fd103, %fd812;
	// inline asm
	mul.rn.f64 	%fd814, %fd809, %fd103;
	// inline asm
	fma.rn.f64 	%fd813, %fd814, %fd1442, %fd1442;
	// inline asm
	mov.f64 	%fd1443, %fd813;

BB6_269:
	and.b32  	%r629, %r182, 2;
	setp.eq.s32 	%p205, %r629, 0;
	neg.f64 	%fd817, %fd1443;
	selp.f64 	%fd1444, %fd1443, %fd817, %p205;
	bra.uni 	BB6_271;

BB6_270:
	mov.f64 	%fd1444, 0dFFF8000000000000;

BB6_271:
	setp.eq.f64 	%p206, %fd99, 0d0000000000000000;
	or.pred  	%p207, %p14, %p206;
	or.pred  	%p208, %p13, %p207;
	@%p208 bra 	BB6_294;

	// inline asm
	abs.f64 	%fd819, %fd99;
	// inline asm
	setp.gt.f64 	%p209, %fd819, 0d41E0000000000000;
	@%p209 bra 	BB6_274;

	mov.f64 	%fd834, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd821, %fd99, %fd834;
	// inline asm
	cvt.rni.s32.f64 	%r630, %fd821;
	// inline asm
	cvt.rn.f64.s32 	%fd835, %r630;
	neg.f64 	%fd831, %fd835;
	mov.f64 	%fd824, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd822, %fd831, %fd824, %fd99;
	// inline asm
	mov.f64 	%fd828, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd826, %fd831, %fd828, %fd822;
	// inline asm
	mov.f64 	%fd832, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd830, %fd831, %fd832, %fd826;
	// inline asm
	mov.u32 	%r779, %r630;
	mov.f64 	%fd1445, %fd830;
	bra.uni 	BB6_290;

BB6_274:
	mov.b64 	 %rl285, %fd99;
	and.b64  	%rl979, %rl285, -9223372036854775808;
	shr.u64 	%rl287, %rl285, 52;
	and.b64  	%rl763, %rl287, 2047;
	add.s64 	%rl764, %rl763, 4294966272;
	cvt.u32.u64 	%r184, %rl764;
	shl.b64 	%rl765, %rl285, 11;
	or.b64  	%rl288, %rl765, -9223372036854775808;
	shr.u32 	%r634, %r184, 6;
	mov.u32 	%r635, 16;
	sub.s32 	%r185, %r635, %r634;
	mov.u32 	%r636, 15;
	sub.s32 	%r777, %r636, %r634;
	mov.u32 	%r637, 19;
	sub.s32 	%r187, %r637, %r634;
	mov.u32 	%r632, 18;
	// inline asm
	min.s32 	%r631, %r632, %r187;
	// inline asm
	setp.lt.s32 	%p210, %r777, %r631;
	@%p210 bra 	BB6_276;

	mov.u64 	%rl976, 0;
	bra.uni 	BB6_278;

BB6_276:
	mov.u32 	%r638, 1;
	sub.s32 	%r188, %r638, %r185;
	mov.u64 	%rl976, 0;

BB6_277:
	.pragma "nounroll";
	shl.b32 	%r642, %r777, 3;
	mov.u32 	%r643, __internal_i2opi_d;
	add.s32 	%r644, %r643, %r642;
	ld.const.u64 	%rl769, [%r644];
	mul.lo.s64 	%rl771, %rl769, %rl288;
	// inline asm
	mul.hi.u64 	%rl768, %rl769, %rl288;
	// inline asm
	mad.lo.s64 	%rl772, %rl769, %rl288, %rl976;
	setp.lt.u64 	%p211, %rl772, %rl771;
	selp.u64 	%rl773, 1, 0, %p211;
	add.s64 	%rl976, %rl773, %rl768;
	add.s32 	%r645, %r188, %r777;
	shl.b32 	%r646, %r645, 3;
	add.s32 	%r648, %r33, %r646;
	st.local.u64 	[%r648], %rl772;
	// inline asm
	min.s32 	%r639, %r632, %r187;
	// inline asm
	add.s32 	%r777, %r777, 1;
	setp.lt.s32 	%p212, %r777, %r639;
	@%p212 bra 	BB6_277;

BB6_278:
	mov.u32 	%r649, 1;
	sub.s32 	%r650, %r649, %r185;
	add.s32 	%r651, %r650, %r777;
	shl.b32 	%r652, %r651, 3;
	add.s32 	%r654, %r33, %r652;
	st.local.u64 	[%r654], %rl976;
	ld.local.u64 	%rl977, [%r33+24];
	ld.local.u64 	%rl978, [%r33+16];
	and.b32  	%r655, %r184, 63;
	setp.eq.s32 	%p213, %r655, 0;
	@%p213 bra 	BB6_280;

	and.b64  	%rl774, %rl287, 63;
	cvt.u32.u64 	%r656, %rl774;
	shl.b64 	%rl775, %rl977, %r656;
	neg.s32 	%r657, %r184;
	and.b32  	%r658, %r657, 63;
	shr.u64 	%rl776, %rl978, %r658;
	or.b64  	%rl977, %rl776, %rl775;
	shl.b64 	%rl777, %rl978, %r656;
	ld.local.u64 	%rl778, [%r33+8];
	shr.u64 	%rl779, %rl778, %r658;
	or.b64  	%rl978, %rl779, %rl777;

BB6_280:
	shr.u64 	%rl780, %rl977, 62;
	cvt.u32.u64 	%r659, %rl780;
	shr.u64 	%rl781, %rl978, 62;
	shl.b64 	%rl782, %rl977, 2;
	or.b64  	%rl983, %rl781, %rl782;
	shl.b64 	%rl299, %rl978, 2;
	setp.ne.s64 	%p214, %rl299, 0;
	selp.u64 	%rl783, 1, 0, %p214;
	or.b64  	%rl784, %rl783, %rl983;
	setp.gt.u64 	%p215, %rl784, -9223372036854775808;
	selp.u32 	%r660, 1, 0, %p215;
	add.s32 	%r661, %r660, %r659;
	neg.s32 	%r662, %r661;
	setp.lt.s64 	%p216, %rl285, 0;
	selp.b32 	%r779, %r662, %r661, %p216;
	@%p215 bra 	BB6_282;

	mov.u64 	%rl982, %rl299;
	bra.uni 	BB6_283;

BB6_282:
	not.b64 	%rl785, %rl983;
	neg.s64 	%rl300, %rl299;
	setp.eq.s64 	%p217, %rl299, 0;
	selp.u64 	%rl786, 1, 0, %p217;
	add.s64 	%rl983, %rl786, %rl785;
	xor.b64  	%rl979, %rl979, -9223372036854775808;
	mov.u64 	%rl982, %rl300;

BB6_283:
	mov.u64 	%rl981, %rl982;
	setp.gt.s64 	%p218, %rl983, 0;
	@%p218 bra 	BB6_285;

	mov.u32 	%r778, 0;
	bra.uni 	BB6_287;

BB6_285:
	mov.u32 	%r778, 0;

BB6_286:
	shr.u64 	%rl787, %rl981, 63;
	shl.b64 	%rl788, %rl983, 1;
	or.b64  	%rl983, %rl787, %rl788;
	shl.b64 	%rl981, %rl981, 1;
	add.s32 	%r778, %r778, -1;
	setp.gt.s64 	%p219, %rl983, 0;
	@%p219 bra 	BB6_286;

BB6_287:
	mul.lo.s64 	%rl985, %rl983, -3958705157555305931;
	mov.u64 	%rl791, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl789, %rl983, %rl791;
	// inline asm
	setp.gt.s64 	%p220, %rl789, 0;
	mov.u64 	%rl984, %rl789;
	@%p220 bra 	BB6_288;
	bra.uni 	BB6_289;

BB6_288:
	shl.b64 	%rl792, %rl789, 1;
	shr.u64 	%rl793, %rl985, 63;
	or.b64  	%rl984, %rl792, %rl793;
	mul.lo.s64 	%rl985, %rl983, -7917410315110611862;
	add.s32 	%r778, %r778, -1;

BB6_289:
	setp.ne.s64 	%p221, %rl985, 0;
	selp.u64 	%rl794, 1, 0, %p221;
	add.s64 	%rl795, %rl794, %rl984;
	add.s32 	%r665, %r778, 1022;
	cvt.u64.u32 	%rl796, %r665;
	shl.b64 	%rl797, %rl796, 52;
	shr.u64 	%rl798, %rl795, 11;
	shr.u64 	%rl799, %rl795, 10;
	and.b64  	%rl800, %rl799, 1;
	add.s64 	%rl801, %rl797, %rl798;
	add.s64 	%rl802, %rl801, %rl800;
	or.b64  	%rl803, %rl802, %rl979;
	mov.b64 	 %fd1445, %rl803;

BB6_290:
	and.b32  	%r666, %r779, 1;
	setp.eq.s32 	%p222, %r666, 0;
	mul.rn.f64 	%fd113, %fd1445, %fd1445;
	@%p222 bra 	BB6_292;

	mov.f64 	%fd837, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd839, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd836, %fd837, %fd113, %fd839;
	// inline asm
	mov.f64 	%fd843, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd840, %fd836, %fd113, %fd843;
	// inline asm
	mov.f64 	%fd847, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd844, %fd840, %fd113, %fd847;
	// inline asm
	mov.f64 	%fd851, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd848, %fd844, %fd113, %fd851;
	// inline asm
	mov.f64 	%fd855, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd852, %fd848, %fd113, %fd855;
	// inline asm
	mov.f64 	%fd859, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd856, %fd852, %fd113, %fd859;
	// inline asm
	mov.f64 	%fd863, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd860, %fd856, %fd113, %fd863;
	// inline asm
	mov.f64 	%fd1446, %fd860;
	bra.uni 	BB6_293;

BB6_292:
	mov.f64 	%fd865, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd867, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd864, %fd865, %fd113, %fd867;
	// inline asm
	mov.f64 	%fd871, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd868, %fd864, %fd113, %fd871;
	// inline asm
	mov.f64 	%fd875, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd872, %fd868, %fd113, %fd875;
	// inline asm
	mov.f64 	%fd879, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd876, %fd872, %fd113, %fd879;
	// inline asm
	mov.f64 	%fd883, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd880, %fd876, %fd113, %fd883;
	// inline asm
	mul.rn.f64 	%fd885, %fd880, %fd113;
	// inline asm
	fma.rn.f64 	%fd884, %fd885, %fd1445, %fd1445;
	// inline asm
	mov.f64 	%fd1446, %fd884;

BB6_293:
	and.b32  	%r667, %r779, 2;
	setp.eq.s32 	%p223, %r667, 0;
	neg.f64 	%fd888, %fd1446;
	selp.f64 	%fd1447, %fd1446, %fd888, %p223;
	bra.uni 	BB6_295;

BB6_294:
	mov.f64 	%fd889, 0d0000000000000000;
	mul.rn.f64 	%fd1447, %fd99, %fd889;

BB6_295:
	neg.f64 	%fd890, %fd1447;
	mov.f64 	%fd1470, %fd1444;
	mov.f64 	%fd1471, %fd890;
	@%p7 bra 	BB6_296;
	bra.uni 	BB6_297;

BB6_296:
	mov.f64 	%fd1470, %fd1444;
	mov.f64 	%fd1471, %fd1447;

BB6_297:
	mul.f64 	%fd892, %fd9, %fd1470;
	neg.f64 	%fd894, %fd10;
	fma.rn.f64 	%fd895, %fd894, %fd1471, %fd892;
	mul.f64 	%fd896, %fd10, %fd1470;
	fma.rn.f64 	%fd897, %fd9, %fd1471, %fd896;
	mov.f64 	%fd1464, %fd895;
	mov.f64 	%fd1465, %fd897;
	mul.f64 	%fd898, %fd13, 0d4042D97C7F3321D2;
	div.rn.f64 	%fd120, %fd898, %fd14;
	setp.eq.f64 	%p15, %fd120, 0d7FF0000000000000;
	setp.eq.f64 	%p16, %fd120, 0dFFF0000000000000;
	or.pred  	%p224, %p15, %p16;
	@%p224 bra 	BB6_320;

	// inline asm
	abs.f64 	%fd899, %fd120;
	// inline asm
	setp.gt.f64 	%p225, %fd899, 0d41E0000000000000;
	@%p225 bra 	BB6_300;

	mov.f64 	%fd914, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd901, %fd120, %fd914;
	// inline asm
	cvt.rni.s32.f64 	%r668, %fd901;
	// inline asm
	cvt.rn.f64.s32 	%fd915, %r668;
	neg.f64 	%fd911, %fd915;
	mov.f64 	%fd904, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd902, %fd911, %fd904, %fd120;
	// inline asm
	mov.f64 	%fd908, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd906, %fd911, %fd908, %fd902;
	// inline asm
	mov.f64 	%fd912, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd910, %fd911, %fd912, %fd906;
	// inline asm
	mov.u32 	%r782, %r668;
	mov.f64 	%fd1448, %fd910;
	bra.uni 	BB6_316;

BB6_300:
	mov.b64 	 %rl317, %fd120;
	and.b64  	%rl989, %rl317, -9223372036854775808;
	shr.u64 	%rl319, %rl317, 52;
	and.b64  	%rl804, %rl319, 2047;
	add.s64 	%rl805, %rl804, 4294966272;
	cvt.u32.u64 	%r200, %rl805;
	shl.b64 	%rl806, %rl317, 11;
	or.b64  	%rl320, %rl806, -9223372036854775808;
	shr.u32 	%r672, %r200, 6;
	mov.u32 	%r673, 16;
	sub.s32 	%r201, %r673, %r672;
	mov.u32 	%r674, 15;
	sub.s32 	%r780, %r674, %r672;
	mov.u32 	%r675, 19;
	sub.s32 	%r203, %r675, %r672;
	mov.u32 	%r670, 18;
	// inline asm
	min.s32 	%r669, %r670, %r203;
	// inline asm
	setp.lt.s32 	%p226, %r780, %r669;
	@%p226 bra 	BB6_302;

	mov.u64 	%rl986, 0;
	bra.uni 	BB6_304;

BB6_302:
	mov.u32 	%r676, 1;
	sub.s32 	%r204, %r676, %r201;
	mov.u64 	%rl986, 0;

BB6_303:
	.pragma "nounroll";
	shl.b32 	%r680, %r780, 3;
	mov.u32 	%r681, __internal_i2opi_d;
	add.s32 	%r682, %r681, %r680;
	ld.const.u64 	%rl810, [%r682];
	mul.lo.s64 	%rl812, %rl810, %rl320;
	// inline asm
	mul.hi.u64 	%rl809, %rl810, %rl320;
	// inline asm
	mad.lo.s64 	%rl813, %rl810, %rl320, %rl986;
	setp.lt.u64 	%p227, %rl813, %rl812;
	selp.u64 	%rl814, 1, 0, %p227;
	add.s64 	%rl986, %rl814, %rl809;
	add.s32 	%r683, %r204, %r780;
	shl.b32 	%r684, %r683, 3;
	add.s32 	%r686, %r33, %r684;
	st.local.u64 	[%r686], %rl813;
	// inline asm
	min.s32 	%r677, %r670, %r203;
	// inline asm
	add.s32 	%r780, %r780, 1;
	setp.lt.s32 	%p228, %r780, %r677;
	@%p228 bra 	BB6_303;

BB6_304:
	mov.u32 	%r687, 1;
	sub.s32 	%r688, %r687, %r201;
	add.s32 	%r689, %r688, %r780;
	shl.b32 	%r690, %r689, 3;
	add.s32 	%r692, %r33, %r690;
	st.local.u64 	[%r692], %rl986;
	ld.local.u64 	%rl987, [%r33+24];
	ld.local.u64 	%rl988, [%r33+16];
	and.b32  	%r693, %r200, 63;
	setp.eq.s32 	%p229, %r693, 0;
	@%p229 bra 	BB6_306;

	and.b64  	%rl815, %rl319, 63;
	cvt.u32.u64 	%r694, %rl815;
	shl.b64 	%rl816, %rl987, %r694;
	neg.s32 	%r695, %r200;
	and.b32  	%r696, %r695, 63;
	shr.u64 	%rl817, %rl988, %r696;
	or.b64  	%rl987, %rl817, %rl816;
	shl.b64 	%rl818, %rl988, %r694;
	ld.local.u64 	%rl819, [%r33+8];
	shr.u64 	%rl820, %rl819, %r696;
	or.b64  	%rl988, %rl820, %rl818;

BB6_306:
	shr.u64 	%rl821, %rl987, 62;
	cvt.u32.u64 	%r697, %rl821;
	shr.u64 	%rl822, %rl988, 62;
	shl.b64 	%rl823, %rl987, 2;
	or.b64  	%rl993, %rl822, %rl823;
	shl.b64 	%rl331, %rl988, 2;
	setp.ne.s64 	%p230, %rl331, 0;
	selp.u64 	%rl824, 1, 0, %p230;
	or.b64  	%rl825, %rl824, %rl993;
	setp.gt.u64 	%p231, %rl825, -9223372036854775808;
	selp.u32 	%r698, 1, 0, %p231;
	add.s32 	%r699, %r698, %r697;
	neg.s32 	%r700, %r699;
	setp.lt.s64 	%p232, %rl317, 0;
	selp.b32 	%r782, %r700, %r699, %p232;
	@%p231 bra 	BB6_308;

	mov.u64 	%rl992, %rl331;
	bra.uni 	BB6_309;

BB6_308:
	not.b64 	%rl826, %rl993;
	neg.s64 	%rl332, %rl331;
	setp.eq.s64 	%p233, %rl331, 0;
	selp.u64 	%rl827, 1, 0, %p233;
	add.s64 	%rl993, %rl827, %rl826;
	xor.b64  	%rl989, %rl989, -9223372036854775808;
	mov.u64 	%rl992, %rl332;

BB6_309:
	mov.u64 	%rl991, %rl992;
	setp.gt.s64 	%p234, %rl993, 0;
	@%p234 bra 	BB6_311;

	mov.u32 	%r781, 0;
	bra.uni 	BB6_313;

BB6_311:
	mov.u32 	%r781, 0;

BB6_312:
	shr.u64 	%rl828, %rl991, 63;
	shl.b64 	%rl829, %rl993, 1;
	or.b64  	%rl993, %rl828, %rl829;
	shl.b64 	%rl991, %rl991, 1;
	add.s32 	%r781, %r781, -1;
	setp.gt.s64 	%p235, %rl993, 0;
	@%p235 bra 	BB6_312;

BB6_313:
	mul.lo.s64 	%rl995, %rl993, -3958705157555305931;
	mov.u64 	%rl832, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl830, %rl993, %rl832;
	// inline asm
	setp.gt.s64 	%p236, %rl830, 0;
	mov.u64 	%rl994, %rl830;
	@%p236 bra 	BB6_314;
	bra.uni 	BB6_315;

BB6_314:
	shl.b64 	%rl833, %rl830, 1;
	shr.u64 	%rl834, %rl995, 63;
	or.b64  	%rl994, %rl833, %rl834;
	mul.lo.s64 	%rl995, %rl993, -7917410315110611862;
	add.s32 	%r781, %r781, -1;

BB6_315:
	setp.ne.s64 	%p237, %rl995, 0;
	selp.u64 	%rl835, 1, 0, %p237;
	add.s64 	%rl836, %rl835, %rl994;
	add.s32 	%r703, %r781, 1022;
	cvt.u64.u32 	%rl837, %r703;
	shl.b64 	%rl838, %rl837, 52;
	shr.u64 	%rl839, %rl836, 11;
	shr.u64 	%rl840, %rl836, 10;
	and.b64  	%rl841, %rl840, 1;
	add.s64 	%rl842, %rl838, %rl839;
	add.s64 	%rl843, %rl842, %rl841;
	or.b64  	%rl844, %rl843, %rl989;
	mov.b64 	 %fd1448, %rl844;

BB6_316:
	add.s32 	%r215, %r782, 1;
	and.b32  	%r704, %r215, 1;
	setp.eq.s32 	%p238, %r704, 0;
	mul.rn.f64 	%fd124, %fd1448, %fd1448;
	@%p238 bra 	BB6_318;

	mov.f64 	%fd917, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd919, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd916, %fd917, %fd124, %fd919;
	// inline asm
	mov.f64 	%fd923, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd920, %fd916, %fd124, %fd923;
	// inline asm
	mov.f64 	%fd927, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd924, %fd920, %fd124, %fd927;
	// inline asm
	mov.f64 	%fd931, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd928, %fd924, %fd124, %fd931;
	// inline asm
	mov.f64 	%fd935, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd932, %fd928, %fd124, %fd935;
	// inline asm
	mov.f64 	%fd939, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd936, %fd932, %fd124, %fd939;
	// inline asm
	mov.f64 	%fd943, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd940, %fd936, %fd124, %fd943;
	// inline asm
	mov.f64 	%fd1449, %fd940;
	bra.uni 	BB6_319;

BB6_318:
	mov.f64 	%fd945, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd947, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd944, %fd945, %fd124, %fd947;
	// inline asm
	mov.f64 	%fd951, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd948, %fd944, %fd124, %fd951;
	// inline asm
	mov.f64 	%fd955, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd952, %fd948, %fd124, %fd955;
	// inline asm
	mov.f64 	%fd959, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd956, %fd952, %fd124, %fd959;
	// inline asm
	mov.f64 	%fd963, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd960, %fd956, %fd124, %fd963;
	// inline asm
	mul.rn.f64 	%fd965, %fd960, %fd124;
	// inline asm
	fma.rn.f64 	%fd964, %fd965, %fd1448, %fd1448;
	// inline asm
	mov.f64 	%fd1449, %fd964;

BB6_319:
	and.b32  	%r705, %r215, 2;
	setp.eq.s32 	%p239, %r705, 0;
	neg.f64 	%fd968, %fd1449;
	selp.f64 	%fd1450, %fd1449, %fd968, %p239;
	bra.uni 	BB6_321;

BB6_320:
	mov.f64 	%fd1450, 0dFFF8000000000000;

BB6_321:
	setp.eq.f64 	%p240, %fd120, 0d0000000000000000;
	or.pred  	%p241, %p16, %p240;
	or.pred  	%p242, %p15, %p241;
	@%p242 bra 	BB6_344;

	// inline asm
	abs.f64 	%fd970, %fd120;
	// inline asm
	setp.gt.f64 	%p243, %fd970, 0d41E0000000000000;
	@%p243 bra 	BB6_324;

	mov.f64 	%fd985, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd972, %fd120, %fd985;
	// inline asm
	cvt.rni.s32.f64 	%r706, %fd972;
	// inline asm
	cvt.rn.f64.s32 	%fd986, %r706;
	neg.f64 	%fd982, %fd986;
	mov.f64 	%fd975, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd973, %fd982, %fd975, %fd120;
	// inline asm
	mov.f64 	%fd979, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd977, %fd982, %fd979, %fd973;
	// inline asm
	mov.f64 	%fd983, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd981, %fd982, %fd983, %fd977;
	// inline asm
	mov.u32 	%r785, %r706;
	mov.f64 	%fd1451, %fd981;
	bra.uni 	BB6_340;

BB6_324:
	mov.b64 	 %rl349, %fd120;
	and.b64  	%rl999, %rl349, -9223372036854775808;
	shr.u64 	%rl351, %rl349, 52;
	and.b64  	%rl845, %rl351, 2047;
	add.s64 	%rl846, %rl845, 4294966272;
	cvt.u32.u64 	%r217, %rl846;
	shl.b64 	%rl847, %rl349, 11;
	or.b64  	%rl352, %rl847, -9223372036854775808;
	shr.u32 	%r710, %r217, 6;
	mov.u32 	%r711, 16;
	sub.s32 	%r218, %r711, %r710;
	mov.u32 	%r712, 15;
	sub.s32 	%r783, %r712, %r710;
	mov.u32 	%r713, 19;
	sub.s32 	%r220, %r713, %r710;
	mov.u32 	%r708, 18;
	// inline asm
	min.s32 	%r707, %r708, %r220;
	// inline asm
	setp.lt.s32 	%p244, %r783, %r707;
	@%p244 bra 	BB6_326;

	mov.u64 	%rl996, 0;
	bra.uni 	BB6_328;

BB6_326:
	mov.u32 	%r714, 1;
	sub.s32 	%r221, %r714, %r218;
	mov.u64 	%rl996, 0;

BB6_327:
	.pragma "nounroll";
	shl.b32 	%r718, %r783, 3;
	mov.u32 	%r719, __internal_i2opi_d;
	add.s32 	%r720, %r719, %r718;
	ld.const.u64 	%rl851, [%r720];
	mul.lo.s64 	%rl853, %rl851, %rl352;
	// inline asm
	mul.hi.u64 	%rl850, %rl851, %rl352;
	// inline asm
	mad.lo.s64 	%rl854, %rl851, %rl352, %rl996;
	setp.lt.u64 	%p245, %rl854, %rl853;
	selp.u64 	%rl855, 1, 0, %p245;
	add.s64 	%rl996, %rl855, %rl850;
	add.s32 	%r721, %r221, %r783;
	shl.b32 	%r722, %r721, 3;
	add.s32 	%r724, %r33, %r722;
	st.local.u64 	[%r724], %rl854;
	// inline asm
	min.s32 	%r715, %r708, %r220;
	// inline asm
	add.s32 	%r783, %r783, 1;
	setp.lt.s32 	%p246, %r783, %r715;
	@%p246 bra 	BB6_327;

BB6_328:
	mov.u32 	%r725, 1;
	sub.s32 	%r726, %r725, %r218;
	add.s32 	%r727, %r726, %r783;
	shl.b32 	%r728, %r727, 3;
	add.s32 	%r730, %r33, %r728;
	st.local.u64 	[%r730], %rl996;
	ld.local.u64 	%rl997, [%r33+24];
	ld.local.u64 	%rl998, [%r33+16];
	and.b32  	%r731, %r217, 63;
	setp.eq.s32 	%p247, %r731, 0;
	@%p247 bra 	BB6_330;

	and.b64  	%rl856, %rl351, 63;
	cvt.u32.u64 	%r732, %rl856;
	shl.b64 	%rl857, %rl997, %r732;
	neg.s32 	%r733, %r217;
	and.b32  	%r734, %r733, 63;
	shr.u64 	%rl858, %rl998, %r734;
	or.b64  	%rl997, %rl858, %rl857;
	shl.b64 	%rl859, %rl998, %r732;
	ld.local.u64 	%rl860, [%r33+8];
	shr.u64 	%rl861, %rl860, %r734;
	or.b64  	%rl998, %rl861, %rl859;

BB6_330:
	shr.u64 	%rl862, %rl997, 62;
	cvt.u32.u64 	%r735, %rl862;
	shr.u64 	%rl863, %rl998, 62;
	shl.b64 	%rl864, %rl997, 2;
	or.b64  	%rl1003, %rl863, %rl864;
	shl.b64 	%rl363, %rl998, 2;
	setp.ne.s64 	%p248, %rl363, 0;
	selp.u64 	%rl865, 1, 0, %p248;
	or.b64  	%rl866, %rl865, %rl1003;
	setp.gt.u64 	%p249, %rl866, -9223372036854775808;
	selp.u32 	%r736, 1, 0, %p249;
	add.s32 	%r737, %r736, %r735;
	neg.s32 	%r738, %r737;
	setp.lt.s64 	%p250, %rl349, 0;
	selp.b32 	%r785, %r738, %r737, %p250;
	@%p249 bra 	BB6_332;

	mov.u64 	%rl1002, %rl363;
	bra.uni 	BB6_333;

BB6_332:
	not.b64 	%rl867, %rl1003;
	neg.s64 	%rl364, %rl363;
	setp.eq.s64 	%p251, %rl363, 0;
	selp.u64 	%rl868, 1, 0, %p251;
	add.s64 	%rl1003, %rl868, %rl867;
	xor.b64  	%rl999, %rl999, -9223372036854775808;
	mov.u64 	%rl1002, %rl364;

BB6_333:
	mov.u64 	%rl1001, %rl1002;
	setp.gt.s64 	%p252, %rl1003, 0;
	@%p252 bra 	BB6_335;

	mov.u32 	%r784, 0;
	bra.uni 	BB6_337;

BB6_335:
	mov.u32 	%r784, 0;

BB6_336:
	shr.u64 	%rl869, %rl1001, 63;
	shl.b64 	%rl870, %rl1003, 1;
	or.b64  	%rl1003, %rl869, %rl870;
	shl.b64 	%rl1001, %rl1001, 1;
	add.s32 	%r784, %r784, -1;
	setp.gt.s64 	%p253, %rl1003, 0;
	@%p253 bra 	BB6_336;

BB6_337:
	mul.lo.s64 	%rl1005, %rl1003, -3958705157555305931;
	mov.u64 	%rl873, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl871, %rl1003, %rl873;
	// inline asm
	setp.gt.s64 	%p254, %rl871, 0;
	mov.u64 	%rl1004, %rl871;
	@%p254 bra 	BB6_338;
	bra.uni 	BB6_339;

BB6_338:
	shl.b64 	%rl874, %rl871, 1;
	shr.u64 	%rl875, %rl1005, 63;
	or.b64  	%rl1004, %rl874, %rl875;
	mul.lo.s64 	%rl1005, %rl1003, -7917410315110611862;
	add.s32 	%r784, %r784, -1;

BB6_339:
	setp.ne.s64 	%p255, %rl1005, 0;
	selp.u64 	%rl876, 1, 0, %p255;
	add.s64 	%rl877, %rl876, %rl1004;
	add.s32 	%r741, %r784, 1022;
	cvt.u64.u32 	%rl878, %r741;
	shl.b64 	%rl879, %rl878, 52;
	shr.u64 	%rl880, %rl877, 11;
	shr.u64 	%rl881, %rl877, 10;
	and.b64  	%rl882, %rl881, 1;
	add.s64 	%rl883, %rl879, %rl880;
	add.s64 	%rl884, %rl883, %rl882;
	or.b64  	%rl885, %rl884, %rl999;
	mov.b64 	 %fd1451, %rl885;

BB6_340:
	and.b32  	%r742, %r785, 1;
	setp.eq.s32 	%p256, %r742, 0;
	mul.rn.f64 	%fd134, %fd1451, %fd1451;
	@%p256 bra 	BB6_342;

	mov.f64 	%fd988, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd990, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd987, %fd988, %fd134, %fd990;
	// inline asm
	mov.f64 	%fd994, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd991, %fd987, %fd134, %fd994;
	// inline asm
	mov.f64 	%fd998, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd995, %fd991, %fd134, %fd998;
	// inline asm
	mov.f64 	%fd1002, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd999, %fd995, %fd134, %fd1002;
	// inline asm
	mov.f64 	%fd1006, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1003, %fd999, %fd134, %fd1006;
	// inline asm
	mov.f64 	%fd1010, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1007, %fd1003, %fd134, %fd1010;
	// inline asm
	mov.f64 	%fd1014, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1011, %fd1007, %fd134, %fd1014;
	// inline asm
	mov.f64 	%fd1452, %fd1011;
	bra.uni 	BB6_343;

BB6_342:
	mov.f64 	%fd1016, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1018, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1015, %fd1016, %fd134, %fd1018;
	// inline asm
	mov.f64 	%fd1022, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1019, %fd1015, %fd134, %fd1022;
	// inline asm
	mov.f64 	%fd1026, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1023, %fd1019, %fd134, %fd1026;
	// inline asm
	mov.f64 	%fd1030, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1027, %fd1023, %fd134, %fd1030;
	// inline asm
	mov.f64 	%fd1034, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1031, %fd1027, %fd134, %fd1034;
	// inline asm
	mul.rn.f64 	%fd1036, %fd1031, %fd134;
	// inline asm
	fma.rn.f64 	%fd1035, %fd1036, %fd1451, %fd1451;
	// inline asm
	mov.f64 	%fd1452, %fd1035;

BB6_343:
	and.b32  	%r743, %r785, 2;
	setp.eq.s32 	%p257, %r743, 0;
	neg.f64 	%fd1039, %fd1452;
	selp.f64 	%fd1453, %fd1452, %fd1039, %p257;
	bra.uni 	BB6_345;

BB6_344:
	mov.f64 	%fd1040, 0d0000000000000000;
	mul.rn.f64 	%fd1453, %fd120, %fd1040;

BB6_345:
	neg.f64 	%fd1041, %fd1453;
	mov.f64 	%fd1468, %fd1450;
	mov.f64 	%fd1469, %fd1041;
	@%p7 bra 	BB6_346;
	bra.uni 	BB6_347;

BB6_346:
	mov.f64 	%fd1468, %fd1450;
	mov.f64 	%fd1469, %fd1453;

BB6_347:
	mul.f64 	%fd1043, %fd11, %fd1468;
	neg.f64 	%fd1045, %fd12;
	fma.rn.f64 	%fd1046, %fd1045, %fd1469, %fd1043;
	mul.f64 	%fd1047, %fd12, %fd1468;
	fma.rn.f64 	%fd1048, %fd11, %fd1469, %fd1047;
	mov.f64 	%fd1466, %fd1046;
	mov.f64 	%fd1467, %fd1048;

BB6_348:
	add.f64 	%fd1051, %fd1454, %fd1456;
	add.f64 	%fd1053, %fd1051, %fd1458;
	add.f64 	%fd1055, %fd1053, %fd1460;
	add.f64 	%fd1057, %fd1055, %fd1462;
	add.f64 	%fd1059, %fd1057, %fd1464;
	add.f64 	%fd1061, %fd1059, %fd1466;
	st.global.f64 	[%r26], %fd1061;
	add.f64 	%fd1064, %fd1455, %fd1457;
	add.f64 	%fd1066, %fd1064, %fd1459;
	add.f64 	%fd1068, %fd1066, %fd1461;
	add.f64 	%fd1070, %fd1068, %fd1463;
	add.f64 	%fd1072, %fd1070, %fd1465;
	add.f64 	%fd1074, %fd1072, %fd1467;
	st.global.f64 	[%r26+8], %fd1074;
	mul.f64 	%fd1075, %fd1456, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1076, %fd1457, 0d3FE904C37505DE4D;
	mov.f64 	%fd1077, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1078, %fd1456, 0d3FE3F3A0E28BEDD6, %fd1076;
	add.f64 	%fd1079, %fd1454, %fd1078;
	mul.f64 	%fd1080, %fd1458, 0dBFCC7B90E3024574;
	mul.f64 	%fd1081, %fd1459, 0d3FEF329C0558E96C;
	mov.f64 	%fd1082, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1083, %fd1458, 0dBFCC7B90E3024574, %fd1081;
	add.f64 	%fd1084, %fd1079, %fd1083;
	mul.f64 	%fd1085, %fd1460, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1086, %fd1461, 0d3FDBC4C04D71ABBF;
	mov.f64 	%fd1087, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1088, %fd1460, 0dBFECD4BCA9CB5C70, %fd1086;
	add.f64 	%fd1089, %fd1084, %fd1088;
	mul.f64 	%fd1090, %fd1462, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1091, %fd1463, 0d3FDBC4C04D71ABBF;
	neg.f64 	%fd1092, %fd1463;
	fma.rn.f64 	%fd1093, %fd1092, %fd1087, %fd1090;
	add.f64 	%fd1094, %fd1089, %fd1093;
	mul.f64 	%fd1095, %fd1464, 0dBFCC7B90E3024574;
	mul.f64 	%fd1096, %fd1465, 0d3FEF329C0558E96C;
	neg.f64 	%fd1097, %fd1465;
	fma.rn.f64 	%fd1098, %fd1097, %fd1082, %fd1095;
	add.f64 	%fd1099, %fd1094, %fd1098;
	mul.f64 	%fd1100, %fd1466, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1101, %fd1467, 0d3FE904C37505DE4D;
	neg.f64 	%fd1102, %fd1467;
	fma.rn.f64 	%fd1103, %fd1102, %fd1077, %fd1100;
	add.f64 	%fd1104, %fd1099, %fd1103;
	st.global.f64 	[%r27], %fd1104;
	mul.f64 	%fd1105, %fd1457, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1106, %fd1456, 0d3FE904C37505DE4D;
	neg.f64 	%fd1107, %fd1456;
	fma.rn.f64 	%fd1108, %fd1107, %fd1077, %fd1105;
	add.f64 	%fd1109, %fd1455, %fd1108;
	mul.f64 	%fd1110, %fd1459, 0dBFCC7B90E3024574;
	mul.f64 	%fd1111, %fd1458, 0d3FEF329C0558E96C;
	neg.f64 	%fd1112, %fd1458;
	fma.rn.f64 	%fd1113, %fd1112, %fd1082, %fd1110;
	add.f64 	%fd1114, %fd1109, %fd1113;
	mul.f64 	%fd1115, %fd1461, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1116, %fd1460, 0d3FDBC4C04D71ABBF;
	neg.f64 	%fd1117, %fd1460;
	fma.rn.f64 	%fd1118, %fd1117, %fd1087, %fd1115;
	add.f64 	%fd1119, %fd1114, %fd1118;
	mul.f64 	%fd1120, %fd1463, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1121, %fd1462, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1122, %fd1463, 0dBFECD4BCA9CB5C70, %fd1121;
	add.f64 	%fd1123, %fd1119, %fd1122;
	mul.f64 	%fd1124, %fd1465, 0dBFCC7B90E3024574;
	mul.f64 	%fd1125, %fd1464, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1126, %fd1465, 0dBFCC7B90E3024574, %fd1125;
	add.f64 	%fd1127, %fd1123, %fd1126;
	mul.f64 	%fd1128, %fd1467, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1129, %fd1466, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1130, %fd1467, 0d3FE3F3A0E28BEDD6, %fd1129;
	add.f64 	%fd1131, %fd1127, %fd1130;
	st.global.f64 	[%r27+8], %fd1131;
	mul.f64 	%fd1132, %fd1456, 0dBFCC7B90E3024574;
	mul.f64 	%fd1133, %fd1457, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1134, %fd1456, 0dBFCC7B90E3024574, %fd1133;
	add.f64 	%fd1135, %fd1454, %fd1134;
	mul.f64 	%fd1136, %fd1458, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1137, %fd1459, 0d3FDBC4C04D71ABBF;
	neg.f64 	%fd1138, %fd1459;
	fma.rn.f64 	%fd1139, %fd1138, %fd1087, %fd1136;
	add.f64 	%fd1140, %fd1135, %fd1139;
	mul.f64 	%fd1141, %fd1460, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1142, %fd1461, 0d3FE904C37505DE4D;
	neg.f64 	%fd1143, %fd1461;
	fma.rn.f64 	%fd1144, %fd1143, %fd1077, %fd1141;
	add.f64 	%fd1145, %fd1140, %fd1144;
	mul.f64 	%fd1146, %fd1462, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1147, %fd1463, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1148, %fd1462, 0d3FE3F3A0E28BEDD6, %fd1147;
	add.f64 	%fd1149, %fd1145, %fd1148;
	mul.f64 	%fd1150, %fd1464, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1151, %fd1465, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1152, %fd1464, 0dBFECD4BCA9CB5C70, %fd1151;
	add.f64 	%fd1153, %fd1149, %fd1152;
	mul.f64 	%fd1154, %fd1466, 0dBFCC7B90E3024574;
	mul.f64 	%fd1155, %fd1467, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1156, %fd1102, %fd1082, %fd1154;
	add.f64 	%fd1157, %fd1153, %fd1156;
	st.global.f64 	[%r28], %fd1157;
	mul.f64 	%fd1158, %fd1457, 0dBFCC7B90E3024574;
	mul.f64 	%fd1159, %fd1456, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1160, %fd1107, %fd1082, %fd1158;
	add.f64 	%fd1161, %fd1455, %fd1160;
	mul.f64 	%fd1162, %fd1459, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1163, %fd1458, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1164, %fd1459, 0dBFECD4BCA9CB5C70, %fd1163;
	add.f64 	%fd1165, %fd1161, %fd1164;
	mul.f64 	%fd1166, %fd1461, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1167, %fd1460, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1168, %fd1461, 0d3FE3F3A0E28BEDD6, %fd1167;
	add.f64 	%fd1169, %fd1165, %fd1168;
	mul.f64 	%fd1170, %fd1463, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1171, %fd1462, 0d3FE904C37505DE4D;
	neg.f64 	%fd1172, %fd1462;
	fma.rn.f64 	%fd1173, %fd1172, %fd1077, %fd1170;
	add.f64 	%fd1174, %fd1169, %fd1173;
	mul.f64 	%fd1175, %fd1465, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1176, %fd1464, 0d3FDBC4C04D71ABBF;
	neg.f64 	%fd1177, %fd1464;
	fma.rn.f64 	%fd1178, %fd1177, %fd1087, %fd1175;
	add.f64 	%fd1179, %fd1174, %fd1178;
	mul.f64 	%fd1180, %fd1467, 0dBFCC7B90E3024574;
	mul.f64 	%fd1181, %fd1466, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1182, %fd1467, 0dBFCC7B90E3024574, %fd1181;
	add.f64 	%fd1183, %fd1179, %fd1182;
	st.global.f64 	[%r28+8], %fd1183;
	mul.f64 	%fd1184, %fd1456, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1185, %fd1457, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1186, %fd1456, 0dBFECD4BCA9CB5C70, %fd1185;
	add.f64 	%fd1187, %fd1454, %fd1186;
	mul.f64 	%fd1188, %fd1458, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1189, %fd1459, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1190, %fd1138, %fd1077, %fd1188;
	add.f64 	%fd1191, %fd1187, %fd1190;
	mul.f64 	%fd1192, %fd1460, 0dBFCC7B90E3024574;
	mul.f64 	%fd1193, %fd1461, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1194, %fd1460, 0dBFCC7B90E3024574, %fd1193;
	add.f64 	%fd1195, %fd1191, %fd1194;
	mul.f64 	%fd1196, %fd1462, 0dBFCC7B90E3024574;
	mul.f64 	%fd1197, %fd1463, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1198, %fd1092, %fd1082, %fd1196;
	add.f64 	%fd1199, %fd1195, %fd1198;
	mul.f64 	%fd1200, %fd1464, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1201, %fd1465, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1202, %fd1464, 0d3FE3F3A0E28BEDD6, %fd1201;
	add.f64 	%fd1203, %fd1199, %fd1202;
	mul.f64 	%fd1204, %fd1466, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1205, %fd1467, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1206, %fd1102, %fd1087, %fd1204;
	add.f64 	%fd1207, %fd1203, %fd1206;
	st.global.f64 	[%r29], %fd1207;
	mul.f64 	%fd1208, %fd1457, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1209, %fd1456, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1210, %fd1107, %fd1087, %fd1208;
	add.f64 	%fd1211, %fd1455, %fd1210;
	mul.f64 	%fd1212, %fd1459, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1213, %fd1458, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1214, %fd1459, 0d3FE3F3A0E28BEDD6, %fd1213;
	add.f64 	%fd1215, %fd1211, %fd1214;
	mul.f64 	%fd1216, %fd1461, 0dBFCC7B90E3024574;
	mul.f64 	%fd1217, %fd1460, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1218, %fd1117, %fd1082, %fd1216;
	add.f64 	%fd1219, %fd1215, %fd1218;
	mul.f64 	%fd1220, %fd1463, 0dBFCC7B90E3024574;
	mul.f64 	%fd1221, %fd1462, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1222, %fd1463, 0dBFCC7B90E3024574, %fd1221;
	add.f64 	%fd1223, %fd1219, %fd1222;
	mul.f64 	%fd1224, %fd1465, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1225, %fd1464, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1226, %fd1177, %fd1077, %fd1224;
	add.f64 	%fd1227, %fd1223, %fd1226;
	mul.f64 	%fd1228, %fd1467, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1229, %fd1466, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1230, %fd1467, 0dBFECD4BCA9CB5C70, %fd1229;
	add.f64 	%fd1231, %fd1227, %fd1230;
	st.global.f64 	[%r29+8], %fd1231;
	neg.f64 	%fd1232, %fd1457;
	fma.rn.f64 	%fd1233, %fd1232, %fd1087, %fd1184;
	add.f64 	%fd1234, %fd1454, %fd1233;
	fma.rn.f64 	%fd1235, %fd1458, 0d3FE3F3A0E28BEDD6, %fd1189;
	add.f64 	%fd1236, %fd1234, %fd1235;
	fma.rn.f64 	%fd1237, %fd1143, %fd1082, %fd1192;
	add.f64 	%fd1238, %fd1236, %fd1237;
	fma.rn.f64 	%fd1239, %fd1462, 0dBFCC7B90E3024574, %fd1197;
	add.f64 	%fd1240, %fd1238, %fd1239;
	fma.rn.f64 	%fd1241, %fd1097, %fd1077, %fd1200;
	add.f64 	%fd1242, %fd1240, %fd1241;
	fma.rn.f64 	%fd1243, %fd1466, 0dBFECD4BCA9CB5C70, %fd1205;
	add.f64 	%fd1244, %fd1242, %fd1243;
	st.global.f64 	[%r30], %fd1244;
	fma.rn.f64 	%fd1245, %fd1457, 0dBFECD4BCA9CB5C70, %fd1209;
	add.f64 	%fd1246, %fd1455, %fd1245;
	fma.rn.f64 	%fd1247, %fd1112, %fd1077, %fd1212;
	add.f64 	%fd1248, %fd1246, %fd1247;
	fma.rn.f64 	%fd1249, %fd1461, 0dBFCC7B90E3024574, %fd1217;
	add.f64 	%fd1250, %fd1248, %fd1249;
	fma.rn.f64 	%fd1251, %fd1172, %fd1082, %fd1220;
	add.f64 	%fd1252, %fd1250, %fd1251;
	fma.rn.f64 	%fd1253, %fd1465, 0d3FE3F3A0E28BEDD6, %fd1225;
	add.f64 	%fd1254, %fd1252, %fd1253;
	neg.f64 	%fd1255, %fd1466;
	fma.rn.f64 	%fd1256, %fd1255, %fd1087, %fd1228;
	add.f64 	%fd1257, %fd1254, %fd1256;
	st.global.f64 	[%r30+8], %fd1257;
	fma.rn.f64 	%fd1258, %fd1232, %fd1082, %fd1132;
	add.f64 	%fd1259, %fd1454, %fd1258;
	fma.rn.f64 	%fd1260, %fd1458, 0dBFECD4BCA9CB5C70, %fd1137;
	add.f64 	%fd1261, %fd1259, %fd1260;
	fma.rn.f64 	%fd1262, %fd1460, 0d3FE3F3A0E28BEDD6, %fd1142;
	add.f64 	%fd1263, %fd1261, %fd1262;
	fma.rn.f64 	%fd1264, %fd1092, %fd1077, %fd1146;
	add.f64 	%fd1265, %fd1263, %fd1264;
	fma.rn.f64 	%fd1266, %fd1097, %fd1087, %fd1150;
	add.f64 	%fd1267, %fd1265, %fd1266;
	fma.rn.f64 	%fd1268, %fd1466, 0dBFCC7B90E3024574, %fd1155;
	add.f64 	%fd1269, %fd1267, %fd1268;
	st.global.f64 	[%r31], %fd1269;
	fma.rn.f64 	%fd1270, %fd1457, 0dBFCC7B90E3024574, %fd1159;
	add.f64 	%fd1271, %fd1455, %fd1270;
	fma.rn.f64 	%fd1272, %fd1112, %fd1087, %fd1162;
	add.f64 	%fd1273, %fd1271, %fd1272;
	fma.rn.f64 	%fd1274, %fd1117, %fd1077, %fd1166;
	add.f64 	%fd1275, %fd1273, %fd1274;
	fma.rn.f64 	%fd1276, %fd1463, 0d3FE3F3A0E28BEDD6, %fd1171;
	add.f64 	%fd1277, %fd1275, %fd1276;
	fma.rn.f64 	%fd1278, %fd1465, 0dBFECD4BCA9CB5C70, %fd1176;
	add.f64 	%fd1279, %fd1277, %fd1278;
	fma.rn.f64 	%fd1280, %fd1255, %fd1082, %fd1180;
	add.f64 	%fd1281, %fd1279, %fd1280;
	st.global.f64 	[%r31+8], %fd1281;
	fma.rn.f64 	%fd1282, %fd1232, %fd1077, %fd1075;
	add.f64 	%fd1283, %fd1454, %fd1282;
	fma.rn.f64 	%fd1284, %fd1138, %fd1082, %fd1080;
	add.f64 	%fd1285, %fd1283, %fd1284;
	fma.rn.f64 	%fd1286, %fd1143, %fd1087, %fd1085;
	add.f64 	%fd1287, %fd1285, %fd1286;
	fma.rn.f64 	%fd1288, %fd1462, 0dBFECD4BCA9CB5C70, %fd1091;
	add.f64 	%fd1289, %fd1287, %fd1288;
	fma.rn.f64 	%fd1290, %fd1464, 0dBFCC7B90E3024574, %fd1096;
	add.f64 	%fd1291, %fd1289, %fd1290;
	fma.rn.f64 	%fd1292, %fd1466, 0d3FE3F3A0E28BEDD6, %fd1101;
	add.f64 	%fd1293, %fd1291, %fd1292;
	st.global.f64 	[%r32], %fd1293;
	fma.rn.f64 	%fd1294, %fd1457, 0d3FE3F3A0E28BEDD6, %fd1106;
	add.f64 	%fd1295, %fd1455, %fd1294;
	fma.rn.f64 	%fd1296, %fd1459, 0dBFCC7B90E3024574, %fd1111;
	add.f64 	%fd1297, %fd1295, %fd1296;
	fma.rn.f64 	%fd1298, %fd1461, 0dBFECD4BCA9CB5C70, %fd1116;
	add.f64 	%fd1299, %fd1297, %fd1298;
	fma.rn.f64 	%fd1300, %fd1172, %fd1087, %fd1120;
	add.f64 	%fd1301, %fd1299, %fd1300;
	fma.rn.f64 	%fd1302, %fd1177, %fd1082, %fd1124;
	add.f64 	%fd1303, %fd1301, %fd1302;
	fma.rn.f64 	%fd1304, %fd1255, %fd1077, %fd1128;
	add.f64 	%fd1305, %fd1303, %fd1304;
	st.global.f64 	[%r32+8], %fd1305;
	ret;
}

.entry DIT5C2C(
	.param .u32 .ptr .global .align 8 DIT5C2C_param_0,
	.param .u32 DIT5C2C_param_1,
	.param .u32 DIT5C2C_param_2,
	.param .u32 DIT5C2C_param_3
)
{
	.local .align 8 .b8 	__local_depot7[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<953>;
	.reg .pred 	%p<183>;
	.reg .s32 	%r<553>;
	.reg .s64 	%rl<665>;


	mov.u32 	%SP, __local_depot7;
	ld.param.u32 	%r168, [DIT5C2C_param_2];
	// inline asm
	mov.u32 	%r164, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r165, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r166, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r167, %tid.x;
	// inline asm
	add.s32 	%r169, %r167, %r164;
	mad.lo.s32 	%r3, %r166, %r165, %r169;
	mul.hi.s32 	%r170, %r168, 780903145;
	shr.u32 	%r171, %r170, 31;
	shr.s32 	%r172, %r170, 1;
	add.s32 	%r4, %r172, %r171;
	mul.lo.s32 	%r173, %r4, 11;
	sub.s32 	%r5, %r168, %r173;
	setp.gt.s32 	%p16, %r168, 10;
	@%p16 bra 	BB7_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB7_23;

BB7_2:
	mov.f32 	%f1, 0f41300000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40A00000;
	add.f32 	%f2, %f47, 0f41300000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r526, 0;
	mov.u32 	%r525, 1;

BB7_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p17, %p3, %p3;
	@%p17 bra 	BB7_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mov.f32 	%f55, 0f40000000;
	mul.rn.f32 	%f56, %f55, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p18, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p19, %f1, %f52;
	and.pred  	%p4, %p18, %p19;
	setp.eq.f32 	%p20, %f48, 0f00000000;
	@%p20 bra 	BB7_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r8, %f58;
	shr.u32 	%r176, %r8, 23;
	and.b32  	%r177, %r176, 255;
	add.s32 	%r527, %r177, -127;
	setp.eq.s32 	%p21, %r177, 0;
	mov.f32 	%f280, %f58;
	@%p21 bra 	BB7_6;
	bra.uni 	BB7_7;

BB7_6:
	and.b32  	%r178, %r8, -2139095041;
	or.b32  	%r179, %r178, 1065353216;
	mov.b32 	 %f60, %r179;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r180, %f61;
	shr.u32 	%r181, %r180, 23;
	and.b32  	%r182, %r181, 255;
	add.s32 	%r527, %r182, -253;
	and.b32  	%r183, %r180, -2139095041;
	or.b32  	%r184, %r183, 1065353216;
	mov.b32 	 %f280, %r184;

BB7_7:
	mov.b32 	 %r185, %f280;
	and.b32  	%r186, %r185, -2139095041;
	or.b32  	%r187, %r186, 1065353216;
	mov.b32 	 %f281, %r187;
	setp.gt.f32 	%p22, %f281, 0f3FB504F3;
	@%p22 bra 	BB7_8;
	bra.uni 	BB7_9;

BB7_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r527, %r527, 1;

BB7_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f55, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r188, %f73;
	and.b32  	%r189, %r188, -4096;
	mov.b32 	 %f81, %r189;
	mov.b32 	 %r190, %f64;
	and.b32  	%r191, %r190, -4096;
	mov.b32 	 %f82, %r191;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f55, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r527;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p23, %f69, 0f77F684DF;
	@%p23 bra 	BB7_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB7_12;

BB7_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB7_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r14, %f131;
	setp.eq.s32 	%p24, %r14, 1118925336;
	@%p24 bra 	BB7_13;
	bra.uni 	BB7_14;

BB7_13:
	add.s32 	%r192, %r14, -1;
	mov.b32 	 %f133, %r192;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB7_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p25, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p25;
	setp.gt.f32 	%p26, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p26;
	setp.neu.f32 	%p27, %f19, %f3;
	@%p27 bra 	BB7_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB7_21;

BB7_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB7_21;

BB7_17:
	@%p3 bra 	BB7_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB7_21;

BB7_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB7_21;

BB7_20:
	mov.f32 	%f283, %f7;

BB7_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r525;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r525, %f157;
	add.s32 	%r526, %r526, 1;
	setp.lt.s32 	%p28, %r526, %r4;
	@%p28 bra 	BB7_3;

	cvt.rn.f32.s32 	%f284, %r525;

BB7_23:
	mov.f32 	%f159, 0f40A00000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r5;
	setp.eq.f32 	%p29, %f287, 0f00000000;
	@%p29 bra 	BB7_45;

	setp.nan.f32 	%p30, %f287, %f287;
	@%p30 bra 	BB7_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p31, %f287, 0f7F800000;
	setp.eq.f32 	%p32, %f287, 0fFF800000;
	or.pred  	%p33, %p31, %p32;
	@%p33 bra 	BB7_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p36, %f158, 0f00000000;
	@%p36 bra 	BB7_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r17, %f168;
	shr.u32 	%r193, %r17, 23;
	and.b32  	%r194, %r193, 255;
	add.s32 	%r528, %r194, -127;
	setp.eq.s32 	%p37, %r194, 0;
	mov.f32 	%f285, %f168;
	@%p37 bra 	BB7_28;
	bra.uni 	BB7_29;

BB7_28:
	and.b32  	%r195, %r17, -2139095041;
	or.b32  	%r196, %r195, 1065353216;
	mov.b32 	 %f170, %r196;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r197, %f171;
	shr.u32 	%r198, %r197, 23;
	and.b32  	%r199, %r198, 255;
	add.s32 	%r528, %r199, -253;
	and.b32  	%r200, %r197, -2139095041;
	or.b32  	%r201, %r200, 1065353216;
	mov.b32 	 %f285, %r201;

BB7_29:
	mov.b32 	 %r202, %f285;
	and.b32  	%r203, %r202, -2139095041;
	or.b32  	%r204, %r203, 1065353216;
	mov.b32 	 %f286, %r204;
	setp.gt.f32 	%p38, %f286, 0f3FB504F3;
	@%p38 bra 	BB7_30;
	bra.uni 	BB7_31;

BB7_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r528, %r528, 1;

BB7_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mov.f32 	%f182, 0f40000000;
	mul.rn.f32 	%f183, %f182, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r205, %f183;
	and.b32  	%r206, %r205, -4096;
	mov.b32 	 %f191, %r206;
	mov.b32 	 %r207, %f174;
	and.b32  	%r208, %r207, -4096;
	mov.b32 	 %f192, %r208;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f182, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r528;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p39, %f179, 0f77F684DF;
	@%p39 bra 	BB7_32;
	bra.uni 	BB7_33;

BB7_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB7_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r23, %f241;
	setp.eq.s32 	%p40, %r23, 1118925336;
	@%p40 bra 	BB7_34;
	bra.uni 	BB7_35;

BB7_34:
	add.s32 	%r209, %r23, -1;
	mov.b32 	 %f243, %r209;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB7_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p41, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p41;
	setp.gt.f32 	%p42, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p42;
	setp.neu.f32 	%p43, %f39, %f27;
	@%p43 bra 	BB7_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB7_46;

BB7_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB7_46;

BB7_38:
	setp.lt.f32 	%p44, %f287, 0f00000000;
	@%p44 bra 	BB7_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB7_46;

BB7_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB7_46;

BB7_41:
	setp.lt.f32 	%p45, %f158, 0f3F800000;
	mov.b32 	 %r210, %f287;
	setp.lt.s32 	%p6, %r210, 0;
	@%p45 bra 	BB7_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB7_46;

BB7_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB7_46;

BB7_44:
	add.f32 	%f288, %f287, 0f40A00000;
	bra.uni 	BB7_46;

BB7_45:
	mov.f32 	%f288, 0f3F800000;

BB7_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r24, %f267;
	mul.hi.s32 	%r211, %r24, 1717986919;
	shr.u32 	%r212, %r211, 31;
	shr.s32 	%r213, %r211, 1;
	add.s32 	%r214, %r213, %r212;
	div.s32 	%r215, %r3, %r214;
	rem.s32 	%r25, %r3, %r214;
	mad.lo.s32 	%r216, %r215, %r24, %r25;
	shl.b32 	%r217, %r216, 4;
	ld.param.u32 	%r523, [DIT5C2C_param_0];
	add.s32 	%r26, %r523, %r217;
	ld.global.f64 	%fd97, [%r26];
	ld.global.f64 	%fd98, [%r26+8];
	shl.b32 	%r218, %r214, 4;
	add.s32 	%r27, %r26, %r218;
	ld.global.f64 	%fd1, [%r27];
	ld.global.f64 	%fd2, [%r27+8];
	add.s32 	%r28, %r27, %r218;
	ld.global.f64 	%fd3, [%r28];
	ld.global.f64 	%fd4, [%r28+8];
	add.s32 	%r29, %r28, %r218;
	ld.global.f64 	%fd5, [%r29];
	ld.global.f64 	%fd6, [%r29+8];
	add.s32 	%r30, %r29, %r218;
	ld.global.f64 	%fd7, [%r30];
	ld.global.f64 	%fd8, [%r30+8];
	mov.f64 	%fd943, %fd7;
	mov.f64 	%fd944, %fd8;
	setp.eq.s32 	%p46, %r25, 0;
	mov.f64 	%fd935, %fd97;
	mov.f64 	%fd936, %fd98;
	mov.f64 	%fd937, %fd1;
	mov.f64 	%fd938, %fd2;
	mov.f64 	%fd939, %fd3;
	mov.f64 	%fd940, %fd4;
	mov.f64 	%fd941, %fd5;
	mov.f64 	%fd942, %fd6;
	@%p46 bra 	BB7_248;

	cvt.rn.f64.s32 	%fd9, %r25;
	mul.f64 	%fd99, %fd9, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd10, %r24;
	div.rn.f64 	%fd11, %fd99, %fd10;
	mov.f64 	%fd12, 0d7FF0000000000000;
	setp.eq.f64 	%p7, %fd11, 0d7FF0000000000000;
	mov.f64 	%fd13, 0dFFF0000000000000;
	setp.eq.f64 	%p8, %fd11, 0dFFF0000000000000;
	or.pred  	%p47, %p7, %p8;
	add.u32 	%r31, %SP, 0;
	@%p47 bra 	BB7_70;

	// inline asm
	abs.f64 	%fd100, %fd11;
	// inline asm
	setp.gt.f64 	%p48, %fd100, 0d41E0000000000000;
	@%p48 bra 	BB7_50;

	mov.f64 	%fd115, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd102, %fd11, %fd115;
	// inline asm
	cvt.rni.s32.f64 	%r219, %fd102;
	// inline asm
	cvt.rn.f64.s32 	%fd116, %r219;
	neg.f64 	%fd112, %fd116;
	mov.f64 	%fd105, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd103, %fd112, %fd105, %fd11;
	// inline asm
	mov.f64 	%fd109, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd107, %fd112, %fd109, %fd103;
	// inline asm
	mov.f64 	%fd113, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd111, %fd112, %fd113, %fd107;
	// inline asm
	mov.u32 	%r531, %r219;
	mov.f64 	%fd911, %fd111;
	bra.uni 	BB7_66;

BB7_50:
	mov.b64 	 %rl1, %fd11;
	and.b64  	%rl588, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl257, %rl3, 2047;
	add.s64 	%rl258, %rl257, 4294966272;
	cvt.u32.u64 	%r33, %rl258;
	shl.b64 	%rl259, %rl1, 11;
	or.b64  	%rl4, %rl259, -9223372036854775808;
	shr.u32 	%r223, %r33, 6;
	mov.u32 	%r224, 16;
	sub.s32 	%r34, %r224, %r223;
	mov.u32 	%r225, 15;
	sub.s32 	%r529, %r225, %r223;
	mov.u32 	%r226, 19;
	sub.s32 	%r36, %r226, %r223;
	mov.u32 	%r221, 18;
	// inline asm
	min.s32 	%r220, %r221, %r36;
	// inline asm
	setp.lt.s32 	%p49, %r529, %r220;
	@%p49 bra 	BB7_52;

	mov.u64 	%rl585, 0;
	bra.uni 	BB7_54;

BB7_52:
	mov.u32 	%r227, 1;
	sub.s32 	%r37, %r227, %r34;
	mov.u64 	%rl585, 0;

BB7_53:
	.pragma "nounroll";
	shl.b32 	%r231, %r529, 3;
	mov.u32 	%r232, __internal_i2opi_d;
	add.s32 	%r233, %r232, %r231;
	ld.const.u64 	%rl263, [%r233];
	mul.lo.s64 	%rl265, %rl263, %rl4;
	// inline asm
	mul.hi.u64 	%rl262, %rl263, %rl4;
	// inline asm
	mad.lo.s64 	%rl266, %rl263, %rl4, %rl585;
	setp.lt.u64 	%p50, %rl266, %rl265;
	selp.u64 	%rl267, 1, 0, %p50;
	add.s64 	%rl585, %rl267, %rl262;
	add.s32 	%r234, %r37, %r529;
	shl.b32 	%r235, %r234, 3;
	add.s32 	%r237, %r31, %r235;
	st.local.u64 	[%r237], %rl266;
	// inline asm
	min.s32 	%r228, %r221, %r36;
	// inline asm
	add.s32 	%r529, %r529, 1;
	setp.lt.s32 	%p51, %r529, %r228;
	@%p51 bra 	BB7_53;

BB7_54:
	mov.u32 	%r238, 1;
	sub.s32 	%r239, %r238, %r34;
	add.s32 	%r240, %r239, %r529;
	shl.b32 	%r241, %r240, 3;
	add.s32 	%r243, %r31, %r241;
	st.local.u64 	[%r243], %rl585;
	ld.local.u64 	%rl586, [%r31+24];
	ld.local.u64 	%rl587, [%r31+16];
	and.b32  	%r244, %r33, 63;
	setp.eq.s32 	%p52, %r244, 0;
	@%p52 bra 	BB7_56;

	and.b64  	%rl268, %rl3, 63;
	cvt.u32.u64 	%r245, %rl268;
	shl.b64 	%rl269, %rl586, %r245;
	neg.s32 	%r246, %r33;
	and.b32  	%r247, %r246, 63;
	shr.u64 	%rl270, %rl587, %r247;
	or.b64  	%rl586, %rl270, %rl269;
	shl.b64 	%rl271, %rl587, %r245;
	ld.local.u64 	%rl272, [%r31+8];
	shr.u64 	%rl273, %rl272, %r247;
	or.b64  	%rl587, %rl273, %rl271;

BB7_56:
	shr.u64 	%rl274, %rl586, 62;
	cvt.u32.u64 	%r248, %rl274;
	shr.u64 	%rl275, %rl587, 62;
	shl.b64 	%rl276, %rl586, 2;
	or.b64  	%rl592, %rl275, %rl276;
	shl.b64 	%rl15, %rl587, 2;
	setp.ne.s64 	%p53, %rl15, 0;
	selp.u64 	%rl277, 1, 0, %p53;
	or.b64  	%rl278, %rl277, %rl592;
	setp.gt.u64 	%p54, %rl278, -9223372036854775808;
	selp.u32 	%r249, 1, 0, %p54;
	add.s32 	%r250, %r249, %r248;
	neg.s32 	%r251, %r250;
	setp.lt.s64 	%p55, %rl1, 0;
	selp.b32 	%r531, %r251, %r250, %p55;
	@%p54 bra 	BB7_58;

	mov.u64 	%rl591, %rl15;
	bra.uni 	BB7_59;

BB7_58:
	not.b64 	%rl279, %rl592;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p56, %rl15, 0;
	selp.u64 	%rl280, 1, 0, %p56;
	add.s64 	%rl592, %rl280, %rl279;
	xor.b64  	%rl588, %rl588, -9223372036854775808;
	mov.u64 	%rl591, %rl16;

BB7_59:
	mov.u64 	%rl590, %rl591;
	setp.gt.s64 	%p57, %rl592, 0;
	@%p57 bra 	BB7_61;

	mov.u32 	%r530, 0;
	bra.uni 	BB7_63;

BB7_61:
	mov.u32 	%r530, 0;

BB7_62:
	shr.u64 	%rl281, %rl590, 63;
	shl.b64 	%rl282, %rl592, 1;
	or.b64  	%rl592, %rl281, %rl282;
	shl.b64 	%rl590, %rl590, 1;
	add.s32 	%r530, %r530, -1;
	setp.gt.s64 	%p58, %rl592, 0;
	@%p58 bra 	BB7_62;

BB7_63:
	mul.lo.s64 	%rl594, %rl592, -3958705157555305931;
	mov.u64 	%rl285, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl283, %rl592, %rl285;
	// inline asm
	setp.gt.s64 	%p59, %rl283, 0;
	mov.u64 	%rl593, %rl283;
	@%p59 bra 	BB7_64;
	bra.uni 	BB7_65;

BB7_64:
	shl.b64 	%rl286, %rl283, 1;
	shr.u64 	%rl287, %rl594, 63;
	or.b64  	%rl593, %rl286, %rl287;
	mul.lo.s64 	%rl594, %rl592, -7917410315110611862;
	add.s32 	%r530, %r530, -1;

BB7_65:
	setp.ne.s64 	%p60, %rl594, 0;
	selp.u64 	%rl288, 1, 0, %p60;
	add.s64 	%rl289, %rl288, %rl593;
	add.s32 	%r254, %r530, 1022;
	cvt.u64.u32 	%rl290, %r254;
	shl.b64 	%rl291, %rl290, 52;
	shr.u64 	%rl292, %rl289, 11;
	shr.u64 	%rl293, %rl289, 10;
	and.b64  	%rl294, %rl293, 1;
	add.s64 	%rl295, %rl291, %rl292;
	add.s64 	%rl296, %rl295, %rl294;
	or.b64  	%rl297, %rl296, %rl588;
	mov.b64 	 %fd911, %rl297;

BB7_66:
	add.s32 	%r48, %r531, 1;
	and.b32  	%r255, %r48, 1;
	setp.eq.s32 	%p61, %r255, 0;
	mul.rn.f64 	%fd17, %fd911, %fd911;
	@%p61 bra 	BB7_68;

	mov.f64 	%fd118, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd120, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd117, %fd118, %fd17, %fd120;
	// inline asm
	mov.f64 	%fd124, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd121, %fd117, %fd17, %fd124;
	// inline asm
	mov.f64 	%fd128, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd125, %fd121, %fd17, %fd128;
	// inline asm
	mov.f64 	%fd132, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd129, %fd125, %fd17, %fd132;
	// inline asm
	mov.f64 	%fd136, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd133, %fd129, %fd17, %fd136;
	// inline asm
	mov.f64 	%fd140, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd137, %fd133, %fd17, %fd140;
	// inline asm
	mov.f64 	%fd144, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd141, %fd137, %fd17, %fd144;
	// inline asm
	mov.f64 	%fd912, %fd141;
	bra.uni 	BB7_69;

BB7_68:
	mov.f64 	%fd146, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd148, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd145, %fd146, %fd17, %fd148;
	// inline asm
	mov.f64 	%fd152, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd149, %fd145, %fd17, %fd152;
	// inline asm
	mov.f64 	%fd156, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd153, %fd149, %fd17, %fd156;
	// inline asm
	mov.f64 	%fd160, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd157, %fd153, %fd17, %fd160;
	// inline asm
	mov.f64 	%fd164, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd161, %fd157, %fd17, %fd164;
	// inline asm
	mul.rn.f64 	%fd166, %fd161, %fd17;
	// inline asm
	fma.rn.f64 	%fd165, %fd166, %fd911, %fd911;
	// inline asm
	mov.f64 	%fd912, %fd165;

BB7_69:
	and.b32  	%r256, %r48, 2;
	setp.eq.s32 	%p62, %r256, 0;
	neg.f64 	%fd169, %fd912;
	selp.f64 	%fd913, %fd912, %fd169, %p62;
	bra.uni 	BB7_71;

BB7_70:
	mov.f64 	%fd913, 0dFFF8000000000000;

BB7_71:
	setp.eq.f64 	%p63, %fd11, 0d0000000000000000;
	or.pred  	%p64, %p8, %p63;
	or.pred  	%p65, %p7, %p64;
	@%p65 bra 	BB7_94;

	// inline asm
	abs.f64 	%fd171, %fd11;
	// inline asm
	setp.gt.f64 	%p66, %fd171, 0d41E0000000000000;
	@%p66 bra 	BB7_74;

	mov.f64 	%fd186, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd173, %fd11, %fd186;
	// inline asm
	cvt.rni.s32.f64 	%r257, %fd173;
	// inline asm
	cvt.rn.f64.s32 	%fd187, %r257;
	neg.f64 	%fd183, %fd187;
	mov.f64 	%fd176, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd174, %fd183, %fd176, %fd11;
	// inline asm
	mov.f64 	%fd180, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd178, %fd183, %fd180, %fd174;
	// inline asm
	mov.f64 	%fd184, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd182, %fd183, %fd184, %fd178;
	// inline asm
	mov.u32 	%r534, %r257;
	mov.f64 	%fd914, %fd182;
	bra.uni 	BB7_90;

BB7_74:
	mov.b64 	 %rl33, %fd11;
	and.b64  	%rl598, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl298, %rl35, 2047;
	add.s64 	%rl299, %rl298, 4294966272;
	cvt.u32.u64 	%r50, %rl299;
	shl.b64 	%rl300, %rl33, 11;
	or.b64  	%rl36, %rl300, -9223372036854775808;
	shr.u32 	%r261, %r50, 6;
	mov.u32 	%r262, 16;
	sub.s32 	%r51, %r262, %r261;
	mov.u32 	%r263, 15;
	sub.s32 	%r532, %r263, %r261;
	mov.u32 	%r264, 19;
	sub.s32 	%r53, %r264, %r261;
	mov.u32 	%r259, 18;
	// inline asm
	min.s32 	%r258, %r259, %r53;
	// inline asm
	setp.lt.s32 	%p67, %r532, %r258;
	@%p67 bra 	BB7_76;

	mov.u64 	%rl595, 0;
	bra.uni 	BB7_78;

BB7_76:
	mov.u32 	%r265, 1;
	sub.s32 	%r54, %r265, %r51;
	mov.u64 	%rl595, 0;

BB7_77:
	.pragma "nounroll";
	shl.b32 	%r269, %r532, 3;
	mov.u32 	%r270, __internal_i2opi_d;
	add.s32 	%r271, %r270, %r269;
	ld.const.u64 	%rl304, [%r271];
	mul.lo.s64 	%rl306, %rl304, %rl36;
	// inline asm
	mul.hi.u64 	%rl303, %rl304, %rl36;
	// inline asm
	mad.lo.s64 	%rl307, %rl304, %rl36, %rl595;
	setp.lt.u64 	%p68, %rl307, %rl306;
	selp.u64 	%rl308, 1, 0, %p68;
	add.s64 	%rl595, %rl308, %rl303;
	add.s32 	%r272, %r54, %r532;
	shl.b32 	%r273, %r272, 3;
	add.s32 	%r275, %r31, %r273;
	st.local.u64 	[%r275], %rl307;
	// inline asm
	min.s32 	%r266, %r259, %r53;
	// inline asm
	add.s32 	%r532, %r532, 1;
	setp.lt.s32 	%p69, %r532, %r266;
	@%p69 bra 	BB7_77;

BB7_78:
	mov.u32 	%r276, 1;
	sub.s32 	%r277, %r276, %r51;
	add.s32 	%r278, %r277, %r532;
	shl.b32 	%r279, %r278, 3;
	add.s32 	%r281, %r31, %r279;
	st.local.u64 	[%r281], %rl595;
	ld.local.u64 	%rl596, [%r31+24];
	ld.local.u64 	%rl597, [%r31+16];
	and.b32  	%r282, %r50, 63;
	setp.eq.s32 	%p70, %r282, 0;
	@%p70 bra 	BB7_80;

	and.b64  	%rl309, %rl35, 63;
	cvt.u32.u64 	%r283, %rl309;
	shl.b64 	%rl310, %rl596, %r283;
	neg.s32 	%r284, %r50;
	and.b32  	%r285, %r284, 63;
	shr.u64 	%rl311, %rl597, %r285;
	or.b64  	%rl596, %rl311, %rl310;
	shl.b64 	%rl312, %rl597, %r283;
	ld.local.u64 	%rl313, [%r31+8];
	shr.u64 	%rl314, %rl313, %r285;
	or.b64  	%rl597, %rl314, %rl312;

BB7_80:
	shr.u64 	%rl315, %rl596, 62;
	cvt.u32.u64 	%r286, %rl315;
	shr.u64 	%rl316, %rl597, 62;
	shl.b64 	%rl317, %rl596, 2;
	or.b64  	%rl602, %rl316, %rl317;
	shl.b64 	%rl47, %rl597, 2;
	setp.ne.s64 	%p71, %rl47, 0;
	selp.u64 	%rl318, 1, 0, %p71;
	or.b64  	%rl319, %rl318, %rl602;
	setp.gt.u64 	%p72, %rl319, -9223372036854775808;
	selp.u32 	%r287, 1, 0, %p72;
	add.s32 	%r288, %r287, %r286;
	neg.s32 	%r289, %r288;
	setp.lt.s64 	%p73, %rl33, 0;
	selp.b32 	%r534, %r289, %r288, %p73;
	@%p72 bra 	BB7_82;

	mov.u64 	%rl601, %rl47;
	bra.uni 	BB7_83;

BB7_82:
	not.b64 	%rl320, %rl602;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p74, %rl47, 0;
	selp.u64 	%rl321, 1, 0, %p74;
	add.s64 	%rl602, %rl321, %rl320;
	xor.b64  	%rl598, %rl598, -9223372036854775808;
	mov.u64 	%rl601, %rl48;

BB7_83:
	mov.u64 	%rl600, %rl601;
	setp.gt.s64 	%p75, %rl602, 0;
	@%p75 bra 	BB7_85;

	mov.u32 	%r533, 0;
	bra.uni 	BB7_87;

BB7_85:
	mov.u32 	%r533, 0;

BB7_86:
	shr.u64 	%rl322, %rl600, 63;
	shl.b64 	%rl323, %rl602, 1;
	or.b64  	%rl602, %rl322, %rl323;
	shl.b64 	%rl600, %rl600, 1;
	add.s32 	%r533, %r533, -1;
	setp.gt.s64 	%p76, %rl602, 0;
	@%p76 bra 	BB7_86;

BB7_87:
	mul.lo.s64 	%rl604, %rl602, -3958705157555305931;
	mov.u64 	%rl326, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl324, %rl602, %rl326;
	// inline asm
	setp.gt.s64 	%p77, %rl324, 0;
	mov.u64 	%rl603, %rl324;
	@%p77 bra 	BB7_88;
	bra.uni 	BB7_89;

BB7_88:
	shl.b64 	%rl327, %rl324, 1;
	shr.u64 	%rl328, %rl604, 63;
	or.b64  	%rl603, %rl327, %rl328;
	mul.lo.s64 	%rl604, %rl602, -7917410315110611862;
	add.s32 	%r533, %r533, -1;

BB7_89:
	setp.ne.s64 	%p78, %rl604, 0;
	selp.u64 	%rl329, 1, 0, %p78;
	add.s64 	%rl330, %rl329, %rl603;
	add.s32 	%r292, %r533, 1022;
	cvt.u64.u32 	%rl331, %r292;
	shl.b64 	%rl332, %rl331, 52;
	shr.u64 	%rl333, %rl330, 11;
	shr.u64 	%rl334, %rl330, 10;
	and.b64  	%rl335, %rl334, 1;
	add.s64 	%rl336, %rl332, %rl333;
	add.s64 	%rl337, %rl336, %rl335;
	or.b64  	%rl338, %rl337, %rl598;
	mov.b64 	 %fd914, %rl338;

BB7_90:
	and.b32  	%r293, %r534, 1;
	setp.eq.s32 	%p79, %r293, 0;
	mul.rn.f64 	%fd27, %fd914, %fd914;
	@%p79 bra 	BB7_92;

	mov.f64 	%fd189, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd191, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd188, %fd189, %fd27, %fd191;
	// inline asm
	mov.f64 	%fd195, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd192, %fd188, %fd27, %fd195;
	// inline asm
	mov.f64 	%fd199, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd196, %fd192, %fd27, %fd199;
	// inline asm
	mov.f64 	%fd203, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd200, %fd196, %fd27, %fd203;
	// inline asm
	mov.f64 	%fd207, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd204, %fd200, %fd27, %fd207;
	// inline asm
	mov.f64 	%fd211, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd208, %fd204, %fd27, %fd211;
	// inline asm
	mov.f64 	%fd215, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd212, %fd208, %fd27, %fd215;
	// inline asm
	mov.f64 	%fd915, %fd212;
	bra.uni 	BB7_93;

BB7_92:
	mov.f64 	%fd217, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd219, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd216, %fd217, %fd27, %fd219;
	// inline asm
	mov.f64 	%fd223, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd220, %fd216, %fd27, %fd223;
	// inline asm
	mov.f64 	%fd227, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd224, %fd220, %fd27, %fd227;
	// inline asm
	mov.f64 	%fd231, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd228, %fd224, %fd27, %fd231;
	// inline asm
	mov.f64 	%fd235, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd232, %fd228, %fd27, %fd235;
	// inline asm
	mul.rn.f64 	%fd237, %fd232, %fd27;
	// inline asm
	fma.rn.f64 	%fd236, %fd237, %fd914, %fd914;
	// inline asm
	mov.f64 	%fd915, %fd236;

BB7_93:
	and.b32  	%r294, %r534, 2;
	setp.eq.s32 	%p80, %r294, 0;
	neg.f64 	%fd240, %fd915;
	selp.f64 	%fd916, %fd915, %fd240, %p80;
	bra.uni 	BB7_95;

BB7_94:
	mov.f64 	%fd241, 0d0000000000000000;
	mul.rn.f64 	%fd916, %fd11, %fd241;

BB7_95:
	neg.f64 	%fd242, %fd916;
	mov.f64 	%fd951, %fd913;
	mov.f64 	%fd952, %fd242;
	ld.param.u32 	%r524, [DIT5C2C_param_3];
	setp.eq.s32 	%p9, %r524, 0;
	@%p9 bra 	BB7_96;
	bra.uni 	BB7_97;

BB7_96:
	mov.f64 	%fd951, %fd913;
	mov.f64 	%fd952, %fd916;

BB7_97:
	mul.f64 	%fd244, %fd1, %fd951;
	neg.f64 	%fd246, %fd2;
	fma.rn.f64 	%fd247, %fd246, %fd952, %fd244;
	mul.f64 	%fd248, %fd2, %fd951;
	fma.rn.f64 	%fd249, %fd1, %fd952, %fd248;
	mul.f64 	%fd250, %fd9, 0d402921FB54442D18;
	div.rn.f64 	%fd34, %fd250, %fd10;
	setp.eq.f64 	%p10, %fd34, %fd12;
	setp.eq.f64 	%p11, %fd34, %fd13;
	or.pred  	%p81, %p10, %p11;
	@%p81 bra 	BB7_120;

	// inline asm
	abs.f64 	%fd251, %fd34;
	// inline asm
	setp.gt.f64 	%p82, %fd251, 0d41E0000000000000;
	@%p82 bra 	BB7_100;

	mov.f64 	%fd266, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd253, %fd34, %fd266;
	// inline asm
	cvt.rni.s32.f64 	%r295, %fd253;
	// inline asm
	cvt.rn.f64.s32 	%fd267, %r295;
	neg.f64 	%fd263, %fd267;
	mov.f64 	%fd256, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd254, %fd263, %fd256, %fd34;
	// inline asm
	mov.f64 	%fd260, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd258, %fd263, %fd260, %fd254;
	// inline asm
	mov.f64 	%fd264, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd262, %fd263, %fd264, %fd258;
	// inline asm
	mov.u32 	%r537, %r295;
	mov.f64 	%fd917, %fd262;
	bra.uni 	BB7_116;

BB7_100:
	mov.b64 	 %rl65, %fd34;
	and.b64  	%rl608, %rl65, -9223372036854775808;
	shr.u64 	%rl67, %rl65, 52;
	and.b64  	%rl339, %rl67, 2047;
	add.s64 	%rl340, %rl339, 4294966272;
	cvt.u32.u64 	%r66, %rl340;
	shl.b64 	%rl341, %rl65, 11;
	or.b64  	%rl68, %rl341, -9223372036854775808;
	shr.u32 	%r299, %r66, 6;
	mov.u32 	%r300, 16;
	sub.s32 	%r67, %r300, %r299;
	mov.u32 	%r301, 15;
	sub.s32 	%r535, %r301, %r299;
	mov.u32 	%r302, 19;
	sub.s32 	%r69, %r302, %r299;
	mov.u32 	%r297, 18;
	// inline asm
	min.s32 	%r296, %r297, %r69;
	// inline asm
	setp.lt.s32 	%p83, %r535, %r296;
	@%p83 bra 	BB7_102;

	mov.u64 	%rl605, 0;
	bra.uni 	BB7_104;

BB7_102:
	mov.u32 	%r303, 1;
	sub.s32 	%r70, %r303, %r67;
	mov.u64 	%rl605, 0;

BB7_103:
	.pragma "nounroll";
	shl.b32 	%r307, %r535, 3;
	mov.u32 	%r308, __internal_i2opi_d;
	add.s32 	%r309, %r308, %r307;
	ld.const.u64 	%rl345, [%r309];
	mul.lo.s64 	%rl347, %rl345, %rl68;
	// inline asm
	mul.hi.u64 	%rl344, %rl345, %rl68;
	// inline asm
	mad.lo.s64 	%rl348, %rl345, %rl68, %rl605;
	setp.lt.u64 	%p84, %rl348, %rl347;
	selp.u64 	%rl349, 1, 0, %p84;
	add.s64 	%rl605, %rl349, %rl344;
	add.s32 	%r310, %r70, %r535;
	shl.b32 	%r311, %r310, 3;
	add.s32 	%r313, %r31, %r311;
	st.local.u64 	[%r313], %rl348;
	// inline asm
	min.s32 	%r304, %r297, %r69;
	// inline asm
	add.s32 	%r535, %r535, 1;
	setp.lt.s32 	%p85, %r535, %r304;
	@%p85 bra 	BB7_103;

BB7_104:
	mov.u32 	%r314, 1;
	sub.s32 	%r315, %r314, %r67;
	add.s32 	%r316, %r315, %r535;
	shl.b32 	%r317, %r316, 3;
	add.s32 	%r319, %r31, %r317;
	st.local.u64 	[%r319], %rl605;
	ld.local.u64 	%rl606, [%r31+24];
	ld.local.u64 	%rl607, [%r31+16];
	and.b32  	%r320, %r66, 63;
	setp.eq.s32 	%p86, %r320, 0;
	@%p86 bra 	BB7_106;

	and.b64  	%rl350, %rl67, 63;
	cvt.u32.u64 	%r321, %rl350;
	shl.b64 	%rl351, %rl606, %r321;
	neg.s32 	%r322, %r66;
	and.b32  	%r323, %r322, 63;
	shr.u64 	%rl352, %rl607, %r323;
	or.b64  	%rl606, %rl352, %rl351;
	shl.b64 	%rl353, %rl607, %r321;
	ld.local.u64 	%rl354, [%r31+8];
	shr.u64 	%rl355, %rl354, %r323;
	or.b64  	%rl607, %rl355, %rl353;

BB7_106:
	shr.u64 	%rl356, %rl606, 62;
	cvt.u32.u64 	%r324, %rl356;
	shr.u64 	%rl357, %rl607, 62;
	shl.b64 	%rl358, %rl606, 2;
	or.b64  	%rl612, %rl357, %rl358;
	shl.b64 	%rl79, %rl607, 2;
	setp.ne.s64 	%p87, %rl79, 0;
	selp.u64 	%rl359, 1, 0, %p87;
	or.b64  	%rl360, %rl359, %rl612;
	setp.gt.u64 	%p88, %rl360, -9223372036854775808;
	selp.u32 	%r325, 1, 0, %p88;
	add.s32 	%r326, %r325, %r324;
	neg.s32 	%r327, %r326;
	setp.lt.s64 	%p89, %rl65, 0;
	selp.b32 	%r537, %r327, %r326, %p89;
	@%p88 bra 	BB7_108;

	mov.u64 	%rl611, %rl79;
	bra.uni 	BB7_109;

BB7_108:
	not.b64 	%rl361, %rl612;
	neg.s64 	%rl80, %rl79;
	setp.eq.s64 	%p90, %rl79, 0;
	selp.u64 	%rl362, 1, 0, %p90;
	add.s64 	%rl612, %rl362, %rl361;
	xor.b64  	%rl608, %rl608, -9223372036854775808;
	mov.u64 	%rl611, %rl80;

BB7_109:
	mov.u64 	%rl610, %rl611;
	setp.gt.s64 	%p91, %rl612, 0;
	@%p91 bra 	BB7_111;

	mov.u32 	%r536, 0;
	bra.uni 	BB7_113;

BB7_111:
	mov.u32 	%r536, 0;

BB7_112:
	shr.u64 	%rl363, %rl610, 63;
	shl.b64 	%rl364, %rl612, 1;
	or.b64  	%rl612, %rl363, %rl364;
	shl.b64 	%rl610, %rl610, 1;
	add.s32 	%r536, %r536, -1;
	setp.gt.s64 	%p92, %rl612, 0;
	@%p92 bra 	BB7_112;

BB7_113:
	mul.lo.s64 	%rl614, %rl612, -3958705157555305931;
	mov.u64 	%rl367, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl365, %rl612, %rl367;
	// inline asm
	setp.gt.s64 	%p93, %rl365, 0;
	mov.u64 	%rl613, %rl365;
	@%p93 bra 	BB7_114;
	bra.uni 	BB7_115;

BB7_114:
	shl.b64 	%rl368, %rl365, 1;
	shr.u64 	%rl369, %rl614, 63;
	or.b64  	%rl613, %rl368, %rl369;
	mul.lo.s64 	%rl614, %rl612, -7917410315110611862;
	add.s32 	%r536, %r536, -1;

BB7_115:
	setp.ne.s64 	%p94, %rl614, 0;
	selp.u64 	%rl370, 1, 0, %p94;
	add.s64 	%rl371, %rl370, %rl613;
	add.s32 	%r330, %r536, 1022;
	cvt.u64.u32 	%rl372, %r330;
	shl.b64 	%rl373, %rl372, 52;
	shr.u64 	%rl374, %rl371, 11;
	shr.u64 	%rl375, %rl371, 10;
	and.b64  	%rl376, %rl375, 1;
	add.s64 	%rl377, %rl373, %rl374;
	add.s64 	%rl378, %rl377, %rl376;
	or.b64  	%rl379, %rl378, %rl608;
	mov.b64 	 %fd917, %rl379;

BB7_116:
	add.s32 	%r81, %r537, 1;
	and.b32  	%r331, %r81, 1;
	setp.eq.s32 	%p95, %r331, 0;
	mul.rn.f64 	%fd38, %fd917, %fd917;
	@%p95 bra 	BB7_118;

	mov.f64 	%fd269, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd271, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd268, %fd269, %fd38, %fd271;
	// inline asm
	mov.f64 	%fd275, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd272, %fd268, %fd38, %fd275;
	// inline asm
	mov.f64 	%fd279, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd276, %fd272, %fd38, %fd279;
	// inline asm
	mov.f64 	%fd283, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd280, %fd276, %fd38, %fd283;
	// inline asm
	mov.f64 	%fd287, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd284, %fd280, %fd38, %fd287;
	// inline asm
	mov.f64 	%fd291, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd288, %fd284, %fd38, %fd291;
	// inline asm
	mov.f64 	%fd295, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd292, %fd288, %fd38, %fd295;
	// inline asm
	mov.f64 	%fd918, %fd292;
	bra.uni 	BB7_119;

BB7_118:
	mov.f64 	%fd297, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd299, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd296, %fd297, %fd38, %fd299;
	// inline asm
	mov.f64 	%fd303, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd300, %fd296, %fd38, %fd303;
	// inline asm
	mov.f64 	%fd307, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd304, %fd300, %fd38, %fd307;
	// inline asm
	mov.f64 	%fd311, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd308, %fd304, %fd38, %fd311;
	// inline asm
	mov.f64 	%fd315, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd312, %fd308, %fd38, %fd315;
	// inline asm
	mul.rn.f64 	%fd317, %fd312, %fd38;
	// inline asm
	fma.rn.f64 	%fd316, %fd317, %fd917, %fd917;
	// inline asm
	mov.f64 	%fd918, %fd316;

BB7_119:
	and.b32  	%r332, %r81, 2;
	setp.eq.s32 	%p96, %r332, 0;
	neg.f64 	%fd320, %fd918;
	selp.f64 	%fd919, %fd918, %fd320, %p96;
	bra.uni 	BB7_121;

BB7_120:
	mov.f64 	%fd919, 0dFFF8000000000000;

BB7_121:
	setp.eq.f64 	%p97, %fd34, 0d0000000000000000;
	or.pred  	%p98, %p11, %p97;
	or.pred  	%p99, %p10, %p98;
	@%p99 bra 	BB7_144;

	// inline asm
	abs.f64 	%fd322, %fd34;
	// inline asm
	setp.gt.f64 	%p100, %fd322, 0d41E0000000000000;
	@%p100 bra 	BB7_124;

	mov.f64 	%fd337, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd324, %fd34, %fd337;
	// inline asm
	cvt.rni.s32.f64 	%r333, %fd324;
	// inline asm
	cvt.rn.f64.s32 	%fd338, %r333;
	neg.f64 	%fd334, %fd338;
	mov.f64 	%fd327, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd325, %fd334, %fd327, %fd34;
	// inline asm
	mov.f64 	%fd331, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd329, %fd334, %fd331, %fd325;
	// inline asm
	mov.f64 	%fd335, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd333, %fd334, %fd335, %fd329;
	// inline asm
	mov.u32 	%r540, %r333;
	mov.f64 	%fd920, %fd333;
	bra.uni 	BB7_140;

BB7_124:
	mov.b64 	 %rl97, %fd34;
	and.b64  	%rl618, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl380, %rl99, 2047;
	add.s64 	%rl381, %rl380, 4294966272;
	cvt.u32.u64 	%r83, %rl381;
	shl.b64 	%rl382, %rl97, 11;
	or.b64  	%rl100, %rl382, -9223372036854775808;
	shr.u32 	%r337, %r83, 6;
	mov.u32 	%r338, 16;
	sub.s32 	%r84, %r338, %r337;
	mov.u32 	%r339, 15;
	sub.s32 	%r538, %r339, %r337;
	mov.u32 	%r340, 19;
	sub.s32 	%r86, %r340, %r337;
	mov.u32 	%r335, 18;
	// inline asm
	min.s32 	%r334, %r335, %r86;
	// inline asm
	setp.lt.s32 	%p101, %r538, %r334;
	@%p101 bra 	BB7_126;

	mov.u64 	%rl615, 0;
	bra.uni 	BB7_128;

BB7_126:
	mov.u32 	%r341, 1;
	sub.s32 	%r87, %r341, %r84;
	mov.u64 	%rl615, 0;

BB7_127:
	.pragma "nounroll";
	shl.b32 	%r345, %r538, 3;
	mov.u32 	%r346, __internal_i2opi_d;
	add.s32 	%r347, %r346, %r345;
	ld.const.u64 	%rl386, [%r347];
	mul.lo.s64 	%rl388, %rl386, %rl100;
	// inline asm
	mul.hi.u64 	%rl385, %rl386, %rl100;
	// inline asm
	mad.lo.s64 	%rl389, %rl386, %rl100, %rl615;
	setp.lt.u64 	%p102, %rl389, %rl388;
	selp.u64 	%rl390, 1, 0, %p102;
	add.s64 	%rl615, %rl390, %rl385;
	add.s32 	%r348, %r87, %r538;
	shl.b32 	%r349, %r348, 3;
	add.s32 	%r351, %r31, %r349;
	st.local.u64 	[%r351], %rl389;
	// inline asm
	min.s32 	%r342, %r335, %r86;
	// inline asm
	add.s32 	%r538, %r538, 1;
	setp.lt.s32 	%p103, %r538, %r342;
	@%p103 bra 	BB7_127;

BB7_128:
	mov.u32 	%r352, 1;
	sub.s32 	%r353, %r352, %r84;
	add.s32 	%r354, %r353, %r538;
	shl.b32 	%r355, %r354, 3;
	add.s32 	%r357, %r31, %r355;
	st.local.u64 	[%r357], %rl615;
	ld.local.u64 	%rl616, [%r31+24];
	ld.local.u64 	%rl617, [%r31+16];
	and.b32  	%r358, %r83, 63;
	setp.eq.s32 	%p104, %r358, 0;
	@%p104 bra 	BB7_130;

	and.b64  	%rl391, %rl99, 63;
	cvt.u32.u64 	%r359, %rl391;
	shl.b64 	%rl392, %rl616, %r359;
	neg.s32 	%r360, %r83;
	and.b32  	%r361, %r360, 63;
	shr.u64 	%rl393, %rl617, %r361;
	or.b64  	%rl616, %rl393, %rl392;
	shl.b64 	%rl394, %rl617, %r359;
	ld.local.u64 	%rl395, [%r31+8];
	shr.u64 	%rl396, %rl395, %r361;
	or.b64  	%rl617, %rl396, %rl394;

BB7_130:
	shr.u64 	%rl397, %rl616, 62;
	cvt.u32.u64 	%r362, %rl397;
	shr.u64 	%rl398, %rl617, 62;
	shl.b64 	%rl399, %rl616, 2;
	or.b64  	%rl622, %rl398, %rl399;
	shl.b64 	%rl111, %rl617, 2;
	setp.ne.s64 	%p105, %rl111, 0;
	selp.u64 	%rl400, 1, 0, %p105;
	or.b64  	%rl401, %rl400, %rl622;
	setp.gt.u64 	%p106, %rl401, -9223372036854775808;
	selp.u32 	%r363, 1, 0, %p106;
	add.s32 	%r364, %r363, %r362;
	neg.s32 	%r365, %r364;
	setp.lt.s64 	%p107, %rl97, 0;
	selp.b32 	%r540, %r365, %r364, %p107;
	@%p106 bra 	BB7_132;

	mov.u64 	%rl621, %rl111;
	bra.uni 	BB7_133;

BB7_132:
	not.b64 	%rl402, %rl622;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p108, %rl111, 0;
	selp.u64 	%rl403, 1, 0, %p108;
	add.s64 	%rl622, %rl403, %rl402;
	xor.b64  	%rl618, %rl618, -9223372036854775808;
	mov.u64 	%rl621, %rl112;

BB7_133:
	mov.u64 	%rl620, %rl621;
	setp.gt.s64 	%p109, %rl622, 0;
	@%p109 bra 	BB7_135;

	mov.u32 	%r539, 0;
	bra.uni 	BB7_137;

BB7_135:
	mov.u32 	%r539, 0;

BB7_136:
	shr.u64 	%rl404, %rl620, 63;
	shl.b64 	%rl405, %rl622, 1;
	or.b64  	%rl622, %rl404, %rl405;
	shl.b64 	%rl620, %rl620, 1;
	add.s32 	%r539, %r539, -1;
	setp.gt.s64 	%p110, %rl622, 0;
	@%p110 bra 	BB7_136;

BB7_137:
	mul.lo.s64 	%rl624, %rl622, -3958705157555305931;
	mov.u64 	%rl408, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl406, %rl622, %rl408;
	// inline asm
	setp.gt.s64 	%p111, %rl406, 0;
	mov.u64 	%rl623, %rl406;
	@%p111 bra 	BB7_138;
	bra.uni 	BB7_139;

BB7_138:
	shl.b64 	%rl409, %rl406, 1;
	shr.u64 	%rl410, %rl624, 63;
	or.b64  	%rl623, %rl409, %rl410;
	mul.lo.s64 	%rl624, %rl622, -7917410315110611862;
	add.s32 	%r539, %r539, -1;

BB7_139:
	setp.ne.s64 	%p112, %rl624, 0;
	selp.u64 	%rl411, 1, 0, %p112;
	add.s64 	%rl412, %rl411, %rl623;
	add.s32 	%r368, %r539, 1022;
	cvt.u64.u32 	%rl413, %r368;
	shl.b64 	%rl414, %rl413, 52;
	shr.u64 	%rl415, %rl412, 11;
	shr.u64 	%rl416, %rl412, 10;
	and.b64  	%rl417, %rl416, 1;
	add.s64 	%rl418, %rl414, %rl415;
	add.s64 	%rl419, %rl418, %rl417;
	or.b64  	%rl420, %rl419, %rl618;
	mov.b64 	 %fd920, %rl420;

BB7_140:
	and.b32  	%r369, %r540, 1;
	setp.eq.s32 	%p113, %r369, 0;
	mul.rn.f64 	%fd48, %fd920, %fd920;
	@%p113 bra 	BB7_142;

	mov.f64 	%fd340, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd342, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd339, %fd340, %fd48, %fd342;
	// inline asm
	mov.f64 	%fd346, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd343, %fd339, %fd48, %fd346;
	// inline asm
	mov.f64 	%fd350, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd347, %fd343, %fd48, %fd350;
	// inline asm
	mov.f64 	%fd354, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd351, %fd347, %fd48, %fd354;
	// inline asm
	mov.f64 	%fd358, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd355, %fd351, %fd48, %fd358;
	// inline asm
	mov.f64 	%fd362, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd359, %fd355, %fd48, %fd362;
	// inline asm
	mov.f64 	%fd366, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd363, %fd359, %fd48, %fd366;
	// inline asm
	mov.f64 	%fd921, %fd363;
	bra.uni 	BB7_143;

BB7_142:
	mov.f64 	%fd368, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd370, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd367, %fd368, %fd48, %fd370;
	// inline asm
	mov.f64 	%fd374, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd371, %fd367, %fd48, %fd374;
	// inline asm
	mov.f64 	%fd378, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd375, %fd371, %fd48, %fd378;
	// inline asm
	mov.f64 	%fd382, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd379, %fd375, %fd48, %fd382;
	// inline asm
	mov.f64 	%fd386, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd383, %fd379, %fd48, %fd386;
	// inline asm
	mul.rn.f64 	%fd388, %fd383, %fd48;
	// inline asm
	fma.rn.f64 	%fd387, %fd388, %fd920, %fd920;
	// inline asm
	mov.f64 	%fd921, %fd387;

BB7_143:
	and.b32  	%r370, %r540, 2;
	setp.eq.s32 	%p114, %r370, 0;
	neg.f64 	%fd391, %fd921;
	selp.f64 	%fd922, %fd921, %fd391, %p114;
	bra.uni 	BB7_145;

BB7_144:
	mov.f64 	%fd392, 0d0000000000000000;
	mul.rn.f64 	%fd922, %fd34, %fd392;

BB7_145:
	neg.f64 	%fd393, %fd922;
	mov.f64 	%fd949, %fd919;
	mov.f64 	%fd950, %fd393;
	@%p9 bra 	BB7_146;
	bra.uni 	BB7_147;

BB7_146:
	mov.f64 	%fd949, %fd919;
	mov.f64 	%fd950, %fd922;

BB7_147:
	mul.f64 	%fd395, %fd3, %fd949;
	neg.f64 	%fd397, %fd4;
	fma.rn.f64 	%fd398, %fd397, %fd950, %fd395;
	mul.f64 	%fd399, %fd4, %fd949;
	fma.rn.f64 	%fd400, %fd3, %fd950, %fd399;
	mul.f64 	%fd401, %fd9, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd55, %fd401, %fd10;
	setp.eq.f64 	%p12, %fd55, %fd12;
	setp.eq.f64 	%p13, %fd55, %fd13;
	or.pred  	%p115, %p12, %p13;
	@%p115 bra 	BB7_170;

	// inline asm
	abs.f64 	%fd402, %fd55;
	// inline asm
	setp.gt.f64 	%p116, %fd402, 0d41E0000000000000;
	@%p116 bra 	BB7_150;

	mov.f64 	%fd417, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd404, %fd55, %fd417;
	// inline asm
	cvt.rni.s32.f64 	%r371, %fd404;
	// inline asm
	cvt.rn.f64.s32 	%fd418, %r371;
	neg.f64 	%fd414, %fd418;
	mov.f64 	%fd407, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd405, %fd414, %fd407, %fd55;
	// inline asm
	mov.f64 	%fd411, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd409, %fd414, %fd411, %fd405;
	// inline asm
	mov.f64 	%fd415, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd413, %fd414, %fd415, %fd409;
	// inline asm
	mov.u32 	%r543, %r371;
	mov.f64 	%fd923, %fd413;
	bra.uni 	BB7_166;

BB7_150:
	mov.b64 	 %rl129, %fd55;
	and.b64  	%rl628, %rl129, -9223372036854775808;
	shr.u64 	%rl131, %rl129, 52;
	and.b64  	%rl421, %rl131, 2047;
	add.s64 	%rl422, %rl421, 4294966272;
	cvt.u32.u64 	%r99, %rl422;
	shl.b64 	%rl423, %rl129, 11;
	or.b64  	%rl132, %rl423, -9223372036854775808;
	shr.u32 	%r375, %r99, 6;
	mov.u32 	%r376, 16;
	sub.s32 	%r100, %r376, %r375;
	mov.u32 	%r377, 15;
	sub.s32 	%r541, %r377, %r375;
	mov.u32 	%r378, 19;
	sub.s32 	%r102, %r378, %r375;
	mov.u32 	%r373, 18;
	// inline asm
	min.s32 	%r372, %r373, %r102;
	// inline asm
	setp.lt.s32 	%p117, %r541, %r372;
	@%p117 bra 	BB7_152;

	mov.u64 	%rl625, 0;
	bra.uni 	BB7_154;

BB7_152:
	mov.u32 	%r379, 1;
	sub.s32 	%r103, %r379, %r100;
	mov.u64 	%rl625, 0;

BB7_153:
	.pragma "nounroll";
	shl.b32 	%r383, %r541, 3;
	mov.u32 	%r384, __internal_i2opi_d;
	add.s32 	%r385, %r384, %r383;
	ld.const.u64 	%rl427, [%r385];
	mul.lo.s64 	%rl429, %rl427, %rl132;
	// inline asm
	mul.hi.u64 	%rl426, %rl427, %rl132;
	// inline asm
	mad.lo.s64 	%rl430, %rl427, %rl132, %rl625;
	setp.lt.u64 	%p118, %rl430, %rl429;
	selp.u64 	%rl431, 1, 0, %p118;
	add.s64 	%rl625, %rl431, %rl426;
	add.s32 	%r386, %r103, %r541;
	shl.b32 	%r387, %r386, 3;
	add.s32 	%r389, %r31, %r387;
	st.local.u64 	[%r389], %rl430;
	// inline asm
	min.s32 	%r380, %r373, %r102;
	// inline asm
	add.s32 	%r541, %r541, 1;
	setp.lt.s32 	%p119, %r541, %r380;
	@%p119 bra 	BB7_153;

BB7_154:
	mov.u32 	%r390, 1;
	sub.s32 	%r391, %r390, %r100;
	add.s32 	%r392, %r391, %r541;
	shl.b32 	%r393, %r392, 3;
	add.s32 	%r395, %r31, %r393;
	st.local.u64 	[%r395], %rl625;
	ld.local.u64 	%rl626, [%r31+24];
	ld.local.u64 	%rl627, [%r31+16];
	and.b32  	%r396, %r99, 63;
	setp.eq.s32 	%p120, %r396, 0;
	@%p120 bra 	BB7_156;

	and.b64  	%rl432, %rl131, 63;
	cvt.u32.u64 	%r397, %rl432;
	shl.b64 	%rl433, %rl626, %r397;
	neg.s32 	%r398, %r99;
	and.b32  	%r399, %r398, 63;
	shr.u64 	%rl434, %rl627, %r399;
	or.b64  	%rl626, %rl434, %rl433;
	shl.b64 	%rl435, %rl627, %r397;
	ld.local.u64 	%rl436, [%r31+8];
	shr.u64 	%rl437, %rl436, %r399;
	or.b64  	%rl627, %rl437, %rl435;

BB7_156:
	shr.u64 	%rl438, %rl626, 62;
	cvt.u32.u64 	%r400, %rl438;
	shr.u64 	%rl439, %rl627, 62;
	shl.b64 	%rl440, %rl626, 2;
	or.b64  	%rl632, %rl439, %rl440;
	shl.b64 	%rl143, %rl627, 2;
	setp.ne.s64 	%p121, %rl143, 0;
	selp.u64 	%rl441, 1, 0, %p121;
	or.b64  	%rl442, %rl441, %rl632;
	setp.gt.u64 	%p122, %rl442, -9223372036854775808;
	selp.u32 	%r401, 1, 0, %p122;
	add.s32 	%r402, %r401, %r400;
	neg.s32 	%r403, %r402;
	setp.lt.s64 	%p123, %rl129, 0;
	selp.b32 	%r543, %r403, %r402, %p123;
	@%p122 bra 	BB7_158;

	mov.u64 	%rl631, %rl143;
	bra.uni 	BB7_159;

BB7_158:
	not.b64 	%rl443, %rl632;
	neg.s64 	%rl144, %rl143;
	setp.eq.s64 	%p124, %rl143, 0;
	selp.u64 	%rl444, 1, 0, %p124;
	add.s64 	%rl632, %rl444, %rl443;
	xor.b64  	%rl628, %rl628, -9223372036854775808;
	mov.u64 	%rl631, %rl144;

BB7_159:
	mov.u64 	%rl630, %rl631;
	setp.gt.s64 	%p125, %rl632, 0;
	@%p125 bra 	BB7_161;

	mov.u32 	%r542, 0;
	bra.uni 	BB7_163;

BB7_161:
	mov.u32 	%r542, 0;

BB7_162:
	shr.u64 	%rl445, %rl630, 63;
	shl.b64 	%rl446, %rl632, 1;
	or.b64  	%rl632, %rl445, %rl446;
	shl.b64 	%rl630, %rl630, 1;
	add.s32 	%r542, %r542, -1;
	setp.gt.s64 	%p126, %rl632, 0;
	@%p126 bra 	BB7_162;

BB7_163:
	mul.lo.s64 	%rl634, %rl632, -3958705157555305931;
	mov.u64 	%rl449, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl447, %rl632, %rl449;
	// inline asm
	setp.gt.s64 	%p127, %rl447, 0;
	mov.u64 	%rl633, %rl447;
	@%p127 bra 	BB7_164;
	bra.uni 	BB7_165;

BB7_164:
	shl.b64 	%rl450, %rl447, 1;
	shr.u64 	%rl451, %rl634, 63;
	or.b64  	%rl633, %rl450, %rl451;
	mul.lo.s64 	%rl634, %rl632, -7917410315110611862;
	add.s32 	%r542, %r542, -1;

BB7_165:
	setp.ne.s64 	%p128, %rl634, 0;
	selp.u64 	%rl452, 1, 0, %p128;
	add.s64 	%rl453, %rl452, %rl633;
	add.s32 	%r406, %r542, 1022;
	cvt.u64.u32 	%rl454, %r406;
	shl.b64 	%rl455, %rl454, 52;
	shr.u64 	%rl456, %rl453, 11;
	shr.u64 	%rl457, %rl453, 10;
	and.b64  	%rl458, %rl457, 1;
	add.s64 	%rl459, %rl455, %rl456;
	add.s64 	%rl460, %rl459, %rl458;
	or.b64  	%rl461, %rl460, %rl628;
	mov.b64 	 %fd923, %rl461;

BB7_166:
	add.s32 	%r114, %r543, 1;
	and.b32  	%r407, %r114, 1;
	setp.eq.s32 	%p129, %r407, 0;
	mul.rn.f64 	%fd59, %fd923, %fd923;
	@%p129 bra 	BB7_168;

	mov.f64 	%fd420, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd422, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd419, %fd420, %fd59, %fd422;
	// inline asm
	mov.f64 	%fd426, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd423, %fd419, %fd59, %fd426;
	// inline asm
	mov.f64 	%fd430, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd427, %fd423, %fd59, %fd430;
	// inline asm
	mov.f64 	%fd434, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd431, %fd427, %fd59, %fd434;
	// inline asm
	mov.f64 	%fd438, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd435, %fd431, %fd59, %fd438;
	// inline asm
	mov.f64 	%fd442, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd439, %fd435, %fd59, %fd442;
	// inline asm
	mov.f64 	%fd446, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd443, %fd439, %fd59, %fd446;
	// inline asm
	mov.f64 	%fd924, %fd443;
	bra.uni 	BB7_169;

BB7_168:
	mov.f64 	%fd448, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd450, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd447, %fd448, %fd59, %fd450;
	// inline asm
	mov.f64 	%fd454, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd451, %fd447, %fd59, %fd454;
	// inline asm
	mov.f64 	%fd458, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd455, %fd451, %fd59, %fd458;
	// inline asm
	mov.f64 	%fd462, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd459, %fd455, %fd59, %fd462;
	// inline asm
	mov.f64 	%fd466, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd463, %fd459, %fd59, %fd466;
	// inline asm
	mul.rn.f64 	%fd468, %fd463, %fd59;
	// inline asm
	fma.rn.f64 	%fd467, %fd468, %fd923, %fd923;
	// inline asm
	mov.f64 	%fd924, %fd467;

BB7_169:
	and.b32  	%r408, %r114, 2;
	setp.eq.s32 	%p130, %r408, 0;
	neg.f64 	%fd471, %fd924;
	selp.f64 	%fd925, %fd924, %fd471, %p130;
	bra.uni 	BB7_171;

BB7_170:
	mov.f64 	%fd925, 0dFFF8000000000000;

BB7_171:
	setp.eq.f64 	%p131, %fd55, 0d0000000000000000;
	or.pred  	%p132, %p13, %p131;
	or.pred  	%p133, %p12, %p132;
	@%p133 bra 	BB7_194;

	// inline asm
	abs.f64 	%fd473, %fd55;
	// inline asm
	setp.gt.f64 	%p134, %fd473, 0d41E0000000000000;
	@%p134 bra 	BB7_174;

	mov.f64 	%fd488, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd475, %fd55, %fd488;
	// inline asm
	cvt.rni.s32.f64 	%r409, %fd475;
	// inline asm
	cvt.rn.f64.s32 	%fd489, %r409;
	neg.f64 	%fd485, %fd489;
	mov.f64 	%fd478, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd476, %fd485, %fd478, %fd55;
	// inline asm
	mov.f64 	%fd482, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd480, %fd485, %fd482, %fd476;
	// inline asm
	mov.f64 	%fd486, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd484, %fd485, %fd486, %fd480;
	// inline asm
	mov.u32 	%r546, %r409;
	mov.f64 	%fd926, %fd484;
	bra.uni 	BB7_190;

BB7_174:
	mov.b64 	 %rl161, %fd55;
	and.b64  	%rl638, %rl161, -9223372036854775808;
	shr.u64 	%rl163, %rl161, 52;
	and.b64  	%rl462, %rl163, 2047;
	add.s64 	%rl463, %rl462, 4294966272;
	cvt.u32.u64 	%r116, %rl463;
	shl.b64 	%rl464, %rl161, 11;
	or.b64  	%rl164, %rl464, -9223372036854775808;
	shr.u32 	%r413, %r116, 6;
	mov.u32 	%r414, 16;
	sub.s32 	%r117, %r414, %r413;
	mov.u32 	%r415, 15;
	sub.s32 	%r544, %r415, %r413;
	mov.u32 	%r416, 19;
	sub.s32 	%r119, %r416, %r413;
	mov.u32 	%r411, 18;
	// inline asm
	min.s32 	%r410, %r411, %r119;
	// inline asm
	setp.lt.s32 	%p135, %r544, %r410;
	@%p135 bra 	BB7_176;

	mov.u64 	%rl635, 0;
	bra.uni 	BB7_178;

BB7_176:
	mov.u32 	%r417, 1;
	sub.s32 	%r120, %r417, %r117;
	mov.u64 	%rl635, 0;

BB7_177:
	.pragma "nounroll";
	shl.b32 	%r421, %r544, 3;
	mov.u32 	%r422, __internal_i2opi_d;
	add.s32 	%r423, %r422, %r421;
	ld.const.u64 	%rl468, [%r423];
	mul.lo.s64 	%rl470, %rl468, %rl164;
	// inline asm
	mul.hi.u64 	%rl467, %rl468, %rl164;
	// inline asm
	mad.lo.s64 	%rl471, %rl468, %rl164, %rl635;
	setp.lt.u64 	%p136, %rl471, %rl470;
	selp.u64 	%rl472, 1, 0, %p136;
	add.s64 	%rl635, %rl472, %rl467;
	add.s32 	%r424, %r120, %r544;
	shl.b32 	%r425, %r424, 3;
	add.s32 	%r427, %r31, %r425;
	st.local.u64 	[%r427], %rl471;
	// inline asm
	min.s32 	%r418, %r411, %r119;
	// inline asm
	add.s32 	%r544, %r544, 1;
	setp.lt.s32 	%p137, %r544, %r418;
	@%p137 bra 	BB7_177;

BB7_178:
	mov.u32 	%r428, 1;
	sub.s32 	%r429, %r428, %r117;
	add.s32 	%r430, %r429, %r544;
	shl.b32 	%r431, %r430, 3;
	add.s32 	%r433, %r31, %r431;
	st.local.u64 	[%r433], %rl635;
	ld.local.u64 	%rl636, [%r31+24];
	ld.local.u64 	%rl637, [%r31+16];
	and.b32  	%r434, %r116, 63;
	setp.eq.s32 	%p138, %r434, 0;
	@%p138 bra 	BB7_180;

	and.b64  	%rl473, %rl163, 63;
	cvt.u32.u64 	%r435, %rl473;
	shl.b64 	%rl474, %rl636, %r435;
	neg.s32 	%r436, %r116;
	and.b32  	%r437, %r436, 63;
	shr.u64 	%rl475, %rl637, %r437;
	or.b64  	%rl636, %rl475, %rl474;
	shl.b64 	%rl476, %rl637, %r435;
	ld.local.u64 	%rl477, [%r31+8];
	shr.u64 	%rl478, %rl477, %r437;
	or.b64  	%rl637, %rl478, %rl476;

BB7_180:
	shr.u64 	%rl479, %rl636, 62;
	cvt.u32.u64 	%r438, %rl479;
	shr.u64 	%rl480, %rl637, 62;
	shl.b64 	%rl481, %rl636, 2;
	or.b64  	%rl642, %rl480, %rl481;
	shl.b64 	%rl175, %rl637, 2;
	setp.ne.s64 	%p139, %rl175, 0;
	selp.u64 	%rl482, 1, 0, %p139;
	or.b64  	%rl483, %rl482, %rl642;
	setp.gt.u64 	%p140, %rl483, -9223372036854775808;
	selp.u32 	%r439, 1, 0, %p140;
	add.s32 	%r440, %r439, %r438;
	neg.s32 	%r441, %r440;
	setp.lt.s64 	%p141, %rl161, 0;
	selp.b32 	%r546, %r441, %r440, %p141;
	@%p140 bra 	BB7_182;

	mov.u64 	%rl641, %rl175;
	bra.uni 	BB7_183;

BB7_182:
	not.b64 	%rl484, %rl642;
	neg.s64 	%rl176, %rl175;
	setp.eq.s64 	%p142, %rl175, 0;
	selp.u64 	%rl485, 1, 0, %p142;
	add.s64 	%rl642, %rl485, %rl484;
	xor.b64  	%rl638, %rl638, -9223372036854775808;
	mov.u64 	%rl641, %rl176;

BB7_183:
	mov.u64 	%rl640, %rl641;
	setp.gt.s64 	%p143, %rl642, 0;
	@%p143 bra 	BB7_185;

	mov.u32 	%r545, 0;
	bra.uni 	BB7_187;

BB7_185:
	mov.u32 	%r545, 0;

BB7_186:
	shr.u64 	%rl486, %rl640, 63;
	shl.b64 	%rl487, %rl642, 1;
	or.b64  	%rl642, %rl486, %rl487;
	shl.b64 	%rl640, %rl640, 1;
	add.s32 	%r545, %r545, -1;
	setp.gt.s64 	%p144, %rl642, 0;
	@%p144 bra 	BB7_186;

BB7_187:
	mul.lo.s64 	%rl644, %rl642, -3958705157555305931;
	mov.u64 	%rl490, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl488, %rl642, %rl490;
	// inline asm
	setp.gt.s64 	%p145, %rl488, 0;
	mov.u64 	%rl643, %rl488;
	@%p145 bra 	BB7_188;
	bra.uni 	BB7_189;

BB7_188:
	shl.b64 	%rl491, %rl488, 1;
	shr.u64 	%rl492, %rl644, 63;
	or.b64  	%rl643, %rl491, %rl492;
	mul.lo.s64 	%rl644, %rl642, -7917410315110611862;
	add.s32 	%r545, %r545, -1;

BB7_189:
	setp.ne.s64 	%p146, %rl644, 0;
	selp.u64 	%rl493, 1, 0, %p146;
	add.s64 	%rl494, %rl493, %rl643;
	add.s32 	%r444, %r545, 1022;
	cvt.u64.u32 	%rl495, %r444;
	shl.b64 	%rl496, %rl495, 52;
	shr.u64 	%rl497, %rl494, 11;
	shr.u64 	%rl498, %rl494, 10;
	and.b64  	%rl499, %rl498, 1;
	add.s64 	%rl500, %rl496, %rl497;
	add.s64 	%rl501, %rl500, %rl499;
	or.b64  	%rl502, %rl501, %rl638;
	mov.b64 	 %fd926, %rl502;

BB7_190:
	and.b32  	%r445, %r546, 1;
	setp.eq.s32 	%p147, %r445, 0;
	mul.rn.f64 	%fd69, %fd926, %fd926;
	@%p147 bra 	BB7_192;

	mov.f64 	%fd491, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd493, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd490, %fd491, %fd69, %fd493;
	// inline asm
	mov.f64 	%fd497, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd494, %fd490, %fd69, %fd497;
	// inline asm
	mov.f64 	%fd501, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd498, %fd494, %fd69, %fd501;
	// inline asm
	mov.f64 	%fd505, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd502, %fd498, %fd69, %fd505;
	// inline asm
	mov.f64 	%fd509, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd506, %fd502, %fd69, %fd509;
	// inline asm
	mov.f64 	%fd513, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd510, %fd506, %fd69, %fd513;
	// inline asm
	mov.f64 	%fd517, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd514, %fd510, %fd69, %fd517;
	// inline asm
	mov.f64 	%fd927, %fd514;
	bra.uni 	BB7_193;

BB7_192:
	mov.f64 	%fd519, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd521, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd518, %fd519, %fd69, %fd521;
	// inline asm
	mov.f64 	%fd525, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd522, %fd518, %fd69, %fd525;
	// inline asm
	mov.f64 	%fd529, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd526, %fd522, %fd69, %fd529;
	// inline asm
	mov.f64 	%fd533, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd530, %fd526, %fd69, %fd533;
	// inline asm
	mov.f64 	%fd537, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd534, %fd530, %fd69, %fd537;
	// inline asm
	mul.rn.f64 	%fd539, %fd534, %fd69;
	// inline asm
	fma.rn.f64 	%fd538, %fd539, %fd926, %fd926;
	// inline asm
	mov.f64 	%fd927, %fd538;

BB7_193:
	and.b32  	%r446, %r546, 2;
	setp.eq.s32 	%p148, %r446, 0;
	neg.f64 	%fd542, %fd927;
	selp.f64 	%fd928, %fd927, %fd542, %p148;
	bra.uni 	BB7_195;

BB7_194:
	mov.f64 	%fd543, 0d0000000000000000;
	mul.rn.f64 	%fd928, %fd55, %fd543;

BB7_195:
	neg.f64 	%fd544, %fd928;
	mov.f64 	%fd947, %fd925;
	mov.f64 	%fd948, %fd544;
	@%p9 bra 	BB7_196;
	bra.uni 	BB7_197;

BB7_196:
	mov.f64 	%fd947, %fd925;
	mov.f64 	%fd948, %fd928;

BB7_197:
	mul.f64 	%fd546, %fd5, %fd947;
	neg.f64 	%fd548, %fd6;
	fma.rn.f64 	%fd549, %fd548, %fd948, %fd546;
	mul.f64 	%fd550, %fd6, %fd947;
	fma.rn.f64 	%fd551, %fd5, %fd948, %fd550;
	mul.f64 	%fd552, %fd9, 0d403921FB54442D18;
	div.rn.f64 	%fd76, %fd552, %fd10;
	setp.eq.f64 	%p14, %fd76, %fd12;
	setp.eq.f64 	%p15, %fd76, %fd13;
	or.pred  	%p149, %p14, %p15;
	mov.f64 	%fd935, %fd97;
	mov.f64 	%fd936, %fd98;
	mov.f64 	%fd937, %fd247;
	mov.f64 	%fd938, %fd249;
	mov.f64 	%fd939, %fd398;
	mov.f64 	%fd940, %fd400;
	mov.f64 	%fd941, %fd549;
	mov.f64 	%fd942, %fd551;
	@%p149 bra 	BB7_220;

	// inline asm
	abs.f64 	%fd553, %fd76;
	// inline asm
	setp.gt.f64 	%p150, %fd553, 0d41E0000000000000;
	@%p150 bra 	BB7_200;

	mov.f64 	%fd568, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd555, %fd76, %fd568;
	// inline asm
	cvt.rni.s32.f64 	%r447, %fd555;
	// inline asm
	cvt.rn.f64.s32 	%fd569, %r447;
	neg.f64 	%fd565, %fd569;
	mov.f64 	%fd558, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd556, %fd565, %fd558, %fd76;
	// inline asm
	mov.f64 	%fd562, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd560, %fd565, %fd562, %fd556;
	// inline asm
	mov.f64 	%fd566, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd564, %fd565, %fd566, %fd560;
	// inline asm
	mov.u32 	%r549, %r447;
	mov.f64 	%fd929, %fd564;
	bra.uni 	BB7_216;

BB7_200:
	mov.b64 	 %rl193, %fd76;
	and.b64  	%rl648, %rl193, -9223372036854775808;
	shr.u64 	%rl195, %rl193, 52;
	and.b64  	%rl503, %rl195, 2047;
	add.s64 	%rl504, %rl503, 4294966272;
	cvt.u32.u64 	%r132, %rl504;
	shl.b64 	%rl505, %rl193, 11;
	or.b64  	%rl196, %rl505, -9223372036854775808;
	shr.u32 	%r451, %r132, 6;
	mov.u32 	%r452, 16;
	sub.s32 	%r133, %r452, %r451;
	mov.u32 	%r453, 15;
	sub.s32 	%r547, %r453, %r451;
	mov.u32 	%r454, 19;
	sub.s32 	%r135, %r454, %r451;
	mov.u32 	%r449, 18;
	// inline asm
	min.s32 	%r448, %r449, %r135;
	// inline asm
	setp.lt.s32 	%p151, %r547, %r448;
	@%p151 bra 	BB7_202;

	mov.u64 	%rl645, 0;
	bra.uni 	BB7_204;

BB7_202:
	mov.u32 	%r455, 1;
	sub.s32 	%r136, %r455, %r133;
	mov.u64 	%rl645, 0;

BB7_203:
	.pragma "nounroll";
	shl.b32 	%r459, %r547, 3;
	mov.u32 	%r460, __internal_i2opi_d;
	add.s32 	%r461, %r460, %r459;
	ld.const.u64 	%rl509, [%r461];
	mul.lo.s64 	%rl511, %rl509, %rl196;
	// inline asm
	mul.hi.u64 	%rl508, %rl509, %rl196;
	// inline asm
	mad.lo.s64 	%rl512, %rl509, %rl196, %rl645;
	setp.lt.u64 	%p152, %rl512, %rl511;
	selp.u64 	%rl513, 1, 0, %p152;
	add.s64 	%rl645, %rl513, %rl508;
	add.s32 	%r462, %r136, %r547;
	shl.b32 	%r463, %r462, 3;
	add.s32 	%r465, %r31, %r463;
	st.local.u64 	[%r465], %rl512;
	// inline asm
	min.s32 	%r456, %r449, %r135;
	// inline asm
	add.s32 	%r547, %r547, 1;
	setp.lt.s32 	%p153, %r547, %r456;
	@%p153 bra 	BB7_203;

BB7_204:
	mov.u32 	%r466, 1;
	sub.s32 	%r467, %r466, %r133;
	add.s32 	%r468, %r467, %r547;
	shl.b32 	%r469, %r468, 3;
	add.s32 	%r471, %r31, %r469;
	st.local.u64 	[%r471], %rl645;
	ld.local.u64 	%rl646, [%r31+24];
	ld.local.u64 	%rl647, [%r31+16];
	and.b32  	%r472, %r132, 63;
	setp.eq.s32 	%p154, %r472, 0;
	@%p154 bra 	BB7_206;

	and.b64  	%rl514, %rl195, 63;
	cvt.u32.u64 	%r473, %rl514;
	shl.b64 	%rl515, %rl646, %r473;
	neg.s32 	%r474, %r132;
	and.b32  	%r475, %r474, 63;
	shr.u64 	%rl516, %rl647, %r475;
	or.b64  	%rl646, %rl516, %rl515;
	shl.b64 	%rl517, %rl647, %r473;
	ld.local.u64 	%rl518, [%r31+8];
	shr.u64 	%rl519, %rl518, %r475;
	or.b64  	%rl647, %rl519, %rl517;

BB7_206:
	shr.u64 	%rl520, %rl646, 62;
	cvt.u32.u64 	%r476, %rl520;
	shr.u64 	%rl521, %rl647, 62;
	shl.b64 	%rl522, %rl646, 2;
	or.b64  	%rl652, %rl521, %rl522;
	shl.b64 	%rl207, %rl647, 2;
	setp.ne.s64 	%p155, %rl207, 0;
	selp.u64 	%rl523, 1, 0, %p155;
	or.b64  	%rl524, %rl523, %rl652;
	setp.gt.u64 	%p156, %rl524, -9223372036854775808;
	selp.u32 	%r477, 1, 0, %p156;
	add.s32 	%r478, %r477, %r476;
	neg.s32 	%r479, %r478;
	setp.lt.s64 	%p157, %rl193, 0;
	selp.b32 	%r549, %r479, %r478, %p157;
	@%p156 bra 	BB7_208;

	mov.u64 	%rl651, %rl207;
	bra.uni 	BB7_209;

BB7_208:
	not.b64 	%rl525, %rl652;
	neg.s64 	%rl208, %rl207;
	setp.eq.s64 	%p158, %rl207, 0;
	selp.u64 	%rl526, 1, 0, %p158;
	add.s64 	%rl652, %rl526, %rl525;
	xor.b64  	%rl648, %rl648, -9223372036854775808;
	mov.u64 	%rl651, %rl208;

BB7_209:
	mov.u64 	%rl650, %rl651;
	setp.gt.s64 	%p159, %rl652, 0;
	@%p159 bra 	BB7_211;

	mov.u32 	%r548, 0;
	bra.uni 	BB7_213;

BB7_211:
	mov.u32 	%r548, 0;

BB7_212:
	shr.u64 	%rl527, %rl650, 63;
	shl.b64 	%rl528, %rl652, 1;
	or.b64  	%rl652, %rl527, %rl528;
	shl.b64 	%rl650, %rl650, 1;
	add.s32 	%r548, %r548, -1;
	setp.gt.s64 	%p160, %rl652, 0;
	@%p160 bra 	BB7_212;

BB7_213:
	mul.lo.s64 	%rl654, %rl652, -3958705157555305931;
	mov.u64 	%rl531, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl529, %rl652, %rl531;
	// inline asm
	setp.gt.s64 	%p161, %rl529, 0;
	mov.u64 	%rl653, %rl529;
	@%p161 bra 	BB7_214;
	bra.uni 	BB7_215;

BB7_214:
	shl.b64 	%rl532, %rl529, 1;
	shr.u64 	%rl533, %rl654, 63;
	or.b64  	%rl653, %rl532, %rl533;
	mul.lo.s64 	%rl654, %rl652, -7917410315110611862;
	add.s32 	%r548, %r548, -1;

BB7_215:
	setp.ne.s64 	%p162, %rl654, 0;
	selp.u64 	%rl534, 1, 0, %p162;
	add.s64 	%rl535, %rl534, %rl653;
	add.s32 	%r482, %r548, 1022;
	cvt.u64.u32 	%rl536, %r482;
	shl.b64 	%rl537, %rl536, 52;
	shr.u64 	%rl538, %rl535, 11;
	shr.u64 	%rl539, %rl535, 10;
	and.b64  	%rl540, %rl539, 1;
	add.s64 	%rl541, %rl537, %rl538;
	add.s64 	%rl542, %rl541, %rl540;
	or.b64  	%rl543, %rl542, %rl648;
	mov.b64 	 %fd929, %rl543;

BB7_216:
	add.s32 	%r147, %r549, 1;
	and.b32  	%r483, %r147, 1;
	setp.eq.s32 	%p163, %r483, 0;
	mul.rn.f64 	%fd80, %fd929, %fd929;
	@%p163 bra 	BB7_218;

	mov.f64 	%fd571, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd573, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd570, %fd571, %fd80, %fd573;
	// inline asm
	mov.f64 	%fd577, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd574, %fd570, %fd80, %fd577;
	// inline asm
	mov.f64 	%fd581, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd578, %fd574, %fd80, %fd581;
	// inline asm
	mov.f64 	%fd585, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd582, %fd578, %fd80, %fd585;
	// inline asm
	mov.f64 	%fd589, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd586, %fd582, %fd80, %fd589;
	// inline asm
	mov.f64 	%fd593, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd590, %fd586, %fd80, %fd593;
	// inline asm
	mov.f64 	%fd597, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd594, %fd590, %fd80, %fd597;
	// inline asm
	mov.f64 	%fd930, %fd594;
	bra.uni 	BB7_219;

BB7_218:
	mov.f64 	%fd599, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd601, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd598, %fd599, %fd80, %fd601;
	// inline asm
	mov.f64 	%fd605, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd602, %fd598, %fd80, %fd605;
	// inline asm
	mov.f64 	%fd609, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd606, %fd602, %fd80, %fd609;
	// inline asm
	mov.f64 	%fd613, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd610, %fd606, %fd80, %fd613;
	// inline asm
	mov.f64 	%fd617, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd614, %fd610, %fd80, %fd617;
	// inline asm
	mul.rn.f64 	%fd619, %fd614, %fd80;
	// inline asm
	fma.rn.f64 	%fd618, %fd619, %fd929, %fd929;
	// inline asm
	mov.f64 	%fd930, %fd618;

BB7_219:
	and.b32  	%r484, %r147, 2;
	setp.eq.s32 	%p164, %r484, 0;
	neg.f64 	%fd622, %fd930;
	selp.f64 	%fd931, %fd930, %fd622, %p164;
	bra.uni 	BB7_221;

BB7_220:
	mov.f64 	%fd931, 0dFFF8000000000000;

BB7_221:
	setp.eq.f64 	%p165, %fd76, 0d0000000000000000;
	or.pred  	%p166, %p15, %p165;
	or.pred  	%p167, %p14, %p166;
	@%p167 bra 	BB7_244;

	// inline asm
	abs.f64 	%fd624, %fd76;
	// inline asm
	setp.gt.f64 	%p168, %fd624, 0d41E0000000000000;
	@%p168 bra 	BB7_224;

	mov.f64 	%fd639, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd626, %fd76, %fd639;
	// inline asm
	cvt.rni.s32.f64 	%r485, %fd626;
	// inline asm
	cvt.rn.f64.s32 	%fd640, %r485;
	neg.f64 	%fd636, %fd640;
	mov.f64 	%fd629, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd627, %fd636, %fd629, %fd76;
	// inline asm
	mov.f64 	%fd633, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd631, %fd636, %fd633, %fd627;
	// inline asm
	mov.f64 	%fd637, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd635, %fd636, %fd637, %fd631;
	// inline asm
	mov.u32 	%r552, %r485;
	mov.f64 	%fd932, %fd635;
	bra.uni 	BB7_240;

BB7_224:
	mov.b64 	 %rl225, %fd76;
	and.b64  	%rl658, %rl225, -9223372036854775808;
	shr.u64 	%rl227, %rl225, 52;
	and.b64  	%rl544, %rl227, 2047;
	add.s64 	%rl545, %rl544, 4294966272;
	cvt.u32.u64 	%r149, %rl545;
	shl.b64 	%rl546, %rl225, 11;
	or.b64  	%rl228, %rl546, -9223372036854775808;
	shr.u32 	%r489, %r149, 6;
	mov.u32 	%r490, 16;
	sub.s32 	%r150, %r490, %r489;
	mov.u32 	%r491, 15;
	sub.s32 	%r550, %r491, %r489;
	mov.u32 	%r492, 19;
	sub.s32 	%r152, %r492, %r489;
	mov.u32 	%r487, 18;
	// inline asm
	min.s32 	%r486, %r487, %r152;
	// inline asm
	setp.lt.s32 	%p169, %r550, %r486;
	@%p169 bra 	BB7_226;

	mov.u64 	%rl655, 0;
	bra.uni 	BB7_228;

BB7_226:
	mov.u32 	%r493, 1;
	sub.s32 	%r153, %r493, %r150;
	mov.u64 	%rl655, 0;

BB7_227:
	.pragma "nounroll";
	shl.b32 	%r497, %r550, 3;
	mov.u32 	%r498, __internal_i2opi_d;
	add.s32 	%r499, %r498, %r497;
	ld.const.u64 	%rl550, [%r499];
	mul.lo.s64 	%rl552, %rl550, %rl228;
	// inline asm
	mul.hi.u64 	%rl549, %rl550, %rl228;
	// inline asm
	mad.lo.s64 	%rl553, %rl550, %rl228, %rl655;
	setp.lt.u64 	%p170, %rl553, %rl552;
	selp.u64 	%rl554, 1, 0, %p170;
	add.s64 	%rl655, %rl554, %rl549;
	add.s32 	%r500, %r153, %r550;
	shl.b32 	%r501, %r500, 3;
	add.s32 	%r503, %r31, %r501;
	st.local.u64 	[%r503], %rl553;
	// inline asm
	min.s32 	%r494, %r487, %r152;
	// inline asm
	add.s32 	%r550, %r550, 1;
	setp.lt.s32 	%p171, %r550, %r494;
	@%p171 bra 	BB7_227;

BB7_228:
	mov.u32 	%r504, 1;
	sub.s32 	%r505, %r504, %r150;
	add.s32 	%r506, %r505, %r550;
	shl.b32 	%r507, %r506, 3;
	add.s32 	%r509, %r31, %r507;
	st.local.u64 	[%r509], %rl655;
	ld.local.u64 	%rl656, [%r31+24];
	ld.local.u64 	%rl657, [%r31+16];
	and.b32  	%r510, %r149, 63;
	setp.eq.s32 	%p172, %r510, 0;
	@%p172 bra 	BB7_230;

	and.b64  	%rl555, %rl227, 63;
	cvt.u32.u64 	%r511, %rl555;
	shl.b64 	%rl556, %rl656, %r511;
	neg.s32 	%r512, %r149;
	and.b32  	%r513, %r512, 63;
	shr.u64 	%rl557, %rl657, %r513;
	or.b64  	%rl656, %rl557, %rl556;
	shl.b64 	%rl558, %rl657, %r511;
	ld.local.u64 	%rl559, [%r31+8];
	shr.u64 	%rl560, %rl559, %r513;
	or.b64  	%rl657, %rl560, %rl558;

BB7_230:
	shr.u64 	%rl561, %rl656, 62;
	cvt.u32.u64 	%r514, %rl561;
	shr.u64 	%rl562, %rl657, 62;
	shl.b64 	%rl563, %rl656, 2;
	or.b64  	%rl662, %rl562, %rl563;
	shl.b64 	%rl239, %rl657, 2;
	setp.ne.s64 	%p173, %rl239, 0;
	selp.u64 	%rl564, 1, 0, %p173;
	or.b64  	%rl565, %rl564, %rl662;
	setp.gt.u64 	%p174, %rl565, -9223372036854775808;
	selp.u32 	%r515, 1, 0, %p174;
	add.s32 	%r516, %r515, %r514;
	neg.s32 	%r517, %r516;
	setp.lt.s64 	%p175, %rl225, 0;
	selp.b32 	%r552, %r517, %r516, %p175;
	@%p174 bra 	BB7_232;

	mov.u64 	%rl661, %rl239;
	bra.uni 	BB7_233;

BB7_232:
	not.b64 	%rl566, %rl662;
	neg.s64 	%rl240, %rl239;
	setp.eq.s64 	%p176, %rl239, 0;
	selp.u64 	%rl567, 1, 0, %p176;
	add.s64 	%rl662, %rl567, %rl566;
	xor.b64  	%rl658, %rl658, -9223372036854775808;
	mov.u64 	%rl661, %rl240;

BB7_233:
	mov.u64 	%rl660, %rl661;
	setp.gt.s64 	%p177, %rl662, 0;
	@%p177 bra 	BB7_235;

	mov.u32 	%r551, 0;
	bra.uni 	BB7_237;

BB7_235:
	mov.u32 	%r551, 0;

BB7_236:
	shr.u64 	%rl568, %rl660, 63;
	shl.b64 	%rl569, %rl662, 1;
	or.b64  	%rl662, %rl568, %rl569;
	shl.b64 	%rl660, %rl660, 1;
	add.s32 	%r551, %r551, -1;
	setp.gt.s64 	%p178, %rl662, 0;
	@%p178 bra 	BB7_236;

BB7_237:
	mul.lo.s64 	%rl664, %rl662, -3958705157555305931;
	mov.u64 	%rl572, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl570, %rl662, %rl572;
	// inline asm
	setp.gt.s64 	%p179, %rl570, 0;
	mov.u64 	%rl663, %rl570;
	@%p179 bra 	BB7_238;
	bra.uni 	BB7_239;

BB7_238:
	shl.b64 	%rl573, %rl570, 1;
	shr.u64 	%rl574, %rl664, 63;
	or.b64  	%rl663, %rl573, %rl574;
	mul.lo.s64 	%rl664, %rl662, -7917410315110611862;
	add.s32 	%r551, %r551, -1;

BB7_239:
	setp.ne.s64 	%p180, %rl664, 0;
	selp.u64 	%rl575, 1, 0, %p180;
	add.s64 	%rl576, %rl575, %rl663;
	add.s32 	%r520, %r551, 1022;
	cvt.u64.u32 	%rl577, %r520;
	shl.b64 	%rl578, %rl577, 52;
	shr.u64 	%rl579, %rl576, 11;
	shr.u64 	%rl580, %rl576, 10;
	and.b64  	%rl581, %rl580, 1;
	add.s64 	%rl582, %rl578, %rl579;
	add.s64 	%rl583, %rl582, %rl581;
	or.b64  	%rl584, %rl583, %rl658;
	mov.b64 	 %fd932, %rl584;

BB7_240:
	and.b32  	%r521, %r552, 1;
	setp.eq.s32 	%p181, %r521, 0;
	mul.rn.f64 	%fd90, %fd932, %fd932;
	@%p181 bra 	BB7_242;

	mov.f64 	%fd642, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd644, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd641, %fd642, %fd90, %fd644;
	// inline asm
	mov.f64 	%fd648, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd645, %fd641, %fd90, %fd648;
	// inline asm
	mov.f64 	%fd652, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd649, %fd645, %fd90, %fd652;
	// inline asm
	mov.f64 	%fd656, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd653, %fd649, %fd90, %fd656;
	// inline asm
	mov.f64 	%fd660, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd657, %fd653, %fd90, %fd660;
	// inline asm
	mov.f64 	%fd664, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd661, %fd657, %fd90, %fd664;
	// inline asm
	mov.f64 	%fd668, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd665, %fd661, %fd90, %fd668;
	// inline asm
	mov.f64 	%fd933, %fd665;
	bra.uni 	BB7_243;

BB7_242:
	mov.f64 	%fd670, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd672, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd669, %fd670, %fd90, %fd672;
	// inline asm
	mov.f64 	%fd676, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd673, %fd669, %fd90, %fd676;
	// inline asm
	mov.f64 	%fd680, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd677, %fd673, %fd90, %fd680;
	// inline asm
	mov.f64 	%fd684, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd681, %fd677, %fd90, %fd684;
	// inline asm
	mov.f64 	%fd688, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd685, %fd681, %fd90, %fd688;
	// inline asm
	mul.rn.f64 	%fd690, %fd685, %fd90;
	// inline asm
	fma.rn.f64 	%fd689, %fd690, %fd932, %fd932;
	// inline asm
	mov.f64 	%fd933, %fd689;

BB7_243:
	and.b32  	%r522, %r552, 2;
	setp.eq.s32 	%p182, %r522, 0;
	neg.f64 	%fd693, %fd933;
	selp.f64 	%fd934, %fd933, %fd693, %p182;
	bra.uni 	BB7_245;

BB7_244:
	mov.f64 	%fd694, 0d0000000000000000;
	mul.rn.f64 	%fd934, %fd76, %fd694;

BB7_245:
	neg.f64 	%fd695, %fd934;
	mov.f64 	%fd945, %fd931;
	mov.f64 	%fd946, %fd695;
	@%p9 bra 	BB7_246;
	bra.uni 	BB7_247;

BB7_246:
	mov.f64 	%fd945, %fd931;
	mov.f64 	%fd946, %fd934;

BB7_247:
	mul.f64 	%fd697, %fd7, %fd945;
	neg.f64 	%fd699, %fd8;
	fma.rn.f64 	%fd700, %fd699, %fd946, %fd697;
	mul.f64 	%fd701, %fd8, %fd945;
	fma.rn.f64 	%fd702, %fd7, %fd946, %fd701;
	mov.f64 	%fd943, %fd700;
	mov.f64 	%fd944, %fd702;

BB7_248:
	add.f64 	%fd705, %fd935, %fd937;
	add.f64 	%fd707, %fd705, %fd939;
	add.f64 	%fd709, %fd707, %fd941;
	add.f64 	%fd711, %fd709, %fd943;
	st.global.f64 	[%r26], %fd711;
	add.f64 	%fd714, %fd936, %fd938;
	add.f64 	%fd716, %fd714, %fd940;
	add.f64 	%fd718, %fd716, %fd942;
	add.f64 	%fd720, %fd718, %fd944;
	st.global.f64 	[%r26+8], %fd720;
	mul.f64 	%fd721, %fd937, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd722, %fd938, 0d3FEE6F0E1344FF84;
	mov.f64 	%fd723, 0d3FEE6F0E1344FF84;
	fma.rn.f64 	%fd724, %fd937, 0d3FD3C6EF3736CC3B, %fd722;
	add.f64 	%fd725, %fd935, %fd724;
	mul.f64 	%fd726, %fd939, 0dBFE9E3779B9B661D;
	mul.f64 	%fd727, %fd940, 0d3FE2CF2304766332;
	mov.f64 	%fd728, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd729, %fd939, 0dBFE9E3779B9B661D, %fd727;
	add.f64 	%fd730, %fd725, %fd729;
	mul.f64 	%fd731, %fd941, 0dBFE9E3779B9B661D;
	mul.f64 	%fd732, %fd942, 0d3FE2CF2304766332;
	neg.f64 	%fd733, %fd942;
	fma.rn.f64 	%fd734, %fd733, %fd728, %fd731;
	add.f64 	%fd735, %fd730, %fd734;
	mul.f64 	%fd736, %fd943, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd737, %fd944, 0d3FEE6F0E1344FF84;
	neg.f64 	%fd738, %fd944;
	fma.rn.f64 	%fd739, %fd738, %fd723, %fd736;
	add.f64 	%fd740, %fd735, %fd739;
	st.global.f64 	[%r27], %fd740;
	mul.f64 	%fd741, %fd938, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd742, %fd937, 0d3FEE6F0E1344FF84;
	neg.f64 	%fd743, %fd937;
	fma.rn.f64 	%fd744, %fd743, %fd723, %fd741;
	add.f64 	%fd745, %fd936, %fd744;
	mul.f64 	%fd746, %fd940, 0dBFE9E3779B9B661D;
	mul.f64 	%fd747, %fd939, 0d3FE2CF2304766332;
	neg.f64 	%fd748, %fd939;
	fma.rn.f64 	%fd749, %fd748, %fd728, %fd746;
	add.f64 	%fd750, %fd745, %fd749;
	mul.f64 	%fd751, %fd942, 0dBFE9E3779B9B661D;
	mul.f64 	%fd752, %fd941, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd753, %fd942, 0dBFE9E3779B9B661D, %fd752;
	add.f64 	%fd754, %fd750, %fd753;
	mul.f64 	%fd755, %fd944, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd756, %fd943, 0d3FEE6F0E1344FF84;
	fma.rn.f64 	%fd757, %fd944, 0d3FD3C6EF3736CC3B, %fd756;
	add.f64 	%fd758, %fd754, %fd757;
	st.global.f64 	[%r27+8], %fd758;
	mul.f64 	%fd759, %fd937, 0dBFE9E3779B9B661D;
	mul.f64 	%fd760, %fd938, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd761, %fd937, 0dBFE9E3779B9B661D, %fd760;
	add.f64 	%fd762, %fd935, %fd761;
	mul.f64 	%fd763, %fd939, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd764, %fd940, 0d3FEE6F0E1344FF84;
	neg.f64 	%fd765, %fd940;
	fma.rn.f64 	%fd766, %fd765, %fd723, %fd763;
	add.f64 	%fd767, %fd762, %fd766;
	mul.f64 	%fd768, %fd941, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd769, %fd942, 0d3FEE6F0E1344FF84;
	fma.rn.f64 	%fd770, %fd941, 0d3FD3C6EF3736CC3B, %fd769;
	add.f64 	%fd771, %fd767, %fd770;
	mul.f64 	%fd772, %fd943, 0dBFE9E3779B9B661D;
	mul.f64 	%fd773, %fd944, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd774, %fd738, %fd728, %fd772;
	add.f64 	%fd775, %fd771, %fd774;
	st.global.f64 	[%r28], %fd775;
	mul.f64 	%fd776, %fd938, 0dBFE9E3779B9B661D;
	mul.f64 	%fd777, %fd937, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd778, %fd743, %fd728, %fd776;
	add.f64 	%fd779, %fd936, %fd778;
	mul.f64 	%fd780, %fd940, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd781, %fd939, 0d3FEE6F0E1344FF84;
	fma.rn.f64 	%fd782, %fd940, 0d3FD3C6EF3736CC3B, %fd781;
	add.f64 	%fd783, %fd779, %fd782;
	mul.f64 	%fd784, %fd942, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd785, %fd941, 0d3FEE6F0E1344FF84;
	neg.f64 	%fd786, %fd941;
	fma.rn.f64 	%fd787, %fd786, %fd723, %fd784;
	add.f64 	%fd788, %fd783, %fd787;
	mul.f64 	%fd789, %fd944, 0dBFE9E3779B9B661D;
	mul.f64 	%fd790, %fd943, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd791, %fd944, 0dBFE9E3779B9B661D, %fd790;
	add.f64 	%fd792, %fd788, %fd791;
	st.global.f64 	[%r28+8], %fd792;
	neg.f64 	%fd793, %fd938;
	fma.rn.f64 	%fd794, %fd793, %fd728, %fd759;
	add.f64 	%fd795, %fd935, %fd794;
	fma.rn.f64 	%fd796, %fd939, 0d3FD3C6EF3736CC3B, %fd764;
	add.f64 	%fd797, %fd795, %fd796;
	fma.rn.f64 	%fd798, %fd733, %fd723, %fd768;
	add.f64 	%fd799, %fd797, %fd798;
	fma.rn.f64 	%fd800, %fd943, 0dBFE9E3779B9B661D, %fd773;
	add.f64 	%fd801, %fd799, %fd800;
	st.global.f64 	[%r29], %fd801;
	fma.rn.f64 	%fd802, %fd938, 0dBFE9E3779B9B661D, %fd777;
	add.f64 	%fd803, %fd936, %fd802;
	fma.rn.f64 	%fd804, %fd748, %fd723, %fd780;
	add.f64 	%fd805, %fd803, %fd804;
	fma.rn.f64 	%fd806, %fd942, 0d3FD3C6EF3736CC3B, %fd785;
	add.f64 	%fd807, %fd805, %fd806;
	neg.f64 	%fd808, %fd943;
	fma.rn.f64 	%fd809, %fd808, %fd728, %fd789;
	add.f64 	%fd810, %fd807, %fd809;
	st.global.f64 	[%r29+8], %fd810;
	fma.rn.f64 	%fd811, %fd793, %fd723, %fd721;
	add.f64 	%fd812, %fd935, %fd811;
	fma.rn.f64 	%fd813, %fd765, %fd728, %fd726;
	add.f64 	%fd814, %fd812, %fd813;
	fma.rn.f64 	%fd815, %fd941, 0dBFE9E3779B9B661D, %fd732;
	add.f64 	%fd816, %fd814, %fd815;
	fma.rn.f64 	%fd817, %fd943, 0d3FD3C6EF3736CC3B, %fd737;
	add.f64 	%fd818, %fd816, %fd817;
	st.global.f64 	[%r30], %fd818;
	fma.rn.f64 	%fd819, %fd938, 0d3FD3C6EF3736CC3B, %fd742;
	add.f64 	%fd820, %fd936, %fd819;
	fma.rn.f64 	%fd821, %fd940, 0dBFE9E3779B9B661D, %fd747;
	add.f64 	%fd822, %fd820, %fd821;
	fma.rn.f64 	%fd823, %fd786, %fd728, %fd751;
	add.f64 	%fd824, %fd822, %fd823;
	fma.rn.f64 	%fd825, %fd808, %fd723, %fd755;
	add.f64 	%fd826, %fd824, %fd825;
	st.global.f64 	[%r30+8], %fd826;
	ret;
}

.entry DIT3C2C(
	.param .u32 .ptr .global .align 8 DIT3C2C_param_0,
	.param .u32 DIT3C2C_param_1,
	.param .u32 DIT3C2C_param_2,
	.param .u32 DIT3C2C_param_3
)
{
	.local .align 8 .b8 	__local_depot8[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<15>;
	.reg .f64 	%fd<945>;
	.reg .pred 	%p<114>;
	.reg .s32 	%r<354>;
	.reg .s64 	%rl<333>;


	mov.u32 	%SP, __local_depot8;
	ld.param.u32 	%r109, [DIT3C2C_param_2];
	// inline asm
	mov.u32 	%r105, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r106, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r107, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r108, %tid.x;
	// inline asm
	add.s32 	%r110, %r108, %r105;
	mad.lo.s32 	%r3, %r107, %r106, %r110;
	mul.hi.u32 	%r111, %r109, -1171354717;
	shr.u32 	%r4, %r111, 3;
	mul.lo.s32 	%r112, %r4, 11;
	sub.s32 	%r5, %r109, %r112;
	setp.lt.u32 	%p8, %r109, 11;
	@%p8 bra 	BB8_18;

	mov.f64 	%fd1, 0d4026000000000000;
	mov.f64 	%fd2, 0d7FF0000000000000;
	mov.pred 	%p2, 0;
	mov.f64 	%fd3, 0dFFF0000000000000;
	mov.f64 	%fd4, 0d3FB5555555555555;
	mov.f64 	%fd5, 0dBC46A4CB00B9E7B0;
	mov.u32 	%r333, 0;

BB8_2:
	@%p2 bra 	BB8_17;

	@%p2 bra 	BB8_17;

	mov.f64 	%fd101, 0d4008000000000000;
	// inline asm
	abs.f64 	%fd100, %fd101;
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r114}, %fd100; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r115, hi}, %fd100; 
	}
	// inline asm
	shr.u32 	%r116, %r114, 20;
	and.b32  	%r336, %r116, 2047;
	setp.eq.s32 	%p9, %r336, 0;
	mov.u32 	%r334, %r115;
	mov.u32 	%r335, %r114;
	@%p9 bra 	BB8_5;
	bra.uni 	BB8_6;

BB8_5:
	mov.f64 	%fd106, 0d4350000000000000;
	mul.rn.f64 	%fd105, %fd100, %fd106;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r117}, %fd105; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r118, hi}, %fd105; 
	}
	// inline asm
	shr.u32 	%r119, %r117, 20;
	and.b32  	%r120, %r119, 2047;
	add.s32 	%r336, %r120, -54;
	mov.u32 	%r334, %r118;
	mov.u32 	%r335, %r117;

BB8_6:
	add.s32 	%r337, %r336, -1023;
	and.b32  	%r123, %r335, -2146435073;
	or.b32  	%r122, %r123, 1072693248;
	// inline asm
	mov.b64 	%fd107, {%r334, %r122};
	// inline asm
	setp.gt.u32 	%p10, %r122, 1073127582;
	mov.f64 	%fd915, %fd107;
	@%p10 bra 	BB8_7;
	bra.uni 	BB8_8;

BB8_7:
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r124, hi}, %fd107; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r125}, %fd107; 
	}
	// inline asm
	add.s32 	%r127, %r125, -1048576;
	// inline asm
	mov.b64 	%fd110, {%r124, %r127};
	// inline asm
	add.s32 	%r337, %r336, -1022;
	mov.f64 	%fd915, %fd110;

BB8_8:
	add.f64 	%fd189, %fd915, 0d3FF0000000000000;
	rcp.rn.f64 	%fd190, %fd189;
	add.f64 	%fd137, %fd915, 0dBFF0000000000000;
	mul.rn.f64 	%fd191, %fd137, %fd190;
	add.f64 	%fd185, %fd191, %fd191;
	mul.rn.f64 	%fd133, %fd185, %fd185;
	mov.f64 	%fd112, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd114, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd111, %fd112, %fd133, %fd114;
	// inline asm
	mov.f64 	%fd118, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd115, %fd111, %fd133, %fd118;
	// inline asm
	mov.f64 	%fd122, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd119, %fd115, %fd133, %fd122;
	// inline asm
	mov.f64 	%fd126, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd123, %fd119, %fd133, %fd126;
	// inline asm
	mov.f64 	%fd130, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd127, %fd123, %fd133, %fd130;
	// inline asm
	mov.f64 	%fd134, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd131, %fd127, %fd133, %fd134;
	// inline asm
	mul.rn.f64 	%fd192, %fd131, %fd133;
	sub.f64 	%fd193, %fd137, %fd185;
	mov.f64 	%fd194, 0d4000000000000000;
	mul.rn.f64 	%fd138, %fd194, %fd193;
	neg.f64 	%fd136, %fd185;
	// inline asm
	fma.rn.f64 	%fd135, %fd136, %fd137, %fd138;
	// inline asm
	mul.rn.f64 	%fd181, %fd190, %fd135;
	add.f64 	%fd195, %fd4, %fd192;
	sub.f64 	%fd196, %fd4, %fd195;
	add.f64 	%fd197, %fd196, %fd192;
	add.f64 	%fd198, %fd197, 0d0000000000000000;
	add.f64 	%fd199, %fd198, %fd5;
	add.f64 	%fd148, %fd195, %fd199;
	sub.f64 	%fd200, %fd195, %fd148;
	add.f64 	%fd152, %fd200, %fd199;
	mul.rn.f64 	%fd201, %fd148, %fd185;
	neg.f64 	%fd142, %fd201;
	// inline asm
	fma.rn.f64 	%fd139, %fd148, %fd185, %fd142;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd143, %fd152, %fd181, %fd139;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd147, %fd148, %fd181, %fd143;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd151, %fd152, %fd185, %fd147;
	// inline asm
	add.f64 	%fd164, %fd201, %fd151;
	sub.f64 	%fd202, %fd201, %fd164;
	add.f64 	%fd168, %fd202, %fd151;
	mul.rn.f64 	%fd203, %fd164, %fd185;
	neg.f64 	%fd158, %fd203;
	// inline asm
	fma.rn.f64 	%fd155, %fd164, %fd185, %fd158;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd159, %fd168, %fd181, %fd155;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd163, %fd164, %fd181, %fd159;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd167, %fd168, %fd185, %fd163;
	// inline asm
	add.f64 	%fd180, %fd203, %fd167;
	sub.f64 	%fd204, %fd203, %fd180;
	add.f64 	%fd184, %fd204, %fd167;
	mul.rn.f64 	%fd205, %fd180, %fd185;
	neg.f64 	%fd174, %fd205;
	// inline asm
	fma.rn.f64 	%fd171, %fd180, %fd185, %fd174;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd175, %fd184, %fd181, %fd171;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd179, %fd180, %fd181, %fd175;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd183, %fd184, %fd185, %fd179;
	// inline asm
	add.f64 	%fd206, %fd205, %fd183;
	sub.f64 	%fd207, %fd205, %fd206;
	add.f64 	%fd208, %fd207, %fd183;
	add.f64 	%fd209, %fd185, %fd206;
	sub.f64 	%fd210, %fd185, %fd209;
	add.f64 	%fd211, %fd210, %fd206;
	add.f64 	%fd212, %fd211, %fd208;
	add.f64 	%fd213, %fd212, %fd181;
	add.f64 	%fd214, %fd209, %fd213;
	sub.f64 	%fd215, %fd209, %fd214;
	add.f64 	%fd216, %fd215, %fd213;
	cvt.rn.f64.s32 	%fd217, %r337;
	mov.f64 	%fd218, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd219, %fd217, %fd218;
	mov.f64 	%fd220, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd221, %fd217, %fd220;
	add.f64 	%fd222, %fd219, %fd214;
	sub.f64 	%fd223, %fd219, %fd222;
	add.f64 	%fd224, %fd223, %fd214;
	add.f64 	%fd225, %fd224, %fd216;
	add.f64 	%fd226, %fd225, %fd221;
	add.f64 	%fd10, %fd222, %fd226;
	sub.f64 	%fd227, %fd222, %fd10;
	add.f64 	%fd11, %fd227, %fd226;
	// inline asm
	abs.f64 	%fd187, %fd1;
	// inline asm
	setp.gt.f64 	%p11, %fd187, 0d7F0D2A1BE4048F90;
	@%p11 bra 	BB8_10;

	mov.f64 	%fd916, %fd1;
	bra.uni 	BB8_11;

BB8_10:
	mov.f64 	%fd228, 0d3F20000000000000;
	mul.rn.f64 	%fd12, %fd1, %fd228;
	mov.f64 	%fd916, %fd12;

BB8_11:
	mov.f64 	%fd13, %fd916;
	mul.rn.f64 	%fd238, %fd10, %fd13;
	neg.f64 	%fd232, %fd238;
	// inline asm
	fma.rn.f64 	%fd229, %fd10, %fd13, %fd232;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd233, %fd11, %fd13, %fd229;
	// inline asm
	add.f64 	%fd237, %fd238, %fd233;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r128}, %fd237; 
	}
	// inline asm
	setp.lt.u32 	%p12, %r128, 1082535491;
	setp.lt.s32 	%p13, %r128, -1064875759;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	BB8_13;

	setp.lt.s32 	%p15, %r128, 0;
	selp.f64 	%fd240, 0d0000000000000000, %fd2, %p15;
	add.f64 	%fd241, %fd237, %fd237;
	setp.nan.f64 	%p16, %fd237, %fd237;
	selp.f64 	%fd918, %fd241, %fd240, %p16;
	bra.uni 	BB8_16;

BB8_13:
	mov.f64 	%fd244, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd243, %fd237, %fd244;
	// inline asm
	abs.f64 	%fd242, %fd243;
	// inline asm
	setp.gt.f64 	%p17, %fd242, 0d4330000000000000;
	mov.f64 	%fd917, %fd243;
	@%p17 bra 	BB8_15;

	add.f64 	%fd246, %fd242, 0d3FE0000000000000;
	// inline asm
	cvt.rmi.f64.f64 	%fd245, %fd246;
	// inline asm
	setp.lt.f64 	%p18, %fd242, 0d3FE0000000000000;
	selp.f64 	%fd249, 0d0000000000000000, %fd245, %p18;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r129}, %fd243; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r130, hi}, %fd249; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r131}, %fd249; 
	}
	// inline asm
	and.b32  	%r134, %r129, -2147483648;
	and.b32  	%r135, %r131, 2147483647;
	or.b32  	%r133, %r135, %r134;
	// inline asm
	mov.b64 	%fd250, {%r130, %r133};
	// inline asm
	mov.f64 	%fd917, %fd250;

BB8_15:
	mov.f64 	%fd253, 0dBFE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd251, %fd917, %fd253, %fd237;
	// inline asm
	mov.f64 	%fd257, 0dBC7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd255, %fd917, %fd257, %fd251;
	// inline asm
	cvt.rzi.s32.f64 	%r138, %fd917;
	add.s32 	%r139, %r138, 55;
	setp.lt.s32 	%p19, %r138, -1020;
	selp.b32 	%r140, %r139, %r138, %p19;
	selp.f64 	 %fd308, 0d3C90000000000000, 0d4000000000000000, %p19;
	mov.f64 	%fd260, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd262, 0d3E5AFD81DA6C3BAF;
	// inline asm
	fma.rn.f64 	%fd259, %fd260, %fd255, %fd262;
	// inline asm
	mov.f64 	%fd266, 0d3E927E55F60F80E6;
	// inline asm
	fma.rn.f64 	%fd263, %fd259, %fd255, %fd266;
	// inline asm
	mov.f64 	%fd270, 0d3EC71DDA8F02D666;
	// inline asm
	fma.rn.f64 	%fd267, %fd263, %fd255, %fd270;
	// inline asm
	mov.f64 	%fd274, 0d3EFA01A013B894E0;
	// inline asm
	fma.rn.f64 	%fd271, %fd267, %fd255, %fd274;
	// inline asm
	mov.f64 	%fd278, 0d3F2A01A01D3AF788;
	// inline asm
	fma.rn.f64 	%fd275, %fd271, %fd255, %fd278;
	// inline asm
	mov.f64 	%fd282, 0d3F56C16C16C3A1EC;
	// inline asm
	fma.rn.f64 	%fd279, %fd275, %fd255, %fd282;
	// inline asm
	mov.f64 	%fd286, 0d3F81111111109161;
	// inline asm
	fma.rn.f64 	%fd283, %fd279, %fd255, %fd286;
	// inline asm
	mov.f64 	%fd290, 0d3FA55555555554C1;
	// inline asm
	fma.rn.f64 	%fd287, %fd283, %fd255, %fd290;
	// inline asm
	mov.f64 	%fd294, 0d3FC555555555556F;
	// inline asm
	fma.rn.f64 	%fd291, %fd287, %fd255, %fd294;
	// inline asm
	mov.f64 	%fd298, 0d3FE0000000000000;
	// inline asm
	fma.rn.f64 	%fd295, %fd291, %fd255, %fd298;
	// inline asm
	mul.rn.f64 	%fd300, %fd295, %fd255;
	// inline asm
	fma.rn.f64 	%fd299, %fd300, %fd255, %fd255;
	// inline asm
	shl.b32 	%r141, %r140, 20;
	add.s32 	%r137, %r141, 1071644672;
	mov.u32 	%r136, 0;
	// inline asm
	mov.b64 	%fd303, {%r136, %r137};
	// inline asm
	// inline asm
	fma.rn.f64 	%fd304, %fd299, %fd303, %fd303;
	// inline asm
	mul.rn.f64 	%fd918, %fd304, %fd308;

BB8_16:
	setp.eq.f64 	%p20, %fd918, %fd3;
	setp.eq.f64 	%p21, %fd918, %fd2;
	or.pred  	%p22, %p21, %p20;

BB8_17:
	add.s32 	%r333, %r333, 1;
	setp.lt.s32 	%p23, %r333, %r4;
	@%p23 bra 	BB8_2;

BB8_18:
	cvt.rn.f64.s32 	%fd920, %r5;
	setp.eq.s32 	%p24, %r5, 0;
	@%p24 bra 	BB8_36;

	mov.f64 	%fd24, 0d7FF0000000000000;
	mov.pred 	%p25, 0;
	@%p25 bra 	BB8_37;

	setp.eq.f64 	%p26, %fd920, %fd24;
	mov.f64 	%fd25, 0dFFF0000000000000;
	setp.eq.f64 	%p27, %fd920, 0dFFF0000000000000;
	or.pred  	%p28, %p26, %p27;
	@%p28 bra 	BB8_35;

	neg.f64 	%fd317, %fd24;
	setp.eq.f64 	%p29, %fd317, 0d4008000000000000;
	@%p29 bra 	BB8_37;

	mov.f64 	%fd319, 0d4008000000000000;
	// inline asm
	abs.f64 	%fd318, %fd319;
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r142}, %fd318; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r143, hi}, %fd318; 
	}
	// inline asm
	shr.u32 	%r144, %r142, 20;
	and.b32  	%r340, %r144, 2047;
	setp.eq.s32 	%p30, %r340, 0;
	mov.u32 	%r338, %r143;
	mov.u32 	%r339, %r142;
	@%p30 bra 	BB8_23;
	bra.uni 	BB8_24;

BB8_23:
	mov.f64 	%fd324, 0d4350000000000000;
	mul.rn.f64 	%fd323, %fd318, %fd324;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r145}, %fd323; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r146, hi}, %fd323; 
	}
	// inline asm
	shr.u32 	%r147, %r145, 20;
	and.b32  	%r148, %r147, 2047;
	add.s32 	%r340, %r148, -54;
	mov.u32 	%r338, %r146;
	mov.u32 	%r339, %r145;

BB8_24:
	add.s32 	%r341, %r340, -1023;
	and.b32  	%r151, %r339, -2146435073;
	or.b32  	%r150, %r151, 1072693248;
	// inline asm
	mov.b64 	%fd325, {%r338, %r150};
	// inline asm
	setp.gt.u32 	%p31, %r150, 1073127582;
	mov.f64 	%fd919, %fd325;
	@%p31 bra 	BB8_25;
	bra.uni 	BB8_26;

BB8_25:
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r152, hi}, %fd325; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r153}, %fd325; 
	}
	// inline asm
	add.s32 	%r155, %r153, -1048576;
	// inline asm
	mov.b64 	%fd328, {%r152, %r155};
	// inline asm
	add.s32 	%r341, %r340, -1022;
	mov.f64 	%fd919, %fd328;

BB8_26:
	add.f64 	%fd407, %fd919, 0d3FF0000000000000;
	rcp.rn.f64 	%fd408, %fd407;
	add.f64 	%fd355, %fd919, 0dBFF0000000000000;
	mul.rn.f64 	%fd409, %fd355, %fd408;
	add.f64 	%fd403, %fd409, %fd409;
	mul.rn.f64 	%fd351, %fd403, %fd403;
	mov.f64 	%fd330, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd332, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd329, %fd330, %fd351, %fd332;
	// inline asm
	mov.f64 	%fd336, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd333, %fd329, %fd351, %fd336;
	// inline asm
	mov.f64 	%fd340, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd337, %fd333, %fd351, %fd340;
	// inline asm
	mov.f64 	%fd344, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd341, %fd337, %fd351, %fd344;
	// inline asm
	mov.f64 	%fd348, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd345, %fd341, %fd351, %fd348;
	// inline asm
	mov.f64 	%fd352, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd349, %fd345, %fd351, %fd352;
	// inline asm
	mul.rn.f64 	%fd410, %fd349, %fd351;
	sub.f64 	%fd411, %fd355, %fd403;
	mov.f64 	%fd412, 0d4000000000000000;
	mul.rn.f64 	%fd356, %fd412, %fd411;
	neg.f64 	%fd354, %fd403;
	// inline asm
	fma.rn.f64 	%fd353, %fd354, %fd355, %fd356;
	// inline asm
	mul.rn.f64 	%fd399, %fd408, %fd353;
	mov.f64 	%fd413, 0d3FB5555555555555;
	add.f64 	%fd414, %fd410, 0d3FB5555555555555;
	sub.f64 	%fd415, %fd413, %fd414;
	add.f64 	%fd416, %fd415, %fd410;
	add.f64 	%fd417, %fd416, 0d0000000000000000;
	add.f64 	%fd418, %fd417, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd366, %fd414, %fd418;
	sub.f64 	%fd419, %fd414, %fd366;
	add.f64 	%fd370, %fd419, %fd418;
	mul.rn.f64 	%fd420, %fd366, %fd403;
	neg.f64 	%fd360, %fd420;
	// inline asm
	fma.rn.f64 	%fd357, %fd366, %fd403, %fd360;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd361, %fd370, %fd399, %fd357;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd365, %fd366, %fd399, %fd361;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd369, %fd370, %fd403, %fd365;
	// inline asm
	add.f64 	%fd382, %fd420, %fd369;
	sub.f64 	%fd421, %fd420, %fd382;
	add.f64 	%fd386, %fd421, %fd369;
	mul.rn.f64 	%fd422, %fd382, %fd403;
	neg.f64 	%fd376, %fd422;
	// inline asm
	fma.rn.f64 	%fd373, %fd382, %fd403, %fd376;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd377, %fd386, %fd399, %fd373;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd381, %fd382, %fd399, %fd377;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd385, %fd386, %fd403, %fd381;
	// inline asm
	add.f64 	%fd398, %fd422, %fd385;
	sub.f64 	%fd423, %fd422, %fd398;
	add.f64 	%fd402, %fd423, %fd385;
	mul.rn.f64 	%fd424, %fd398, %fd403;
	neg.f64 	%fd392, %fd424;
	// inline asm
	fma.rn.f64 	%fd389, %fd398, %fd403, %fd392;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd393, %fd402, %fd399, %fd389;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd397, %fd398, %fd399, %fd393;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd401, %fd402, %fd403, %fd397;
	// inline asm
	add.f64 	%fd425, %fd424, %fd401;
	sub.f64 	%fd426, %fd424, %fd425;
	add.f64 	%fd427, %fd426, %fd401;
	add.f64 	%fd428, %fd403, %fd425;
	sub.f64 	%fd429, %fd403, %fd428;
	add.f64 	%fd430, %fd429, %fd425;
	add.f64 	%fd431, %fd430, %fd427;
	add.f64 	%fd432, %fd431, %fd399;
	add.f64 	%fd433, %fd428, %fd432;
	sub.f64 	%fd434, %fd428, %fd433;
	add.f64 	%fd435, %fd434, %fd432;
	cvt.rn.f64.s32 	%fd436, %r341;
	mov.f64 	%fd437, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd438, %fd436, %fd437;
	mov.f64 	%fd439, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd440, %fd436, %fd439;
	add.f64 	%fd441, %fd438, %fd433;
	sub.f64 	%fd442, %fd438, %fd441;
	add.f64 	%fd443, %fd442, %fd433;
	add.f64 	%fd444, %fd443, %fd435;
	add.f64 	%fd445, %fd444, %fd440;
	add.f64 	%fd30, %fd441, %fd445;
	sub.f64 	%fd446, %fd441, %fd30;
	add.f64 	%fd31, %fd446, %fd445;
	// inline asm
	abs.f64 	%fd405, %fd920;
	// inline asm
	setp.gt.f64 	%p32, %fd405, 0d7F0D2A1BE4048F90;
	@%p32 bra 	BB8_27;
	bra.uni 	BB8_28;

BB8_27:
	mov.f64 	%fd447, 0d3F20000000000000;
	mul.rn.f64 	%fd920, %fd920, %fd447;

BB8_28:
	mul.rn.f64 	%fd457, %fd30, %fd920;
	neg.f64 	%fd451, %fd457;
	// inline asm
	fma.rn.f64 	%fd448, %fd30, %fd920, %fd451;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd452, %fd31, %fd920, %fd448;
	// inline asm
	add.f64 	%fd456, %fd457, %fd452;
	sub.f64 	%fd458, %fd457, %fd456;
	add.f64 	%fd35, %fd458, %fd452;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r156}, %fd456; 
	}
	// inline asm
	setp.lt.u32 	%p33, %r156, 1082535491;
	setp.lt.s32 	%p34, %r156, -1064875759;
	or.pred  	%p35, %p33, %p34;
	@%p35 bra 	BB8_30;

	setp.lt.s32 	%p36, %r156, 0;
	selp.f64 	%fd459, 0d0000000000000000, %fd24, %p36;
	add.f64 	%fd460, %fd456, %fd456;
	setp.nan.f64 	%p37, %fd456, %fd456;
	selp.f64 	%fd922, %fd460, %fd459, %p37;
	bra.uni 	BB8_33;

BB8_30:
	mov.f64 	%fd463, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd462, %fd456, %fd463;
	// inline asm
	abs.f64 	%fd461, %fd462;
	// inline asm
	setp.gt.f64 	%p38, %fd461, 0d4330000000000000;
	mov.f64 	%fd921, %fd462;
	@%p38 bra 	BB8_32;

	add.f64 	%fd465, %fd461, 0d3FE0000000000000;
	// inline asm
	cvt.rmi.f64.f64 	%fd464, %fd465;
	// inline asm
	setp.lt.f64 	%p39, %fd461, 0d3FE0000000000000;
	selp.f64 	%fd468, 0d0000000000000000, %fd464, %p39;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r157}, %fd462; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r158, hi}, %fd468; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r159}, %fd468; 
	}
	// inline asm
	and.b32  	%r162, %r157, -2147483648;
	and.b32  	%r163, %r159, 2147483647;
	or.b32  	%r161, %r163, %r162;
	// inline asm
	mov.b64 	%fd469, {%r158, %r161};
	// inline asm
	mov.f64 	%fd921, %fd469;

BB8_32:
	mov.f64 	%fd472, 0dBFE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd470, %fd921, %fd472, %fd456;
	// inline asm
	mov.f64 	%fd476, 0dBC7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd474, %fd921, %fd476, %fd470;
	// inline asm
	cvt.rzi.s32.f64 	%r166, %fd921;
	add.s32 	%r167, %r166, 55;
	setp.lt.s32 	%p40, %r166, -1020;
	selp.b32 	%r168, %r167, %r166, %p40;
	selp.f64 	 %fd527, 0d3C90000000000000, 0d4000000000000000, %p40;
	mov.f64 	%fd479, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd481, 0d3E5AFD81DA6C3BAF;
	// inline asm
	fma.rn.f64 	%fd478, %fd479, %fd474, %fd481;
	// inline asm
	mov.f64 	%fd485, 0d3E927E55F60F80E6;
	// inline asm
	fma.rn.f64 	%fd482, %fd478, %fd474, %fd485;
	// inline asm
	mov.f64 	%fd489, 0d3EC71DDA8F02D666;
	// inline asm
	fma.rn.f64 	%fd486, %fd482, %fd474, %fd489;
	// inline asm
	mov.f64 	%fd493, 0d3EFA01A013B894E0;
	// inline asm
	fma.rn.f64 	%fd490, %fd486, %fd474, %fd493;
	// inline asm
	mov.f64 	%fd497, 0d3F2A01A01D3AF788;
	// inline asm
	fma.rn.f64 	%fd494, %fd490, %fd474, %fd497;
	// inline asm
	mov.f64 	%fd501, 0d3F56C16C16C3A1EC;
	// inline asm
	fma.rn.f64 	%fd498, %fd494, %fd474, %fd501;
	// inline asm
	mov.f64 	%fd505, 0d3F81111111109161;
	// inline asm
	fma.rn.f64 	%fd502, %fd498, %fd474, %fd505;
	// inline asm
	mov.f64 	%fd509, 0d3FA55555555554C1;
	// inline asm
	fma.rn.f64 	%fd506, %fd502, %fd474, %fd509;
	// inline asm
	mov.f64 	%fd513, 0d3FC555555555556F;
	// inline asm
	fma.rn.f64 	%fd510, %fd506, %fd474, %fd513;
	// inline asm
	mov.f64 	%fd517, 0d3FE0000000000000;
	// inline asm
	fma.rn.f64 	%fd514, %fd510, %fd474, %fd517;
	// inline asm
	mul.rn.f64 	%fd519, %fd514, %fd474;
	// inline asm
	fma.rn.f64 	%fd518, %fd519, %fd474, %fd474;
	// inline asm
	shl.b32 	%r169, %r168, 20;
	add.s32 	%r165, %r169, 1071644672;
	mov.u32 	%r164, 0;
	// inline asm
	mov.b64 	%fd522, {%r164, %r165};
	// inline asm
	// inline asm
	fma.rn.f64 	%fd523, %fd518, %fd522, %fd522;
	// inline asm
	mul.rn.f64 	%fd922, %fd523, %fd527;

BB8_33:
	setp.eq.f64 	%p41, %fd922, %fd25;
	setp.eq.f64 	%p42, %fd922, %fd24;
	or.pred  	%p43, %p42, %p41;
	@%p43 bra 	BB8_37;

	// inline asm
	fma.rn.f64 	%fd528, %fd922, %fd35, %fd922;
	// inline asm
	mov.f64 	%fd922, %fd528;
	bra.uni 	BB8_37;

BB8_35:
	mov.f64 	%fd533, 0d4008000000000000;
	// inline asm
	abs.f64 	%fd532, %fd533;
	// inline asm
	setp.gt.f64 	%p44, %fd532, 0d3FF0000000000000;
	selp.f64 	%fd922, %fd24, 0d0000000000000000, %p44;
	bra.uni 	BB8_37;

BB8_36:
	mov.f64 	%fd922, 0d3FF0000000000000;

BB8_37:
	mul.f64 	%fd536, %fd922, 0d3FF0000000000000;
	cvt.rn.f32.f64 	%f1, %fd536;
	div.full.f32 	%f2, %f1, 0f40400000;
	cvt.rzi.s32.f32 	%r170, %f2;
	div.s32 	%r171, %r3, %r170;
	rem.s32 	%r34, %r3, %r170;
	cvt.rn.f32.s32 	%f3, %r34;
	cvt.rn.f32.s32 	%f4, %r171;
	mul.f32 	%f5, %f4, %f1;
	add.f32 	%f6, %f3, %f5;
	add.f32 	%f7, %f6, 0f00000000;
	add.f32 	%f8, %f7, %f7;
	cvt.rzi.s32.f32 	%r172, %f8;
	cvt.rn.f32.s32 	%f9, %r170;
	add.f32 	%f10, %f6, %f9;
	add.f32 	%f11, %f10, %f10;
	cvt.rzi.s32.f32 	%r173, %f11;
	shl.b32 	%r174, %r170, 1;
	cvt.rn.f32.s32 	%f12, %r174;
	add.f32 	%f13, %f6, %f12;
	add.f32 	%f14, %f13, %f13;
	cvt.rzi.s32.f32 	%r175, %f14;
	shl.b32 	%r176, %r172, 3;
	ld.param.u32 	%r331, [DIT3C2C_param_0];
	add.s32 	%r35, %r331, %r176;
	ld.global.f64 	%fd537, [%r35];
	ld.global.f64 	%fd538, [%r35+8];
	shl.b32 	%r177, %r173, 3;
	add.s32 	%r36, %r331, %r177;
	ld.global.f64 	%fd46, [%r36];
	ld.global.f64 	%fd47, [%r36+8];
	shl.b32 	%r178, %r175, 3;
	add.s32 	%r37, %r331, %r178;
	ld.global.f64 	%fd48, [%r37];
	ld.global.f64 	%fd49, [%r37+8];
	mov.f64 	%fd939, %fd48;
	mov.f64 	%fd940, %fd49;
	setp.eq.s32 	%p45, %r34, 0;
	mov.f64 	%fd937, %fd46;
	mov.f64 	%fd938, %fd47;
	mov.f64 	%fd935, %fd537;
	mov.f64 	%fd936, %fd538;
	@%p45 bra 	BB8_139;

	cvt.rn.f64.s32 	%fd50, %r34;
	mul.f64 	%fd539, %fd50, 0d401921FB54442D18;
	cvt.f64.f32 	%fd51, %f1;
	div.rn.f64 	%fd52, %fd539, %fd51;
	mov.f64 	%fd53, 0d7FF0000000000000;
	setp.eq.f64 	%p3, %fd52, 0d7FF0000000000000;
	mov.f64 	%fd54, 0dFFF0000000000000;
	setp.eq.f64 	%p4, %fd52, 0dFFF0000000000000;
	or.pred  	%p46, %p3, %p4;
	add.u32 	%r38, %SP, 0;
	@%p46 bra 	BB8_61;

	// inline asm
	abs.f64 	%fd540, %fd52;
	// inline asm
	setp.gt.f64 	%p47, %fd540, 0d41E0000000000000;
	@%p47 bra 	BB8_41;

	mov.f64 	%fd555, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd542, %fd52, %fd555;
	// inline asm
	cvt.rni.s32.f64 	%r179, %fd542;
	// inline asm
	cvt.rn.f64.s32 	%fd556, %r179;
	neg.f64 	%fd552, %fd556;
	mov.f64 	%fd545, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd543, %fd552, %fd545, %fd52;
	// inline asm
	mov.f64 	%fd549, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd547, %fd552, %fd549, %fd543;
	// inline asm
	mov.f64 	%fd553, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd551, %fd552, %fd553, %fd547;
	// inline asm
	mov.u32 	%r344, %r179;
	mov.f64 	%fd923, %fd551;
	bra.uni 	BB8_57;

BB8_41:
	mov.b64 	 %rl1, %fd52;
	and.b64  	%rl296, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl129, %rl3, 2047;
	add.s64 	%rl130, %rl129, 4294966272;
	cvt.u32.u64 	%r40, %rl130;
	shl.b64 	%rl131, %rl1, 11;
	or.b64  	%rl4, %rl131, -9223372036854775808;
	shr.u32 	%r183, %r40, 6;
	mov.u32 	%r184, 16;
	sub.s32 	%r41, %r184, %r183;
	mov.u32 	%r185, 15;
	sub.s32 	%r342, %r185, %r183;
	mov.u32 	%r186, 19;
	sub.s32 	%r43, %r186, %r183;
	mov.u32 	%r181, 18;
	// inline asm
	min.s32 	%r180, %r181, %r43;
	// inline asm
	setp.lt.s32 	%p48, %r342, %r180;
	@%p48 bra 	BB8_43;

	mov.u64 	%rl293, 0;
	bra.uni 	BB8_45;

BB8_43:
	mov.u32 	%r187, 1;
	sub.s32 	%r44, %r187, %r41;
	mov.u64 	%rl293, 0;

BB8_44:
	.pragma "nounroll";
	shl.b32 	%r191, %r342, 3;
	mov.u32 	%r192, __internal_i2opi_d;
	add.s32 	%r193, %r192, %r191;
	ld.const.u64 	%rl135, [%r193];
	mul.lo.s64 	%rl137, %rl135, %rl4;
	// inline asm
	mul.hi.u64 	%rl134, %rl135, %rl4;
	// inline asm
	mad.lo.s64 	%rl138, %rl135, %rl4, %rl293;
	setp.lt.u64 	%p49, %rl138, %rl137;
	selp.u64 	%rl139, 1, 0, %p49;
	add.s64 	%rl293, %rl139, %rl134;
	add.s32 	%r194, %r44, %r342;
	shl.b32 	%r195, %r194, 3;
	add.s32 	%r197, %r38, %r195;
	st.local.u64 	[%r197], %rl138;
	// inline asm
	min.s32 	%r188, %r181, %r43;
	// inline asm
	add.s32 	%r342, %r342, 1;
	setp.lt.s32 	%p50, %r342, %r188;
	@%p50 bra 	BB8_44;

BB8_45:
	mov.u32 	%r198, 1;
	sub.s32 	%r199, %r198, %r41;
	add.s32 	%r200, %r199, %r342;
	shl.b32 	%r201, %r200, 3;
	add.s32 	%r203, %r38, %r201;
	st.local.u64 	[%r203], %rl293;
	ld.local.u64 	%rl294, [%r38+24];
	ld.local.u64 	%rl295, [%r38+16];
	and.b32  	%r204, %r40, 63;
	setp.eq.s32 	%p51, %r204, 0;
	@%p51 bra 	BB8_47;

	and.b64  	%rl140, %rl3, 63;
	cvt.u32.u64 	%r205, %rl140;
	shl.b64 	%rl141, %rl294, %r205;
	neg.s32 	%r206, %r40;
	and.b32  	%r207, %r206, 63;
	shr.u64 	%rl142, %rl295, %r207;
	or.b64  	%rl294, %rl142, %rl141;
	shl.b64 	%rl143, %rl295, %r205;
	ld.local.u64 	%rl144, [%r38+8];
	shr.u64 	%rl145, %rl144, %r207;
	or.b64  	%rl295, %rl145, %rl143;

BB8_47:
	shr.u64 	%rl146, %rl294, 62;
	cvt.u32.u64 	%r208, %rl146;
	shr.u64 	%rl147, %rl295, 62;
	shl.b64 	%rl148, %rl294, 2;
	or.b64  	%rl300, %rl147, %rl148;
	shl.b64 	%rl15, %rl295, 2;
	setp.ne.s64 	%p52, %rl15, 0;
	selp.u64 	%rl149, 1, 0, %p52;
	or.b64  	%rl150, %rl149, %rl300;
	setp.gt.u64 	%p53, %rl150, -9223372036854775808;
	selp.u32 	%r209, 1, 0, %p53;
	add.s32 	%r210, %r209, %r208;
	neg.s32 	%r211, %r210;
	setp.lt.s64 	%p54, %rl1, 0;
	selp.b32 	%r344, %r211, %r210, %p54;
	@%p53 bra 	BB8_49;

	mov.u64 	%rl299, %rl15;
	bra.uni 	BB8_50;

BB8_49:
	not.b64 	%rl151, %rl300;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p55, %rl15, 0;
	selp.u64 	%rl152, 1, 0, %p55;
	add.s64 	%rl300, %rl152, %rl151;
	xor.b64  	%rl296, %rl296, -9223372036854775808;
	mov.u64 	%rl299, %rl16;

BB8_50:
	mov.u64 	%rl298, %rl299;
	setp.gt.s64 	%p56, %rl300, 0;
	@%p56 bra 	BB8_52;

	mov.u32 	%r343, 0;
	bra.uni 	BB8_54;

BB8_52:
	mov.u32 	%r343, 0;

BB8_53:
	shr.u64 	%rl153, %rl298, 63;
	shl.b64 	%rl154, %rl300, 1;
	or.b64  	%rl300, %rl153, %rl154;
	shl.b64 	%rl298, %rl298, 1;
	add.s32 	%r343, %r343, -1;
	setp.gt.s64 	%p57, %rl300, 0;
	@%p57 bra 	BB8_53;

BB8_54:
	mul.lo.s64 	%rl302, %rl300, -3958705157555305931;
	mov.u64 	%rl157, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl155, %rl300, %rl157;
	// inline asm
	setp.gt.s64 	%p58, %rl155, 0;
	mov.u64 	%rl301, %rl155;
	@%p58 bra 	BB8_55;
	bra.uni 	BB8_56;

BB8_55:
	shl.b64 	%rl158, %rl155, 1;
	shr.u64 	%rl159, %rl302, 63;
	or.b64  	%rl301, %rl158, %rl159;
	mul.lo.s64 	%rl302, %rl300, -7917410315110611862;
	add.s32 	%r343, %r343, -1;

BB8_56:
	setp.ne.s64 	%p59, %rl302, 0;
	selp.u64 	%rl160, 1, 0, %p59;
	add.s64 	%rl161, %rl160, %rl301;
	add.s32 	%r214, %r343, 1022;
	cvt.u64.u32 	%rl162, %r214;
	shl.b64 	%rl163, %rl162, 52;
	shr.u64 	%rl164, %rl161, 11;
	shr.u64 	%rl165, %rl161, 10;
	and.b64  	%rl166, %rl165, 1;
	add.s64 	%rl167, %rl163, %rl164;
	add.s64 	%rl168, %rl167, %rl166;
	or.b64  	%rl169, %rl168, %rl296;
	mov.b64 	 %fd923, %rl169;

BB8_57:
	add.s32 	%r55, %r344, 1;
	and.b32  	%r215, %r55, 1;
	setp.eq.s32 	%p60, %r215, 0;
	mul.rn.f64 	%fd58, %fd923, %fd923;
	@%p60 bra 	BB8_59;

	mov.f64 	%fd558, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd560, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd557, %fd558, %fd58, %fd560;
	// inline asm
	mov.f64 	%fd564, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd561, %fd557, %fd58, %fd564;
	// inline asm
	mov.f64 	%fd568, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd565, %fd561, %fd58, %fd568;
	// inline asm
	mov.f64 	%fd572, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd569, %fd565, %fd58, %fd572;
	// inline asm
	mov.f64 	%fd576, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd573, %fd569, %fd58, %fd576;
	// inline asm
	mov.f64 	%fd580, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd577, %fd573, %fd58, %fd580;
	// inline asm
	mov.f64 	%fd584, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd581, %fd577, %fd58, %fd584;
	// inline asm
	mov.f64 	%fd924, %fd581;
	bra.uni 	BB8_60;

BB8_59:
	mov.f64 	%fd586, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd588, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd585, %fd586, %fd58, %fd588;
	// inline asm
	mov.f64 	%fd592, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd589, %fd585, %fd58, %fd592;
	// inline asm
	mov.f64 	%fd596, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd593, %fd589, %fd58, %fd596;
	// inline asm
	mov.f64 	%fd600, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd597, %fd593, %fd58, %fd600;
	// inline asm
	mov.f64 	%fd604, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd601, %fd597, %fd58, %fd604;
	// inline asm
	mul.rn.f64 	%fd606, %fd601, %fd58;
	// inline asm
	fma.rn.f64 	%fd605, %fd606, %fd923, %fd923;
	// inline asm
	mov.f64 	%fd924, %fd605;

BB8_60:
	and.b32  	%r216, %r55, 2;
	setp.eq.s32 	%p61, %r216, 0;
	neg.f64 	%fd609, %fd924;
	selp.f64 	%fd925, %fd924, %fd609, %p61;
	bra.uni 	BB8_62;

BB8_61:
	mov.f64 	%fd925, 0dFFF8000000000000;

BB8_62:
	setp.eq.f64 	%p62, %fd52, 0d0000000000000000;
	or.pred  	%p63, %p4, %p62;
	or.pred  	%p64, %p3, %p63;
	@%p64 bra 	BB8_85;

	// inline asm
	abs.f64 	%fd611, %fd52;
	// inline asm
	setp.gt.f64 	%p65, %fd611, 0d41E0000000000000;
	@%p65 bra 	BB8_65;

	mov.f64 	%fd626, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd613, %fd52, %fd626;
	// inline asm
	cvt.rni.s32.f64 	%r217, %fd613;
	// inline asm
	cvt.rn.f64.s32 	%fd627, %r217;
	neg.f64 	%fd623, %fd627;
	mov.f64 	%fd616, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd614, %fd623, %fd616, %fd52;
	// inline asm
	mov.f64 	%fd620, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd618, %fd623, %fd620, %fd614;
	// inline asm
	mov.f64 	%fd624, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd622, %fd623, %fd624, %fd618;
	// inline asm
	mov.u32 	%r347, %r217;
	mov.f64 	%fd926, %fd622;
	bra.uni 	BB8_81;

BB8_65:
	mov.b64 	 %rl33, %fd52;
	and.b64  	%rl306, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl170, %rl35, 2047;
	add.s64 	%rl171, %rl170, 4294966272;
	cvt.u32.u64 	%r57, %rl171;
	shl.b64 	%rl172, %rl33, 11;
	or.b64  	%rl36, %rl172, -9223372036854775808;
	shr.u32 	%r221, %r57, 6;
	mov.u32 	%r222, 16;
	sub.s32 	%r58, %r222, %r221;
	mov.u32 	%r223, 15;
	sub.s32 	%r345, %r223, %r221;
	mov.u32 	%r224, 19;
	sub.s32 	%r60, %r224, %r221;
	mov.u32 	%r219, 18;
	// inline asm
	min.s32 	%r218, %r219, %r60;
	// inline asm
	setp.lt.s32 	%p66, %r345, %r218;
	@%p66 bra 	BB8_67;

	mov.u64 	%rl303, 0;
	bra.uni 	BB8_69;

BB8_67:
	mov.u32 	%r225, 1;
	sub.s32 	%r61, %r225, %r58;
	mov.u64 	%rl303, 0;

BB8_68:
	.pragma "nounroll";
	shl.b32 	%r229, %r345, 3;
	mov.u32 	%r230, __internal_i2opi_d;
	add.s32 	%r231, %r230, %r229;
	ld.const.u64 	%rl176, [%r231];
	mul.lo.s64 	%rl178, %rl176, %rl36;
	// inline asm
	mul.hi.u64 	%rl175, %rl176, %rl36;
	// inline asm
	mad.lo.s64 	%rl179, %rl176, %rl36, %rl303;
	setp.lt.u64 	%p67, %rl179, %rl178;
	selp.u64 	%rl180, 1, 0, %p67;
	add.s64 	%rl303, %rl180, %rl175;
	add.s32 	%r232, %r61, %r345;
	shl.b32 	%r233, %r232, 3;
	add.s32 	%r235, %r38, %r233;
	st.local.u64 	[%r235], %rl179;
	// inline asm
	min.s32 	%r226, %r219, %r60;
	// inline asm
	add.s32 	%r345, %r345, 1;
	setp.lt.s32 	%p68, %r345, %r226;
	@%p68 bra 	BB8_68;

BB8_69:
	mov.u32 	%r236, 1;
	sub.s32 	%r237, %r236, %r58;
	add.s32 	%r238, %r237, %r345;
	shl.b32 	%r239, %r238, 3;
	add.s32 	%r241, %r38, %r239;
	st.local.u64 	[%r241], %rl303;
	ld.local.u64 	%rl304, [%r38+24];
	ld.local.u64 	%rl305, [%r38+16];
	and.b32  	%r242, %r57, 63;
	setp.eq.s32 	%p69, %r242, 0;
	@%p69 bra 	BB8_71;

	and.b64  	%rl181, %rl35, 63;
	cvt.u32.u64 	%r243, %rl181;
	shl.b64 	%rl182, %rl304, %r243;
	neg.s32 	%r244, %r57;
	and.b32  	%r245, %r244, 63;
	shr.u64 	%rl183, %rl305, %r245;
	or.b64  	%rl304, %rl183, %rl182;
	shl.b64 	%rl184, %rl305, %r243;
	ld.local.u64 	%rl185, [%r38+8];
	shr.u64 	%rl186, %rl185, %r245;
	or.b64  	%rl305, %rl186, %rl184;

BB8_71:
	shr.u64 	%rl187, %rl304, 62;
	cvt.u32.u64 	%r246, %rl187;
	shr.u64 	%rl188, %rl305, 62;
	shl.b64 	%rl189, %rl304, 2;
	or.b64  	%rl310, %rl188, %rl189;
	shl.b64 	%rl47, %rl305, 2;
	setp.ne.s64 	%p70, %rl47, 0;
	selp.u64 	%rl190, 1, 0, %p70;
	or.b64  	%rl191, %rl190, %rl310;
	setp.gt.u64 	%p71, %rl191, -9223372036854775808;
	selp.u32 	%r247, 1, 0, %p71;
	add.s32 	%r248, %r247, %r246;
	neg.s32 	%r249, %r248;
	setp.lt.s64 	%p72, %rl33, 0;
	selp.b32 	%r347, %r249, %r248, %p72;
	@%p71 bra 	BB8_73;

	mov.u64 	%rl309, %rl47;
	bra.uni 	BB8_74;

BB8_73:
	not.b64 	%rl192, %rl310;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p73, %rl47, 0;
	selp.u64 	%rl193, 1, 0, %p73;
	add.s64 	%rl310, %rl193, %rl192;
	xor.b64  	%rl306, %rl306, -9223372036854775808;
	mov.u64 	%rl309, %rl48;

BB8_74:
	mov.u64 	%rl308, %rl309;
	setp.gt.s64 	%p74, %rl310, 0;
	@%p74 bra 	BB8_76;

	mov.u32 	%r346, 0;
	bra.uni 	BB8_78;

BB8_76:
	mov.u32 	%r346, 0;

BB8_77:
	shr.u64 	%rl194, %rl308, 63;
	shl.b64 	%rl195, %rl310, 1;
	or.b64  	%rl310, %rl194, %rl195;
	shl.b64 	%rl308, %rl308, 1;
	add.s32 	%r346, %r346, -1;
	setp.gt.s64 	%p75, %rl310, 0;
	@%p75 bra 	BB8_77;

BB8_78:
	mul.lo.s64 	%rl312, %rl310, -3958705157555305931;
	mov.u64 	%rl198, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl196, %rl310, %rl198;
	// inline asm
	setp.gt.s64 	%p76, %rl196, 0;
	mov.u64 	%rl311, %rl196;
	@%p76 bra 	BB8_79;
	bra.uni 	BB8_80;

BB8_79:
	shl.b64 	%rl199, %rl196, 1;
	shr.u64 	%rl200, %rl312, 63;
	or.b64  	%rl311, %rl199, %rl200;
	mul.lo.s64 	%rl312, %rl310, -7917410315110611862;
	add.s32 	%r346, %r346, -1;

BB8_80:
	setp.ne.s64 	%p77, %rl312, 0;
	selp.u64 	%rl201, 1, 0, %p77;
	add.s64 	%rl202, %rl201, %rl311;
	add.s32 	%r252, %r346, 1022;
	cvt.u64.u32 	%rl203, %r252;
	shl.b64 	%rl204, %rl203, 52;
	shr.u64 	%rl205, %rl202, 11;
	shr.u64 	%rl206, %rl202, 10;
	and.b64  	%rl207, %rl206, 1;
	add.s64 	%rl208, %rl204, %rl205;
	add.s64 	%rl209, %rl208, %rl207;
	or.b64  	%rl210, %rl209, %rl306;
	mov.b64 	 %fd926, %rl210;

BB8_81:
	and.b32  	%r253, %r347, 1;
	setp.eq.s32 	%p78, %r253, 0;
	mul.rn.f64 	%fd68, %fd926, %fd926;
	@%p78 bra 	BB8_83;

	mov.f64 	%fd629, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd631, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd628, %fd629, %fd68, %fd631;
	// inline asm
	mov.f64 	%fd635, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd632, %fd628, %fd68, %fd635;
	// inline asm
	mov.f64 	%fd639, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd636, %fd632, %fd68, %fd639;
	// inline asm
	mov.f64 	%fd643, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd640, %fd636, %fd68, %fd643;
	// inline asm
	mov.f64 	%fd647, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd644, %fd640, %fd68, %fd647;
	// inline asm
	mov.f64 	%fd651, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd648, %fd644, %fd68, %fd651;
	// inline asm
	mov.f64 	%fd655, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd652, %fd648, %fd68, %fd655;
	// inline asm
	mov.f64 	%fd927, %fd652;
	bra.uni 	BB8_84;

BB8_83:
	mov.f64 	%fd657, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd659, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd656, %fd657, %fd68, %fd659;
	// inline asm
	mov.f64 	%fd663, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd660, %fd656, %fd68, %fd663;
	// inline asm
	mov.f64 	%fd667, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd664, %fd660, %fd68, %fd667;
	// inline asm
	mov.f64 	%fd671, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd668, %fd664, %fd68, %fd671;
	// inline asm
	mov.f64 	%fd675, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd672, %fd668, %fd68, %fd675;
	// inline asm
	mul.rn.f64 	%fd677, %fd672, %fd68;
	// inline asm
	fma.rn.f64 	%fd676, %fd677, %fd926, %fd926;
	// inline asm
	mov.f64 	%fd927, %fd676;

BB8_84:
	and.b32  	%r254, %r347, 2;
	setp.eq.s32 	%p79, %r254, 0;
	neg.f64 	%fd680, %fd927;
	selp.f64 	%fd928, %fd927, %fd680, %p79;
	bra.uni 	BB8_86;

BB8_85:
	mov.f64 	%fd681, 0d0000000000000000;
	mul.rn.f64 	%fd928, %fd52, %fd681;

BB8_86:
	neg.f64 	%fd682, %fd928;
	mov.f64 	%fd943, %fd925;
	mov.f64 	%fd944, %fd682;
	ld.param.u32 	%r332, [DIT3C2C_param_3];
	setp.eq.s32 	%p5, %r332, 0;
	@%p5 bra 	BB8_87;
	bra.uni 	BB8_88;

BB8_87:
	mov.f64 	%fd943, %fd925;
	mov.f64 	%fd944, %fd928;

BB8_88:
	mul.f64 	%fd684, %fd46, %fd943;
	neg.f64 	%fd686, %fd47;
	fma.rn.f64 	%fd687, %fd686, %fd944, %fd684;
	mul.f64 	%fd688, %fd47, %fd943;
	fma.rn.f64 	%fd689, %fd46, %fd944, %fd688;
	mul.f64 	%fd690, %fd50, 0d402921FB54442D18;
	div.rn.f64 	%fd75, %fd690, %fd51;
	setp.eq.f64 	%p6, %fd75, %fd53;
	setp.eq.f64 	%p7, %fd75, %fd54;
	or.pred  	%p80, %p6, %p7;
	mov.f64 	%fd935, %fd537;
	mov.f64 	%fd936, %fd538;
	mov.f64 	%fd937, %fd687;
	mov.f64 	%fd938, %fd689;
	@%p80 bra 	BB8_111;

	// inline asm
	abs.f64 	%fd691, %fd75;
	// inline asm
	setp.gt.f64 	%p81, %fd691, 0d41E0000000000000;
	@%p81 bra 	BB8_91;

	mov.f64 	%fd706, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd693, %fd75, %fd706;
	// inline asm
	cvt.rni.s32.f64 	%r255, %fd693;
	// inline asm
	cvt.rn.f64.s32 	%fd707, %r255;
	neg.f64 	%fd703, %fd707;
	mov.f64 	%fd696, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd694, %fd703, %fd696, %fd75;
	// inline asm
	mov.f64 	%fd700, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd698, %fd703, %fd700, %fd694;
	// inline asm
	mov.f64 	%fd704, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd702, %fd703, %fd704, %fd698;
	// inline asm
	mov.u32 	%r350, %r255;
	mov.f64 	%fd929, %fd702;
	bra.uni 	BB8_107;

BB8_91:
	mov.b64 	 %rl65, %fd75;
	and.b64  	%rl316, %rl65, -9223372036854775808;
	shr.u64 	%rl67, %rl65, 52;
	and.b64  	%rl211, %rl67, 2047;
	add.s64 	%rl212, %rl211, 4294966272;
	cvt.u32.u64 	%r73, %rl212;
	shl.b64 	%rl213, %rl65, 11;
	or.b64  	%rl68, %rl213, -9223372036854775808;
	shr.u32 	%r259, %r73, 6;
	mov.u32 	%r260, 16;
	sub.s32 	%r74, %r260, %r259;
	mov.u32 	%r261, 15;
	sub.s32 	%r348, %r261, %r259;
	mov.u32 	%r262, 19;
	sub.s32 	%r76, %r262, %r259;
	mov.u32 	%r257, 18;
	// inline asm
	min.s32 	%r256, %r257, %r76;
	// inline asm
	setp.lt.s32 	%p82, %r348, %r256;
	@%p82 bra 	BB8_93;

	mov.u64 	%rl313, 0;
	bra.uni 	BB8_95;

BB8_93:
	mov.u32 	%r263, 1;
	sub.s32 	%r77, %r263, %r74;
	mov.u64 	%rl313, 0;

BB8_94:
	.pragma "nounroll";
	shl.b32 	%r267, %r348, 3;
	mov.u32 	%r268, __internal_i2opi_d;
	add.s32 	%r269, %r268, %r267;
	ld.const.u64 	%rl217, [%r269];
	mul.lo.s64 	%rl219, %rl217, %rl68;
	// inline asm
	mul.hi.u64 	%rl216, %rl217, %rl68;
	// inline asm
	mad.lo.s64 	%rl220, %rl217, %rl68, %rl313;
	setp.lt.u64 	%p83, %rl220, %rl219;
	selp.u64 	%rl221, 1, 0, %p83;
	add.s64 	%rl313, %rl221, %rl216;
	add.s32 	%r270, %r77, %r348;
	shl.b32 	%r271, %r270, 3;
	add.s32 	%r273, %r38, %r271;
	st.local.u64 	[%r273], %rl220;
	// inline asm
	min.s32 	%r264, %r257, %r76;
	// inline asm
	add.s32 	%r348, %r348, 1;
	setp.lt.s32 	%p84, %r348, %r264;
	@%p84 bra 	BB8_94;

BB8_95:
	mov.u32 	%r274, 1;
	sub.s32 	%r275, %r274, %r74;
	add.s32 	%r276, %r275, %r348;
	shl.b32 	%r277, %r276, 3;
	add.s32 	%r279, %r38, %r277;
	st.local.u64 	[%r279], %rl313;
	ld.local.u64 	%rl314, [%r38+24];
	ld.local.u64 	%rl315, [%r38+16];
	and.b32  	%r280, %r73, 63;
	setp.eq.s32 	%p85, %r280, 0;
	@%p85 bra 	BB8_97;

	and.b64  	%rl222, %rl67, 63;
	cvt.u32.u64 	%r281, %rl222;
	shl.b64 	%rl223, %rl314, %r281;
	neg.s32 	%r282, %r73;
	and.b32  	%r283, %r282, 63;
	shr.u64 	%rl224, %rl315, %r283;
	or.b64  	%rl314, %rl224, %rl223;
	shl.b64 	%rl225, %rl315, %r281;
	ld.local.u64 	%rl226, [%r38+8];
	shr.u64 	%rl227, %rl226, %r283;
	or.b64  	%rl315, %rl227, %rl225;

BB8_97:
	shr.u64 	%rl228, %rl314, 62;
	cvt.u32.u64 	%r284, %rl228;
	shr.u64 	%rl229, %rl315, 62;
	shl.b64 	%rl230, %rl314, 2;
	or.b64  	%rl320, %rl229, %rl230;
	shl.b64 	%rl79, %rl315, 2;
	setp.ne.s64 	%p86, %rl79, 0;
	selp.u64 	%rl231, 1, 0, %p86;
	or.b64  	%rl232, %rl231, %rl320;
	setp.gt.u64 	%p87, %rl232, -9223372036854775808;
	selp.u32 	%r285, 1, 0, %p87;
	add.s32 	%r286, %r285, %r284;
	neg.s32 	%r287, %r286;
	setp.lt.s64 	%p88, %rl65, 0;
	selp.b32 	%r350, %r287, %r286, %p88;
	@%p87 bra 	BB8_99;

	mov.u64 	%rl319, %rl79;
	bra.uni 	BB8_100;

BB8_99:
	not.b64 	%rl233, %rl320;
	neg.s64 	%rl80, %rl79;
	setp.eq.s64 	%p89, %rl79, 0;
	selp.u64 	%rl234, 1, 0, %p89;
	add.s64 	%rl320, %rl234, %rl233;
	xor.b64  	%rl316, %rl316, -9223372036854775808;
	mov.u64 	%rl319, %rl80;

BB8_100:
	mov.u64 	%rl318, %rl319;
	setp.gt.s64 	%p90, %rl320, 0;
	@%p90 bra 	BB8_102;

	mov.u32 	%r349, 0;
	bra.uni 	BB8_104;

BB8_102:
	mov.u32 	%r349, 0;

BB8_103:
	shr.u64 	%rl235, %rl318, 63;
	shl.b64 	%rl236, %rl320, 1;
	or.b64  	%rl320, %rl235, %rl236;
	shl.b64 	%rl318, %rl318, 1;
	add.s32 	%r349, %r349, -1;
	setp.gt.s64 	%p91, %rl320, 0;
	@%p91 bra 	BB8_103;

BB8_104:
	mul.lo.s64 	%rl322, %rl320, -3958705157555305931;
	mov.u64 	%rl239, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl237, %rl320, %rl239;
	// inline asm
	setp.gt.s64 	%p92, %rl237, 0;
	mov.u64 	%rl321, %rl237;
	@%p92 bra 	BB8_105;
	bra.uni 	BB8_106;

BB8_105:
	shl.b64 	%rl240, %rl237, 1;
	shr.u64 	%rl241, %rl322, 63;
	or.b64  	%rl321, %rl240, %rl241;
	mul.lo.s64 	%rl322, %rl320, -7917410315110611862;
	add.s32 	%r349, %r349, -1;

BB8_106:
	setp.ne.s64 	%p93, %rl322, 0;
	selp.u64 	%rl242, 1, 0, %p93;
	add.s64 	%rl243, %rl242, %rl321;
	add.s32 	%r290, %r349, 1022;
	cvt.u64.u32 	%rl244, %r290;
	shl.b64 	%rl245, %rl244, 52;
	shr.u64 	%rl246, %rl243, 11;
	shr.u64 	%rl247, %rl243, 10;
	and.b64  	%rl248, %rl247, 1;
	add.s64 	%rl249, %rl245, %rl246;
	add.s64 	%rl250, %rl249, %rl248;
	or.b64  	%rl251, %rl250, %rl316;
	mov.b64 	 %fd929, %rl251;

BB8_107:
	add.s32 	%r88, %r350, 1;
	and.b32  	%r291, %r88, 1;
	setp.eq.s32 	%p94, %r291, 0;
	mul.rn.f64 	%fd79, %fd929, %fd929;
	@%p94 bra 	BB8_109;

	mov.f64 	%fd709, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd711, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd708, %fd709, %fd79, %fd711;
	// inline asm
	mov.f64 	%fd715, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd712, %fd708, %fd79, %fd715;
	// inline asm
	mov.f64 	%fd719, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd716, %fd712, %fd79, %fd719;
	// inline asm
	mov.f64 	%fd723, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd720, %fd716, %fd79, %fd723;
	// inline asm
	mov.f64 	%fd727, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd724, %fd720, %fd79, %fd727;
	// inline asm
	mov.f64 	%fd731, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd728, %fd724, %fd79, %fd731;
	// inline asm
	mov.f64 	%fd735, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd732, %fd728, %fd79, %fd735;
	// inline asm
	mov.f64 	%fd930, %fd732;
	bra.uni 	BB8_110;

BB8_109:
	mov.f64 	%fd737, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd739, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd736, %fd737, %fd79, %fd739;
	// inline asm
	mov.f64 	%fd743, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd740, %fd736, %fd79, %fd743;
	// inline asm
	mov.f64 	%fd747, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd744, %fd740, %fd79, %fd747;
	// inline asm
	mov.f64 	%fd751, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd748, %fd744, %fd79, %fd751;
	// inline asm
	mov.f64 	%fd755, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd752, %fd748, %fd79, %fd755;
	// inline asm
	mul.rn.f64 	%fd757, %fd752, %fd79;
	// inline asm
	fma.rn.f64 	%fd756, %fd757, %fd929, %fd929;
	// inline asm
	mov.f64 	%fd930, %fd756;

BB8_110:
	and.b32  	%r292, %r88, 2;
	setp.eq.s32 	%p95, %r292, 0;
	neg.f64 	%fd760, %fd930;
	selp.f64 	%fd931, %fd930, %fd760, %p95;
	bra.uni 	BB8_112;

BB8_111:
	mov.f64 	%fd931, 0dFFF8000000000000;

BB8_112:
	setp.eq.f64 	%p96, %fd75, 0d0000000000000000;
	or.pred  	%p97, %p7, %p96;
	or.pred  	%p98, %p6, %p97;
	@%p98 bra 	BB8_135;

	// inline asm
	abs.f64 	%fd762, %fd75;
	// inline asm
	setp.gt.f64 	%p99, %fd762, 0d41E0000000000000;
	@%p99 bra 	BB8_115;

	mov.f64 	%fd777, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd764, %fd75, %fd777;
	// inline asm
	cvt.rni.s32.f64 	%r293, %fd764;
	// inline asm
	cvt.rn.f64.s32 	%fd778, %r293;
	neg.f64 	%fd774, %fd778;
	mov.f64 	%fd767, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd765, %fd774, %fd767, %fd75;
	// inline asm
	mov.f64 	%fd771, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd769, %fd774, %fd771, %fd765;
	// inline asm
	mov.f64 	%fd775, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd773, %fd774, %fd775, %fd769;
	// inline asm
	mov.u32 	%r353, %r293;
	mov.f64 	%fd932, %fd773;
	bra.uni 	BB8_131;

BB8_115:
	mov.b64 	 %rl97, %fd75;
	and.b64  	%rl326, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl252, %rl99, 2047;
	add.s64 	%rl253, %rl252, 4294966272;
	cvt.u32.u64 	%r90, %rl253;
	shl.b64 	%rl254, %rl97, 11;
	or.b64  	%rl100, %rl254, -9223372036854775808;
	shr.u32 	%r297, %r90, 6;
	mov.u32 	%r298, 16;
	sub.s32 	%r91, %r298, %r297;
	mov.u32 	%r299, 15;
	sub.s32 	%r351, %r299, %r297;
	mov.u32 	%r300, 19;
	sub.s32 	%r93, %r300, %r297;
	mov.u32 	%r295, 18;
	// inline asm
	min.s32 	%r294, %r295, %r93;
	// inline asm
	setp.lt.s32 	%p100, %r351, %r294;
	@%p100 bra 	BB8_117;

	mov.u64 	%rl323, 0;
	bra.uni 	BB8_119;

BB8_117:
	mov.u32 	%r301, 1;
	sub.s32 	%r94, %r301, %r91;
	mov.u64 	%rl323, 0;

BB8_118:
	.pragma "nounroll";
	shl.b32 	%r305, %r351, 3;
	mov.u32 	%r306, __internal_i2opi_d;
	add.s32 	%r307, %r306, %r305;
	ld.const.u64 	%rl258, [%r307];
	mul.lo.s64 	%rl260, %rl258, %rl100;
	// inline asm
	mul.hi.u64 	%rl257, %rl258, %rl100;
	// inline asm
	mad.lo.s64 	%rl261, %rl258, %rl100, %rl323;
	setp.lt.u64 	%p101, %rl261, %rl260;
	selp.u64 	%rl262, 1, 0, %p101;
	add.s64 	%rl323, %rl262, %rl257;
	add.s32 	%r308, %r94, %r351;
	shl.b32 	%r309, %r308, 3;
	add.s32 	%r311, %r38, %r309;
	st.local.u64 	[%r311], %rl261;
	// inline asm
	min.s32 	%r302, %r295, %r93;
	// inline asm
	add.s32 	%r351, %r351, 1;
	setp.lt.s32 	%p102, %r351, %r302;
	@%p102 bra 	BB8_118;

BB8_119:
	mov.u32 	%r312, 1;
	sub.s32 	%r313, %r312, %r91;
	add.s32 	%r314, %r313, %r351;
	shl.b32 	%r315, %r314, 3;
	add.s32 	%r317, %r38, %r315;
	st.local.u64 	[%r317], %rl323;
	ld.local.u64 	%rl324, [%r38+24];
	ld.local.u64 	%rl325, [%r38+16];
	and.b32  	%r318, %r90, 63;
	setp.eq.s32 	%p103, %r318, 0;
	@%p103 bra 	BB8_121;

	and.b64  	%rl263, %rl99, 63;
	cvt.u32.u64 	%r319, %rl263;
	shl.b64 	%rl264, %rl324, %r319;
	neg.s32 	%r320, %r90;
	and.b32  	%r321, %r320, 63;
	shr.u64 	%rl265, %rl325, %r321;
	or.b64  	%rl324, %rl265, %rl264;
	shl.b64 	%rl266, %rl325, %r319;
	ld.local.u64 	%rl267, [%r38+8];
	shr.u64 	%rl268, %rl267, %r321;
	or.b64  	%rl325, %rl268, %rl266;

BB8_121:
	shr.u64 	%rl269, %rl324, 62;
	cvt.u32.u64 	%r322, %rl269;
	shr.u64 	%rl270, %rl325, 62;
	shl.b64 	%rl271, %rl324, 2;
	or.b64  	%rl330, %rl270, %rl271;
	shl.b64 	%rl111, %rl325, 2;
	setp.ne.s64 	%p104, %rl111, 0;
	selp.u64 	%rl272, 1, 0, %p104;
	or.b64  	%rl273, %rl272, %rl330;
	setp.gt.u64 	%p105, %rl273, -9223372036854775808;
	selp.u32 	%r323, 1, 0, %p105;
	add.s32 	%r324, %r323, %r322;
	neg.s32 	%r325, %r324;
	setp.lt.s64 	%p106, %rl97, 0;
	selp.b32 	%r353, %r325, %r324, %p106;
	@%p105 bra 	BB8_123;

	mov.u64 	%rl329, %rl111;
	bra.uni 	BB8_124;

BB8_123:
	not.b64 	%rl274, %rl330;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p107, %rl111, 0;
	selp.u64 	%rl275, 1, 0, %p107;
	add.s64 	%rl330, %rl275, %rl274;
	xor.b64  	%rl326, %rl326, -9223372036854775808;
	mov.u64 	%rl329, %rl112;

BB8_124:
	mov.u64 	%rl328, %rl329;
	setp.gt.s64 	%p108, %rl330, 0;
	@%p108 bra 	BB8_126;

	mov.u32 	%r352, 0;
	bra.uni 	BB8_128;

BB8_126:
	mov.u32 	%r352, 0;

BB8_127:
	shr.u64 	%rl276, %rl328, 63;
	shl.b64 	%rl277, %rl330, 1;
	or.b64  	%rl330, %rl276, %rl277;
	shl.b64 	%rl328, %rl328, 1;
	add.s32 	%r352, %r352, -1;
	setp.gt.s64 	%p109, %rl330, 0;
	@%p109 bra 	BB8_127;

BB8_128:
	mul.lo.s64 	%rl332, %rl330, -3958705157555305931;
	mov.u64 	%rl280, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl278, %rl330, %rl280;
	// inline asm
	setp.gt.s64 	%p110, %rl278, 0;
	mov.u64 	%rl331, %rl278;
	@%p110 bra 	BB8_129;
	bra.uni 	BB8_130;

BB8_129:
	shl.b64 	%rl281, %rl278, 1;
	shr.u64 	%rl282, %rl332, 63;
	or.b64  	%rl331, %rl281, %rl282;
	mul.lo.s64 	%rl332, %rl330, -7917410315110611862;
	add.s32 	%r352, %r352, -1;

BB8_130:
	setp.ne.s64 	%p111, %rl332, 0;
	selp.u64 	%rl283, 1, 0, %p111;
	add.s64 	%rl284, %rl283, %rl331;
	add.s32 	%r328, %r352, 1022;
	cvt.u64.u32 	%rl285, %r328;
	shl.b64 	%rl286, %rl285, 52;
	shr.u64 	%rl287, %rl284, 11;
	shr.u64 	%rl288, %rl284, 10;
	and.b64  	%rl289, %rl288, 1;
	add.s64 	%rl290, %rl286, %rl287;
	add.s64 	%rl291, %rl290, %rl289;
	or.b64  	%rl292, %rl291, %rl326;
	mov.b64 	 %fd932, %rl292;

BB8_131:
	and.b32  	%r329, %r353, 1;
	setp.eq.s32 	%p112, %r329, 0;
	mul.rn.f64 	%fd89, %fd932, %fd932;
	@%p112 bra 	BB8_133;

	mov.f64 	%fd780, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd782, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd779, %fd780, %fd89, %fd782;
	// inline asm
	mov.f64 	%fd786, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd783, %fd779, %fd89, %fd786;
	// inline asm
	mov.f64 	%fd790, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd787, %fd783, %fd89, %fd790;
	// inline asm
	mov.f64 	%fd794, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd791, %fd787, %fd89, %fd794;
	// inline asm
	mov.f64 	%fd798, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd795, %fd791, %fd89, %fd798;
	// inline asm
	mov.f64 	%fd802, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd799, %fd795, %fd89, %fd802;
	// inline asm
	mov.f64 	%fd806, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd803, %fd799, %fd89, %fd806;
	// inline asm
	mov.f64 	%fd933, %fd803;
	bra.uni 	BB8_134;

BB8_133:
	mov.f64 	%fd808, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd810, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd807, %fd808, %fd89, %fd810;
	// inline asm
	mov.f64 	%fd814, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd811, %fd807, %fd89, %fd814;
	// inline asm
	mov.f64 	%fd818, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd815, %fd811, %fd89, %fd818;
	// inline asm
	mov.f64 	%fd822, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd819, %fd815, %fd89, %fd822;
	// inline asm
	mov.f64 	%fd826, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd823, %fd819, %fd89, %fd826;
	// inline asm
	mul.rn.f64 	%fd828, %fd823, %fd89;
	// inline asm
	fma.rn.f64 	%fd827, %fd828, %fd932, %fd932;
	// inline asm
	mov.f64 	%fd933, %fd827;

BB8_134:
	and.b32  	%r330, %r353, 2;
	setp.eq.s32 	%p113, %r330, 0;
	neg.f64 	%fd831, %fd933;
	selp.f64 	%fd934, %fd933, %fd831, %p113;
	bra.uni 	BB8_136;

BB8_135:
	mov.f64 	%fd832, 0d0000000000000000;
	mul.rn.f64 	%fd934, %fd75, %fd832;

BB8_136:
	neg.f64 	%fd833, %fd934;
	mov.f64 	%fd941, %fd931;
	mov.f64 	%fd942, %fd833;
	@%p5 bra 	BB8_137;
	bra.uni 	BB8_138;

BB8_137:
	mov.f64 	%fd941, %fd931;
	mov.f64 	%fd942, %fd934;

BB8_138:
	mul.f64 	%fd835, %fd48, %fd941;
	neg.f64 	%fd837, %fd49;
	fma.rn.f64 	%fd838, %fd837, %fd942, %fd835;
	mul.f64 	%fd839, %fd49, %fd941;
	fma.rn.f64 	%fd840, %fd48, %fd942, %fd839;
	mov.f64 	%fd939, %fd838;
	mov.f64 	%fd940, %fd840;

BB8_139:
	add.f64 	%fd843, %fd935, %fd937;
	add.f64 	%fd845, %fd843, %fd939;
	st.global.f64 	[%r35], %fd845;
	add.f64 	%fd848, %fd936, %fd938;
	add.f64 	%fd850, %fd848, %fd940;
	st.global.f64 	[%r35+8], %fd850;
	mul.f64 	%fd851, %fd937, 0dBFE0000000000000;
	mul.f64 	%fd852, %fd938, 0d3FEBB645A1CAC083;
	mov.f64 	%fd853, 0d3FEBB645A1CAC083;
	fma.rn.f64 	%fd854, %fd937, 0dBFE0000000000000, %fd852;
	add.f64 	%fd855, %fd935, %fd854;
	mul.f64 	%fd856, %fd939, 0dBFE0000000000000;
	mul.f64 	%fd857, %fd940, 0d3FEBB645A1CAC083;
	neg.f64 	%fd858, %fd940;
	fma.rn.f64 	%fd859, %fd858, %fd853, %fd856;
	add.f64 	%fd860, %fd855, %fd859;
	st.global.f64 	[%r36], %fd860;
	mul.f64 	%fd861, %fd937, 0dBFEBB645A1CAC083;
	mov.f64 	%fd862, 0d3FE0000000000000;
	neg.f64 	%fd863, %fd938;
	fma.rn.f64 	%fd864, %fd863, %fd862, %fd861;
	add.f64 	%fd865, %fd936, %fd864;
	mul.f64 	%fd866, %fd939, 0d3FEBB645A1CAC083;
	fma.rn.f64 	%fd867, %fd858, %fd862, %fd866;
	add.f64 	%fd868, %fd865, %fd867;
	st.global.f64 	[%r36+8], %fd868;
	fma.rn.f64 	%fd869, %fd863, %fd853, %fd851;
	add.f64 	%fd870, %fd935, %fd869;
	fma.rn.f64 	%fd871, %fd939, 0dBFE0000000000000, %fd857;
	add.f64 	%fd872, %fd870, %fd871;
	st.global.f64 	[%r37], %fd872;
	mul.f64 	%fd873, %fd937, 0d3FEBB645A1CAC083;
	fma.rn.f64 	%fd874, %fd863, %fd862, %fd873;
	add.f64 	%fd875, %fd936, %fd874;
	mul.f64 	%fd876, %fd939, 0dBFEBB645A1CAC083;
	fma.rn.f64 	%fd877, %fd858, %fd862, %fd876;
	add.f64 	%fd878, %fd875, %fd877;
	st.global.f64 	[%r37+8], %fd878;
	ret;
}

.entry DIT3C2CM(
	.param .u32 .ptr .global .align 8 DIT3C2CM_param_0,
	.param .u32 DIT3C2CM_param_1,
	.param .u32 DIT3C2CM_param_2,
	.param .u32 DIT3C2CM_param_3,
	.param .u32 DIT3C2CM_param_4,
	.param .u32 DIT3C2CM_param_5
)
{
	.local .align 8 .b8 	__local_depot9[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<451>;
	.reg .pred 	%p<113>;
	.reg .s32 	%r<355>;
	.reg .s64 	%rl<333>;


	mov.u32 	%SP, __local_depot9;
	ld.param.u32 	%r116, [DIT3C2CM_param_3];
	// inline asm
	mov.u32 	%r108, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r109, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r110, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r111, %tid.x;
	// inline asm
	add.s32 	%r117, %r111, %r108;
	mad.lo.s32 	%r5, %r110, %r109, %r117;
	// inline asm
	mov.u32 	%r112, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r113, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r114, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r115, %tid.y;
	// inline asm
	add.s32 	%r118, %r115, %r112;
	mad.lo.s32 	%r6, %r114, %r113, %r118;
	mul.hi.s32 	%r119, %r116, -1431655765;
	add.s32 	%r120, %r119, %r116;
	shr.u32 	%r121, %r120, 31;
	shr.s32 	%r122, %r120, 2;
	add.s32 	%r7, %r122, %r121;
	mul.lo.s32 	%r123, %r7, 6;
	sub.s32 	%r8, %r116, %r123;
	setp.gt.s32 	%p12, %r116, 5;
	@%p12 bra 	BB9_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB9_23;

BB9_2:
	mov.f32 	%f1, 0f40C00000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40400000;
	add.f32 	%f2, %f47, 0f40C00000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r336, 0;
	mov.u32 	%r335, 1;

BB9_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p13, %p3, %p3;
	@%p13 bra 	BB9_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mov.f32 	%f55, 0f40000000;
	mul.rn.f32 	%f56, %f55, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p14, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p15, %f1, %f52;
	and.pred  	%p4, %p14, %p15;
	setp.eq.f32 	%p16, %f48, 0f00000000;
	@%p16 bra 	BB9_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r11, %f58;
	shr.u32 	%r126, %r11, 23;
	and.b32  	%r127, %r126, 255;
	add.s32 	%r337, %r127, -127;
	setp.eq.s32 	%p17, %r127, 0;
	mov.f32 	%f280, %f58;
	@%p17 bra 	BB9_6;
	bra.uni 	BB9_7;

BB9_6:
	and.b32  	%r128, %r11, -2139095041;
	or.b32  	%r129, %r128, 1065353216;
	mov.b32 	 %f60, %r129;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r130, %f61;
	shr.u32 	%r131, %r130, 23;
	and.b32  	%r132, %r131, 255;
	add.s32 	%r337, %r132, -253;
	and.b32  	%r133, %r130, -2139095041;
	or.b32  	%r134, %r133, 1065353216;
	mov.b32 	 %f280, %r134;

BB9_7:
	mov.b32 	 %r135, %f280;
	and.b32  	%r136, %r135, -2139095041;
	or.b32  	%r137, %r136, 1065353216;
	mov.b32 	 %f281, %r137;
	setp.gt.f32 	%p18, %f281, 0f3FB504F3;
	@%p18 bra 	BB9_8;
	bra.uni 	BB9_9;

BB9_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r337, %r337, 1;

BB9_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f55, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r138, %f73;
	and.b32  	%r139, %r138, -4096;
	mov.b32 	 %f81, %r139;
	mov.b32 	 %r140, %f64;
	and.b32  	%r141, %r140, -4096;
	mov.b32 	 %f82, %r141;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f55, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r337;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p19, %f69, 0f77F684DF;
	@%p19 bra 	BB9_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB9_12;

BB9_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB9_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r17, %f131;
	setp.eq.s32 	%p20, %r17, 1118925336;
	@%p20 bra 	BB9_13;
	bra.uni 	BB9_14;

BB9_13:
	add.s32 	%r142, %r17, -1;
	mov.b32 	 %f133, %r142;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB9_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p21, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p21;
	setp.gt.f32 	%p22, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p22;
	setp.neu.f32 	%p23, %f19, %f3;
	@%p23 bra 	BB9_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB9_21;

BB9_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB9_21;

BB9_17:
	@%p3 bra 	BB9_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB9_21;

BB9_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB9_21;

BB9_20:
	mov.f32 	%f283, %f7;

BB9_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r335;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r335, %f157;
	add.s32 	%r336, %r336, 1;
	setp.lt.s32 	%p24, %r336, %r7;
	@%p24 bra 	BB9_3;

	cvt.rn.f32.s32 	%f284, %r335;

BB9_23:
	mov.f32 	%f159, 0f40400000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r8;
	setp.eq.f32 	%p25, %f287, 0f00000000;
	@%p25 bra 	BB9_45;

	setp.nan.f32 	%p26, %f287, %f287;
	@%p26 bra 	BB9_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p27, %f287, 0f7F800000;
	setp.eq.f32 	%p28, %f287, 0fFF800000;
	or.pred  	%p29, %p27, %p28;
	@%p29 bra 	BB9_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p32, %f158, 0f00000000;
	@%p32 bra 	BB9_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r20, %f168;
	shr.u32 	%r143, %r20, 23;
	and.b32  	%r144, %r143, 255;
	add.s32 	%r338, %r144, -127;
	setp.eq.s32 	%p33, %r144, 0;
	mov.f32 	%f285, %f168;
	@%p33 bra 	BB9_28;
	bra.uni 	BB9_29;

BB9_28:
	and.b32  	%r145, %r20, -2139095041;
	or.b32  	%r146, %r145, 1065353216;
	mov.b32 	 %f170, %r146;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r147, %f171;
	shr.u32 	%r148, %r147, 23;
	and.b32  	%r149, %r148, 255;
	add.s32 	%r338, %r149, -253;
	and.b32  	%r150, %r147, -2139095041;
	or.b32  	%r151, %r150, 1065353216;
	mov.b32 	 %f285, %r151;

BB9_29:
	mov.b32 	 %r152, %f285;
	and.b32  	%r153, %r152, -2139095041;
	or.b32  	%r154, %r153, 1065353216;
	mov.b32 	 %f286, %r154;
	setp.gt.f32 	%p34, %f286, 0f3FB504F3;
	@%p34 bra 	BB9_30;
	bra.uni 	BB9_31;

BB9_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r338, %r338, 1;

BB9_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mov.f32 	%f182, 0f40000000;
	mul.rn.f32 	%f183, %f182, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r155, %f183;
	and.b32  	%r156, %r155, -4096;
	mov.b32 	 %f191, %r156;
	mov.b32 	 %r157, %f174;
	and.b32  	%r158, %r157, -4096;
	mov.b32 	 %f192, %r158;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f182, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r338;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p35, %f179, 0f77F684DF;
	@%p35 bra 	BB9_32;
	bra.uni 	BB9_33;

BB9_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB9_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r26, %f241;
	setp.eq.s32 	%p36, %r26, 1118925336;
	@%p36 bra 	BB9_34;
	bra.uni 	BB9_35;

BB9_34:
	add.s32 	%r159, %r26, -1;
	mov.b32 	 %f243, %r159;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB9_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p37, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p37;
	setp.gt.f32 	%p38, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p38;
	setp.neu.f32 	%p39, %f39, %f27;
	@%p39 bra 	BB9_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB9_46;

BB9_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB9_46;

BB9_38:
	setp.lt.f32 	%p40, %f287, 0f00000000;
	@%p40 bra 	BB9_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB9_46;

BB9_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB9_46;

BB9_41:
	setp.lt.f32 	%p41, %f158, 0f3F800000;
	mov.b32 	 %r160, %f287;
	setp.lt.s32 	%p6, %r160, 0;
	@%p41 bra 	BB9_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB9_46;

BB9_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB9_46;

BB9_44:
	add.f32 	%f288, %f287, 0f40400000;
	bra.uni 	BB9_46;

BB9_45:
	mov.f32 	%f288, 0f3F800000;

BB9_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r27, %f267;
	mul.hi.s32 	%r161, %r27, -1431655765;
	add.s32 	%r162, %r161, %r27;
	shr.u32 	%r163, %r162, 31;
	shr.s32 	%r164, %r162, 1;
	add.s32 	%r28, %r164, %r163;
	ld.param.u32 	%r334, [DIT3C2CM_param_5];
	setp.eq.s32 	%p42, %r334, 0;
	@%p42 bra 	BB9_49;

	ld.param.u32 	%r333, [DIT3C2CM_param_5];
	setp.ne.s32 	%p43, %r333, 1;
	@%p43 bra 	BB9_50;

	div.s32 	%r339, %r6, %r28;
	rem.s32 	%r340, %r6, %r28;
	ld.param.u32 	%r342, [DIT3C2CM_param_1];
	mov.u32 	%r341, %r5;
	bra.uni 	BB9_51;

BB9_49:
	ld.param.u32 	%r331, [DIT3C2CM_param_1];
	mul.lo.s32 	%r31, %r6, %r331;
	div.s32 	%r339, %r5, %r28;
	rem.s32 	%r340, %r5, %r28;
	mov.u32 	%r342, 1;
	mov.u32 	%r341, %r31;
	bra.uni 	BB9_51;

BB9_50:
	mov.u32 	%r342, 1;
	mov.u32 	%r341, 0;
	mov.u32 	%r340, %r170;
	mov.u32 	%r339, %r171;

BB9_51:
	mad.lo.s32 	%r172, %r339, %r27, %r340;
	mad.lo.s32 	%r173, %r342, %r172, %r341;
	shl.b32 	%r174, %r173, 4;
	ld.param.u32 	%r329, [DIT3C2CM_param_0];
	add.s32 	%r38, %r329, %r174;
	ld.global.f64 	%fd51, [%r38];
	ld.global.f64 	%fd52, [%r38+8];
	mul.lo.s32 	%r175, %r342, %r28;
	shl.b32 	%r176, %r175, 4;
	add.s32 	%r39, %r38, %r176;
	ld.global.f64 	%fd1, [%r39];
	ld.global.f64 	%fd2, [%r39+8];
	add.s32 	%r40, %r39, %r176;
	ld.global.f64 	%fd3, [%r40];
	ld.global.f64 	%fd4, [%r40+8];
	mov.f64 	%fd445, %fd3;
	mov.f64 	%fd446, %fd4;
	setp.eq.s32 	%p44, %r340, 0;
	mov.f64 	%fd441, %fd51;
	mov.f64 	%fd442, %fd52;
	mov.f64 	%fd443, %fd1;
	mov.f64 	%fd444, %fd2;
	@%p44 bra 	BB9_153;

	cvt.rn.f64.s32 	%fd5, %r340;
	mul.f64 	%fd53, %fd5, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd6, %r27;
	div.rn.f64 	%fd7, %fd53, %fd6;
	mov.f64 	%fd8, 0d7FF0000000000000;
	setp.eq.f64 	%p7, %fd7, 0d7FF0000000000000;
	mov.f64 	%fd9, 0dFFF0000000000000;
	setp.eq.f64 	%p8, %fd7, 0dFFF0000000000000;
	or.pred  	%p45, %p7, %p8;
	add.u32 	%r41, %SP, 0;
	@%p45 bra 	BB9_75;

	// inline asm
	abs.f64 	%fd54, %fd7;
	// inline asm
	setp.gt.f64 	%p46, %fd54, 0d41E0000000000000;
	@%p46 bra 	BB9_55;

	mov.f64 	%fd69, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd56, %fd7, %fd69;
	// inline asm
	cvt.rni.s32.f64 	%r177, %fd56;
	// inline asm
	cvt.rn.f64.s32 	%fd70, %r177;
	neg.f64 	%fd66, %fd70;
	mov.f64 	%fd59, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd57, %fd66, %fd59, %fd7;
	// inline asm
	mov.f64 	%fd63, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd61, %fd66, %fd63, %fd57;
	// inline asm
	mov.f64 	%fd67, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd65, %fd66, %fd67, %fd61;
	// inline asm
	mov.u32 	%r345, %r177;
	mov.f64 	%fd429, %fd65;
	bra.uni 	BB9_71;

BB9_55:
	mov.b64 	 %rl1, %fd7;
	and.b64  	%rl296, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl129, %rl3, 2047;
	add.s64 	%rl130, %rl129, 4294966272;
	cvt.u32.u64 	%r43, %rl130;
	shl.b64 	%rl131, %rl1, 11;
	or.b64  	%rl4, %rl131, -9223372036854775808;
	shr.u32 	%r181, %r43, 6;
	mov.u32 	%r182, 16;
	sub.s32 	%r44, %r182, %r181;
	mov.u32 	%r183, 15;
	sub.s32 	%r343, %r183, %r181;
	mov.u32 	%r184, 19;
	sub.s32 	%r46, %r184, %r181;
	mov.u32 	%r179, 18;
	// inline asm
	min.s32 	%r178, %r179, %r46;
	// inline asm
	setp.lt.s32 	%p47, %r343, %r178;
	@%p47 bra 	BB9_57;

	mov.u64 	%rl293, 0;
	bra.uni 	BB9_59;

BB9_57:
	mov.u32 	%r185, 1;
	sub.s32 	%r47, %r185, %r44;
	mov.u64 	%rl293, 0;

BB9_58:
	.pragma "nounroll";
	shl.b32 	%r189, %r343, 3;
	mov.u32 	%r190, __internal_i2opi_d;
	add.s32 	%r191, %r190, %r189;
	ld.const.u64 	%rl135, [%r191];
	mul.lo.s64 	%rl137, %rl135, %rl4;
	// inline asm
	mul.hi.u64 	%rl134, %rl135, %rl4;
	// inline asm
	mad.lo.s64 	%rl138, %rl135, %rl4, %rl293;
	setp.lt.u64 	%p48, %rl138, %rl137;
	selp.u64 	%rl139, 1, 0, %p48;
	add.s64 	%rl293, %rl139, %rl134;
	add.s32 	%r192, %r47, %r343;
	shl.b32 	%r193, %r192, 3;
	add.s32 	%r195, %r41, %r193;
	st.local.u64 	[%r195], %rl138;
	// inline asm
	min.s32 	%r186, %r179, %r46;
	// inline asm
	add.s32 	%r343, %r343, 1;
	setp.lt.s32 	%p49, %r343, %r186;
	@%p49 bra 	BB9_58;

BB9_59:
	mov.u32 	%r196, 1;
	sub.s32 	%r197, %r196, %r44;
	add.s32 	%r198, %r197, %r343;
	shl.b32 	%r199, %r198, 3;
	add.s32 	%r201, %r41, %r199;
	st.local.u64 	[%r201], %rl293;
	ld.local.u64 	%rl294, [%r41+24];
	ld.local.u64 	%rl295, [%r41+16];
	and.b32  	%r202, %r43, 63;
	setp.eq.s32 	%p50, %r202, 0;
	@%p50 bra 	BB9_61;

	and.b64  	%rl140, %rl3, 63;
	cvt.u32.u64 	%r203, %rl140;
	shl.b64 	%rl141, %rl294, %r203;
	neg.s32 	%r204, %r43;
	and.b32  	%r205, %r204, 63;
	shr.u64 	%rl142, %rl295, %r205;
	or.b64  	%rl294, %rl142, %rl141;
	shl.b64 	%rl143, %rl295, %r203;
	ld.local.u64 	%rl144, [%r41+8];
	shr.u64 	%rl145, %rl144, %r205;
	or.b64  	%rl295, %rl145, %rl143;

BB9_61:
	shr.u64 	%rl146, %rl294, 62;
	cvt.u32.u64 	%r206, %rl146;
	shr.u64 	%rl147, %rl295, 62;
	shl.b64 	%rl148, %rl294, 2;
	or.b64  	%rl300, %rl147, %rl148;
	shl.b64 	%rl15, %rl295, 2;
	setp.ne.s64 	%p51, %rl15, 0;
	selp.u64 	%rl149, 1, 0, %p51;
	or.b64  	%rl150, %rl149, %rl300;
	setp.gt.u64 	%p52, %rl150, -9223372036854775808;
	selp.u32 	%r207, 1, 0, %p52;
	add.s32 	%r208, %r207, %r206;
	neg.s32 	%r209, %r208;
	setp.lt.s64 	%p53, %rl1, 0;
	selp.b32 	%r345, %r209, %r208, %p53;
	@%p52 bra 	BB9_63;

	mov.u64 	%rl299, %rl15;
	bra.uni 	BB9_64;

BB9_63:
	not.b64 	%rl151, %rl300;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p54, %rl15, 0;
	selp.u64 	%rl152, 1, 0, %p54;
	add.s64 	%rl300, %rl152, %rl151;
	xor.b64  	%rl296, %rl296, -9223372036854775808;
	mov.u64 	%rl299, %rl16;

BB9_64:
	mov.u64 	%rl298, %rl299;
	setp.gt.s64 	%p55, %rl300, 0;
	@%p55 bra 	BB9_66;

	mov.u32 	%r344, 0;
	bra.uni 	BB9_68;

BB9_66:
	mov.u32 	%r344, 0;

BB9_67:
	shr.u64 	%rl153, %rl298, 63;
	shl.b64 	%rl154, %rl300, 1;
	or.b64  	%rl300, %rl153, %rl154;
	shl.b64 	%rl298, %rl298, 1;
	add.s32 	%r344, %r344, -1;
	setp.gt.s64 	%p56, %rl300, 0;
	@%p56 bra 	BB9_67;

BB9_68:
	mul.lo.s64 	%rl302, %rl300, -3958705157555305931;
	mov.u64 	%rl157, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl155, %rl300, %rl157;
	// inline asm
	setp.gt.s64 	%p57, %rl155, 0;
	mov.u64 	%rl301, %rl155;
	@%p57 bra 	BB9_69;
	bra.uni 	BB9_70;

BB9_69:
	shl.b64 	%rl158, %rl155, 1;
	shr.u64 	%rl159, %rl302, 63;
	or.b64  	%rl301, %rl158, %rl159;
	mul.lo.s64 	%rl302, %rl300, -7917410315110611862;
	add.s32 	%r344, %r344, -1;

BB9_70:
	setp.ne.s64 	%p58, %rl302, 0;
	selp.u64 	%rl160, 1, 0, %p58;
	add.s64 	%rl161, %rl160, %rl301;
	add.s32 	%r212, %r344, 1022;
	cvt.u64.u32 	%rl162, %r212;
	shl.b64 	%rl163, %rl162, 52;
	shr.u64 	%rl164, %rl161, 11;
	shr.u64 	%rl165, %rl161, 10;
	and.b64  	%rl166, %rl165, 1;
	add.s64 	%rl167, %rl163, %rl164;
	add.s64 	%rl168, %rl167, %rl166;
	or.b64  	%rl169, %rl168, %rl296;
	mov.b64 	 %fd429, %rl169;

BB9_71:
	add.s32 	%r58, %r345, 1;
	and.b32  	%r213, %r58, 1;
	setp.eq.s32 	%p59, %r213, 0;
	mul.rn.f64 	%fd13, %fd429, %fd429;
	@%p59 bra 	BB9_73;

	mov.f64 	%fd72, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd74, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd71, %fd72, %fd13, %fd74;
	// inline asm
	mov.f64 	%fd78, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd75, %fd71, %fd13, %fd78;
	// inline asm
	mov.f64 	%fd82, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd79, %fd75, %fd13, %fd82;
	// inline asm
	mov.f64 	%fd86, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd83, %fd79, %fd13, %fd86;
	// inline asm
	mov.f64 	%fd90, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd87, %fd83, %fd13, %fd90;
	// inline asm
	mov.f64 	%fd94, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd91, %fd87, %fd13, %fd94;
	// inline asm
	mov.f64 	%fd98, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd95, %fd91, %fd13, %fd98;
	// inline asm
	mov.f64 	%fd430, %fd95;
	bra.uni 	BB9_74;

BB9_73:
	mov.f64 	%fd100, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd102, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd99, %fd100, %fd13, %fd102;
	// inline asm
	mov.f64 	%fd106, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd103, %fd99, %fd13, %fd106;
	// inline asm
	mov.f64 	%fd110, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd107, %fd103, %fd13, %fd110;
	// inline asm
	mov.f64 	%fd114, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd111, %fd107, %fd13, %fd114;
	// inline asm
	mov.f64 	%fd118, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd115, %fd111, %fd13, %fd118;
	// inline asm
	mul.rn.f64 	%fd120, %fd115, %fd13;
	// inline asm
	fma.rn.f64 	%fd119, %fd120, %fd429, %fd429;
	// inline asm
	mov.f64 	%fd430, %fd119;

BB9_74:
	and.b32  	%r214, %r58, 2;
	setp.eq.s32 	%p60, %r214, 0;
	neg.f64 	%fd123, %fd430;
	selp.f64 	%fd431, %fd430, %fd123, %p60;
	bra.uni 	BB9_76;

BB9_75:
	mov.f64 	%fd431, 0dFFF8000000000000;

BB9_76:
	setp.eq.f64 	%p61, %fd7, 0d0000000000000000;
	or.pred  	%p62, %p8, %p61;
	or.pred  	%p63, %p7, %p62;
	@%p63 bra 	BB9_99;

	// inline asm
	abs.f64 	%fd125, %fd7;
	// inline asm
	setp.gt.f64 	%p64, %fd125, 0d41E0000000000000;
	@%p64 bra 	BB9_79;

	mov.f64 	%fd140, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd127, %fd7, %fd140;
	// inline asm
	cvt.rni.s32.f64 	%r215, %fd127;
	// inline asm
	cvt.rn.f64.s32 	%fd141, %r215;
	neg.f64 	%fd137, %fd141;
	mov.f64 	%fd130, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd128, %fd137, %fd130, %fd7;
	// inline asm
	mov.f64 	%fd134, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd132, %fd137, %fd134, %fd128;
	// inline asm
	mov.f64 	%fd138, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd136, %fd137, %fd138, %fd132;
	// inline asm
	mov.u32 	%r348, %r215;
	mov.f64 	%fd432, %fd136;
	bra.uni 	BB9_95;

BB9_79:
	mov.b64 	 %rl33, %fd7;
	and.b64  	%rl306, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl170, %rl35, 2047;
	add.s64 	%rl171, %rl170, 4294966272;
	cvt.u32.u64 	%r60, %rl171;
	shl.b64 	%rl172, %rl33, 11;
	or.b64  	%rl36, %rl172, -9223372036854775808;
	shr.u32 	%r219, %r60, 6;
	mov.u32 	%r220, 16;
	sub.s32 	%r61, %r220, %r219;
	mov.u32 	%r221, 15;
	sub.s32 	%r346, %r221, %r219;
	mov.u32 	%r222, 19;
	sub.s32 	%r63, %r222, %r219;
	mov.u32 	%r217, 18;
	// inline asm
	min.s32 	%r216, %r217, %r63;
	// inline asm
	setp.lt.s32 	%p65, %r346, %r216;
	@%p65 bra 	BB9_81;

	mov.u64 	%rl303, 0;
	bra.uni 	BB9_83;

BB9_81:
	mov.u32 	%r223, 1;
	sub.s32 	%r64, %r223, %r61;
	mov.u64 	%rl303, 0;

BB9_82:
	.pragma "nounroll";
	shl.b32 	%r227, %r346, 3;
	mov.u32 	%r228, __internal_i2opi_d;
	add.s32 	%r229, %r228, %r227;
	ld.const.u64 	%rl176, [%r229];
	mul.lo.s64 	%rl178, %rl176, %rl36;
	// inline asm
	mul.hi.u64 	%rl175, %rl176, %rl36;
	// inline asm
	mad.lo.s64 	%rl179, %rl176, %rl36, %rl303;
	setp.lt.u64 	%p66, %rl179, %rl178;
	selp.u64 	%rl180, 1, 0, %p66;
	add.s64 	%rl303, %rl180, %rl175;
	add.s32 	%r230, %r64, %r346;
	shl.b32 	%r231, %r230, 3;
	add.s32 	%r233, %r41, %r231;
	st.local.u64 	[%r233], %rl179;
	// inline asm
	min.s32 	%r224, %r217, %r63;
	// inline asm
	add.s32 	%r346, %r346, 1;
	setp.lt.s32 	%p67, %r346, %r224;
	@%p67 bra 	BB9_82;

BB9_83:
	mov.u32 	%r234, 1;
	sub.s32 	%r235, %r234, %r61;
	add.s32 	%r236, %r235, %r346;
	shl.b32 	%r237, %r236, 3;
	add.s32 	%r239, %r41, %r237;
	st.local.u64 	[%r239], %rl303;
	ld.local.u64 	%rl304, [%r41+24];
	ld.local.u64 	%rl305, [%r41+16];
	and.b32  	%r240, %r60, 63;
	setp.eq.s32 	%p68, %r240, 0;
	@%p68 bra 	BB9_85;

	and.b64  	%rl181, %rl35, 63;
	cvt.u32.u64 	%r241, %rl181;
	shl.b64 	%rl182, %rl304, %r241;
	neg.s32 	%r242, %r60;
	and.b32  	%r243, %r242, 63;
	shr.u64 	%rl183, %rl305, %r243;
	or.b64  	%rl304, %rl183, %rl182;
	shl.b64 	%rl184, %rl305, %r241;
	ld.local.u64 	%rl185, [%r41+8];
	shr.u64 	%rl186, %rl185, %r243;
	or.b64  	%rl305, %rl186, %rl184;

BB9_85:
	shr.u64 	%rl187, %rl304, 62;
	cvt.u32.u64 	%r244, %rl187;
	shr.u64 	%rl188, %rl305, 62;
	shl.b64 	%rl189, %rl304, 2;
	or.b64  	%rl310, %rl188, %rl189;
	shl.b64 	%rl47, %rl305, 2;
	setp.ne.s64 	%p69, %rl47, 0;
	selp.u64 	%rl190, 1, 0, %p69;
	or.b64  	%rl191, %rl190, %rl310;
	setp.gt.u64 	%p70, %rl191, -9223372036854775808;
	selp.u32 	%r245, 1, 0, %p70;
	add.s32 	%r246, %r245, %r244;
	neg.s32 	%r247, %r246;
	setp.lt.s64 	%p71, %rl33, 0;
	selp.b32 	%r348, %r247, %r246, %p71;
	@%p70 bra 	BB9_87;

	mov.u64 	%rl309, %rl47;
	bra.uni 	BB9_88;

BB9_87:
	not.b64 	%rl192, %rl310;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p72, %rl47, 0;
	selp.u64 	%rl193, 1, 0, %p72;
	add.s64 	%rl310, %rl193, %rl192;
	xor.b64  	%rl306, %rl306, -9223372036854775808;
	mov.u64 	%rl309, %rl48;

BB9_88:
	mov.u64 	%rl308, %rl309;
	setp.gt.s64 	%p73, %rl310, 0;
	@%p73 bra 	BB9_90;

	mov.u32 	%r347, 0;
	bra.uni 	BB9_92;

BB9_90:
	mov.u32 	%r347, 0;

BB9_91:
	shr.u64 	%rl194, %rl308, 63;
	shl.b64 	%rl195, %rl310, 1;
	or.b64  	%rl310, %rl194, %rl195;
	shl.b64 	%rl308, %rl308, 1;
	add.s32 	%r347, %r347, -1;
	setp.gt.s64 	%p74, %rl310, 0;
	@%p74 bra 	BB9_91;

BB9_92:
	mul.lo.s64 	%rl312, %rl310, -3958705157555305931;
	mov.u64 	%rl198, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl196, %rl310, %rl198;
	// inline asm
	setp.gt.s64 	%p75, %rl196, 0;
	mov.u64 	%rl311, %rl196;
	@%p75 bra 	BB9_93;
	bra.uni 	BB9_94;

BB9_93:
	shl.b64 	%rl199, %rl196, 1;
	shr.u64 	%rl200, %rl312, 63;
	or.b64  	%rl311, %rl199, %rl200;
	mul.lo.s64 	%rl312, %rl310, -7917410315110611862;
	add.s32 	%r347, %r347, -1;

BB9_94:
	setp.ne.s64 	%p76, %rl312, 0;
	selp.u64 	%rl201, 1, 0, %p76;
	add.s64 	%rl202, %rl201, %rl311;
	add.s32 	%r250, %r347, 1022;
	cvt.u64.u32 	%rl203, %r250;
	shl.b64 	%rl204, %rl203, 52;
	shr.u64 	%rl205, %rl202, 11;
	shr.u64 	%rl206, %rl202, 10;
	and.b64  	%rl207, %rl206, 1;
	add.s64 	%rl208, %rl204, %rl205;
	add.s64 	%rl209, %rl208, %rl207;
	or.b64  	%rl210, %rl209, %rl306;
	mov.b64 	 %fd432, %rl210;

BB9_95:
	and.b32  	%r251, %r348, 1;
	setp.eq.s32 	%p77, %r251, 0;
	mul.rn.f64 	%fd23, %fd432, %fd432;
	@%p77 bra 	BB9_97;

	mov.f64 	%fd143, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd145, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd142, %fd143, %fd23, %fd145;
	// inline asm
	mov.f64 	%fd149, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd146, %fd142, %fd23, %fd149;
	// inline asm
	mov.f64 	%fd153, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd150, %fd146, %fd23, %fd153;
	// inline asm
	mov.f64 	%fd157, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd154, %fd150, %fd23, %fd157;
	// inline asm
	mov.f64 	%fd161, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd158, %fd154, %fd23, %fd161;
	// inline asm
	mov.f64 	%fd165, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd162, %fd158, %fd23, %fd165;
	// inline asm
	mov.f64 	%fd169, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd166, %fd162, %fd23, %fd169;
	// inline asm
	mov.f64 	%fd433, %fd166;
	bra.uni 	BB9_98;

BB9_97:
	mov.f64 	%fd171, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd173, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd170, %fd171, %fd23, %fd173;
	// inline asm
	mov.f64 	%fd177, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd174, %fd170, %fd23, %fd177;
	// inline asm
	mov.f64 	%fd181, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd178, %fd174, %fd23, %fd181;
	// inline asm
	mov.f64 	%fd185, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd182, %fd178, %fd23, %fd185;
	// inline asm
	mov.f64 	%fd189, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd186, %fd182, %fd23, %fd189;
	// inline asm
	mul.rn.f64 	%fd191, %fd186, %fd23;
	// inline asm
	fma.rn.f64 	%fd190, %fd191, %fd432, %fd432;
	// inline asm
	mov.f64 	%fd433, %fd190;

BB9_98:
	and.b32  	%r252, %r348, 2;
	setp.eq.s32 	%p78, %r252, 0;
	neg.f64 	%fd194, %fd433;
	selp.f64 	%fd434, %fd433, %fd194, %p78;
	bra.uni 	BB9_100;

BB9_99:
	mov.f64 	%fd195, 0d0000000000000000;
	mul.rn.f64 	%fd434, %fd7, %fd195;

BB9_100:
	neg.f64 	%fd196, %fd434;
	mov.f64 	%fd449, %fd431;
	mov.f64 	%fd450, %fd196;
	ld.param.u32 	%r332, [DIT3C2CM_param_4];
	setp.eq.s32 	%p9, %r332, 0;
	@%p9 bra 	BB9_101;
	bra.uni 	BB9_102;

BB9_101:
	mov.f64 	%fd449, %fd431;
	mov.f64 	%fd450, %fd434;

BB9_102:
	mul.f64 	%fd198, %fd1, %fd449;
	neg.f64 	%fd200, %fd2;
	fma.rn.f64 	%fd201, %fd200, %fd450, %fd198;
	mul.f64 	%fd202, %fd2, %fd449;
	fma.rn.f64 	%fd203, %fd1, %fd450, %fd202;
	mul.f64 	%fd204, %fd5, 0d402921FB54442D18;
	div.rn.f64 	%fd30, %fd204, %fd6;
	setp.eq.f64 	%p10, %fd30, %fd8;
	setp.eq.f64 	%p11, %fd30, %fd9;
	or.pred  	%p79, %p10, %p11;
	mov.f64 	%fd441, %fd51;
	mov.f64 	%fd442, %fd52;
	mov.f64 	%fd443, %fd201;
	mov.f64 	%fd444, %fd203;
	@%p79 bra 	BB9_125;

	// inline asm
	abs.f64 	%fd205, %fd30;
	// inline asm
	setp.gt.f64 	%p80, %fd205, 0d41E0000000000000;
	@%p80 bra 	BB9_105;

	mov.f64 	%fd220, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd207, %fd30, %fd220;
	// inline asm
	cvt.rni.s32.f64 	%r253, %fd207;
	// inline asm
	cvt.rn.f64.s32 	%fd221, %r253;
	neg.f64 	%fd217, %fd221;
	mov.f64 	%fd210, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd208, %fd217, %fd210, %fd30;
	// inline asm
	mov.f64 	%fd214, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd212, %fd217, %fd214, %fd208;
	// inline asm
	mov.f64 	%fd218, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd216, %fd217, %fd218, %fd212;
	// inline asm
	mov.u32 	%r351, %r253;
	mov.f64 	%fd435, %fd216;
	bra.uni 	BB9_121;

BB9_105:
	mov.b64 	 %rl65, %fd30;
	and.b64  	%rl316, %rl65, -9223372036854775808;
	shr.u64 	%rl67, %rl65, 52;
	and.b64  	%rl211, %rl67, 2047;
	add.s64 	%rl212, %rl211, 4294966272;
	cvt.u32.u64 	%r76, %rl212;
	shl.b64 	%rl213, %rl65, 11;
	or.b64  	%rl68, %rl213, -9223372036854775808;
	shr.u32 	%r257, %r76, 6;
	mov.u32 	%r258, 16;
	sub.s32 	%r77, %r258, %r257;
	mov.u32 	%r259, 15;
	sub.s32 	%r349, %r259, %r257;
	mov.u32 	%r260, 19;
	sub.s32 	%r79, %r260, %r257;
	mov.u32 	%r255, 18;
	// inline asm
	min.s32 	%r254, %r255, %r79;
	// inline asm
	setp.lt.s32 	%p81, %r349, %r254;
	@%p81 bra 	BB9_107;

	mov.u64 	%rl313, 0;
	bra.uni 	BB9_109;

BB9_107:
	mov.u32 	%r261, 1;
	sub.s32 	%r80, %r261, %r77;
	mov.u64 	%rl313, 0;

BB9_108:
	.pragma "nounroll";
	shl.b32 	%r265, %r349, 3;
	mov.u32 	%r266, __internal_i2opi_d;
	add.s32 	%r267, %r266, %r265;
	ld.const.u64 	%rl217, [%r267];
	mul.lo.s64 	%rl219, %rl217, %rl68;
	// inline asm
	mul.hi.u64 	%rl216, %rl217, %rl68;
	// inline asm
	mad.lo.s64 	%rl220, %rl217, %rl68, %rl313;
	setp.lt.u64 	%p82, %rl220, %rl219;
	selp.u64 	%rl221, 1, 0, %p82;
	add.s64 	%rl313, %rl221, %rl216;
	add.s32 	%r268, %r80, %r349;
	shl.b32 	%r269, %r268, 3;
	add.s32 	%r271, %r41, %r269;
	st.local.u64 	[%r271], %rl220;
	// inline asm
	min.s32 	%r262, %r255, %r79;
	// inline asm
	add.s32 	%r349, %r349, 1;
	setp.lt.s32 	%p83, %r349, %r262;
	@%p83 bra 	BB9_108;

BB9_109:
	mov.u32 	%r272, 1;
	sub.s32 	%r273, %r272, %r77;
	add.s32 	%r274, %r273, %r349;
	shl.b32 	%r275, %r274, 3;
	add.s32 	%r277, %r41, %r275;
	st.local.u64 	[%r277], %rl313;
	ld.local.u64 	%rl314, [%r41+24];
	ld.local.u64 	%rl315, [%r41+16];
	and.b32  	%r278, %r76, 63;
	setp.eq.s32 	%p84, %r278, 0;
	@%p84 bra 	BB9_111;

	and.b64  	%rl222, %rl67, 63;
	cvt.u32.u64 	%r279, %rl222;
	shl.b64 	%rl223, %rl314, %r279;
	neg.s32 	%r280, %r76;
	and.b32  	%r281, %r280, 63;
	shr.u64 	%rl224, %rl315, %r281;
	or.b64  	%rl314, %rl224, %rl223;
	shl.b64 	%rl225, %rl315, %r279;
	ld.local.u64 	%rl226, [%r41+8];
	shr.u64 	%rl227, %rl226, %r281;
	or.b64  	%rl315, %rl227, %rl225;

BB9_111:
	shr.u64 	%rl228, %rl314, 62;
	cvt.u32.u64 	%r282, %rl228;
	shr.u64 	%rl229, %rl315, 62;
	shl.b64 	%rl230, %rl314, 2;
	or.b64  	%rl320, %rl229, %rl230;
	shl.b64 	%rl79, %rl315, 2;
	setp.ne.s64 	%p85, %rl79, 0;
	selp.u64 	%rl231, 1, 0, %p85;
	or.b64  	%rl232, %rl231, %rl320;
	setp.gt.u64 	%p86, %rl232, -9223372036854775808;
	selp.u32 	%r283, 1, 0, %p86;
	add.s32 	%r284, %r283, %r282;
	neg.s32 	%r285, %r284;
	setp.lt.s64 	%p87, %rl65, 0;
	selp.b32 	%r351, %r285, %r284, %p87;
	@%p86 bra 	BB9_113;

	mov.u64 	%rl319, %rl79;
	bra.uni 	BB9_114;

BB9_113:
	not.b64 	%rl233, %rl320;
	neg.s64 	%rl80, %rl79;
	setp.eq.s64 	%p88, %rl79, 0;
	selp.u64 	%rl234, 1, 0, %p88;
	add.s64 	%rl320, %rl234, %rl233;
	xor.b64  	%rl316, %rl316, -9223372036854775808;
	mov.u64 	%rl319, %rl80;

BB9_114:
	mov.u64 	%rl318, %rl319;
	setp.gt.s64 	%p89, %rl320, 0;
	@%p89 bra 	BB9_116;

	mov.u32 	%r350, 0;
	bra.uni 	BB9_118;

BB9_116:
	mov.u32 	%r350, 0;

BB9_117:
	shr.u64 	%rl235, %rl318, 63;
	shl.b64 	%rl236, %rl320, 1;
	or.b64  	%rl320, %rl235, %rl236;
	shl.b64 	%rl318, %rl318, 1;
	add.s32 	%r350, %r350, -1;
	setp.gt.s64 	%p90, %rl320, 0;
	@%p90 bra 	BB9_117;

BB9_118:
	mul.lo.s64 	%rl322, %rl320, -3958705157555305931;
	mov.u64 	%rl239, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl237, %rl320, %rl239;
	// inline asm
	setp.gt.s64 	%p91, %rl237, 0;
	mov.u64 	%rl321, %rl237;
	@%p91 bra 	BB9_119;
	bra.uni 	BB9_120;

BB9_119:
	shl.b64 	%rl240, %rl237, 1;
	shr.u64 	%rl241, %rl322, 63;
	or.b64  	%rl321, %rl240, %rl241;
	mul.lo.s64 	%rl322, %rl320, -7917410315110611862;
	add.s32 	%r350, %r350, -1;

BB9_120:
	setp.ne.s64 	%p92, %rl322, 0;
	selp.u64 	%rl242, 1, 0, %p92;
	add.s64 	%rl243, %rl242, %rl321;
	add.s32 	%r288, %r350, 1022;
	cvt.u64.u32 	%rl244, %r288;
	shl.b64 	%rl245, %rl244, 52;
	shr.u64 	%rl246, %rl243, 11;
	shr.u64 	%rl247, %rl243, 10;
	and.b64  	%rl248, %rl247, 1;
	add.s64 	%rl249, %rl245, %rl246;
	add.s64 	%rl250, %rl249, %rl248;
	or.b64  	%rl251, %rl250, %rl316;
	mov.b64 	 %fd435, %rl251;

BB9_121:
	add.s32 	%r91, %r351, 1;
	and.b32  	%r289, %r91, 1;
	setp.eq.s32 	%p93, %r289, 0;
	mul.rn.f64 	%fd34, %fd435, %fd435;
	@%p93 bra 	BB9_123;

	mov.f64 	%fd223, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd225, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd222, %fd223, %fd34, %fd225;
	// inline asm
	mov.f64 	%fd229, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd226, %fd222, %fd34, %fd229;
	// inline asm
	mov.f64 	%fd233, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd230, %fd226, %fd34, %fd233;
	// inline asm
	mov.f64 	%fd237, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd234, %fd230, %fd34, %fd237;
	// inline asm
	mov.f64 	%fd241, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd238, %fd234, %fd34, %fd241;
	// inline asm
	mov.f64 	%fd245, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd242, %fd238, %fd34, %fd245;
	// inline asm
	mov.f64 	%fd249, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd246, %fd242, %fd34, %fd249;
	// inline asm
	mov.f64 	%fd436, %fd246;
	bra.uni 	BB9_124;

BB9_123:
	mov.f64 	%fd251, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd253, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd250, %fd251, %fd34, %fd253;
	// inline asm
	mov.f64 	%fd257, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd254, %fd250, %fd34, %fd257;
	// inline asm
	mov.f64 	%fd261, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd258, %fd254, %fd34, %fd261;
	// inline asm
	mov.f64 	%fd265, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd262, %fd258, %fd34, %fd265;
	// inline asm
	mov.f64 	%fd269, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd266, %fd262, %fd34, %fd269;
	// inline asm
	mul.rn.f64 	%fd271, %fd266, %fd34;
	// inline asm
	fma.rn.f64 	%fd270, %fd271, %fd435, %fd435;
	// inline asm
	mov.f64 	%fd436, %fd270;

BB9_124:
	and.b32  	%r290, %r91, 2;
	setp.eq.s32 	%p94, %r290, 0;
	neg.f64 	%fd274, %fd436;
	selp.f64 	%fd437, %fd436, %fd274, %p94;
	bra.uni 	BB9_126;

BB9_125:
	mov.f64 	%fd437, 0dFFF8000000000000;

BB9_126:
	setp.eq.f64 	%p95, %fd30, 0d0000000000000000;
	or.pred  	%p96, %p11, %p95;
	or.pred  	%p97, %p10, %p96;
	@%p97 bra 	BB9_149;

	// inline asm
	abs.f64 	%fd276, %fd30;
	// inline asm
	setp.gt.f64 	%p98, %fd276, 0d41E0000000000000;
	@%p98 bra 	BB9_129;

	mov.f64 	%fd291, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd278, %fd30, %fd291;
	// inline asm
	cvt.rni.s32.f64 	%r291, %fd278;
	// inline asm
	cvt.rn.f64.s32 	%fd292, %r291;
	neg.f64 	%fd288, %fd292;
	mov.f64 	%fd281, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd279, %fd288, %fd281, %fd30;
	// inline asm
	mov.f64 	%fd285, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd283, %fd288, %fd285, %fd279;
	// inline asm
	mov.f64 	%fd289, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd287, %fd288, %fd289, %fd283;
	// inline asm
	mov.u32 	%r354, %r291;
	mov.f64 	%fd438, %fd287;
	bra.uni 	BB9_145;

BB9_129:
	mov.b64 	 %rl97, %fd30;
	and.b64  	%rl326, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl252, %rl99, 2047;
	add.s64 	%rl253, %rl252, 4294966272;
	cvt.u32.u64 	%r93, %rl253;
	shl.b64 	%rl254, %rl97, 11;
	or.b64  	%rl100, %rl254, -9223372036854775808;
	shr.u32 	%r295, %r93, 6;
	mov.u32 	%r296, 16;
	sub.s32 	%r94, %r296, %r295;
	mov.u32 	%r297, 15;
	sub.s32 	%r352, %r297, %r295;
	mov.u32 	%r298, 19;
	sub.s32 	%r96, %r298, %r295;
	mov.u32 	%r293, 18;
	// inline asm
	min.s32 	%r292, %r293, %r96;
	// inline asm
	setp.lt.s32 	%p99, %r352, %r292;
	@%p99 bra 	BB9_131;

	mov.u64 	%rl323, 0;
	bra.uni 	BB9_133;

BB9_131:
	mov.u32 	%r299, 1;
	sub.s32 	%r97, %r299, %r94;
	mov.u64 	%rl323, 0;

BB9_132:
	.pragma "nounroll";
	shl.b32 	%r303, %r352, 3;
	mov.u32 	%r304, __internal_i2opi_d;
	add.s32 	%r305, %r304, %r303;
	ld.const.u64 	%rl258, [%r305];
	mul.lo.s64 	%rl260, %rl258, %rl100;
	// inline asm
	mul.hi.u64 	%rl257, %rl258, %rl100;
	// inline asm
	mad.lo.s64 	%rl261, %rl258, %rl100, %rl323;
	setp.lt.u64 	%p100, %rl261, %rl260;
	selp.u64 	%rl262, 1, 0, %p100;
	add.s64 	%rl323, %rl262, %rl257;
	add.s32 	%r306, %r97, %r352;
	shl.b32 	%r307, %r306, 3;
	add.s32 	%r309, %r41, %r307;
	st.local.u64 	[%r309], %rl261;
	// inline asm
	min.s32 	%r300, %r293, %r96;
	// inline asm
	add.s32 	%r352, %r352, 1;
	setp.lt.s32 	%p101, %r352, %r300;
	@%p101 bra 	BB9_132;

BB9_133:
	mov.u32 	%r310, 1;
	sub.s32 	%r311, %r310, %r94;
	add.s32 	%r312, %r311, %r352;
	shl.b32 	%r313, %r312, 3;
	add.s32 	%r315, %r41, %r313;
	st.local.u64 	[%r315], %rl323;
	ld.local.u64 	%rl324, [%r41+24];
	ld.local.u64 	%rl325, [%r41+16];
	and.b32  	%r316, %r93, 63;
	setp.eq.s32 	%p102, %r316, 0;
	@%p102 bra 	BB9_135;

	and.b64  	%rl263, %rl99, 63;
	cvt.u32.u64 	%r317, %rl263;
	shl.b64 	%rl264, %rl324, %r317;
	neg.s32 	%r318, %r93;
	and.b32  	%r319, %r318, 63;
	shr.u64 	%rl265, %rl325, %r319;
	or.b64  	%rl324, %rl265, %rl264;
	shl.b64 	%rl266, %rl325, %r317;
	ld.local.u64 	%rl267, [%r41+8];
	shr.u64 	%rl268, %rl267, %r319;
	or.b64  	%rl325, %rl268, %rl266;

BB9_135:
	shr.u64 	%rl269, %rl324, 62;
	cvt.u32.u64 	%r320, %rl269;
	shr.u64 	%rl270, %rl325, 62;
	shl.b64 	%rl271, %rl324, 2;
	or.b64  	%rl330, %rl270, %rl271;
	shl.b64 	%rl111, %rl325, 2;
	setp.ne.s64 	%p103, %rl111, 0;
	selp.u64 	%rl272, 1, 0, %p103;
	or.b64  	%rl273, %rl272, %rl330;
	setp.gt.u64 	%p104, %rl273, -9223372036854775808;
	selp.u32 	%r321, 1, 0, %p104;
	add.s32 	%r322, %r321, %r320;
	neg.s32 	%r323, %r322;
	setp.lt.s64 	%p105, %rl97, 0;
	selp.b32 	%r354, %r323, %r322, %p105;
	@%p104 bra 	BB9_137;

	mov.u64 	%rl329, %rl111;
	bra.uni 	BB9_138;

BB9_137:
	not.b64 	%rl274, %rl330;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p106, %rl111, 0;
	selp.u64 	%rl275, 1, 0, %p106;
	add.s64 	%rl330, %rl275, %rl274;
	xor.b64  	%rl326, %rl326, -9223372036854775808;
	mov.u64 	%rl329, %rl112;

BB9_138:
	mov.u64 	%rl328, %rl329;
	setp.gt.s64 	%p107, %rl330, 0;
	@%p107 bra 	BB9_140;

	mov.u32 	%r353, 0;
	bra.uni 	BB9_142;

BB9_140:
	mov.u32 	%r353, 0;

BB9_141:
	shr.u64 	%rl276, %rl328, 63;
	shl.b64 	%rl277, %rl330, 1;
	or.b64  	%rl330, %rl276, %rl277;
	shl.b64 	%rl328, %rl328, 1;
	add.s32 	%r353, %r353, -1;
	setp.gt.s64 	%p108, %rl330, 0;
	@%p108 bra 	BB9_141;

BB9_142:
	mul.lo.s64 	%rl332, %rl330, -3958705157555305931;
	mov.u64 	%rl280, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl278, %rl330, %rl280;
	// inline asm
	setp.gt.s64 	%p109, %rl278, 0;
	mov.u64 	%rl331, %rl278;
	@%p109 bra 	BB9_143;
	bra.uni 	BB9_144;

BB9_143:
	shl.b64 	%rl281, %rl278, 1;
	shr.u64 	%rl282, %rl332, 63;
	or.b64  	%rl331, %rl281, %rl282;
	mul.lo.s64 	%rl332, %rl330, -7917410315110611862;
	add.s32 	%r353, %r353, -1;

BB9_144:
	setp.ne.s64 	%p110, %rl332, 0;
	selp.u64 	%rl283, 1, 0, %p110;
	add.s64 	%rl284, %rl283, %rl331;
	add.s32 	%r326, %r353, 1022;
	cvt.u64.u32 	%rl285, %r326;
	shl.b64 	%rl286, %rl285, 52;
	shr.u64 	%rl287, %rl284, 11;
	shr.u64 	%rl288, %rl284, 10;
	and.b64  	%rl289, %rl288, 1;
	add.s64 	%rl290, %rl286, %rl287;
	add.s64 	%rl291, %rl290, %rl289;
	or.b64  	%rl292, %rl291, %rl326;
	mov.b64 	 %fd438, %rl292;

BB9_145:
	and.b32  	%r327, %r354, 1;
	setp.eq.s32 	%p111, %r327, 0;
	mul.rn.f64 	%fd44, %fd438, %fd438;
	@%p111 bra 	BB9_147;

	mov.f64 	%fd294, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd296, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd293, %fd294, %fd44, %fd296;
	// inline asm
	mov.f64 	%fd300, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd297, %fd293, %fd44, %fd300;
	// inline asm
	mov.f64 	%fd304, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd301, %fd297, %fd44, %fd304;
	// inline asm
	mov.f64 	%fd308, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd305, %fd301, %fd44, %fd308;
	// inline asm
	mov.f64 	%fd312, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd309, %fd305, %fd44, %fd312;
	// inline asm
	mov.f64 	%fd316, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd313, %fd309, %fd44, %fd316;
	// inline asm
	mov.f64 	%fd320, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd317, %fd313, %fd44, %fd320;
	// inline asm
	mov.f64 	%fd439, %fd317;
	bra.uni 	BB9_148;

BB9_147:
	mov.f64 	%fd322, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd324, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd321, %fd322, %fd44, %fd324;
	// inline asm
	mov.f64 	%fd328, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd325, %fd321, %fd44, %fd328;
	// inline asm
	mov.f64 	%fd332, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd329, %fd325, %fd44, %fd332;
	// inline asm
	mov.f64 	%fd336, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd333, %fd329, %fd44, %fd336;
	// inline asm
	mov.f64 	%fd340, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd337, %fd333, %fd44, %fd340;
	// inline asm
	mul.rn.f64 	%fd342, %fd337, %fd44;
	// inline asm
	fma.rn.f64 	%fd341, %fd342, %fd438, %fd438;
	// inline asm
	mov.f64 	%fd439, %fd341;

BB9_148:
	and.b32  	%r328, %r354, 2;
	setp.eq.s32 	%p112, %r328, 0;
	neg.f64 	%fd345, %fd439;
	selp.f64 	%fd440, %fd439, %fd345, %p112;
	bra.uni 	BB9_150;

BB9_149:
	mov.f64 	%fd346, 0d0000000000000000;
	mul.rn.f64 	%fd440, %fd30, %fd346;

BB9_150:
	neg.f64 	%fd347, %fd440;
	mov.f64 	%fd447, %fd437;
	mov.f64 	%fd448, %fd347;
	@%p9 bra 	BB9_151;
	bra.uni 	BB9_152;

BB9_151:
	mov.f64 	%fd447, %fd437;
	mov.f64 	%fd448, %fd440;

BB9_152:
	mul.f64 	%fd349, %fd3, %fd447;
	neg.f64 	%fd351, %fd4;
	fma.rn.f64 	%fd352, %fd351, %fd448, %fd349;
	mul.f64 	%fd353, %fd4, %fd447;
	fma.rn.f64 	%fd354, %fd3, %fd448, %fd353;
	mov.f64 	%fd445, %fd352;
	mov.f64 	%fd446, %fd354;

BB9_153:
	add.f64 	%fd357, %fd441, %fd443;
	add.f64 	%fd359, %fd357, %fd445;
	st.global.f64 	[%r38], %fd359;
	add.f64 	%fd362, %fd442, %fd444;
	add.f64 	%fd364, %fd362, %fd446;
	st.global.f64 	[%r38+8], %fd364;
	mul.f64 	%fd365, %fd443, 0dBFE0000000000000;
	mul.f64 	%fd366, %fd444, 0d3FEBB645A1CAC083;
	mov.f64 	%fd367, 0d3FEBB645A1CAC083;
	fma.rn.f64 	%fd368, %fd443, 0dBFE0000000000000, %fd366;
	add.f64 	%fd369, %fd441, %fd368;
	mul.f64 	%fd370, %fd445, 0dBFE0000000000000;
	mul.f64 	%fd371, %fd446, 0d3FEBB645A1CAC083;
	neg.f64 	%fd372, %fd446;
	fma.rn.f64 	%fd373, %fd372, %fd367, %fd370;
	add.f64 	%fd374, %fd369, %fd373;
	st.global.f64 	[%r39], %fd374;
	mul.f64 	%fd375, %fd443, 0dBFEBB645A1CAC083;
	mov.f64 	%fd376, 0d3FE0000000000000;
	neg.f64 	%fd377, %fd444;
	fma.rn.f64 	%fd378, %fd377, %fd376, %fd375;
	add.f64 	%fd379, %fd442, %fd378;
	mul.f64 	%fd380, %fd445, 0d3FEBB645A1CAC083;
	fma.rn.f64 	%fd381, %fd372, %fd376, %fd380;
	add.f64 	%fd382, %fd379, %fd381;
	st.global.f64 	[%r39+8], %fd382;
	fma.rn.f64 	%fd383, %fd377, %fd367, %fd365;
	add.f64 	%fd384, %fd441, %fd383;
	fma.rn.f64 	%fd385, %fd445, 0dBFE0000000000000, %fd371;
	add.f64 	%fd386, %fd384, %fd385;
	st.global.f64 	[%r40], %fd386;
	mul.f64 	%fd387, %fd443, 0d3FEBB645A1CAC083;
	fma.rn.f64 	%fd388, %fd377, %fd376, %fd387;
	add.f64 	%fd389, %fd442, %fd388;
	mul.f64 	%fd390, %fd445, 0dBFEBB645A1CAC083;
	fma.rn.f64 	%fd391, %fd372, %fd376, %fd390;
	add.f64 	%fd392, %fd389, %fd391;
	st.global.f64 	[%r40+8], %fd392;
	ret;
}

.entry DIT4C2CM(
	.param .u32 .ptr .global .align 8 DIT4C2CM_param_0,
	.param .u32 DIT4C2CM_param_1,
	.param .u32 DIT4C2CM_param_2,
	.param .u32 DIT4C2CM_param_3,
	.param .u32 DIT4C2CM_param_4,
	.param .u32 DIT4C2CM_param_5
)
{
	.local .align 8 .b8 	__local_depot10[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<645>;
	.reg .pred 	%p<150>;
	.reg .s32 	%r<471>;
	.reg .s64 	%rl<499>;


	mov.u32 	%SP, __local_depot10;
	ld.param.u32 	%r150, [DIT4C2CM_param_3];
	// inline asm
	mov.u32 	%r142, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r143, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r144, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r145, %tid.x;
	// inline asm
	add.s32 	%r151, %r145, %r142;
	mad.lo.s32 	%r5, %r144, %r143, %r151;
	// inline asm
	mov.u32 	%r146, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r147, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r148, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r149, %tid.y;
	// inline asm
	add.s32 	%r152, %r149, %r146;
	mad.lo.s32 	%r6, %r148, %r147, %r152;
	mul.hi.s32 	%r153, %r150, -1840700269;
	add.s32 	%r154, %r153, %r150;
	shr.u32 	%r155, %r154, 31;
	shr.s32 	%r156, %r154, 2;
	add.s32 	%r7, %r156, %r155;
	mul.lo.s32 	%r157, %r7, 7;
	sub.s32 	%r8, %r150, %r157;
	setp.gt.s32 	%p13, %r150, 6;
	@%p13 bra 	BB10_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB10_23;

BB10_2:
	mov.f32 	%f1, 0f40E00000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40800000;
	add.f32 	%f2, %f47, 0f40E00000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r446, 0;
	mov.u32 	%r445, 1;

BB10_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p14, %p3, %p3;
	@%p14 bra 	BB10_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mov.f32 	%f55, 0f40000000;
	mul.rn.f32 	%f56, %f55, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p15, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p16, %f1, %f52;
	and.pred  	%p4, %p15, %p16;
	setp.eq.f32 	%p17, %f48, 0f00000000;
	@%p17 bra 	BB10_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r11, %f58;
	shr.u32 	%r160, %r11, 23;
	and.b32  	%r161, %r160, 255;
	add.s32 	%r447, %r161, -127;
	setp.eq.s32 	%p18, %r161, 0;
	mov.f32 	%f280, %f58;
	@%p18 bra 	BB10_6;
	bra.uni 	BB10_7;

BB10_6:
	and.b32  	%r162, %r11, -2139095041;
	or.b32  	%r163, %r162, 1065353216;
	mov.b32 	 %f60, %r163;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r164, %f61;
	shr.u32 	%r165, %r164, 23;
	and.b32  	%r166, %r165, 255;
	add.s32 	%r447, %r166, -253;
	and.b32  	%r167, %r164, -2139095041;
	or.b32  	%r168, %r167, 1065353216;
	mov.b32 	 %f280, %r168;

BB10_7:
	mov.b32 	 %r169, %f280;
	and.b32  	%r170, %r169, -2139095041;
	or.b32  	%r171, %r170, 1065353216;
	mov.b32 	 %f281, %r171;
	setp.gt.f32 	%p19, %f281, 0f3FB504F3;
	@%p19 bra 	BB10_8;
	bra.uni 	BB10_9;

BB10_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r447, %r447, 1;

BB10_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f55, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r172, %f73;
	and.b32  	%r173, %r172, -4096;
	mov.b32 	 %f81, %r173;
	mov.b32 	 %r174, %f64;
	and.b32  	%r175, %r174, -4096;
	mov.b32 	 %f82, %r175;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f55, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r447;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p20, %f69, 0f77F684DF;
	@%p20 bra 	BB10_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB10_12;

BB10_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB10_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r17, %f131;
	setp.eq.s32 	%p21, %r17, 1118925336;
	@%p21 bra 	BB10_13;
	bra.uni 	BB10_14;

BB10_13:
	add.s32 	%r176, %r17, -1;
	mov.b32 	 %f133, %r176;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB10_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p22, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p22;
	setp.gt.f32 	%p23, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p23;
	setp.neu.f32 	%p24, %f19, %f3;
	@%p24 bra 	BB10_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB10_21;

BB10_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB10_21;

BB10_17:
	@%p3 bra 	BB10_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB10_21;

BB10_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB10_21;

BB10_20:
	mov.f32 	%f283, %f7;

BB10_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r445;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r445, %f157;
	add.s32 	%r446, %r446, 1;
	setp.lt.s32 	%p25, %r446, %r7;
	@%p25 bra 	BB10_3;

	cvt.rn.f32.s32 	%f284, %r445;

BB10_23:
	mov.f32 	%f159, 0f40800000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r8;
	setp.eq.f32 	%p26, %f287, 0f00000000;
	@%p26 bra 	BB10_45;

	setp.nan.f32 	%p27, %f287, %f287;
	@%p27 bra 	BB10_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p28, %f287, 0f7F800000;
	setp.eq.f32 	%p29, %f287, 0fFF800000;
	or.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB10_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p33, %f158, 0f00000000;
	@%p33 bra 	BB10_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r20, %f168;
	shr.u32 	%r177, %r20, 23;
	and.b32  	%r178, %r177, 255;
	add.s32 	%r448, %r178, -127;
	setp.eq.s32 	%p34, %r178, 0;
	mov.f32 	%f285, %f168;
	@%p34 bra 	BB10_28;
	bra.uni 	BB10_29;

BB10_28:
	and.b32  	%r179, %r20, -2139095041;
	or.b32  	%r180, %r179, 1065353216;
	mov.b32 	 %f170, %r180;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r181, %f171;
	shr.u32 	%r182, %r181, 23;
	and.b32  	%r183, %r182, 255;
	add.s32 	%r448, %r183, -253;
	and.b32  	%r184, %r181, -2139095041;
	or.b32  	%r185, %r184, 1065353216;
	mov.b32 	 %f285, %r185;

BB10_29:
	mov.b32 	 %r186, %f285;
	and.b32  	%r187, %r186, -2139095041;
	or.b32  	%r188, %r187, 1065353216;
	mov.b32 	 %f286, %r188;
	setp.gt.f32 	%p35, %f286, 0f3FB504F3;
	@%p35 bra 	BB10_30;
	bra.uni 	BB10_31;

BB10_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r448, %r448, 1;

BB10_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mov.f32 	%f182, 0f40000000;
	mul.rn.f32 	%f183, %f182, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r189, %f183;
	and.b32  	%r190, %r189, -4096;
	mov.b32 	 %f191, %r190;
	mov.b32 	 %r191, %f174;
	and.b32  	%r192, %r191, -4096;
	mov.b32 	 %f192, %r192;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f182, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r448;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p36, %f179, 0f77F684DF;
	@%p36 bra 	BB10_32;
	bra.uni 	BB10_33;

BB10_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB10_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r26, %f241;
	setp.eq.s32 	%p37, %r26, 1118925336;
	@%p37 bra 	BB10_34;
	bra.uni 	BB10_35;

BB10_34:
	add.s32 	%r193, %r26, -1;
	mov.b32 	 %f243, %r193;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB10_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p38, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p38;
	setp.gt.f32 	%p39, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p39;
	setp.neu.f32 	%p40, %f39, %f27;
	@%p40 bra 	BB10_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB10_46;

BB10_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB10_46;

BB10_38:
	setp.lt.f32 	%p41, %f287, 0f00000000;
	@%p41 bra 	BB10_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB10_46;

BB10_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB10_46;

BB10_41:
	setp.lt.f32 	%p42, %f158, 0f3F800000;
	mov.b32 	 %r194, %f287;
	setp.lt.s32 	%p6, %r194, 0;
	@%p42 bra 	BB10_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB10_46;

BB10_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB10_46;

BB10_44:
	add.f32 	%f288, %f287, 0f40800000;
	bra.uni 	BB10_46;

BB10_45:
	mov.f32 	%f288, 0f3F800000;

BB10_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r27, %f267;
	shr.s32 	%r195, %r27, 31;
	shr.u32 	%r196, %r195, 30;
	add.s32 	%r197, %r27, %r196;
	shr.s32 	%r28, %r197, 2;
	ld.param.u32 	%r444, [DIT4C2CM_param_5];
	setp.eq.s32 	%p43, %r444, 0;
	@%p43 bra 	BB10_49;

	ld.param.u32 	%r443, [DIT4C2CM_param_5];
	setp.ne.s32 	%p44, %r443, 1;
	@%p44 bra 	BB10_50;

	div.s32 	%r449, %r6, %r28;
	rem.s32 	%r450, %r6, %r28;
	ld.param.u32 	%r452, [DIT4C2CM_param_1];
	mov.u32 	%r451, %r5;
	bra.uni 	BB10_51;

BB10_49:
	ld.param.u32 	%r440, [DIT4C2CM_param_1];
	mul.lo.s32 	%r31, %r6, %r440;
	div.s32 	%r449, %r5, %r28;
	rem.s32 	%r450, %r5, %r28;
	mov.u32 	%r452, 1;
	mov.u32 	%r451, %r31;
	bra.uni 	BB10_51;

BB10_50:
	mov.u32 	%r452, 1;
	mov.u32 	%r451, 0;
	mov.u32 	%r450, %r203;
	mov.u32 	%r449, %r204;

BB10_51:
	mad.lo.s32 	%r205, %r449, %r27, %r450;
	mad.lo.s32 	%r206, %r452, %r205, %r451;
	shl.b32 	%r207, %r206, 4;
	ld.param.u32 	%r438, [DIT4C2CM_param_0];
	add.s32 	%r38, %r438, %r207;
	ld.global.f64 	%fd74, [%r38];
	ld.global.f64 	%fd75, [%r38+8];
	mul.lo.s32 	%r208, %r452, %r28;
	shl.b32 	%r209, %r208, 4;
	add.s32 	%r39, %r38, %r209;
	ld.global.f64 	%fd1, [%r39];
	ld.global.f64 	%fd2, [%r39+8];
	add.s32 	%r40, %r39, %r209;
	ld.global.f64 	%fd3, [%r40];
	ld.global.f64 	%fd4, [%r40+8];
	add.s32 	%r41, %r40, %r209;
	ld.global.f64 	%fd5, [%r41];
	ld.global.f64 	%fd6, [%r41+8];
	setp.eq.s32 	%p45, %r450, 0;
	mov.f64 	%fd637, %fd74;
	mov.f64 	%fd638, %fd75;
	mov.f64 	%fd639, %fd1;
	mov.f64 	%fd640, %fd2;
	mov.f64 	%fd641, %fd3;
	mov.f64 	%fd642, %fd4;
	mov.f64 	%fd643, %fd5;
	mov.f64 	%fd644, %fd6;
	@%p45 bra 	BB10_197;

	cvt.rn.f64.s32 	%fd7, %r450;
	mul.f64 	%fd76, %fd7, 0d402921FB54442D18;
	cvt.rn.f64.s32 	%fd8, %r27;
	div.rn.f64 	%fd9, %fd76, %fd8;
	mov.f64 	%fd10, 0d7FF0000000000000;
	setp.eq.f64 	%p7, %fd9, 0d7FF0000000000000;
	mov.f64 	%fd11, 0dFFF0000000000000;
	setp.eq.f64 	%p8, %fd9, 0dFFF0000000000000;
	or.pred  	%p46, %p7, %p8;
	add.u32 	%r42, %SP, 0;
	@%p46 bra 	BB10_75;

	// inline asm
	abs.f64 	%fd77, %fd9;
	// inline asm
	setp.gt.f64 	%p47, %fd77, 0d41E0000000000000;
	@%p47 bra 	BB10_55;

	mov.f64 	%fd92, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd79, %fd9, %fd92;
	// inline asm
	cvt.rni.s32.f64 	%r210, %fd79;
	// inline asm
	cvt.rn.f64.s32 	%fd93, %r210;
	neg.f64 	%fd89, %fd93;
	mov.f64 	%fd82, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd80, %fd89, %fd82, %fd9;
	// inline asm
	mov.f64 	%fd86, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd84, %fd89, %fd86, %fd80;
	// inline asm
	mov.f64 	%fd90, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd88, %fd89, %fd90, %fd84;
	// inline asm
	mov.u32 	%r455, %r210;
	mov.f64 	%fd619, %fd88;
	bra.uni 	BB10_71;

BB10_55:
	mov.b64 	 %rl1, %fd9;
	and.b64  	%rl442, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl193, %rl3, 2047;
	add.s64 	%rl194, %rl193, 4294966272;
	cvt.u32.u64 	%r44, %rl194;
	shl.b64 	%rl195, %rl1, 11;
	or.b64  	%rl4, %rl195, -9223372036854775808;
	shr.u32 	%r214, %r44, 6;
	mov.u32 	%r215, 16;
	sub.s32 	%r45, %r215, %r214;
	mov.u32 	%r216, 15;
	sub.s32 	%r453, %r216, %r214;
	mov.u32 	%r217, 19;
	sub.s32 	%r47, %r217, %r214;
	mov.u32 	%r212, 18;
	// inline asm
	min.s32 	%r211, %r212, %r47;
	// inline asm
	setp.lt.s32 	%p48, %r453, %r211;
	@%p48 bra 	BB10_57;

	mov.u64 	%rl439, 0;
	bra.uni 	BB10_59;

BB10_57:
	mov.u32 	%r218, 1;
	sub.s32 	%r48, %r218, %r45;
	mov.u64 	%rl439, 0;

BB10_58:
	.pragma "nounroll";
	shl.b32 	%r222, %r453, 3;
	mov.u32 	%r223, __internal_i2opi_d;
	add.s32 	%r224, %r223, %r222;
	ld.const.u64 	%rl199, [%r224];
	mul.lo.s64 	%rl201, %rl199, %rl4;
	// inline asm
	mul.hi.u64 	%rl198, %rl199, %rl4;
	// inline asm
	mad.lo.s64 	%rl202, %rl199, %rl4, %rl439;
	setp.lt.u64 	%p49, %rl202, %rl201;
	selp.u64 	%rl203, 1, 0, %p49;
	add.s64 	%rl439, %rl203, %rl198;
	add.s32 	%r225, %r48, %r453;
	shl.b32 	%r226, %r225, 3;
	add.s32 	%r228, %r42, %r226;
	st.local.u64 	[%r228], %rl202;
	// inline asm
	min.s32 	%r219, %r212, %r47;
	// inline asm
	add.s32 	%r453, %r453, 1;
	setp.lt.s32 	%p50, %r453, %r219;
	@%p50 bra 	BB10_58;

BB10_59:
	mov.u32 	%r229, 1;
	sub.s32 	%r230, %r229, %r45;
	add.s32 	%r231, %r230, %r453;
	shl.b32 	%r232, %r231, 3;
	add.s32 	%r234, %r42, %r232;
	st.local.u64 	[%r234], %rl439;
	ld.local.u64 	%rl440, [%r42+24];
	ld.local.u64 	%rl441, [%r42+16];
	and.b32  	%r235, %r44, 63;
	setp.eq.s32 	%p51, %r235, 0;
	@%p51 bra 	BB10_61;

	and.b64  	%rl204, %rl3, 63;
	cvt.u32.u64 	%r236, %rl204;
	shl.b64 	%rl205, %rl440, %r236;
	neg.s32 	%r237, %r44;
	and.b32  	%r238, %r237, 63;
	shr.u64 	%rl206, %rl441, %r238;
	or.b64  	%rl440, %rl206, %rl205;
	shl.b64 	%rl207, %rl441, %r236;
	ld.local.u64 	%rl208, [%r42+8];
	shr.u64 	%rl209, %rl208, %r238;
	or.b64  	%rl441, %rl209, %rl207;

BB10_61:
	shr.u64 	%rl210, %rl440, 62;
	cvt.u32.u64 	%r239, %rl210;
	shr.u64 	%rl211, %rl441, 62;
	shl.b64 	%rl212, %rl440, 2;
	or.b64  	%rl446, %rl211, %rl212;
	shl.b64 	%rl15, %rl441, 2;
	setp.ne.s64 	%p52, %rl15, 0;
	selp.u64 	%rl213, 1, 0, %p52;
	or.b64  	%rl214, %rl213, %rl446;
	setp.gt.u64 	%p53, %rl214, -9223372036854775808;
	selp.u32 	%r240, 1, 0, %p53;
	add.s32 	%r241, %r240, %r239;
	neg.s32 	%r242, %r241;
	setp.lt.s64 	%p54, %rl1, 0;
	selp.b32 	%r455, %r242, %r241, %p54;
	@%p53 bra 	BB10_63;

	mov.u64 	%rl445, %rl15;
	bra.uni 	BB10_64;

BB10_63:
	not.b64 	%rl215, %rl446;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p55, %rl15, 0;
	selp.u64 	%rl216, 1, 0, %p55;
	add.s64 	%rl446, %rl216, %rl215;
	xor.b64  	%rl442, %rl442, -9223372036854775808;
	mov.u64 	%rl445, %rl16;

BB10_64:
	mov.u64 	%rl444, %rl445;
	setp.gt.s64 	%p56, %rl446, 0;
	@%p56 bra 	BB10_66;

	mov.u32 	%r454, 0;
	bra.uni 	BB10_68;

BB10_66:
	mov.u32 	%r454, 0;

BB10_67:
	shr.u64 	%rl217, %rl444, 63;
	shl.b64 	%rl218, %rl446, 1;
	or.b64  	%rl446, %rl217, %rl218;
	shl.b64 	%rl444, %rl444, 1;
	add.s32 	%r454, %r454, -1;
	setp.gt.s64 	%p57, %rl446, 0;
	@%p57 bra 	BB10_67;

BB10_68:
	mul.lo.s64 	%rl448, %rl446, -3958705157555305931;
	mov.u64 	%rl221, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl219, %rl446, %rl221;
	// inline asm
	setp.gt.s64 	%p58, %rl219, 0;
	mov.u64 	%rl447, %rl219;
	@%p58 bra 	BB10_69;
	bra.uni 	BB10_70;

BB10_69:
	shl.b64 	%rl222, %rl219, 1;
	shr.u64 	%rl223, %rl448, 63;
	or.b64  	%rl447, %rl222, %rl223;
	mul.lo.s64 	%rl448, %rl446, -7917410315110611862;
	add.s32 	%r454, %r454, -1;

BB10_70:
	setp.ne.s64 	%p59, %rl448, 0;
	selp.u64 	%rl224, 1, 0, %p59;
	add.s64 	%rl225, %rl224, %rl447;
	add.s32 	%r245, %r454, 1022;
	cvt.u64.u32 	%rl226, %r245;
	shl.b64 	%rl227, %rl226, 52;
	shr.u64 	%rl228, %rl225, 11;
	shr.u64 	%rl229, %rl225, 10;
	and.b64  	%rl230, %rl229, 1;
	add.s64 	%rl231, %rl227, %rl228;
	add.s64 	%rl232, %rl231, %rl230;
	or.b64  	%rl233, %rl232, %rl442;
	mov.b64 	 %fd619, %rl233;

BB10_71:
	add.s32 	%r59, %r455, 1;
	and.b32  	%r246, %r59, 1;
	setp.eq.s32 	%p60, %r246, 0;
	mul.rn.f64 	%fd15, %fd619, %fd619;
	@%p60 bra 	BB10_73;

	mov.f64 	%fd95, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd97, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd94, %fd95, %fd15, %fd97;
	// inline asm
	mov.f64 	%fd101, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd98, %fd94, %fd15, %fd101;
	// inline asm
	mov.f64 	%fd105, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd102, %fd98, %fd15, %fd105;
	// inline asm
	mov.f64 	%fd109, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd106, %fd102, %fd15, %fd109;
	// inline asm
	mov.f64 	%fd113, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd110, %fd106, %fd15, %fd113;
	// inline asm
	mov.f64 	%fd117, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd114, %fd110, %fd15, %fd117;
	// inline asm
	mov.f64 	%fd121, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd118, %fd114, %fd15, %fd121;
	// inline asm
	mov.f64 	%fd620, %fd118;
	bra.uni 	BB10_74;

BB10_73:
	mov.f64 	%fd123, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd125, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd122, %fd123, %fd15, %fd125;
	// inline asm
	mov.f64 	%fd129, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd126, %fd122, %fd15, %fd129;
	// inline asm
	mov.f64 	%fd133, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd130, %fd126, %fd15, %fd133;
	// inline asm
	mov.f64 	%fd137, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd134, %fd130, %fd15, %fd137;
	// inline asm
	mov.f64 	%fd141, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd138, %fd134, %fd15, %fd141;
	// inline asm
	mul.rn.f64 	%fd143, %fd138, %fd15;
	// inline asm
	fma.rn.f64 	%fd142, %fd143, %fd619, %fd619;
	// inline asm
	mov.f64 	%fd620, %fd142;

BB10_74:
	and.b32  	%r247, %r59, 2;
	setp.eq.s32 	%p61, %r247, 0;
	neg.f64 	%fd146, %fd620;
	selp.f64 	%fd621, %fd620, %fd146, %p61;
	bra.uni 	BB10_76;

BB10_75:
	mov.f64 	%fd621, 0dFFF8000000000000;

BB10_76:
	setp.eq.f64 	%p62, %fd9, 0d0000000000000000;
	or.pred  	%p63, %p8, %p62;
	or.pred  	%p64, %p7, %p63;
	@%p64 bra 	BB10_99;

	// inline asm
	abs.f64 	%fd147, %fd9;
	// inline asm
	setp.gt.f64 	%p65, %fd147, 0d41E0000000000000;
	@%p65 bra 	BB10_79;

	mov.f64 	%fd162, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd149, %fd9, %fd162;
	// inline asm
	cvt.rni.s32.f64 	%r248, %fd149;
	// inline asm
	cvt.rn.f64.s32 	%fd163, %r248;
	neg.f64 	%fd159, %fd163;
	mov.f64 	%fd152, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd150, %fd159, %fd152, %fd9;
	// inline asm
	mov.f64 	%fd156, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd154, %fd159, %fd156, %fd150;
	// inline asm
	mov.f64 	%fd160, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd158, %fd159, %fd160, %fd154;
	// inline asm
	mov.u32 	%r458, %r248;
	mov.f64 	%fd622, %fd158;
	bra.uni 	BB10_95;

BB10_79:
	mov.b64 	 %rl33, %fd9;
	and.b64  	%rl452, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl234, %rl35, 2047;
	add.s64 	%rl235, %rl234, 4294966272;
	cvt.u32.u64 	%r61, %rl235;
	shl.b64 	%rl236, %rl33, 11;
	or.b64  	%rl36, %rl236, -9223372036854775808;
	shr.u32 	%r252, %r61, 6;
	mov.u32 	%r253, 16;
	sub.s32 	%r62, %r253, %r252;
	mov.u32 	%r254, 15;
	sub.s32 	%r456, %r254, %r252;
	mov.u32 	%r255, 19;
	sub.s32 	%r64, %r255, %r252;
	mov.u32 	%r250, 18;
	// inline asm
	min.s32 	%r249, %r250, %r64;
	// inline asm
	setp.lt.s32 	%p66, %r456, %r249;
	@%p66 bra 	BB10_81;

	mov.u64 	%rl449, 0;
	bra.uni 	BB10_83;

BB10_81:
	mov.u32 	%r256, 1;
	sub.s32 	%r65, %r256, %r62;
	mov.u64 	%rl449, 0;

BB10_82:
	.pragma "nounroll";
	shl.b32 	%r260, %r456, 3;
	mov.u32 	%r261, __internal_i2opi_d;
	add.s32 	%r262, %r261, %r260;
	ld.const.u64 	%rl240, [%r262];
	mul.lo.s64 	%rl242, %rl240, %rl36;
	// inline asm
	mul.hi.u64 	%rl239, %rl240, %rl36;
	// inline asm
	mad.lo.s64 	%rl243, %rl240, %rl36, %rl449;
	setp.lt.u64 	%p67, %rl243, %rl242;
	selp.u64 	%rl244, 1, 0, %p67;
	add.s64 	%rl449, %rl244, %rl239;
	add.s32 	%r263, %r65, %r456;
	shl.b32 	%r264, %r263, 3;
	add.s32 	%r266, %r42, %r264;
	st.local.u64 	[%r266], %rl243;
	// inline asm
	min.s32 	%r257, %r250, %r64;
	// inline asm
	add.s32 	%r456, %r456, 1;
	setp.lt.s32 	%p68, %r456, %r257;
	@%p68 bra 	BB10_82;

BB10_83:
	mov.u32 	%r267, 1;
	sub.s32 	%r268, %r267, %r62;
	add.s32 	%r269, %r268, %r456;
	shl.b32 	%r270, %r269, 3;
	add.s32 	%r272, %r42, %r270;
	st.local.u64 	[%r272], %rl449;
	ld.local.u64 	%rl450, [%r42+24];
	ld.local.u64 	%rl451, [%r42+16];
	and.b32  	%r273, %r61, 63;
	setp.eq.s32 	%p69, %r273, 0;
	@%p69 bra 	BB10_85;

	and.b64  	%rl245, %rl35, 63;
	cvt.u32.u64 	%r274, %rl245;
	shl.b64 	%rl246, %rl450, %r274;
	neg.s32 	%r275, %r61;
	and.b32  	%r276, %r275, 63;
	shr.u64 	%rl247, %rl451, %r276;
	or.b64  	%rl450, %rl247, %rl246;
	shl.b64 	%rl248, %rl451, %r274;
	ld.local.u64 	%rl249, [%r42+8];
	shr.u64 	%rl250, %rl249, %r276;
	or.b64  	%rl451, %rl250, %rl248;

BB10_85:
	shr.u64 	%rl251, %rl450, 62;
	cvt.u32.u64 	%r277, %rl251;
	shr.u64 	%rl252, %rl451, 62;
	shl.b64 	%rl253, %rl450, 2;
	or.b64  	%rl456, %rl252, %rl253;
	shl.b64 	%rl47, %rl451, 2;
	setp.ne.s64 	%p70, %rl47, 0;
	selp.u64 	%rl254, 1, 0, %p70;
	or.b64  	%rl255, %rl254, %rl456;
	setp.gt.u64 	%p71, %rl255, -9223372036854775808;
	selp.u32 	%r278, 1, 0, %p71;
	add.s32 	%r279, %r278, %r277;
	neg.s32 	%r280, %r279;
	setp.lt.s64 	%p72, %rl33, 0;
	selp.b32 	%r458, %r280, %r279, %p72;
	@%p71 bra 	BB10_87;

	mov.u64 	%rl455, %rl47;
	bra.uni 	BB10_88;

BB10_87:
	not.b64 	%rl256, %rl456;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p73, %rl47, 0;
	selp.u64 	%rl257, 1, 0, %p73;
	add.s64 	%rl456, %rl257, %rl256;
	xor.b64  	%rl452, %rl452, -9223372036854775808;
	mov.u64 	%rl455, %rl48;

BB10_88:
	mov.u64 	%rl454, %rl455;
	setp.gt.s64 	%p74, %rl456, 0;
	@%p74 bra 	BB10_90;

	mov.u32 	%r457, 0;
	bra.uni 	BB10_92;

BB10_90:
	mov.u32 	%r457, 0;

BB10_91:
	shr.u64 	%rl258, %rl454, 63;
	shl.b64 	%rl259, %rl456, 1;
	or.b64  	%rl456, %rl258, %rl259;
	shl.b64 	%rl454, %rl454, 1;
	add.s32 	%r457, %r457, -1;
	setp.gt.s64 	%p75, %rl456, 0;
	@%p75 bra 	BB10_91;

BB10_92:
	mul.lo.s64 	%rl458, %rl456, -3958705157555305931;
	mov.u64 	%rl262, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl260, %rl456, %rl262;
	// inline asm
	setp.gt.s64 	%p76, %rl260, 0;
	mov.u64 	%rl457, %rl260;
	@%p76 bra 	BB10_93;
	bra.uni 	BB10_94;

BB10_93:
	shl.b64 	%rl263, %rl260, 1;
	shr.u64 	%rl264, %rl458, 63;
	or.b64  	%rl457, %rl263, %rl264;
	mul.lo.s64 	%rl458, %rl456, -7917410315110611862;
	add.s32 	%r457, %r457, -1;

BB10_94:
	setp.ne.s64 	%p77, %rl458, 0;
	selp.u64 	%rl265, 1, 0, %p77;
	add.s64 	%rl266, %rl265, %rl457;
	add.s32 	%r283, %r457, 1022;
	cvt.u64.u32 	%rl267, %r283;
	shl.b64 	%rl268, %rl267, 52;
	shr.u64 	%rl269, %rl266, 11;
	shr.u64 	%rl270, %rl266, 10;
	and.b64  	%rl271, %rl270, 1;
	add.s64 	%rl272, %rl268, %rl269;
	add.s64 	%rl273, %rl272, %rl271;
	or.b64  	%rl274, %rl273, %rl452;
	mov.b64 	 %fd622, %rl274;

BB10_95:
	and.b32  	%r284, %r458, 1;
	setp.eq.s32 	%p78, %r284, 0;
	mul.rn.f64 	%fd25, %fd622, %fd622;
	@%p78 bra 	BB10_97;

	mov.f64 	%fd165, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd167, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd164, %fd165, %fd25, %fd167;
	// inline asm
	mov.f64 	%fd171, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd168, %fd164, %fd25, %fd171;
	// inline asm
	mov.f64 	%fd175, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd172, %fd168, %fd25, %fd175;
	// inline asm
	mov.f64 	%fd179, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd176, %fd172, %fd25, %fd179;
	// inline asm
	mov.f64 	%fd183, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd180, %fd176, %fd25, %fd183;
	// inline asm
	mov.f64 	%fd187, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd184, %fd180, %fd25, %fd187;
	// inline asm
	mov.f64 	%fd191, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd188, %fd184, %fd25, %fd191;
	// inline asm
	mov.f64 	%fd623, %fd188;
	bra.uni 	BB10_98;

BB10_97:
	mov.f64 	%fd193, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd195, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd192, %fd193, %fd25, %fd195;
	// inline asm
	mov.f64 	%fd199, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd196, %fd192, %fd25, %fd199;
	// inline asm
	mov.f64 	%fd203, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd200, %fd196, %fd25, %fd203;
	// inline asm
	mov.f64 	%fd207, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd204, %fd200, %fd25, %fd207;
	// inline asm
	mov.f64 	%fd211, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd208, %fd204, %fd25, %fd211;
	// inline asm
	mul.rn.f64 	%fd213, %fd208, %fd25;
	// inline asm
	fma.rn.f64 	%fd212, %fd213, %fd622, %fd622;
	// inline asm
	mov.f64 	%fd623, %fd212;

BB10_98:
	and.b32  	%r285, %r458, 2;
	setp.eq.s32 	%p79, %r285, 0;
	neg.f64 	%fd216, %fd623;
	selp.f64 	%fd624, %fd623, %fd216, %p79;
	bra.uni 	BB10_100;

BB10_99:
	mov.f64 	%fd217, 0d0000000000000000;
	mul.rn.f64 	%fd624, %fd9, %fd217;

BB10_100:
	mul.f64 	%fd218, %fd2, %fd624;
	fma.rn.f64 	%fd219, %fd1, %fd621, %fd218;
	mul.f64 	%fd220, %fd2, %fd621;
	neg.f64 	%fd221, %fd1;
	fma.rn.f64 	%fd222, %fd221, %fd624, %fd220;
	mul.f64 	%fd223, %fd7, 0d401921FB54442D18;
	div.rn.f64 	%fd32, %fd223, %fd8;
	setp.eq.f64 	%p9, %fd32, %fd10;
	setp.eq.f64 	%p10, %fd32, %fd11;
	or.pred  	%p80, %p9, %p10;
	@%p80 bra 	BB10_123;

	// inline asm
	abs.f64 	%fd224, %fd32;
	// inline asm
	setp.gt.f64 	%p81, %fd224, 0d41E0000000000000;
	@%p81 bra 	BB10_103;

	mov.f64 	%fd239, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd226, %fd32, %fd239;
	// inline asm
	cvt.rni.s32.f64 	%r286, %fd226;
	// inline asm
	cvt.rn.f64.s32 	%fd240, %r286;
	neg.f64 	%fd236, %fd240;
	mov.f64 	%fd229, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd227, %fd236, %fd229, %fd32;
	// inline asm
	mov.f64 	%fd233, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd231, %fd236, %fd233, %fd227;
	// inline asm
	mov.f64 	%fd237, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd235, %fd236, %fd237, %fd231;
	// inline asm
	mov.u32 	%r461, %r286;
	mov.f64 	%fd625, %fd235;
	bra.uni 	BB10_119;

BB10_103:
	mov.b64 	 %rl65, %fd32;
	and.b64  	%rl462, %rl65, -9223372036854775808;
	shr.u64 	%rl67, %rl65, 52;
	and.b64  	%rl275, %rl67, 2047;
	add.s64 	%rl276, %rl275, 4294966272;
	cvt.u32.u64 	%r77, %rl276;
	shl.b64 	%rl277, %rl65, 11;
	or.b64  	%rl68, %rl277, -9223372036854775808;
	shr.u32 	%r290, %r77, 6;
	mov.u32 	%r291, 16;
	sub.s32 	%r78, %r291, %r290;
	mov.u32 	%r292, 15;
	sub.s32 	%r459, %r292, %r290;
	mov.u32 	%r293, 19;
	sub.s32 	%r80, %r293, %r290;
	mov.u32 	%r288, 18;
	// inline asm
	min.s32 	%r287, %r288, %r80;
	// inline asm
	setp.lt.s32 	%p82, %r459, %r287;
	@%p82 bra 	BB10_105;

	mov.u64 	%rl459, 0;
	bra.uni 	BB10_107;

BB10_105:
	mov.u32 	%r294, 1;
	sub.s32 	%r81, %r294, %r78;
	mov.u64 	%rl459, 0;

BB10_106:
	.pragma "nounroll";
	shl.b32 	%r298, %r459, 3;
	mov.u32 	%r299, __internal_i2opi_d;
	add.s32 	%r300, %r299, %r298;
	ld.const.u64 	%rl281, [%r300];
	mul.lo.s64 	%rl283, %rl281, %rl68;
	// inline asm
	mul.hi.u64 	%rl280, %rl281, %rl68;
	// inline asm
	mad.lo.s64 	%rl284, %rl281, %rl68, %rl459;
	setp.lt.u64 	%p83, %rl284, %rl283;
	selp.u64 	%rl285, 1, 0, %p83;
	add.s64 	%rl459, %rl285, %rl280;
	add.s32 	%r301, %r81, %r459;
	shl.b32 	%r302, %r301, 3;
	add.s32 	%r304, %r42, %r302;
	st.local.u64 	[%r304], %rl284;
	// inline asm
	min.s32 	%r295, %r288, %r80;
	// inline asm
	add.s32 	%r459, %r459, 1;
	setp.lt.s32 	%p84, %r459, %r295;
	@%p84 bra 	BB10_106;

BB10_107:
	mov.u32 	%r305, 1;
	sub.s32 	%r306, %r305, %r78;
	add.s32 	%r307, %r306, %r459;
	shl.b32 	%r308, %r307, 3;
	add.s32 	%r310, %r42, %r308;
	st.local.u64 	[%r310], %rl459;
	ld.local.u64 	%rl460, [%r42+24];
	ld.local.u64 	%rl461, [%r42+16];
	and.b32  	%r311, %r77, 63;
	setp.eq.s32 	%p85, %r311, 0;
	@%p85 bra 	BB10_109;

	and.b64  	%rl286, %rl67, 63;
	cvt.u32.u64 	%r312, %rl286;
	shl.b64 	%rl287, %rl460, %r312;
	neg.s32 	%r313, %r77;
	and.b32  	%r314, %r313, 63;
	shr.u64 	%rl288, %rl461, %r314;
	or.b64  	%rl460, %rl288, %rl287;
	shl.b64 	%rl289, %rl461, %r312;
	ld.local.u64 	%rl290, [%r42+8];
	shr.u64 	%rl291, %rl290, %r314;
	or.b64  	%rl461, %rl291, %rl289;

BB10_109:
	shr.u64 	%rl292, %rl460, 62;
	cvt.u32.u64 	%r315, %rl292;
	shr.u64 	%rl293, %rl461, 62;
	shl.b64 	%rl294, %rl460, 2;
	or.b64  	%rl466, %rl293, %rl294;
	shl.b64 	%rl79, %rl461, 2;
	setp.ne.s64 	%p86, %rl79, 0;
	selp.u64 	%rl295, 1, 0, %p86;
	or.b64  	%rl296, %rl295, %rl466;
	setp.gt.u64 	%p87, %rl296, -9223372036854775808;
	selp.u32 	%r316, 1, 0, %p87;
	add.s32 	%r317, %r316, %r315;
	neg.s32 	%r318, %r317;
	setp.lt.s64 	%p88, %rl65, 0;
	selp.b32 	%r461, %r318, %r317, %p88;
	@%p87 bra 	BB10_111;

	mov.u64 	%rl465, %rl79;
	bra.uni 	BB10_112;

BB10_111:
	not.b64 	%rl297, %rl466;
	neg.s64 	%rl80, %rl79;
	setp.eq.s64 	%p89, %rl79, 0;
	selp.u64 	%rl298, 1, 0, %p89;
	add.s64 	%rl466, %rl298, %rl297;
	xor.b64  	%rl462, %rl462, -9223372036854775808;
	mov.u64 	%rl465, %rl80;

BB10_112:
	mov.u64 	%rl464, %rl465;
	setp.gt.s64 	%p90, %rl466, 0;
	@%p90 bra 	BB10_114;

	mov.u32 	%r460, 0;
	bra.uni 	BB10_116;

BB10_114:
	mov.u32 	%r460, 0;

BB10_115:
	shr.u64 	%rl299, %rl464, 63;
	shl.b64 	%rl300, %rl466, 1;
	or.b64  	%rl466, %rl299, %rl300;
	shl.b64 	%rl464, %rl464, 1;
	add.s32 	%r460, %r460, -1;
	setp.gt.s64 	%p91, %rl466, 0;
	@%p91 bra 	BB10_115;

BB10_116:
	mul.lo.s64 	%rl468, %rl466, -3958705157555305931;
	mov.u64 	%rl303, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl301, %rl466, %rl303;
	// inline asm
	setp.gt.s64 	%p92, %rl301, 0;
	mov.u64 	%rl467, %rl301;
	@%p92 bra 	BB10_117;
	bra.uni 	BB10_118;

BB10_117:
	shl.b64 	%rl304, %rl301, 1;
	shr.u64 	%rl305, %rl468, 63;
	or.b64  	%rl467, %rl304, %rl305;
	mul.lo.s64 	%rl468, %rl466, -7917410315110611862;
	add.s32 	%r460, %r460, -1;

BB10_118:
	setp.ne.s64 	%p93, %rl468, 0;
	selp.u64 	%rl306, 1, 0, %p93;
	add.s64 	%rl307, %rl306, %rl467;
	add.s32 	%r321, %r460, 1022;
	cvt.u64.u32 	%rl308, %r321;
	shl.b64 	%rl309, %rl308, 52;
	shr.u64 	%rl310, %rl307, 11;
	shr.u64 	%rl311, %rl307, 10;
	and.b64  	%rl312, %rl311, 1;
	add.s64 	%rl313, %rl309, %rl310;
	add.s64 	%rl314, %rl313, %rl312;
	or.b64  	%rl315, %rl314, %rl462;
	mov.b64 	 %fd625, %rl315;

BB10_119:
	add.s32 	%r92, %r461, 1;
	and.b32  	%r322, %r92, 1;
	setp.eq.s32 	%p94, %r322, 0;
	mul.rn.f64 	%fd36, %fd625, %fd625;
	@%p94 bra 	BB10_121;

	mov.f64 	%fd242, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd244, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd241, %fd242, %fd36, %fd244;
	// inline asm
	mov.f64 	%fd248, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd245, %fd241, %fd36, %fd248;
	// inline asm
	mov.f64 	%fd252, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd249, %fd245, %fd36, %fd252;
	// inline asm
	mov.f64 	%fd256, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd253, %fd249, %fd36, %fd256;
	// inline asm
	mov.f64 	%fd260, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd257, %fd253, %fd36, %fd260;
	// inline asm
	mov.f64 	%fd264, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd261, %fd257, %fd36, %fd264;
	// inline asm
	mov.f64 	%fd268, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd265, %fd261, %fd36, %fd268;
	// inline asm
	mov.f64 	%fd626, %fd265;
	bra.uni 	BB10_122;

BB10_121:
	mov.f64 	%fd270, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd272, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd269, %fd270, %fd36, %fd272;
	// inline asm
	mov.f64 	%fd276, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd273, %fd269, %fd36, %fd276;
	// inline asm
	mov.f64 	%fd280, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd277, %fd273, %fd36, %fd280;
	// inline asm
	mov.f64 	%fd284, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd281, %fd277, %fd36, %fd284;
	// inline asm
	mov.f64 	%fd288, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd285, %fd281, %fd36, %fd288;
	// inline asm
	mul.rn.f64 	%fd290, %fd285, %fd36;
	// inline asm
	fma.rn.f64 	%fd289, %fd290, %fd625, %fd625;
	// inline asm
	mov.f64 	%fd626, %fd289;

BB10_122:
	and.b32  	%r323, %r92, 2;
	setp.eq.s32 	%p95, %r323, 0;
	neg.f64 	%fd293, %fd626;
	selp.f64 	%fd627, %fd626, %fd293, %p95;
	bra.uni 	BB10_124;

BB10_123:
	mov.f64 	%fd627, 0dFFF8000000000000;

BB10_124:
	setp.eq.f64 	%p96, %fd32, 0d0000000000000000;
	or.pred  	%p97, %p10, %p96;
	or.pred  	%p98, %p9, %p97;
	@%p98 bra 	BB10_147;

	// inline asm
	abs.f64 	%fd294, %fd32;
	// inline asm
	setp.gt.f64 	%p99, %fd294, 0d41E0000000000000;
	@%p99 bra 	BB10_127;

	mov.f64 	%fd309, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd296, %fd32, %fd309;
	// inline asm
	cvt.rni.s32.f64 	%r324, %fd296;
	// inline asm
	cvt.rn.f64.s32 	%fd310, %r324;
	neg.f64 	%fd306, %fd310;
	mov.f64 	%fd299, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd297, %fd306, %fd299, %fd32;
	// inline asm
	mov.f64 	%fd303, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd301, %fd306, %fd303, %fd297;
	// inline asm
	mov.f64 	%fd307, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd305, %fd306, %fd307, %fd301;
	// inline asm
	mov.u32 	%r464, %r324;
	mov.f64 	%fd628, %fd305;
	bra.uni 	BB10_143;

BB10_127:
	mov.b64 	 %rl97, %fd32;
	and.b64  	%rl472, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl316, %rl99, 2047;
	add.s64 	%rl317, %rl316, 4294966272;
	cvt.u32.u64 	%r94, %rl317;
	shl.b64 	%rl318, %rl97, 11;
	or.b64  	%rl100, %rl318, -9223372036854775808;
	shr.u32 	%r328, %r94, 6;
	mov.u32 	%r329, 16;
	sub.s32 	%r95, %r329, %r328;
	mov.u32 	%r330, 15;
	sub.s32 	%r462, %r330, %r328;
	mov.u32 	%r331, 19;
	sub.s32 	%r97, %r331, %r328;
	mov.u32 	%r326, 18;
	// inline asm
	min.s32 	%r325, %r326, %r97;
	// inline asm
	setp.lt.s32 	%p100, %r462, %r325;
	@%p100 bra 	BB10_129;

	mov.u64 	%rl469, 0;
	bra.uni 	BB10_131;

BB10_129:
	mov.u32 	%r332, 1;
	sub.s32 	%r98, %r332, %r95;
	mov.u64 	%rl469, 0;

BB10_130:
	.pragma "nounroll";
	shl.b32 	%r336, %r462, 3;
	mov.u32 	%r337, __internal_i2opi_d;
	add.s32 	%r338, %r337, %r336;
	ld.const.u64 	%rl322, [%r338];
	mul.lo.s64 	%rl324, %rl322, %rl100;
	// inline asm
	mul.hi.u64 	%rl321, %rl322, %rl100;
	// inline asm
	mad.lo.s64 	%rl325, %rl322, %rl100, %rl469;
	setp.lt.u64 	%p101, %rl325, %rl324;
	selp.u64 	%rl326, 1, 0, %p101;
	add.s64 	%rl469, %rl326, %rl321;
	add.s32 	%r339, %r98, %r462;
	shl.b32 	%r340, %r339, 3;
	add.s32 	%r342, %r42, %r340;
	st.local.u64 	[%r342], %rl325;
	// inline asm
	min.s32 	%r333, %r326, %r97;
	// inline asm
	add.s32 	%r462, %r462, 1;
	setp.lt.s32 	%p102, %r462, %r333;
	@%p102 bra 	BB10_130;

BB10_131:
	mov.u32 	%r343, 1;
	sub.s32 	%r344, %r343, %r95;
	add.s32 	%r345, %r344, %r462;
	shl.b32 	%r346, %r345, 3;
	add.s32 	%r348, %r42, %r346;
	st.local.u64 	[%r348], %rl469;
	ld.local.u64 	%rl470, [%r42+24];
	ld.local.u64 	%rl471, [%r42+16];
	and.b32  	%r349, %r94, 63;
	setp.eq.s32 	%p103, %r349, 0;
	@%p103 bra 	BB10_133;

	and.b64  	%rl327, %rl99, 63;
	cvt.u32.u64 	%r350, %rl327;
	shl.b64 	%rl328, %rl470, %r350;
	neg.s32 	%r351, %r94;
	and.b32  	%r352, %r351, 63;
	shr.u64 	%rl329, %rl471, %r352;
	or.b64  	%rl470, %rl329, %rl328;
	shl.b64 	%rl330, %rl471, %r350;
	ld.local.u64 	%rl331, [%r42+8];
	shr.u64 	%rl332, %rl331, %r352;
	or.b64  	%rl471, %rl332, %rl330;

BB10_133:
	shr.u64 	%rl333, %rl470, 62;
	cvt.u32.u64 	%r353, %rl333;
	shr.u64 	%rl334, %rl471, 62;
	shl.b64 	%rl335, %rl470, 2;
	or.b64  	%rl476, %rl334, %rl335;
	shl.b64 	%rl111, %rl471, 2;
	setp.ne.s64 	%p104, %rl111, 0;
	selp.u64 	%rl336, 1, 0, %p104;
	or.b64  	%rl337, %rl336, %rl476;
	setp.gt.u64 	%p105, %rl337, -9223372036854775808;
	selp.u32 	%r354, 1, 0, %p105;
	add.s32 	%r355, %r354, %r353;
	neg.s32 	%r356, %r355;
	setp.lt.s64 	%p106, %rl97, 0;
	selp.b32 	%r464, %r356, %r355, %p106;
	@%p105 bra 	BB10_135;

	mov.u64 	%rl475, %rl111;
	bra.uni 	BB10_136;

BB10_135:
	not.b64 	%rl338, %rl476;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p107, %rl111, 0;
	selp.u64 	%rl339, 1, 0, %p107;
	add.s64 	%rl476, %rl339, %rl338;
	xor.b64  	%rl472, %rl472, -9223372036854775808;
	mov.u64 	%rl475, %rl112;

BB10_136:
	mov.u64 	%rl474, %rl475;
	setp.gt.s64 	%p108, %rl476, 0;
	@%p108 bra 	BB10_138;

	mov.u32 	%r463, 0;
	bra.uni 	BB10_140;

BB10_138:
	mov.u32 	%r463, 0;

BB10_139:
	shr.u64 	%rl340, %rl474, 63;
	shl.b64 	%rl341, %rl476, 1;
	or.b64  	%rl476, %rl340, %rl341;
	shl.b64 	%rl474, %rl474, 1;
	add.s32 	%r463, %r463, -1;
	setp.gt.s64 	%p109, %rl476, 0;
	@%p109 bra 	BB10_139;

BB10_140:
	mul.lo.s64 	%rl478, %rl476, -3958705157555305931;
	mov.u64 	%rl344, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl342, %rl476, %rl344;
	// inline asm
	setp.gt.s64 	%p110, %rl342, 0;
	mov.u64 	%rl477, %rl342;
	@%p110 bra 	BB10_141;
	bra.uni 	BB10_142;

BB10_141:
	shl.b64 	%rl345, %rl342, 1;
	shr.u64 	%rl346, %rl478, 63;
	or.b64  	%rl477, %rl345, %rl346;
	mul.lo.s64 	%rl478, %rl476, -7917410315110611862;
	add.s32 	%r463, %r463, -1;

BB10_142:
	setp.ne.s64 	%p111, %rl478, 0;
	selp.u64 	%rl347, 1, 0, %p111;
	add.s64 	%rl348, %rl347, %rl477;
	add.s32 	%r359, %r463, 1022;
	cvt.u64.u32 	%rl349, %r359;
	shl.b64 	%rl350, %rl349, 52;
	shr.u64 	%rl351, %rl348, 11;
	shr.u64 	%rl352, %rl348, 10;
	and.b64  	%rl353, %rl352, 1;
	add.s64 	%rl354, %rl350, %rl351;
	add.s64 	%rl355, %rl354, %rl353;
	or.b64  	%rl356, %rl355, %rl472;
	mov.b64 	 %fd628, %rl356;

BB10_143:
	and.b32  	%r360, %r464, 1;
	setp.eq.s32 	%p112, %r360, 0;
	mul.rn.f64 	%fd46, %fd628, %fd628;
	@%p112 bra 	BB10_145;

	mov.f64 	%fd312, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd314, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd311, %fd312, %fd46, %fd314;
	// inline asm
	mov.f64 	%fd318, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd315, %fd311, %fd46, %fd318;
	// inline asm
	mov.f64 	%fd322, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd319, %fd315, %fd46, %fd322;
	// inline asm
	mov.f64 	%fd326, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd323, %fd319, %fd46, %fd326;
	// inline asm
	mov.f64 	%fd330, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd327, %fd323, %fd46, %fd330;
	// inline asm
	mov.f64 	%fd334, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd331, %fd327, %fd46, %fd334;
	// inline asm
	mov.f64 	%fd338, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd335, %fd331, %fd46, %fd338;
	// inline asm
	mov.f64 	%fd629, %fd335;
	bra.uni 	BB10_146;

BB10_145:
	mov.f64 	%fd340, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd342, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd339, %fd340, %fd46, %fd342;
	// inline asm
	mov.f64 	%fd346, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd343, %fd339, %fd46, %fd346;
	// inline asm
	mov.f64 	%fd350, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd347, %fd343, %fd46, %fd350;
	// inline asm
	mov.f64 	%fd354, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd351, %fd347, %fd46, %fd354;
	// inline asm
	mov.f64 	%fd358, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd355, %fd351, %fd46, %fd358;
	// inline asm
	mul.rn.f64 	%fd360, %fd355, %fd46;
	// inline asm
	fma.rn.f64 	%fd359, %fd360, %fd628, %fd628;
	// inline asm
	mov.f64 	%fd629, %fd359;

BB10_146:
	and.b32  	%r361, %r464, 2;
	setp.eq.s32 	%p113, %r361, 0;
	neg.f64 	%fd363, %fd629;
	selp.f64 	%fd630, %fd629, %fd363, %p113;
	bra.uni 	BB10_148;

BB10_147:
	mov.f64 	%fd364, 0d0000000000000000;
	mul.rn.f64 	%fd630, %fd32, %fd364;

BB10_148:
	mul.f64 	%fd365, %fd4, %fd630;
	fma.rn.f64 	%fd366, %fd3, %fd627, %fd365;
	mul.f64 	%fd367, %fd4, %fd627;
	neg.f64 	%fd368, %fd3;
	fma.rn.f64 	%fd369, %fd368, %fd630, %fd367;
	mul.f64 	%fd370, %fd7, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd53, %fd370, %fd8;
	setp.eq.f64 	%p11, %fd53, %fd10;
	setp.eq.f64 	%p12, %fd53, %fd11;
	or.pred  	%p114, %p11, %p12;
	@%p114 bra 	BB10_171;

	// inline asm
	abs.f64 	%fd371, %fd53;
	// inline asm
	setp.gt.f64 	%p115, %fd371, 0d41E0000000000000;
	@%p115 bra 	BB10_151;

	mov.f64 	%fd386, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd373, %fd53, %fd386;
	// inline asm
	cvt.rni.s32.f64 	%r362, %fd373;
	// inline asm
	cvt.rn.f64.s32 	%fd387, %r362;
	neg.f64 	%fd383, %fd387;
	mov.f64 	%fd376, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd374, %fd383, %fd376, %fd53;
	// inline asm
	mov.f64 	%fd380, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd378, %fd383, %fd380, %fd374;
	// inline asm
	mov.f64 	%fd384, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd382, %fd383, %fd384, %fd378;
	// inline asm
	mov.u32 	%r467, %r362;
	mov.f64 	%fd631, %fd382;
	bra.uni 	BB10_167;

BB10_151:
	mov.b64 	 %rl129, %fd53;
	and.b64  	%rl482, %rl129, -9223372036854775808;
	shr.u64 	%rl131, %rl129, 52;
	and.b64  	%rl357, %rl131, 2047;
	add.s64 	%rl358, %rl357, 4294966272;
	cvt.u32.u64 	%r110, %rl358;
	shl.b64 	%rl359, %rl129, 11;
	or.b64  	%rl132, %rl359, -9223372036854775808;
	shr.u32 	%r366, %r110, 6;
	mov.u32 	%r367, 16;
	sub.s32 	%r111, %r367, %r366;
	mov.u32 	%r368, 15;
	sub.s32 	%r465, %r368, %r366;
	mov.u32 	%r369, 19;
	sub.s32 	%r113, %r369, %r366;
	mov.u32 	%r364, 18;
	// inline asm
	min.s32 	%r363, %r364, %r113;
	// inline asm
	setp.lt.s32 	%p116, %r465, %r363;
	@%p116 bra 	BB10_153;

	mov.u64 	%rl479, 0;
	bra.uni 	BB10_155;

BB10_153:
	mov.u32 	%r370, 1;
	sub.s32 	%r114, %r370, %r111;
	mov.u64 	%rl479, 0;

BB10_154:
	.pragma "nounroll";
	shl.b32 	%r374, %r465, 3;
	mov.u32 	%r375, __internal_i2opi_d;
	add.s32 	%r376, %r375, %r374;
	ld.const.u64 	%rl363, [%r376];
	mul.lo.s64 	%rl365, %rl363, %rl132;
	// inline asm
	mul.hi.u64 	%rl362, %rl363, %rl132;
	// inline asm
	mad.lo.s64 	%rl366, %rl363, %rl132, %rl479;
	setp.lt.u64 	%p117, %rl366, %rl365;
	selp.u64 	%rl367, 1, 0, %p117;
	add.s64 	%rl479, %rl367, %rl362;
	add.s32 	%r377, %r114, %r465;
	shl.b32 	%r378, %r377, 3;
	add.s32 	%r380, %r42, %r378;
	st.local.u64 	[%r380], %rl366;
	// inline asm
	min.s32 	%r371, %r364, %r113;
	// inline asm
	add.s32 	%r465, %r465, 1;
	setp.lt.s32 	%p118, %r465, %r371;
	@%p118 bra 	BB10_154;

BB10_155:
	mov.u32 	%r381, 1;
	sub.s32 	%r382, %r381, %r111;
	add.s32 	%r383, %r382, %r465;
	shl.b32 	%r384, %r383, 3;
	add.s32 	%r386, %r42, %r384;
	st.local.u64 	[%r386], %rl479;
	ld.local.u64 	%rl480, [%r42+24];
	ld.local.u64 	%rl481, [%r42+16];
	and.b32  	%r387, %r110, 63;
	setp.eq.s32 	%p119, %r387, 0;
	@%p119 bra 	BB10_157;

	and.b64  	%rl368, %rl131, 63;
	cvt.u32.u64 	%r388, %rl368;
	shl.b64 	%rl369, %rl480, %r388;
	neg.s32 	%r389, %r110;
	and.b32  	%r390, %r389, 63;
	shr.u64 	%rl370, %rl481, %r390;
	or.b64  	%rl480, %rl370, %rl369;
	shl.b64 	%rl371, %rl481, %r388;
	ld.local.u64 	%rl372, [%r42+8];
	shr.u64 	%rl373, %rl372, %r390;
	or.b64  	%rl481, %rl373, %rl371;

BB10_157:
	shr.u64 	%rl374, %rl480, 62;
	cvt.u32.u64 	%r391, %rl374;
	shr.u64 	%rl375, %rl481, 62;
	shl.b64 	%rl376, %rl480, 2;
	or.b64  	%rl486, %rl375, %rl376;
	shl.b64 	%rl143, %rl481, 2;
	setp.ne.s64 	%p120, %rl143, 0;
	selp.u64 	%rl377, 1, 0, %p120;
	or.b64  	%rl378, %rl377, %rl486;
	setp.gt.u64 	%p121, %rl378, -9223372036854775808;
	selp.u32 	%r392, 1, 0, %p121;
	add.s32 	%r393, %r392, %r391;
	neg.s32 	%r394, %r393;
	setp.lt.s64 	%p122, %rl129, 0;
	selp.b32 	%r467, %r394, %r393, %p122;
	@%p121 bra 	BB10_159;

	mov.u64 	%rl485, %rl143;
	bra.uni 	BB10_160;

BB10_159:
	not.b64 	%rl379, %rl486;
	neg.s64 	%rl144, %rl143;
	setp.eq.s64 	%p123, %rl143, 0;
	selp.u64 	%rl380, 1, 0, %p123;
	add.s64 	%rl486, %rl380, %rl379;
	xor.b64  	%rl482, %rl482, -9223372036854775808;
	mov.u64 	%rl485, %rl144;

BB10_160:
	mov.u64 	%rl484, %rl485;
	setp.gt.s64 	%p124, %rl486, 0;
	@%p124 bra 	BB10_162;

	mov.u32 	%r466, 0;
	bra.uni 	BB10_164;

BB10_162:
	mov.u32 	%r466, 0;

BB10_163:
	shr.u64 	%rl381, %rl484, 63;
	shl.b64 	%rl382, %rl486, 1;
	or.b64  	%rl486, %rl381, %rl382;
	shl.b64 	%rl484, %rl484, 1;
	add.s32 	%r466, %r466, -1;
	setp.gt.s64 	%p125, %rl486, 0;
	@%p125 bra 	BB10_163;

BB10_164:
	mul.lo.s64 	%rl488, %rl486, -3958705157555305931;
	mov.u64 	%rl385, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl383, %rl486, %rl385;
	// inline asm
	setp.gt.s64 	%p126, %rl383, 0;
	mov.u64 	%rl487, %rl383;
	@%p126 bra 	BB10_165;
	bra.uni 	BB10_166;

BB10_165:
	shl.b64 	%rl386, %rl383, 1;
	shr.u64 	%rl387, %rl488, 63;
	or.b64  	%rl487, %rl386, %rl387;
	mul.lo.s64 	%rl488, %rl486, -7917410315110611862;
	add.s32 	%r466, %r466, -1;

BB10_166:
	setp.ne.s64 	%p127, %rl488, 0;
	selp.u64 	%rl388, 1, 0, %p127;
	add.s64 	%rl389, %rl388, %rl487;
	add.s32 	%r397, %r466, 1022;
	cvt.u64.u32 	%rl390, %r397;
	shl.b64 	%rl391, %rl390, 52;
	shr.u64 	%rl392, %rl389, 11;
	shr.u64 	%rl393, %rl389, 10;
	and.b64  	%rl394, %rl393, 1;
	add.s64 	%rl395, %rl391, %rl392;
	add.s64 	%rl396, %rl395, %rl394;
	or.b64  	%rl397, %rl396, %rl482;
	mov.b64 	 %fd631, %rl397;

BB10_167:
	add.s32 	%r125, %r467, 1;
	and.b32  	%r398, %r125, 1;
	setp.eq.s32 	%p128, %r398, 0;
	mul.rn.f64 	%fd57, %fd631, %fd631;
	@%p128 bra 	BB10_169;

	mov.f64 	%fd389, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd391, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd388, %fd389, %fd57, %fd391;
	// inline asm
	mov.f64 	%fd395, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd392, %fd388, %fd57, %fd395;
	// inline asm
	mov.f64 	%fd399, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd396, %fd392, %fd57, %fd399;
	// inline asm
	mov.f64 	%fd403, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd400, %fd396, %fd57, %fd403;
	// inline asm
	mov.f64 	%fd407, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd404, %fd400, %fd57, %fd407;
	// inline asm
	mov.f64 	%fd411, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd408, %fd404, %fd57, %fd411;
	// inline asm
	mov.f64 	%fd415, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd412, %fd408, %fd57, %fd415;
	// inline asm
	mov.f64 	%fd632, %fd412;
	bra.uni 	BB10_170;

BB10_169:
	mov.f64 	%fd417, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd419, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd416, %fd417, %fd57, %fd419;
	// inline asm
	mov.f64 	%fd423, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd420, %fd416, %fd57, %fd423;
	// inline asm
	mov.f64 	%fd427, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd424, %fd420, %fd57, %fd427;
	// inline asm
	mov.f64 	%fd431, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd428, %fd424, %fd57, %fd431;
	// inline asm
	mov.f64 	%fd435, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd432, %fd428, %fd57, %fd435;
	// inline asm
	mul.rn.f64 	%fd437, %fd432, %fd57;
	// inline asm
	fma.rn.f64 	%fd436, %fd437, %fd631, %fd631;
	// inline asm
	mov.f64 	%fd632, %fd436;

BB10_170:
	and.b32  	%r399, %r125, 2;
	setp.eq.s32 	%p129, %r399, 0;
	neg.f64 	%fd440, %fd632;
	selp.f64 	%fd633, %fd632, %fd440, %p129;
	bra.uni 	BB10_172;

BB10_171:
	mov.f64 	%fd633, 0dFFF8000000000000;

BB10_172:
	setp.eq.f64 	%p130, %fd53, 0d0000000000000000;
	or.pred  	%p131, %p12, %p130;
	or.pred  	%p132, %p11, %p131;
	@%p132 bra 	BB10_195;

	// inline asm
	abs.f64 	%fd441, %fd53;
	// inline asm
	setp.gt.f64 	%p133, %fd441, 0d41E0000000000000;
	@%p133 bra 	BB10_175;

	mov.f64 	%fd456, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd443, %fd53, %fd456;
	// inline asm
	cvt.rni.s32.f64 	%r400, %fd443;
	// inline asm
	cvt.rn.f64.s32 	%fd457, %r400;
	neg.f64 	%fd453, %fd457;
	mov.f64 	%fd446, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd444, %fd453, %fd446, %fd53;
	// inline asm
	mov.f64 	%fd450, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd448, %fd453, %fd450, %fd444;
	// inline asm
	mov.f64 	%fd454, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd452, %fd453, %fd454, %fd448;
	// inline asm
	mov.u32 	%r470, %r400;
	mov.f64 	%fd634, %fd452;
	bra.uni 	BB10_191;

BB10_175:
	mov.b64 	 %rl161, %fd53;
	and.b64  	%rl492, %rl161, -9223372036854775808;
	shr.u64 	%rl163, %rl161, 52;
	and.b64  	%rl398, %rl163, 2047;
	add.s64 	%rl399, %rl398, 4294966272;
	cvt.u32.u64 	%r127, %rl399;
	shl.b64 	%rl400, %rl161, 11;
	or.b64  	%rl164, %rl400, -9223372036854775808;
	shr.u32 	%r404, %r127, 6;
	mov.u32 	%r405, 16;
	sub.s32 	%r128, %r405, %r404;
	mov.u32 	%r406, 15;
	sub.s32 	%r468, %r406, %r404;
	mov.u32 	%r407, 19;
	sub.s32 	%r130, %r407, %r404;
	mov.u32 	%r402, 18;
	// inline asm
	min.s32 	%r401, %r402, %r130;
	// inline asm
	setp.lt.s32 	%p134, %r468, %r401;
	@%p134 bra 	BB10_177;

	mov.u64 	%rl489, 0;
	bra.uni 	BB10_179;

BB10_177:
	mov.u32 	%r408, 1;
	sub.s32 	%r131, %r408, %r128;
	mov.u64 	%rl489, 0;

BB10_178:
	.pragma "nounroll";
	shl.b32 	%r412, %r468, 3;
	mov.u32 	%r413, __internal_i2opi_d;
	add.s32 	%r414, %r413, %r412;
	ld.const.u64 	%rl404, [%r414];
	mul.lo.s64 	%rl406, %rl404, %rl164;
	// inline asm
	mul.hi.u64 	%rl403, %rl404, %rl164;
	// inline asm
	mad.lo.s64 	%rl407, %rl404, %rl164, %rl489;
	setp.lt.u64 	%p135, %rl407, %rl406;
	selp.u64 	%rl408, 1, 0, %p135;
	add.s64 	%rl489, %rl408, %rl403;
	add.s32 	%r415, %r131, %r468;
	shl.b32 	%r416, %r415, 3;
	add.s32 	%r418, %r42, %r416;
	st.local.u64 	[%r418], %rl407;
	// inline asm
	min.s32 	%r409, %r402, %r130;
	// inline asm
	add.s32 	%r468, %r468, 1;
	setp.lt.s32 	%p136, %r468, %r409;
	@%p136 bra 	BB10_178;

BB10_179:
	mov.u32 	%r419, 1;
	sub.s32 	%r420, %r419, %r128;
	add.s32 	%r421, %r420, %r468;
	shl.b32 	%r422, %r421, 3;
	add.s32 	%r424, %r42, %r422;
	st.local.u64 	[%r424], %rl489;
	ld.local.u64 	%rl490, [%r42+24];
	ld.local.u64 	%rl491, [%r42+16];
	and.b32  	%r425, %r127, 63;
	setp.eq.s32 	%p137, %r425, 0;
	@%p137 bra 	BB10_181;

	and.b64  	%rl409, %rl163, 63;
	cvt.u32.u64 	%r426, %rl409;
	shl.b64 	%rl410, %rl490, %r426;
	neg.s32 	%r427, %r127;
	and.b32  	%r428, %r427, 63;
	shr.u64 	%rl411, %rl491, %r428;
	or.b64  	%rl490, %rl411, %rl410;
	shl.b64 	%rl412, %rl491, %r426;
	ld.local.u64 	%rl413, [%r42+8];
	shr.u64 	%rl414, %rl413, %r428;
	or.b64  	%rl491, %rl414, %rl412;

BB10_181:
	shr.u64 	%rl415, %rl490, 62;
	cvt.u32.u64 	%r429, %rl415;
	shr.u64 	%rl416, %rl491, 62;
	shl.b64 	%rl417, %rl490, 2;
	or.b64  	%rl496, %rl416, %rl417;
	shl.b64 	%rl175, %rl491, 2;
	setp.ne.s64 	%p138, %rl175, 0;
	selp.u64 	%rl418, 1, 0, %p138;
	or.b64  	%rl419, %rl418, %rl496;
	setp.gt.u64 	%p139, %rl419, -9223372036854775808;
	selp.u32 	%r430, 1, 0, %p139;
	add.s32 	%r431, %r430, %r429;
	neg.s32 	%r432, %r431;
	setp.lt.s64 	%p140, %rl161, 0;
	selp.b32 	%r470, %r432, %r431, %p140;
	@%p139 bra 	BB10_183;

	mov.u64 	%rl495, %rl175;
	bra.uni 	BB10_184;

BB10_183:
	not.b64 	%rl420, %rl496;
	neg.s64 	%rl176, %rl175;
	setp.eq.s64 	%p141, %rl175, 0;
	selp.u64 	%rl421, 1, 0, %p141;
	add.s64 	%rl496, %rl421, %rl420;
	xor.b64  	%rl492, %rl492, -9223372036854775808;
	mov.u64 	%rl495, %rl176;

BB10_184:
	mov.u64 	%rl494, %rl495;
	setp.gt.s64 	%p142, %rl496, 0;
	@%p142 bra 	BB10_186;

	mov.u32 	%r469, 0;
	bra.uni 	BB10_188;

BB10_186:
	mov.u32 	%r469, 0;

BB10_187:
	shr.u64 	%rl422, %rl494, 63;
	shl.b64 	%rl423, %rl496, 1;
	or.b64  	%rl496, %rl422, %rl423;
	shl.b64 	%rl494, %rl494, 1;
	add.s32 	%r469, %r469, -1;
	setp.gt.s64 	%p143, %rl496, 0;
	@%p143 bra 	BB10_187;

BB10_188:
	mul.lo.s64 	%rl498, %rl496, -3958705157555305931;
	mov.u64 	%rl426, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl424, %rl496, %rl426;
	// inline asm
	setp.gt.s64 	%p144, %rl424, 0;
	mov.u64 	%rl497, %rl424;
	@%p144 bra 	BB10_189;
	bra.uni 	BB10_190;

BB10_189:
	shl.b64 	%rl427, %rl424, 1;
	shr.u64 	%rl428, %rl498, 63;
	or.b64  	%rl497, %rl427, %rl428;
	mul.lo.s64 	%rl498, %rl496, -7917410315110611862;
	add.s32 	%r469, %r469, -1;

BB10_190:
	setp.ne.s64 	%p145, %rl498, 0;
	selp.u64 	%rl429, 1, 0, %p145;
	add.s64 	%rl430, %rl429, %rl497;
	add.s32 	%r435, %r469, 1022;
	cvt.u64.u32 	%rl431, %r435;
	shl.b64 	%rl432, %rl431, 52;
	shr.u64 	%rl433, %rl430, 11;
	shr.u64 	%rl434, %rl430, 10;
	and.b64  	%rl435, %rl434, 1;
	add.s64 	%rl436, %rl432, %rl433;
	add.s64 	%rl437, %rl436, %rl435;
	or.b64  	%rl438, %rl437, %rl492;
	mov.b64 	 %fd634, %rl438;

BB10_191:
	and.b32  	%r436, %r470, 1;
	setp.eq.s32 	%p146, %r436, 0;
	mul.rn.f64 	%fd67, %fd634, %fd634;
	@%p146 bra 	BB10_193;

	mov.f64 	%fd459, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd461, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd458, %fd459, %fd67, %fd461;
	// inline asm
	mov.f64 	%fd465, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd462, %fd458, %fd67, %fd465;
	// inline asm
	mov.f64 	%fd469, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd466, %fd462, %fd67, %fd469;
	// inline asm
	mov.f64 	%fd473, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd470, %fd466, %fd67, %fd473;
	// inline asm
	mov.f64 	%fd477, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd474, %fd470, %fd67, %fd477;
	// inline asm
	mov.f64 	%fd481, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd478, %fd474, %fd67, %fd481;
	// inline asm
	mov.f64 	%fd485, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd482, %fd478, %fd67, %fd485;
	// inline asm
	mov.f64 	%fd635, %fd482;
	bra.uni 	BB10_194;

BB10_193:
	mov.f64 	%fd487, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd489, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd486, %fd487, %fd67, %fd489;
	// inline asm
	mov.f64 	%fd493, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd490, %fd486, %fd67, %fd493;
	// inline asm
	mov.f64 	%fd497, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd494, %fd490, %fd67, %fd497;
	// inline asm
	mov.f64 	%fd501, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd498, %fd494, %fd67, %fd501;
	// inline asm
	mov.f64 	%fd505, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd502, %fd498, %fd67, %fd505;
	// inline asm
	mul.rn.f64 	%fd507, %fd502, %fd67;
	// inline asm
	fma.rn.f64 	%fd506, %fd507, %fd634, %fd634;
	// inline asm
	mov.f64 	%fd635, %fd506;

BB10_194:
	and.b32  	%r437, %r470, 2;
	setp.eq.s32 	%p147, %r437, 0;
	neg.f64 	%fd510, %fd635;
	selp.f64 	%fd636, %fd635, %fd510, %p147;
	bra.uni 	BB10_196;

BB10_195:
	mov.f64 	%fd511, 0d0000000000000000;
	mul.rn.f64 	%fd636, %fd53, %fd511;

BB10_196:
	mul.f64 	%fd512, %fd6, %fd636;
	fma.rn.f64 	%fd513, %fd5, %fd633, %fd512;
	mul.f64 	%fd514, %fd6, %fd633;
	neg.f64 	%fd515, %fd5;
	fma.rn.f64 	%fd516, %fd515, %fd636, %fd514;
	mov.f64 	%fd637, %fd74;
	mov.f64 	%fd638, %fd75;
	mov.f64 	%fd639, %fd219;
	mov.f64 	%fd640, %fd222;
	mov.f64 	%fd641, %fd366;
	mov.f64 	%fd642, %fd369;
	mov.f64 	%fd643, %fd513;
	mov.f64 	%fd644, %fd516;

BB10_197:
	ld.param.u32 	%r442, [DIT4C2CM_param_4];
	setp.eq.s32 	%p148, %r442, 1;
	@%p148 bra 	BB10_200;

	ld.param.u32 	%r441, [DIT4C2CM_param_4];
	setp.ne.s32 	%p149, %r441, 0;
	@%p149 bra 	BB10_201;

	add.f64 	%fd519, %fd637, %fd639;
	add.f64 	%fd521, %fd519, %fd641;
	add.f64 	%fd523, %fd521, %fd643;
	st.global.f64 	[%r38], %fd523;
	add.f64 	%fd526, %fd638, %fd640;
	add.f64 	%fd528, %fd526, %fd642;
	add.f64 	%fd530, %fd528, %fd644;
	st.global.f64 	[%r38+8], %fd530;
	sub.f64 	%fd531, %fd637, %fd639;
	sub.f64 	%fd532, %fd531, %fd642;
	add.f64 	%fd533, %fd532, %fd644;
	st.global.f64 	[%r39], %fd533;
	sub.f64 	%fd534, %fd638, %fd640;
	add.f64 	%fd535, %fd534, %fd641;
	sub.f64 	%fd536, %fd535, %fd643;
	st.global.f64 	[%r39+8], %fd536;
	sub.f64 	%fd537, %fd519, %fd641;
	sub.f64 	%fd538, %fd537, %fd643;
	st.global.f64 	[%r40], %fd538;
	sub.f64 	%fd539, %fd526, %fd642;
	sub.f64 	%fd540, %fd539, %fd644;
	st.global.f64 	[%r40+8], %fd540;
	add.f64 	%fd541, %fd531, %fd642;
	sub.f64 	%fd542, %fd541, %fd644;
	st.global.f64 	[%r41], %fd542;
	sub.f64 	%fd543, %fd534, %fd641;
	add.f64 	%fd544, %fd543, %fd643;
	st.global.f64 	[%r41+8], %fd544;
	ret;

BB10_200:
	add.f64 	%fd547, %fd637, %fd639;
	add.f64 	%fd549, %fd547, %fd641;
	add.f64 	%fd551, %fd549, %fd643;
	st.global.f64 	[%r38], %fd551;
	add.f64 	%fd554, %fd638, %fd640;
	add.f64 	%fd556, %fd554, %fd642;
	add.f64 	%fd558, %fd556, %fd644;
	st.global.f64 	[%r38+8], %fd558;
	sub.f64 	%fd559, %fd637, %fd639;
	add.f64 	%fd560, %fd559, %fd642;
	sub.f64 	%fd561, %fd560, %fd644;
	st.global.f64 	[%r39], %fd561;
	sub.f64 	%fd562, %fd638, %fd640;
	sub.f64 	%fd563, %fd562, %fd641;
	add.f64 	%fd564, %fd563, %fd643;
	st.global.f64 	[%r39+8], %fd564;
	sub.f64 	%fd565, %fd547, %fd641;
	sub.f64 	%fd566, %fd565, %fd643;
	st.global.f64 	[%r40], %fd566;
	sub.f64 	%fd567, %fd554, %fd642;
	sub.f64 	%fd568, %fd567, %fd644;
	st.global.f64 	[%r40+8], %fd568;
	sub.f64 	%fd569, %fd559, %fd642;
	add.f64 	%fd570, %fd569, %fd644;
	st.global.f64 	[%r41], %fd570;
	add.f64 	%fd571, %fd562, %fd641;
	sub.f64 	%fd572, %fd571, %fd643;
	st.global.f64 	[%r41+8], %fd572;

BB10_201:
	ret;
}

.entry DIT5C2CM(
	.param .u32 .ptr .global .align 8 DIT5C2CM_param_0,
	.param .u32 DIT5C2CM_param_1,
	.param .u32 DIT5C2CM_param_2,
	.param .u32 DIT5C2CM_param_3,
	.param .u32 DIT5C2CM_param_4,
	.param .u32 DIT5C2CM_param_5
)
{
	.local .align 8 .b8 	__local_depot11[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<953>;
	.reg .pred 	%p<185>;
	.reg .s32 	%r<585>;
	.reg .s64 	%rl<665>;


	mov.u32 	%SP, __local_depot11;
	ld.param.u32 	%r184, [DIT5C2CM_param_3];
	// inline asm
	mov.u32 	%r176, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r177, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r178, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r179, %tid.x;
	// inline asm
	add.s32 	%r185, %r179, %r176;
	mad.lo.s32 	%r5, %r178, %r177, %r185;
	// inline asm
	mov.u32 	%r180, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r181, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r182, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r183, %tid.y;
	// inline asm
	add.s32 	%r186, %r183, %r180;
	mad.lo.s32 	%r6, %r182, %r181, %r186;
	mul.hi.s32 	%r187, %r184, 780903145;
	shr.u32 	%r188, %r187, 31;
	shr.s32 	%r189, %r187, 1;
	add.s32 	%r7, %r189, %r188;
	mul.lo.s32 	%r190, %r7, 11;
	sub.s32 	%r8, %r184, %r190;
	setp.gt.s32 	%p16, %r184, 10;
	@%p16 bra 	BB11_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB11_23;

BB11_2:
	mov.f32 	%f1, 0f41300000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40A00000;
	add.f32 	%f2, %f47, 0f41300000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r554, 0;
	mov.u32 	%r553, 1;

BB11_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p17, %p3, %p3;
	@%p17 bra 	BB11_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mov.f32 	%f55, 0f40000000;
	mul.rn.f32 	%f56, %f55, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p18, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p19, %f1, %f52;
	and.pred  	%p4, %p18, %p19;
	setp.eq.f32 	%p20, %f48, 0f00000000;
	@%p20 bra 	BB11_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r11, %f58;
	shr.u32 	%r193, %r11, 23;
	and.b32  	%r194, %r193, 255;
	add.s32 	%r555, %r194, -127;
	setp.eq.s32 	%p21, %r194, 0;
	mov.f32 	%f280, %f58;
	@%p21 bra 	BB11_6;
	bra.uni 	BB11_7;

BB11_6:
	and.b32  	%r195, %r11, -2139095041;
	or.b32  	%r196, %r195, 1065353216;
	mov.b32 	 %f60, %r196;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r197, %f61;
	shr.u32 	%r198, %r197, 23;
	and.b32  	%r199, %r198, 255;
	add.s32 	%r555, %r199, -253;
	and.b32  	%r200, %r197, -2139095041;
	or.b32  	%r201, %r200, 1065353216;
	mov.b32 	 %f280, %r201;

BB11_7:
	mov.b32 	 %r202, %f280;
	and.b32  	%r203, %r202, -2139095041;
	or.b32  	%r204, %r203, 1065353216;
	mov.b32 	 %f281, %r204;
	setp.gt.f32 	%p22, %f281, 0f3FB504F3;
	@%p22 bra 	BB11_8;
	bra.uni 	BB11_9;

BB11_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r555, %r555, 1;

BB11_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f55, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r205, %f73;
	and.b32  	%r206, %r205, -4096;
	mov.b32 	 %f81, %r206;
	mov.b32 	 %r207, %f64;
	and.b32  	%r208, %r207, -4096;
	mov.b32 	 %f82, %r208;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f55, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r555;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p23, %f69, 0f77F684DF;
	@%p23 bra 	BB11_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB11_12;

BB11_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB11_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r17, %f131;
	setp.eq.s32 	%p24, %r17, 1118925336;
	@%p24 bra 	BB11_13;
	bra.uni 	BB11_14;

BB11_13:
	add.s32 	%r209, %r17, -1;
	mov.b32 	 %f133, %r209;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB11_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p25, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p25;
	setp.gt.f32 	%p26, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p26;
	setp.neu.f32 	%p27, %f19, %f3;
	@%p27 bra 	BB11_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB11_21;

BB11_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB11_21;

BB11_17:
	@%p3 bra 	BB11_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB11_21;

BB11_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB11_21;

BB11_20:
	mov.f32 	%f283, %f7;

BB11_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r553;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r553, %f157;
	add.s32 	%r554, %r554, 1;
	setp.lt.s32 	%p28, %r554, %r7;
	@%p28 bra 	BB11_3;

	cvt.rn.f32.s32 	%f284, %r553;

BB11_23:
	mov.f32 	%f159, 0f40A00000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r8;
	setp.eq.f32 	%p29, %f287, 0f00000000;
	@%p29 bra 	BB11_45;

	setp.nan.f32 	%p30, %f287, %f287;
	@%p30 bra 	BB11_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p31, %f287, 0f7F800000;
	setp.eq.f32 	%p32, %f287, 0fFF800000;
	or.pred  	%p33, %p31, %p32;
	@%p33 bra 	BB11_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p36, %f158, 0f00000000;
	@%p36 bra 	BB11_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r20, %f168;
	shr.u32 	%r210, %r20, 23;
	and.b32  	%r211, %r210, 255;
	add.s32 	%r556, %r211, -127;
	setp.eq.s32 	%p37, %r211, 0;
	mov.f32 	%f285, %f168;
	@%p37 bra 	BB11_28;
	bra.uni 	BB11_29;

BB11_28:
	and.b32  	%r212, %r20, -2139095041;
	or.b32  	%r213, %r212, 1065353216;
	mov.b32 	 %f170, %r213;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r214, %f171;
	shr.u32 	%r215, %r214, 23;
	and.b32  	%r216, %r215, 255;
	add.s32 	%r556, %r216, -253;
	and.b32  	%r217, %r214, -2139095041;
	or.b32  	%r218, %r217, 1065353216;
	mov.b32 	 %f285, %r218;

BB11_29:
	mov.b32 	 %r219, %f285;
	and.b32  	%r220, %r219, -2139095041;
	or.b32  	%r221, %r220, 1065353216;
	mov.b32 	 %f286, %r221;
	setp.gt.f32 	%p38, %f286, 0f3FB504F3;
	@%p38 bra 	BB11_30;
	bra.uni 	BB11_31;

BB11_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r556, %r556, 1;

BB11_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mov.f32 	%f182, 0f40000000;
	mul.rn.f32 	%f183, %f182, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r222, %f183;
	and.b32  	%r223, %r222, -4096;
	mov.b32 	 %f191, %r223;
	mov.b32 	 %r224, %f174;
	and.b32  	%r225, %r224, -4096;
	mov.b32 	 %f192, %r225;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f182, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r556;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p39, %f179, 0f77F684DF;
	@%p39 bra 	BB11_32;
	bra.uni 	BB11_33;

BB11_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB11_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r26, %f241;
	setp.eq.s32 	%p40, %r26, 1118925336;
	@%p40 bra 	BB11_34;
	bra.uni 	BB11_35;

BB11_34:
	add.s32 	%r226, %r26, -1;
	mov.b32 	 %f243, %r226;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB11_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p41, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p41;
	setp.gt.f32 	%p42, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p42;
	setp.neu.f32 	%p43, %f39, %f27;
	@%p43 bra 	BB11_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB11_46;

BB11_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB11_46;

BB11_38:
	setp.lt.f32 	%p44, %f287, 0f00000000;
	@%p44 bra 	BB11_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB11_46;

BB11_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB11_46;

BB11_41:
	setp.lt.f32 	%p45, %f158, 0f3F800000;
	mov.b32 	 %r227, %f287;
	setp.lt.s32 	%p6, %r227, 0;
	@%p45 bra 	BB11_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB11_46;

BB11_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB11_46;

BB11_44:
	add.f32 	%f288, %f287, 0f40A00000;
	bra.uni 	BB11_46;

BB11_45:
	mov.f32 	%f288, 0f3F800000;

BB11_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r27, %f267;
	mul.hi.s32 	%r228, %r27, 1717986919;
	shr.u32 	%r229, %r228, 31;
	shr.s32 	%r230, %r228, 1;
	add.s32 	%r28, %r230, %r229;
	ld.param.u32 	%r552, [DIT5C2CM_param_5];
	setp.eq.s32 	%p46, %r552, 0;
	@%p46 bra 	BB11_49;

	ld.param.u32 	%r551, [DIT5C2CM_param_5];
	setp.ne.s32 	%p47, %r551, 1;
	@%p47 bra 	BB11_50;

	div.s32 	%r559, %r6, %r28;
	rem.s32 	%r560, %r6, %r28;
	ld.param.u32 	%r558, [DIT5C2CM_param_1];
	mov.u32 	%r557, %r5;
	bra.uni 	BB11_51;

BB11_49:
	ld.param.u32 	%r549, [DIT5C2CM_param_1];
	mul.lo.s32 	%r31, %r6, %r549;
	div.s32 	%r559, %r5, %r28;
	rem.s32 	%r560, %r5, %r28;
	mov.u32 	%r558, 1;
	mov.u32 	%r557, %r31;
	bra.uni 	BB11_51;

BB11_50:
	mov.u32 	%r560, %r236;
	mov.u32 	%r559, %r237;
	mov.u32 	%r558, 1;
	mov.u32 	%r557, 0;

BB11_51:
	mad.lo.s32 	%r238, %r559, %r27, %r560;
	mad.lo.s32 	%r239, %r238, %r558, %r557;
	shl.b32 	%r240, %r239, 4;
	ld.param.u32 	%r547, [DIT5C2CM_param_0];
	add.s32 	%r38, %r547, %r240;
	ld.global.f64 	%fd97, [%r38];
	ld.global.f64 	%fd98, [%r38+8];
	mul.lo.s32 	%r241, %r558, %r28;
	shl.b32 	%r242, %r241, 4;
	add.s32 	%r39, %r38, %r242;
	ld.global.f64 	%fd1, [%r39];
	ld.global.f64 	%fd2, [%r39+8];
	add.s32 	%r40, %r39, %r242;
	ld.global.f64 	%fd3, [%r40];
	ld.global.f64 	%fd4, [%r40+8];
	add.s32 	%r41, %r40, %r242;
	ld.global.f64 	%fd5, [%r41];
	ld.global.f64 	%fd6, [%r41+8];
	add.s32 	%r42, %r41, %r242;
	ld.global.f64 	%fd7, [%r42];
	ld.global.f64 	%fd8, [%r42+8];
	mov.f64 	%fd943, %fd7;
	mov.f64 	%fd944, %fd8;
	setp.eq.s32 	%p48, %r560, 0;
	mov.f64 	%fd935, %fd97;
	mov.f64 	%fd936, %fd98;
	mov.f64 	%fd937, %fd1;
	mov.f64 	%fd938, %fd2;
	mov.f64 	%fd939, %fd3;
	mov.f64 	%fd940, %fd4;
	mov.f64 	%fd941, %fd5;
	mov.f64 	%fd942, %fd6;
	@%p48 bra 	BB11_253;

	cvt.rn.f64.s32 	%fd9, %r560;
	mul.f64 	%fd99, %fd9, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd10, %r27;
	div.rn.f64 	%fd11, %fd99, %fd10;
	mov.f64 	%fd12, 0d7FF0000000000000;
	setp.eq.f64 	%p7, %fd11, 0d7FF0000000000000;
	mov.f64 	%fd13, 0dFFF0000000000000;
	setp.eq.f64 	%p8, %fd11, 0dFFF0000000000000;
	or.pred  	%p49, %p7, %p8;
	add.u32 	%r43, %SP, 0;
	@%p49 bra 	BB11_75;

	// inline asm
	abs.f64 	%fd100, %fd11;
	// inline asm
	setp.gt.f64 	%p50, %fd100, 0d41E0000000000000;
	@%p50 bra 	BB11_55;

	mov.f64 	%fd115, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd102, %fd11, %fd115;
	// inline asm
	cvt.rni.s32.f64 	%r243, %fd102;
	// inline asm
	cvt.rn.f64.s32 	%fd116, %r243;
	neg.f64 	%fd112, %fd116;
	mov.f64 	%fd105, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd103, %fd112, %fd105, %fd11;
	// inline asm
	mov.f64 	%fd109, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd107, %fd112, %fd109, %fd103;
	// inline asm
	mov.f64 	%fd113, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd111, %fd112, %fd113, %fd107;
	// inline asm
	mov.u32 	%r563, %r243;
	mov.f64 	%fd911, %fd111;
	bra.uni 	BB11_71;

BB11_55:
	mov.b64 	 %rl1, %fd11;
	and.b64  	%rl588, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl257, %rl3, 2047;
	add.s64 	%rl258, %rl257, 4294966272;
	cvt.u32.u64 	%r45, %rl258;
	shl.b64 	%rl259, %rl1, 11;
	or.b64  	%rl4, %rl259, -9223372036854775808;
	shr.u32 	%r247, %r45, 6;
	mov.u32 	%r248, 16;
	sub.s32 	%r46, %r248, %r247;
	mov.u32 	%r249, 15;
	sub.s32 	%r561, %r249, %r247;
	mov.u32 	%r250, 19;
	sub.s32 	%r48, %r250, %r247;
	mov.u32 	%r245, 18;
	// inline asm
	min.s32 	%r244, %r245, %r48;
	// inline asm
	setp.lt.s32 	%p51, %r561, %r244;
	@%p51 bra 	BB11_57;

	mov.u64 	%rl585, 0;
	bra.uni 	BB11_59;

BB11_57:
	mov.u32 	%r251, 1;
	sub.s32 	%r49, %r251, %r46;
	mov.u64 	%rl585, 0;

BB11_58:
	.pragma "nounroll";
	shl.b32 	%r255, %r561, 3;
	mov.u32 	%r256, __internal_i2opi_d;
	add.s32 	%r257, %r256, %r255;
	ld.const.u64 	%rl263, [%r257];
	mul.lo.s64 	%rl265, %rl263, %rl4;
	// inline asm
	mul.hi.u64 	%rl262, %rl263, %rl4;
	// inline asm
	mad.lo.s64 	%rl266, %rl263, %rl4, %rl585;
	setp.lt.u64 	%p52, %rl266, %rl265;
	selp.u64 	%rl267, 1, 0, %p52;
	add.s64 	%rl585, %rl267, %rl262;
	add.s32 	%r258, %r49, %r561;
	shl.b32 	%r259, %r258, 3;
	add.s32 	%r261, %r43, %r259;
	st.local.u64 	[%r261], %rl266;
	// inline asm
	min.s32 	%r252, %r245, %r48;
	// inline asm
	add.s32 	%r561, %r561, 1;
	setp.lt.s32 	%p53, %r561, %r252;
	@%p53 bra 	BB11_58;

BB11_59:
	mov.u32 	%r262, 1;
	sub.s32 	%r263, %r262, %r46;
	add.s32 	%r264, %r263, %r561;
	shl.b32 	%r265, %r264, 3;
	add.s32 	%r267, %r43, %r265;
	st.local.u64 	[%r267], %rl585;
	ld.local.u64 	%rl586, [%r43+24];
	ld.local.u64 	%rl587, [%r43+16];
	and.b32  	%r268, %r45, 63;
	setp.eq.s32 	%p54, %r268, 0;
	@%p54 bra 	BB11_61;

	and.b64  	%rl268, %rl3, 63;
	cvt.u32.u64 	%r269, %rl268;
	shl.b64 	%rl269, %rl586, %r269;
	neg.s32 	%r270, %r45;
	and.b32  	%r271, %r270, 63;
	shr.u64 	%rl270, %rl587, %r271;
	or.b64  	%rl586, %rl270, %rl269;
	shl.b64 	%rl271, %rl587, %r269;
	ld.local.u64 	%rl272, [%r43+8];
	shr.u64 	%rl273, %rl272, %r271;
	or.b64  	%rl587, %rl273, %rl271;

BB11_61:
	shr.u64 	%rl274, %rl586, 62;
	cvt.u32.u64 	%r272, %rl274;
	shr.u64 	%rl275, %rl587, 62;
	shl.b64 	%rl276, %rl586, 2;
	or.b64  	%rl592, %rl275, %rl276;
	shl.b64 	%rl15, %rl587, 2;
	setp.ne.s64 	%p55, %rl15, 0;
	selp.u64 	%rl277, 1, 0, %p55;
	or.b64  	%rl278, %rl277, %rl592;
	setp.gt.u64 	%p56, %rl278, -9223372036854775808;
	selp.u32 	%r273, 1, 0, %p56;
	add.s32 	%r274, %r273, %r272;
	neg.s32 	%r275, %r274;
	setp.lt.s64 	%p57, %rl1, 0;
	selp.b32 	%r563, %r275, %r274, %p57;
	@%p56 bra 	BB11_63;

	mov.u64 	%rl591, %rl15;
	bra.uni 	BB11_64;

BB11_63:
	not.b64 	%rl279, %rl592;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p58, %rl15, 0;
	selp.u64 	%rl280, 1, 0, %p58;
	add.s64 	%rl592, %rl280, %rl279;
	xor.b64  	%rl588, %rl588, -9223372036854775808;
	mov.u64 	%rl591, %rl16;

BB11_64:
	mov.u64 	%rl590, %rl591;
	setp.gt.s64 	%p59, %rl592, 0;
	@%p59 bra 	BB11_66;

	mov.u32 	%r562, 0;
	bra.uni 	BB11_68;

BB11_66:
	mov.u32 	%r562, 0;

BB11_67:
	shr.u64 	%rl281, %rl590, 63;
	shl.b64 	%rl282, %rl592, 1;
	or.b64  	%rl592, %rl281, %rl282;
	shl.b64 	%rl590, %rl590, 1;
	add.s32 	%r562, %r562, -1;
	setp.gt.s64 	%p60, %rl592, 0;
	@%p60 bra 	BB11_67;

BB11_68:
	mul.lo.s64 	%rl594, %rl592, -3958705157555305931;
	mov.u64 	%rl285, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl283, %rl592, %rl285;
	// inline asm
	setp.gt.s64 	%p61, %rl283, 0;
	mov.u64 	%rl593, %rl283;
	@%p61 bra 	BB11_69;
	bra.uni 	BB11_70;

BB11_69:
	shl.b64 	%rl286, %rl283, 1;
	shr.u64 	%rl287, %rl594, 63;
	or.b64  	%rl593, %rl286, %rl287;
	mul.lo.s64 	%rl594, %rl592, -7917410315110611862;
	add.s32 	%r562, %r562, -1;

BB11_70:
	setp.ne.s64 	%p62, %rl594, 0;
	selp.u64 	%rl288, 1, 0, %p62;
	add.s64 	%rl289, %rl288, %rl593;
	add.s32 	%r278, %r562, 1022;
	cvt.u64.u32 	%rl290, %r278;
	shl.b64 	%rl291, %rl290, 52;
	shr.u64 	%rl292, %rl289, 11;
	shr.u64 	%rl293, %rl289, 10;
	and.b64  	%rl294, %rl293, 1;
	add.s64 	%rl295, %rl291, %rl292;
	add.s64 	%rl296, %rl295, %rl294;
	or.b64  	%rl297, %rl296, %rl588;
	mov.b64 	 %fd911, %rl297;

BB11_71:
	add.s32 	%r60, %r563, 1;
	and.b32  	%r279, %r60, 1;
	setp.eq.s32 	%p63, %r279, 0;
	mul.rn.f64 	%fd17, %fd911, %fd911;
	@%p63 bra 	BB11_73;

	mov.f64 	%fd118, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd120, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd117, %fd118, %fd17, %fd120;
	// inline asm
	mov.f64 	%fd124, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd121, %fd117, %fd17, %fd124;
	// inline asm
	mov.f64 	%fd128, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd125, %fd121, %fd17, %fd128;
	// inline asm
	mov.f64 	%fd132, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd129, %fd125, %fd17, %fd132;
	// inline asm
	mov.f64 	%fd136, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd133, %fd129, %fd17, %fd136;
	// inline asm
	mov.f64 	%fd140, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd137, %fd133, %fd17, %fd140;
	// inline asm
	mov.f64 	%fd144, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd141, %fd137, %fd17, %fd144;
	// inline asm
	mov.f64 	%fd912, %fd141;
	bra.uni 	BB11_74;

BB11_73:
	mov.f64 	%fd146, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd148, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd145, %fd146, %fd17, %fd148;
	// inline asm
	mov.f64 	%fd152, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd149, %fd145, %fd17, %fd152;
	// inline asm
	mov.f64 	%fd156, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd153, %fd149, %fd17, %fd156;
	// inline asm
	mov.f64 	%fd160, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd157, %fd153, %fd17, %fd160;
	// inline asm
	mov.f64 	%fd164, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd161, %fd157, %fd17, %fd164;
	// inline asm
	mul.rn.f64 	%fd166, %fd161, %fd17;
	// inline asm
	fma.rn.f64 	%fd165, %fd166, %fd911, %fd911;
	// inline asm
	mov.f64 	%fd912, %fd165;

BB11_74:
	and.b32  	%r280, %r60, 2;
	setp.eq.s32 	%p64, %r280, 0;
	neg.f64 	%fd169, %fd912;
	selp.f64 	%fd913, %fd912, %fd169, %p64;
	bra.uni 	BB11_76;

BB11_75:
	mov.f64 	%fd913, 0dFFF8000000000000;

BB11_76:
	setp.eq.f64 	%p65, %fd11, 0d0000000000000000;
	or.pred  	%p66, %p8, %p65;
	or.pred  	%p67, %p7, %p66;
	@%p67 bra 	BB11_99;

	// inline asm
	abs.f64 	%fd171, %fd11;
	// inline asm
	setp.gt.f64 	%p68, %fd171, 0d41E0000000000000;
	@%p68 bra 	BB11_79;

	mov.f64 	%fd186, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd173, %fd11, %fd186;
	// inline asm
	cvt.rni.s32.f64 	%r281, %fd173;
	// inline asm
	cvt.rn.f64.s32 	%fd187, %r281;
	neg.f64 	%fd183, %fd187;
	mov.f64 	%fd176, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd174, %fd183, %fd176, %fd11;
	// inline asm
	mov.f64 	%fd180, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd178, %fd183, %fd180, %fd174;
	// inline asm
	mov.f64 	%fd184, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd182, %fd183, %fd184, %fd178;
	// inline asm
	mov.u32 	%r566, %r281;
	mov.f64 	%fd914, %fd182;
	bra.uni 	BB11_95;

BB11_79:
	mov.b64 	 %rl33, %fd11;
	and.b64  	%rl598, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl298, %rl35, 2047;
	add.s64 	%rl299, %rl298, 4294966272;
	cvt.u32.u64 	%r62, %rl299;
	shl.b64 	%rl300, %rl33, 11;
	or.b64  	%rl36, %rl300, -9223372036854775808;
	shr.u32 	%r285, %r62, 6;
	mov.u32 	%r286, 16;
	sub.s32 	%r63, %r286, %r285;
	mov.u32 	%r287, 15;
	sub.s32 	%r564, %r287, %r285;
	mov.u32 	%r288, 19;
	sub.s32 	%r65, %r288, %r285;
	mov.u32 	%r283, 18;
	// inline asm
	min.s32 	%r282, %r283, %r65;
	// inline asm
	setp.lt.s32 	%p69, %r564, %r282;
	@%p69 bra 	BB11_81;

	mov.u64 	%rl595, 0;
	bra.uni 	BB11_83;

BB11_81:
	mov.u32 	%r289, 1;
	sub.s32 	%r66, %r289, %r63;
	mov.u64 	%rl595, 0;

BB11_82:
	.pragma "nounroll";
	shl.b32 	%r293, %r564, 3;
	mov.u32 	%r294, __internal_i2opi_d;
	add.s32 	%r295, %r294, %r293;
	ld.const.u64 	%rl304, [%r295];
	mul.lo.s64 	%rl306, %rl304, %rl36;
	// inline asm
	mul.hi.u64 	%rl303, %rl304, %rl36;
	// inline asm
	mad.lo.s64 	%rl307, %rl304, %rl36, %rl595;
	setp.lt.u64 	%p70, %rl307, %rl306;
	selp.u64 	%rl308, 1, 0, %p70;
	add.s64 	%rl595, %rl308, %rl303;
	add.s32 	%r296, %r66, %r564;
	shl.b32 	%r297, %r296, 3;
	add.s32 	%r299, %r43, %r297;
	st.local.u64 	[%r299], %rl307;
	// inline asm
	min.s32 	%r290, %r283, %r65;
	// inline asm
	add.s32 	%r564, %r564, 1;
	setp.lt.s32 	%p71, %r564, %r290;
	@%p71 bra 	BB11_82;

BB11_83:
	mov.u32 	%r300, 1;
	sub.s32 	%r301, %r300, %r63;
	add.s32 	%r302, %r301, %r564;
	shl.b32 	%r303, %r302, 3;
	add.s32 	%r305, %r43, %r303;
	st.local.u64 	[%r305], %rl595;
	ld.local.u64 	%rl596, [%r43+24];
	ld.local.u64 	%rl597, [%r43+16];
	and.b32  	%r306, %r62, 63;
	setp.eq.s32 	%p72, %r306, 0;
	@%p72 bra 	BB11_85;

	and.b64  	%rl309, %rl35, 63;
	cvt.u32.u64 	%r307, %rl309;
	shl.b64 	%rl310, %rl596, %r307;
	neg.s32 	%r308, %r62;
	and.b32  	%r309, %r308, 63;
	shr.u64 	%rl311, %rl597, %r309;
	or.b64  	%rl596, %rl311, %rl310;
	shl.b64 	%rl312, %rl597, %r307;
	ld.local.u64 	%rl313, [%r43+8];
	shr.u64 	%rl314, %rl313, %r309;
	or.b64  	%rl597, %rl314, %rl312;

BB11_85:
	shr.u64 	%rl315, %rl596, 62;
	cvt.u32.u64 	%r310, %rl315;
	shr.u64 	%rl316, %rl597, 62;
	shl.b64 	%rl317, %rl596, 2;
	or.b64  	%rl602, %rl316, %rl317;
	shl.b64 	%rl47, %rl597, 2;
	setp.ne.s64 	%p73, %rl47, 0;
	selp.u64 	%rl318, 1, 0, %p73;
	or.b64  	%rl319, %rl318, %rl602;
	setp.gt.u64 	%p74, %rl319, -9223372036854775808;
	selp.u32 	%r311, 1, 0, %p74;
	add.s32 	%r312, %r311, %r310;
	neg.s32 	%r313, %r312;
	setp.lt.s64 	%p75, %rl33, 0;
	selp.b32 	%r566, %r313, %r312, %p75;
	@%p74 bra 	BB11_87;

	mov.u64 	%rl601, %rl47;
	bra.uni 	BB11_88;

BB11_87:
	not.b64 	%rl320, %rl602;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p76, %rl47, 0;
	selp.u64 	%rl321, 1, 0, %p76;
	add.s64 	%rl602, %rl321, %rl320;
	xor.b64  	%rl598, %rl598, -9223372036854775808;
	mov.u64 	%rl601, %rl48;

BB11_88:
	mov.u64 	%rl600, %rl601;
	setp.gt.s64 	%p77, %rl602, 0;
	@%p77 bra 	BB11_90;

	mov.u32 	%r565, 0;
	bra.uni 	BB11_92;

BB11_90:
	mov.u32 	%r565, 0;

BB11_91:
	shr.u64 	%rl322, %rl600, 63;
	shl.b64 	%rl323, %rl602, 1;
	or.b64  	%rl602, %rl322, %rl323;
	shl.b64 	%rl600, %rl600, 1;
	add.s32 	%r565, %r565, -1;
	setp.gt.s64 	%p78, %rl602, 0;
	@%p78 bra 	BB11_91;

BB11_92:
	mul.lo.s64 	%rl604, %rl602, -3958705157555305931;
	mov.u64 	%rl326, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl324, %rl602, %rl326;
	// inline asm
	setp.gt.s64 	%p79, %rl324, 0;
	mov.u64 	%rl603, %rl324;
	@%p79 bra 	BB11_93;
	bra.uni 	BB11_94;

BB11_93:
	shl.b64 	%rl327, %rl324, 1;
	shr.u64 	%rl328, %rl604, 63;
	or.b64  	%rl603, %rl327, %rl328;
	mul.lo.s64 	%rl604, %rl602, -7917410315110611862;
	add.s32 	%r565, %r565, -1;

BB11_94:
	setp.ne.s64 	%p80, %rl604, 0;
	selp.u64 	%rl329, 1, 0, %p80;
	add.s64 	%rl330, %rl329, %rl603;
	add.s32 	%r316, %r565, 1022;
	cvt.u64.u32 	%rl331, %r316;
	shl.b64 	%rl332, %rl331, 52;
	shr.u64 	%rl333, %rl330, 11;
	shr.u64 	%rl334, %rl330, 10;
	and.b64  	%rl335, %rl334, 1;
	add.s64 	%rl336, %rl332, %rl333;
	add.s64 	%rl337, %rl336, %rl335;
	or.b64  	%rl338, %rl337, %rl598;
	mov.b64 	 %fd914, %rl338;

BB11_95:
	and.b32  	%r317, %r566, 1;
	setp.eq.s32 	%p81, %r317, 0;
	mul.rn.f64 	%fd27, %fd914, %fd914;
	@%p81 bra 	BB11_97;

	mov.f64 	%fd189, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd191, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd188, %fd189, %fd27, %fd191;
	// inline asm
	mov.f64 	%fd195, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd192, %fd188, %fd27, %fd195;
	// inline asm
	mov.f64 	%fd199, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd196, %fd192, %fd27, %fd199;
	// inline asm
	mov.f64 	%fd203, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd200, %fd196, %fd27, %fd203;
	// inline asm
	mov.f64 	%fd207, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd204, %fd200, %fd27, %fd207;
	// inline asm
	mov.f64 	%fd211, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd208, %fd204, %fd27, %fd211;
	// inline asm
	mov.f64 	%fd215, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd212, %fd208, %fd27, %fd215;
	// inline asm
	mov.f64 	%fd915, %fd212;
	bra.uni 	BB11_98;

BB11_97:
	mov.f64 	%fd217, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd219, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd216, %fd217, %fd27, %fd219;
	// inline asm
	mov.f64 	%fd223, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd220, %fd216, %fd27, %fd223;
	// inline asm
	mov.f64 	%fd227, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd224, %fd220, %fd27, %fd227;
	// inline asm
	mov.f64 	%fd231, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd228, %fd224, %fd27, %fd231;
	// inline asm
	mov.f64 	%fd235, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd232, %fd228, %fd27, %fd235;
	// inline asm
	mul.rn.f64 	%fd237, %fd232, %fd27;
	// inline asm
	fma.rn.f64 	%fd236, %fd237, %fd914, %fd914;
	// inline asm
	mov.f64 	%fd915, %fd236;

BB11_98:
	and.b32  	%r318, %r566, 2;
	setp.eq.s32 	%p82, %r318, 0;
	neg.f64 	%fd240, %fd915;
	selp.f64 	%fd916, %fd915, %fd240, %p82;
	bra.uni 	BB11_100;

BB11_99:
	mov.f64 	%fd241, 0d0000000000000000;
	mul.rn.f64 	%fd916, %fd11, %fd241;

BB11_100:
	neg.f64 	%fd242, %fd916;
	mov.f64 	%fd951, %fd913;
	mov.f64 	%fd952, %fd242;
	ld.param.u32 	%r550, [DIT5C2CM_param_4];
	setp.eq.s32 	%p9, %r550, 0;
	@%p9 bra 	BB11_101;
	bra.uni 	BB11_102;

BB11_101:
	mov.f64 	%fd951, %fd913;
	mov.f64 	%fd952, %fd916;

BB11_102:
	mul.f64 	%fd244, %fd1, %fd951;
	neg.f64 	%fd246, %fd2;
	fma.rn.f64 	%fd247, %fd246, %fd952, %fd244;
	mul.f64 	%fd248, %fd2, %fd951;
	fma.rn.f64 	%fd249, %fd1, %fd952, %fd248;
	mul.f64 	%fd250, %fd9, 0d402921FB54442D18;
	div.rn.f64 	%fd34, %fd250, %fd10;
	setp.eq.f64 	%p10, %fd34, %fd12;
	setp.eq.f64 	%p11, %fd34, %fd13;
	or.pred  	%p83, %p10, %p11;
	@%p83 bra 	BB11_125;

	// inline asm
	abs.f64 	%fd251, %fd34;
	// inline asm
	setp.gt.f64 	%p84, %fd251, 0d41E0000000000000;
	@%p84 bra 	BB11_105;

	mov.f64 	%fd266, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd253, %fd34, %fd266;
	// inline asm
	cvt.rni.s32.f64 	%r319, %fd253;
	// inline asm
	cvt.rn.f64.s32 	%fd267, %r319;
	neg.f64 	%fd263, %fd267;
	mov.f64 	%fd256, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd254, %fd263, %fd256, %fd34;
	// inline asm
	mov.f64 	%fd260, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd258, %fd263, %fd260, %fd254;
	// inline asm
	mov.f64 	%fd264, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd262, %fd263, %fd264, %fd258;
	// inline asm
	mov.u32 	%r569, %r319;
	mov.f64 	%fd917, %fd262;
	bra.uni 	BB11_121;

BB11_105:
	mov.b64 	 %rl65, %fd34;
	and.b64  	%rl608, %rl65, -9223372036854775808;
	shr.u64 	%rl67, %rl65, 52;
	and.b64  	%rl339, %rl67, 2047;
	add.s64 	%rl340, %rl339, 4294966272;
	cvt.u32.u64 	%r78, %rl340;
	shl.b64 	%rl341, %rl65, 11;
	or.b64  	%rl68, %rl341, -9223372036854775808;
	shr.u32 	%r323, %r78, 6;
	mov.u32 	%r324, 16;
	sub.s32 	%r79, %r324, %r323;
	mov.u32 	%r325, 15;
	sub.s32 	%r567, %r325, %r323;
	mov.u32 	%r326, 19;
	sub.s32 	%r81, %r326, %r323;
	mov.u32 	%r321, 18;
	// inline asm
	min.s32 	%r320, %r321, %r81;
	// inline asm
	setp.lt.s32 	%p85, %r567, %r320;
	@%p85 bra 	BB11_107;

	mov.u64 	%rl605, 0;
	bra.uni 	BB11_109;

BB11_107:
	mov.u32 	%r327, 1;
	sub.s32 	%r82, %r327, %r79;
	mov.u64 	%rl605, 0;

BB11_108:
	.pragma "nounroll";
	shl.b32 	%r331, %r567, 3;
	mov.u32 	%r332, __internal_i2opi_d;
	add.s32 	%r333, %r332, %r331;
	ld.const.u64 	%rl345, [%r333];
	mul.lo.s64 	%rl347, %rl345, %rl68;
	// inline asm
	mul.hi.u64 	%rl344, %rl345, %rl68;
	// inline asm
	mad.lo.s64 	%rl348, %rl345, %rl68, %rl605;
	setp.lt.u64 	%p86, %rl348, %rl347;
	selp.u64 	%rl349, 1, 0, %p86;
	add.s64 	%rl605, %rl349, %rl344;
	add.s32 	%r334, %r82, %r567;
	shl.b32 	%r335, %r334, 3;
	add.s32 	%r337, %r43, %r335;
	st.local.u64 	[%r337], %rl348;
	// inline asm
	min.s32 	%r328, %r321, %r81;
	// inline asm
	add.s32 	%r567, %r567, 1;
	setp.lt.s32 	%p87, %r567, %r328;
	@%p87 bra 	BB11_108;

BB11_109:
	mov.u32 	%r338, 1;
	sub.s32 	%r339, %r338, %r79;
	add.s32 	%r340, %r339, %r567;
	shl.b32 	%r341, %r340, 3;
	add.s32 	%r343, %r43, %r341;
	st.local.u64 	[%r343], %rl605;
	ld.local.u64 	%rl606, [%r43+24];
	ld.local.u64 	%rl607, [%r43+16];
	and.b32  	%r344, %r78, 63;
	setp.eq.s32 	%p88, %r344, 0;
	@%p88 bra 	BB11_111;

	and.b64  	%rl350, %rl67, 63;
	cvt.u32.u64 	%r345, %rl350;
	shl.b64 	%rl351, %rl606, %r345;
	neg.s32 	%r346, %r78;
	and.b32  	%r347, %r346, 63;
	shr.u64 	%rl352, %rl607, %r347;
	or.b64  	%rl606, %rl352, %rl351;
	shl.b64 	%rl353, %rl607, %r345;
	ld.local.u64 	%rl354, [%r43+8];
	shr.u64 	%rl355, %rl354, %r347;
	or.b64  	%rl607, %rl355, %rl353;

BB11_111:
	shr.u64 	%rl356, %rl606, 62;
	cvt.u32.u64 	%r348, %rl356;
	shr.u64 	%rl357, %rl607, 62;
	shl.b64 	%rl358, %rl606, 2;
	or.b64  	%rl612, %rl357, %rl358;
	shl.b64 	%rl79, %rl607, 2;
	setp.ne.s64 	%p89, %rl79, 0;
	selp.u64 	%rl359, 1, 0, %p89;
	or.b64  	%rl360, %rl359, %rl612;
	setp.gt.u64 	%p90, %rl360, -9223372036854775808;
	selp.u32 	%r349, 1, 0, %p90;
	add.s32 	%r350, %r349, %r348;
	neg.s32 	%r351, %r350;
	setp.lt.s64 	%p91, %rl65, 0;
	selp.b32 	%r569, %r351, %r350, %p91;
	@%p90 bra 	BB11_113;

	mov.u64 	%rl611, %rl79;
	bra.uni 	BB11_114;

BB11_113:
	not.b64 	%rl361, %rl612;
	neg.s64 	%rl80, %rl79;
	setp.eq.s64 	%p92, %rl79, 0;
	selp.u64 	%rl362, 1, 0, %p92;
	add.s64 	%rl612, %rl362, %rl361;
	xor.b64  	%rl608, %rl608, -9223372036854775808;
	mov.u64 	%rl611, %rl80;

BB11_114:
	mov.u64 	%rl610, %rl611;
	setp.gt.s64 	%p93, %rl612, 0;
	@%p93 bra 	BB11_116;

	mov.u32 	%r568, 0;
	bra.uni 	BB11_118;

BB11_116:
	mov.u32 	%r568, 0;

BB11_117:
	shr.u64 	%rl363, %rl610, 63;
	shl.b64 	%rl364, %rl612, 1;
	or.b64  	%rl612, %rl363, %rl364;
	shl.b64 	%rl610, %rl610, 1;
	add.s32 	%r568, %r568, -1;
	setp.gt.s64 	%p94, %rl612, 0;
	@%p94 bra 	BB11_117;

BB11_118:
	mul.lo.s64 	%rl614, %rl612, -3958705157555305931;
	mov.u64 	%rl367, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl365, %rl612, %rl367;
	// inline asm
	setp.gt.s64 	%p95, %rl365, 0;
	mov.u64 	%rl613, %rl365;
	@%p95 bra 	BB11_119;
	bra.uni 	BB11_120;

BB11_119:
	shl.b64 	%rl368, %rl365, 1;
	shr.u64 	%rl369, %rl614, 63;
	or.b64  	%rl613, %rl368, %rl369;
	mul.lo.s64 	%rl614, %rl612, -7917410315110611862;
	add.s32 	%r568, %r568, -1;

BB11_120:
	setp.ne.s64 	%p96, %rl614, 0;
	selp.u64 	%rl370, 1, 0, %p96;
	add.s64 	%rl371, %rl370, %rl613;
	add.s32 	%r354, %r568, 1022;
	cvt.u64.u32 	%rl372, %r354;
	shl.b64 	%rl373, %rl372, 52;
	shr.u64 	%rl374, %rl371, 11;
	shr.u64 	%rl375, %rl371, 10;
	and.b64  	%rl376, %rl375, 1;
	add.s64 	%rl377, %rl373, %rl374;
	add.s64 	%rl378, %rl377, %rl376;
	or.b64  	%rl379, %rl378, %rl608;
	mov.b64 	 %fd917, %rl379;

BB11_121:
	add.s32 	%r93, %r569, 1;
	and.b32  	%r355, %r93, 1;
	setp.eq.s32 	%p97, %r355, 0;
	mul.rn.f64 	%fd38, %fd917, %fd917;
	@%p97 bra 	BB11_123;

	mov.f64 	%fd269, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd271, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd268, %fd269, %fd38, %fd271;
	// inline asm
	mov.f64 	%fd275, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd272, %fd268, %fd38, %fd275;
	// inline asm
	mov.f64 	%fd279, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd276, %fd272, %fd38, %fd279;
	// inline asm
	mov.f64 	%fd283, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd280, %fd276, %fd38, %fd283;
	// inline asm
	mov.f64 	%fd287, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd284, %fd280, %fd38, %fd287;
	// inline asm
	mov.f64 	%fd291, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd288, %fd284, %fd38, %fd291;
	// inline asm
	mov.f64 	%fd295, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd292, %fd288, %fd38, %fd295;
	// inline asm
	mov.f64 	%fd918, %fd292;
	bra.uni 	BB11_124;

BB11_123:
	mov.f64 	%fd297, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd299, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd296, %fd297, %fd38, %fd299;
	// inline asm
	mov.f64 	%fd303, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd300, %fd296, %fd38, %fd303;
	// inline asm
	mov.f64 	%fd307, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd304, %fd300, %fd38, %fd307;
	// inline asm
	mov.f64 	%fd311, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd308, %fd304, %fd38, %fd311;
	// inline asm
	mov.f64 	%fd315, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd312, %fd308, %fd38, %fd315;
	// inline asm
	mul.rn.f64 	%fd317, %fd312, %fd38;
	// inline asm
	fma.rn.f64 	%fd316, %fd317, %fd917, %fd917;
	// inline asm
	mov.f64 	%fd918, %fd316;

BB11_124:
	and.b32  	%r356, %r93, 2;
	setp.eq.s32 	%p98, %r356, 0;
	neg.f64 	%fd320, %fd918;
	selp.f64 	%fd919, %fd918, %fd320, %p98;
	bra.uni 	BB11_126;

BB11_125:
	mov.f64 	%fd919, 0dFFF8000000000000;

BB11_126:
	setp.eq.f64 	%p99, %fd34, 0d0000000000000000;
	or.pred  	%p100, %p11, %p99;
	or.pred  	%p101, %p10, %p100;
	@%p101 bra 	BB11_149;

	// inline asm
	abs.f64 	%fd322, %fd34;
	// inline asm
	setp.gt.f64 	%p102, %fd322, 0d41E0000000000000;
	@%p102 bra 	BB11_129;

	mov.f64 	%fd337, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd324, %fd34, %fd337;
	// inline asm
	cvt.rni.s32.f64 	%r357, %fd324;
	// inline asm
	cvt.rn.f64.s32 	%fd338, %r357;
	neg.f64 	%fd334, %fd338;
	mov.f64 	%fd327, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd325, %fd334, %fd327, %fd34;
	// inline asm
	mov.f64 	%fd331, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd329, %fd334, %fd331, %fd325;
	// inline asm
	mov.f64 	%fd335, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd333, %fd334, %fd335, %fd329;
	// inline asm
	mov.u32 	%r572, %r357;
	mov.f64 	%fd920, %fd333;
	bra.uni 	BB11_145;

BB11_129:
	mov.b64 	 %rl97, %fd34;
	and.b64  	%rl618, %rl97, -9223372036854775808;
	shr.u64 	%rl99, %rl97, 52;
	and.b64  	%rl380, %rl99, 2047;
	add.s64 	%rl381, %rl380, 4294966272;
	cvt.u32.u64 	%r95, %rl381;
	shl.b64 	%rl382, %rl97, 11;
	or.b64  	%rl100, %rl382, -9223372036854775808;
	shr.u32 	%r361, %r95, 6;
	mov.u32 	%r362, 16;
	sub.s32 	%r96, %r362, %r361;
	mov.u32 	%r363, 15;
	sub.s32 	%r570, %r363, %r361;
	mov.u32 	%r364, 19;
	sub.s32 	%r98, %r364, %r361;
	mov.u32 	%r359, 18;
	// inline asm
	min.s32 	%r358, %r359, %r98;
	// inline asm
	setp.lt.s32 	%p103, %r570, %r358;
	@%p103 bra 	BB11_131;

	mov.u64 	%rl615, 0;
	bra.uni 	BB11_133;

BB11_131:
	mov.u32 	%r365, 1;
	sub.s32 	%r99, %r365, %r96;
	mov.u64 	%rl615, 0;

BB11_132:
	.pragma "nounroll";
	shl.b32 	%r369, %r570, 3;
	mov.u32 	%r370, __internal_i2opi_d;
	add.s32 	%r371, %r370, %r369;
	ld.const.u64 	%rl386, [%r371];
	mul.lo.s64 	%rl388, %rl386, %rl100;
	// inline asm
	mul.hi.u64 	%rl385, %rl386, %rl100;
	// inline asm
	mad.lo.s64 	%rl389, %rl386, %rl100, %rl615;
	setp.lt.u64 	%p104, %rl389, %rl388;
	selp.u64 	%rl390, 1, 0, %p104;
	add.s64 	%rl615, %rl390, %rl385;
	add.s32 	%r372, %r99, %r570;
	shl.b32 	%r373, %r372, 3;
	add.s32 	%r375, %r43, %r373;
	st.local.u64 	[%r375], %rl389;
	// inline asm
	min.s32 	%r366, %r359, %r98;
	// inline asm
	add.s32 	%r570, %r570, 1;
	setp.lt.s32 	%p105, %r570, %r366;
	@%p105 bra 	BB11_132;

BB11_133:
	mov.u32 	%r376, 1;
	sub.s32 	%r377, %r376, %r96;
	add.s32 	%r378, %r377, %r570;
	shl.b32 	%r379, %r378, 3;
	add.s32 	%r381, %r43, %r379;
	st.local.u64 	[%r381], %rl615;
	ld.local.u64 	%rl616, [%r43+24];
	ld.local.u64 	%rl617, [%r43+16];
	and.b32  	%r382, %r95, 63;
	setp.eq.s32 	%p106, %r382, 0;
	@%p106 bra 	BB11_135;

	and.b64  	%rl391, %rl99, 63;
	cvt.u32.u64 	%r383, %rl391;
	shl.b64 	%rl392, %rl616, %r383;
	neg.s32 	%r384, %r95;
	and.b32  	%r385, %r384, 63;
	shr.u64 	%rl393, %rl617, %r385;
	or.b64  	%rl616, %rl393, %rl392;
	shl.b64 	%rl394, %rl617, %r383;
	ld.local.u64 	%rl395, [%r43+8];
	shr.u64 	%rl396, %rl395, %r385;
	or.b64  	%rl617, %rl396, %rl394;

BB11_135:
	shr.u64 	%rl397, %rl616, 62;
	cvt.u32.u64 	%r386, %rl397;
	shr.u64 	%rl398, %rl617, 62;
	shl.b64 	%rl399, %rl616, 2;
	or.b64  	%rl622, %rl398, %rl399;
	shl.b64 	%rl111, %rl617, 2;
	setp.ne.s64 	%p107, %rl111, 0;
	selp.u64 	%rl400, 1, 0, %p107;
	or.b64  	%rl401, %rl400, %rl622;
	setp.gt.u64 	%p108, %rl401, -9223372036854775808;
	selp.u32 	%r387, 1, 0, %p108;
	add.s32 	%r388, %r387, %r386;
	neg.s32 	%r389, %r388;
	setp.lt.s64 	%p109, %rl97, 0;
	selp.b32 	%r572, %r389, %r388, %p109;
	@%p108 bra 	BB11_137;

	mov.u64 	%rl621, %rl111;
	bra.uni 	BB11_138;

BB11_137:
	not.b64 	%rl402, %rl622;
	neg.s64 	%rl112, %rl111;
	setp.eq.s64 	%p110, %rl111, 0;
	selp.u64 	%rl403, 1, 0, %p110;
	add.s64 	%rl622, %rl403, %rl402;
	xor.b64  	%rl618, %rl618, -9223372036854775808;
	mov.u64 	%rl621, %rl112;

BB11_138:
	mov.u64 	%rl620, %rl621;
	setp.gt.s64 	%p111, %rl622, 0;
	@%p111 bra 	BB11_140;

	mov.u32 	%r571, 0;
	bra.uni 	BB11_142;

BB11_140:
	mov.u32 	%r571, 0;

BB11_141:
	shr.u64 	%rl404, %rl620, 63;
	shl.b64 	%rl405, %rl622, 1;
	or.b64  	%rl622, %rl404, %rl405;
	shl.b64 	%rl620, %rl620, 1;
	add.s32 	%r571, %r571, -1;
	setp.gt.s64 	%p112, %rl622, 0;
	@%p112 bra 	BB11_141;

BB11_142:
	mul.lo.s64 	%rl624, %rl622, -3958705157555305931;
	mov.u64 	%rl408, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl406, %rl622, %rl408;
	// inline asm
	setp.gt.s64 	%p113, %rl406, 0;
	mov.u64 	%rl623, %rl406;
	@%p113 bra 	BB11_143;
	bra.uni 	BB11_144;

BB11_143:
	shl.b64 	%rl409, %rl406, 1;
	shr.u64 	%rl410, %rl624, 63;
	or.b64  	%rl623, %rl409, %rl410;
	mul.lo.s64 	%rl624, %rl622, -7917410315110611862;
	add.s32 	%r571, %r571, -1;

BB11_144:
	setp.ne.s64 	%p114, %rl624, 0;
	selp.u64 	%rl411, 1, 0, %p114;
	add.s64 	%rl412, %rl411, %rl623;
	add.s32 	%r392, %r571, 1022;
	cvt.u64.u32 	%rl413, %r392;
	shl.b64 	%rl414, %rl413, 52;
	shr.u64 	%rl415, %rl412, 11;
	shr.u64 	%rl416, %rl412, 10;
	and.b64  	%rl417, %rl416, 1;
	add.s64 	%rl418, %rl414, %rl415;
	add.s64 	%rl419, %rl418, %rl417;
	or.b64  	%rl420, %rl419, %rl618;
	mov.b64 	 %fd920, %rl420;

BB11_145:
	and.b32  	%r393, %r572, 1;
	setp.eq.s32 	%p115, %r393, 0;
	mul.rn.f64 	%fd48, %fd920, %fd920;
	@%p115 bra 	BB11_147;

	mov.f64 	%fd340, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd342, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd339, %fd340, %fd48, %fd342;
	// inline asm
	mov.f64 	%fd346, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd343, %fd339, %fd48, %fd346;
	// inline asm
	mov.f64 	%fd350, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd347, %fd343, %fd48, %fd350;
	// inline asm
	mov.f64 	%fd354, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd351, %fd347, %fd48, %fd354;
	// inline asm
	mov.f64 	%fd358, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd355, %fd351, %fd48, %fd358;
	// inline asm
	mov.f64 	%fd362, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd359, %fd355, %fd48, %fd362;
	// inline asm
	mov.f64 	%fd366, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd363, %fd359, %fd48, %fd366;
	// inline asm
	mov.f64 	%fd921, %fd363;
	bra.uni 	BB11_148;

BB11_147:
	mov.f64 	%fd368, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd370, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd367, %fd368, %fd48, %fd370;
	// inline asm
	mov.f64 	%fd374, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd371, %fd367, %fd48, %fd374;
	// inline asm
	mov.f64 	%fd378, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd375, %fd371, %fd48, %fd378;
	// inline asm
	mov.f64 	%fd382, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd379, %fd375, %fd48, %fd382;
	// inline asm
	mov.f64 	%fd386, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd383, %fd379, %fd48, %fd386;
	// inline asm
	mul.rn.f64 	%fd388, %fd383, %fd48;
	// inline asm
	fma.rn.f64 	%fd387, %fd388, %fd920, %fd920;
	// inline asm
	mov.f64 	%fd921, %fd387;

BB11_148:
	and.b32  	%r394, %r572, 2;
	setp.eq.s32 	%p116, %r394, 0;
	neg.f64 	%fd391, %fd921;
	selp.f64 	%fd922, %fd921, %fd391, %p116;
	bra.uni 	BB11_150;

BB11_149:
	mov.f64 	%fd392, 0d0000000000000000;
	mul.rn.f64 	%fd922, %fd34, %fd392;

BB11_150:
	neg.f64 	%fd393, %fd922;
	mov.f64 	%fd949, %fd919;
	mov.f64 	%fd950, %fd393;
	@%p9 bra 	BB11_151;
	bra.uni 	BB11_152;

BB11_151:
	mov.f64 	%fd949, %fd919;
	mov.f64 	%fd950, %fd922;

BB11_152:
	mul.f64 	%fd395, %fd3, %fd949;
	neg.f64 	%fd397, %fd4;
	fma.rn.f64 	%fd398, %fd397, %fd950, %fd395;
	mul.f64 	%fd399, %fd4, %fd949;
	fma.rn.f64 	%fd400, %fd3, %fd950, %fd399;
	mul.f64 	%fd401, %fd9, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd55, %fd401, %fd10;
	setp.eq.f64 	%p12, %fd55, %fd12;
	setp.eq.f64 	%p13, %fd55, %fd13;
	or.pred  	%p117, %p12, %p13;
	@%p117 bra 	BB11_175;

	// inline asm
	abs.f64 	%fd402, %fd55;
	// inline asm
	setp.gt.f64 	%p118, %fd402, 0d41E0000000000000;
	@%p118 bra 	BB11_155;

	mov.f64 	%fd417, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd404, %fd55, %fd417;
	// inline asm
	cvt.rni.s32.f64 	%r395, %fd404;
	// inline asm
	cvt.rn.f64.s32 	%fd418, %r395;
	neg.f64 	%fd414, %fd418;
	mov.f64 	%fd407, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd405, %fd414, %fd407, %fd55;
	// inline asm
	mov.f64 	%fd411, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd409, %fd414, %fd411, %fd405;
	// inline asm
	mov.f64 	%fd415, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd413, %fd414, %fd415, %fd409;
	// inline asm
	mov.u32 	%r575, %r395;
	mov.f64 	%fd923, %fd413;
	bra.uni 	BB11_171;

BB11_155:
	mov.b64 	 %rl129, %fd55;
	and.b64  	%rl628, %rl129, -9223372036854775808;
	shr.u64 	%rl131, %rl129, 52;
	and.b64  	%rl421, %rl131, 2047;
	add.s64 	%rl422, %rl421, 4294966272;
	cvt.u32.u64 	%r111, %rl422;
	shl.b64 	%rl423, %rl129, 11;
	or.b64  	%rl132, %rl423, -9223372036854775808;
	shr.u32 	%r399, %r111, 6;
	mov.u32 	%r400, 16;
	sub.s32 	%r112, %r400, %r399;
	mov.u32 	%r401, 15;
	sub.s32 	%r573, %r401, %r399;
	mov.u32 	%r402, 19;
	sub.s32 	%r114, %r402, %r399;
	mov.u32 	%r397, 18;
	// inline asm
	min.s32 	%r396, %r397, %r114;
	// inline asm
	setp.lt.s32 	%p119, %r573, %r396;
	@%p119 bra 	BB11_157;

	mov.u64 	%rl625, 0;
	bra.uni 	BB11_159;

BB11_157:
	mov.u32 	%r403, 1;
	sub.s32 	%r115, %r403, %r112;
	mov.u64 	%rl625, 0;

BB11_158:
	.pragma "nounroll";
	shl.b32 	%r407, %r573, 3;
	mov.u32 	%r408, __internal_i2opi_d;
	add.s32 	%r409, %r408, %r407;
	ld.const.u64 	%rl427, [%r409];
	mul.lo.s64 	%rl429, %rl427, %rl132;
	// inline asm
	mul.hi.u64 	%rl426, %rl427, %rl132;
	// inline asm
	mad.lo.s64 	%rl430, %rl427, %rl132, %rl625;
	setp.lt.u64 	%p120, %rl430, %rl429;
	selp.u64 	%rl431, 1, 0, %p120;
	add.s64 	%rl625, %rl431, %rl426;
	add.s32 	%r410, %r115, %r573;
	shl.b32 	%r411, %r410, 3;
	add.s32 	%r413, %r43, %r411;
	st.local.u64 	[%r413], %rl430;
	// inline asm
	min.s32 	%r404, %r397, %r114;
	// inline asm
	add.s32 	%r573, %r573, 1;
	setp.lt.s32 	%p121, %r573, %r404;
	@%p121 bra 	BB11_158;

BB11_159:
	mov.u32 	%r414, 1;
	sub.s32 	%r415, %r414, %r112;
	add.s32 	%r416, %r415, %r573;
	shl.b32 	%r417, %r416, 3;
	add.s32 	%r419, %r43, %r417;
	st.local.u64 	[%r419], %rl625;
	ld.local.u64 	%rl626, [%r43+24];
	ld.local.u64 	%rl627, [%r43+16];
	and.b32  	%r420, %r111, 63;
	setp.eq.s32 	%p122, %r420, 0;
	@%p122 bra 	BB11_161;

	and.b64  	%rl432, %rl131, 63;
	cvt.u32.u64 	%r421, %rl432;
	shl.b64 	%rl433, %rl626, %r421;
	neg.s32 	%r422, %r111;
	and.b32  	%r423, %r422, 63;
	shr.u64 	%rl434, %rl627, %r423;
	or.b64  	%rl626, %rl434, %rl433;
	shl.b64 	%rl435, %rl627, %r421;
	ld.local.u64 	%rl436, [%r43+8];
	shr.u64 	%rl437, %rl436, %r423;
	or.b64  	%rl627, %rl437, %rl435;

BB11_161:
	shr.u64 	%rl438, %rl626, 62;
	cvt.u32.u64 	%r424, %rl438;
	shr.u64 	%rl439, %rl627, 62;
	shl.b64 	%rl440, %rl626, 2;
	or.b64  	%rl632, %rl439, %rl440;
	shl.b64 	%rl143, %rl627, 2;
	setp.ne.s64 	%p123, %rl143, 0;
	selp.u64 	%rl441, 1, 0, %p123;
	or.b64  	%rl442, %rl441, %rl632;
	setp.gt.u64 	%p124, %rl442, -9223372036854775808;
	selp.u32 	%r425, 1, 0, %p124;
	add.s32 	%r426, %r425, %r424;
	neg.s32 	%r427, %r426;
	setp.lt.s64 	%p125, %rl129, 0;
	selp.b32 	%r575, %r427, %r426, %p125;
	@%p124 bra 	BB11_163;

	mov.u64 	%rl631, %rl143;
	bra.uni 	BB11_164;

BB11_163:
	not.b64 	%rl443, %rl632;
	neg.s64 	%rl144, %rl143;
	setp.eq.s64 	%p126, %rl143, 0;
	selp.u64 	%rl444, 1, 0, %p126;
	add.s64 	%rl632, %rl444, %rl443;
	xor.b64  	%rl628, %rl628, -9223372036854775808;
	mov.u64 	%rl631, %rl144;

BB11_164:
	mov.u64 	%rl630, %rl631;
	setp.gt.s64 	%p127, %rl632, 0;
	@%p127 bra 	BB11_166;

	mov.u32 	%r574, 0;
	bra.uni 	BB11_168;

BB11_166:
	mov.u32 	%r574, 0;

BB11_167:
	shr.u64 	%rl445, %rl630, 63;
	shl.b64 	%rl446, %rl632, 1;
	or.b64  	%rl632, %rl445, %rl446;
	shl.b64 	%rl630, %rl630, 1;
	add.s32 	%r574, %r574, -1;
	setp.gt.s64 	%p128, %rl632, 0;
	@%p128 bra 	BB11_167;

BB11_168:
	mul.lo.s64 	%rl634, %rl632, -3958705157555305931;
	mov.u64 	%rl449, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl447, %rl632, %rl449;
	// inline asm
	setp.gt.s64 	%p129, %rl447, 0;
	mov.u64 	%rl633, %rl447;
	@%p129 bra 	BB11_169;
	bra.uni 	BB11_170;

BB11_169:
	shl.b64 	%rl450, %rl447, 1;
	shr.u64 	%rl451, %rl634, 63;
	or.b64  	%rl633, %rl450, %rl451;
	mul.lo.s64 	%rl634, %rl632, -7917410315110611862;
	add.s32 	%r574, %r574, -1;

BB11_170:
	setp.ne.s64 	%p130, %rl634, 0;
	selp.u64 	%rl452, 1, 0, %p130;
	add.s64 	%rl453, %rl452, %rl633;
	add.s32 	%r430, %r574, 1022;
	cvt.u64.u32 	%rl454, %r430;
	shl.b64 	%rl455, %rl454, 52;
	shr.u64 	%rl456, %rl453, 11;
	shr.u64 	%rl457, %rl453, 10;
	and.b64  	%rl458, %rl457, 1;
	add.s64 	%rl459, %rl455, %rl456;
	add.s64 	%rl460, %rl459, %rl458;
	or.b64  	%rl461, %rl460, %rl628;
	mov.b64 	 %fd923, %rl461;

BB11_171:
	add.s32 	%r126, %r575, 1;
	and.b32  	%r431, %r126, 1;
	setp.eq.s32 	%p131, %r431, 0;
	mul.rn.f64 	%fd59, %fd923, %fd923;
	@%p131 bra 	BB11_173;

	mov.f64 	%fd420, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd422, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd419, %fd420, %fd59, %fd422;
	// inline asm
	mov.f64 	%fd426, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd423, %fd419, %fd59, %fd426;
	// inline asm
	mov.f64 	%fd430, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd427, %fd423, %fd59, %fd430;
	// inline asm
	mov.f64 	%fd434, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd431, %fd427, %fd59, %fd434;
	// inline asm
	mov.f64 	%fd438, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd435, %fd431, %fd59, %fd438;
	// inline asm
	mov.f64 	%fd442, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd439, %fd435, %fd59, %fd442;
	// inline asm
	mov.f64 	%fd446, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd443, %fd439, %fd59, %fd446;
	// inline asm
	mov.f64 	%fd924, %fd443;
	bra.uni 	BB11_174;

BB11_173:
	mov.f64 	%fd448, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd450, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd447, %fd448, %fd59, %fd450;
	// inline asm
	mov.f64 	%fd454, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd451, %fd447, %fd59, %fd454;
	// inline asm
	mov.f64 	%fd458, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd455, %fd451, %fd59, %fd458;
	// inline asm
	mov.f64 	%fd462, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd459, %fd455, %fd59, %fd462;
	// inline asm
	mov.f64 	%fd466, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd463, %fd459, %fd59, %fd466;
	// inline asm
	mul.rn.f64 	%fd468, %fd463, %fd59;
	// inline asm
	fma.rn.f64 	%fd467, %fd468, %fd923, %fd923;
	// inline asm
	mov.f64 	%fd924, %fd467;

BB11_174:
	and.b32  	%r432, %r126, 2;
	setp.eq.s32 	%p132, %r432, 0;
	neg.f64 	%fd471, %fd924;
	selp.f64 	%fd925, %fd924, %fd471, %p132;
	bra.uni 	BB11_176;

BB11_175:
	mov.f64 	%fd925, 0dFFF8000000000000;

BB11_176:
	setp.eq.f64 	%p133, %fd55, 0d0000000000000000;
	or.pred  	%p134, %p13, %p133;
	or.pred  	%p135, %p12, %p134;
	@%p135 bra 	BB11_199;

	// inline asm
	abs.f64 	%fd473, %fd55;
	// inline asm
	setp.gt.f64 	%p136, %fd473, 0d41E0000000000000;
	@%p136 bra 	BB11_179;

	mov.f64 	%fd488, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd475, %fd55, %fd488;
	// inline asm
	cvt.rni.s32.f64 	%r433, %fd475;
	// inline asm
	cvt.rn.f64.s32 	%fd489, %r433;
	neg.f64 	%fd485, %fd489;
	mov.f64 	%fd478, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd476, %fd485, %fd478, %fd55;
	// inline asm
	mov.f64 	%fd482, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd480, %fd485, %fd482, %fd476;
	// inline asm
	mov.f64 	%fd486, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd484, %fd485, %fd486, %fd480;
	// inline asm
	mov.u32 	%r578, %r433;
	mov.f64 	%fd926, %fd484;
	bra.uni 	BB11_195;

BB11_179:
	mov.b64 	 %rl161, %fd55;
	and.b64  	%rl638, %rl161, -9223372036854775808;
	shr.u64 	%rl163, %rl161, 52;
	and.b64  	%rl462, %rl163, 2047;
	add.s64 	%rl463, %rl462, 4294966272;
	cvt.u32.u64 	%r128, %rl463;
	shl.b64 	%rl464, %rl161, 11;
	or.b64  	%rl164, %rl464, -9223372036854775808;
	shr.u32 	%r437, %r128, 6;
	mov.u32 	%r438, 16;
	sub.s32 	%r129, %r438, %r437;
	mov.u32 	%r439, 15;
	sub.s32 	%r576, %r439, %r437;
	mov.u32 	%r440, 19;
	sub.s32 	%r131, %r440, %r437;
	mov.u32 	%r435, 18;
	// inline asm
	min.s32 	%r434, %r435, %r131;
	// inline asm
	setp.lt.s32 	%p137, %r576, %r434;
	@%p137 bra 	BB11_181;

	mov.u64 	%rl635, 0;
	bra.uni 	BB11_183;

BB11_181:
	mov.u32 	%r441, 1;
	sub.s32 	%r132, %r441, %r129;
	mov.u64 	%rl635, 0;

BB11_182:
	.pragma "nounroll";
	shl.b32 	%r445, %r576, 3;
	mov.u32 	%r446, __internal_i2opi_d;
	add.s32 	%r447, %r446, %r445;
	ld.const.u64 	%rl468, [%r447];
	mul.lo.s64 	%rl470, %rl468, %rl164;
	// inline asm
	mul.hi.u64 	%rl467, %rl468, %rl164;
	// inline asm
	mad.lo.s64 	%rl471, %rl468, %rl164, %rl635;
	setp.lt.u64 	%p138, %rl471, %rl470;
	selp.u64 	%rl472, 1, 0, %p138;
	add.s64 	%rl635, %rl472, %rl467;
	add.s32 	%r448, %r132, %r576;
	shl.b32 	%r449, %r448, 3;
	add.s32 	%r451, %r43, %r449;
	st.local.u64 	[%r451], %rl471;
	// inline asm
	min.s32 	%r442, %r435, %r131;
	// inline asm
	add.s32 	%r576, %r576, 1;
	setp.lt.s32 	%p139, %r576, %r442;
	@%p139 bra 	BB11_182;

BB11_183:
	mov.u32 	%r452, 1;
	sub.s32 	%r453, %r452, %r129;
	add.s32 	%r454, %r453, %r576;
	shl.b32 	%r455, %r454, 3;
	add.s32 	%r457, %r43, %r455;
	st.local.u64 	[%r457], %rl635;
	ld.local.u64 	%rl636, [%r43+24];
	ld.local.u64 	%rl637, [%r43+16];
	and.b32  	%r458, %r128, 63;
	setp.eq.s32 	%p140, %r458, 0;
	@%p140 bra 	BB11_185;

	and.b64  	%rl473, %rl163, 63;
	cvt.u32.u64 	%r459, %rl473;
	shl.b64 	%rl474, %rl636, %r459;
	neg.s32 	%r460, %r128;
	and.b32  	%r461, %r460, 63;
	shr.u64 	%rl475, %rl637, %r461;
	or.b64  	%rl636, %rl475, %rl474;
	shl.b64 	%rl476, %rl637, %r459;
	ld.local.u64 	%rl477, [%r43+8];
	shr.u64 	%rl478, %rl477, %r461;
	or.b64  	%rl637, %rl478, %rl476;

BB11_185:
	shr.u64 	%rl479, %rl636, 62;
	cvt.u32.u64 	%r462, %rl479;
	shr.u64 	%rl480, %rl637, 62;
	shl.b64 	%rl481, %rl636, 2;
	or.b64  	%rl642, %rl480, %rl481;
	shl.b64 	%rl175, %rl637, 2;
	setp.ne.s64 	%p141, %rl175, 0;
	selp.u64 	%rl482, 1, 0, %p141;
	or.b64  	%rl483, %rl482, %rl642;
	setp.gt.u64 	%p142, %rl483, -9223372036854775808;
	selp.u32 	%r463, 1, 0, %p142;
	add.s32 	%r464, %r463, %r462;
	neg.s32 	%r465, %r464;
	setp.lt.s64 	%p143, %rl161, 0;
	selp.b32 	%r578, %r465, %r464, %p143;
	@%p142 bra 	BB11_187;

	mov.u64 	%rl641, %rl175;
	bra.uni 	BB11_188;

BB11_187:
	not.b64 	%rl484, %rl642;
	neg.s64 	%rl176, %rl175;
	setp.eq.s64 	%p144, %rl175, 0;
	selp.u64 	%rl485, 1, 0, %p144;
	add.s64 	%rl642, %rl485, %rl484;
	xor.b64  	%rl638, %rl638, -9223372036854775808;
	mov.u64 	%rl641, %rl176;

BB11_188:
	mov.u64 	%rl640, %rl641;
	setp.gt.s64 	%p145, %rl642, 0;
	@%p145 bra 	BB11_190;

	mov.u32 	%r577, 0;
	bra.uni 	BB11_192;

BB11_190:
	mov.u32 	%r577, 0;

BB11_191:
	shr.u64 	%rl486, %rl640, 63;
	shl.b64 	%rl487, %rl642, 1;
	or.b64  	%rl642, %rl486, %rl487;
	shl.b64 	%rl640, %rl640, 1;
	add.s32 	%r577, %r577, -1;
	setp.gt.s64 	%p146, %rl642, 0;
	@%p146 bra 	BB11_191;

BB11_192:
	mul.lo.s64 	%rl644, %rl642, -3958705157555305931;
	mov.u64 	%rl490, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl488, %rl642, %rl490;
	// inline asm
	setp.gt.s64 	%p147, %rl488, 0;
	mov.u64 	%rl643, %rl488;
	@%p147 bra 	BB11_193;
	bra.uni 	BB11_194;

BB11_193:
	shl.b64 	%rl491, %rl488, 1;
	shr.u64 	%rl492, %rl644, 63;
	or.b64  	%rl643, %rl491, %rl492;
	mul.lo.s64 	%rl644, %rl642, -7917410315110611862;
	add.s32 	%r577, %r577, -1;

BB11_194:
	setp.ne.s64 	%p148, %rl644, 0;
	selp.u64 	%rl493, 1, 0, %p148;
	add.s64 	%rl494, %rl493, %rl643;
	add.s32 	%r468, %r577, 1022;
	cvt.u64.u32 	%rl495, %r468;
	shl.b64 	%rl496, %rl495, 52;
	shr.u64 	%rl497, %rl494, 11;
	shr.u64 	%rl498, %rl494, 10;
	and.b64  	%rl499, %rl498, 1;
	add.s64 	%rl500, %rl496, %rl497;
	add.s64 	%rl501, %rl500, %rl499;
	or.b64  	%rl502, %rl501, %rl638;
	mov.b64 	 %fd926, %rl502;

BB11_195:
	and.b32  	%r469, %r578, 1;
	setp.eq.s32 	%p149, %r469, 0;
	mul.rn.f64 	%fd69, %fd926, %fd926;
	@%p149 bra 	BB11_197;

	mov.f64 	%fd491, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd493, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd490, %fd491, %fd69, %fd493;
	// inline asm
	mov.f64 	%fd497, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd494, %fd490, %fd69, %fd497;
	// inline asm
	mov.f64 	%fd501, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd498, %fd494, %fd69, %fd501;
	// inline asm
	mov.f64 	%fd505, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd502, %fd498, %fd69, %fd505;
	// inline asm
	mov.f64 	%fd509, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd506, %fd502, %fd69, %fd509;
	// inline asm
	mov.f64 	%fd513, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd510, %fd506, %fd69, %fd513;
	// inline asm
	mov.f64 	%fd517, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd514, %fd510, %fd69, %fd517;
	// inline asm
	mov.f64 	%fd927, %fd514;
	bra.uni 	BB11_198;

BB11_197:
	mov.f64 	%fd519, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd521, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd518, %fd519, %fd69, %fd521;
	// inline asm
	mov.f64 	%fd525, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd522, %fd518, %fd69, %fd525;
	// inline asm
	mov.f64 	%fd529, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd526, %fd522, %fd69, %fd529;
	// inline asm
	mov.f64 	%fd533, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd530, %fd526, %fd69, %fd533;
	// inline asm
	mov.f64 	%fd537, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd534, %fd530, %fd69, %fd537;
	// inline asm
	mul.rn.f64 	%fd539, %fd534, %fd69;
	// inline asm
	fma.rn.f64 	%fd538, %fd539, %fd926, %fd926;
	// inline asm
	mov.f64 	%fd927, %fd538;

BB11_198:
	and.b32  	%r470, %r578, 2;
	setp.eq.s32 	%p150, %r470, 0;
	neg.f64 	%fd542, %fd927;
	selp.f64 	%fd928, %fd927, %fd542, %p150;
	bra.uni 	BB11_200;

BB11_199:
	mov.f64 	%fd543, 0d0000000000000000;
	mul.rn.f64 	%fd928, %fd55, %fd543;

BB11_200:
	neg.f64 	%fd544, %fd928;
	mov.f64 	%fd947, %fd925;
	mov.f64 	%fd948, %fd544;
	@%p9 bra 	BB11_201;
	bra.uni 	BB11_202;

BB11_201:
	mov.f64 	%fd947, %fd925;
	mov.f64 	%fd948, %fd928;

BB11_202:
	mul.f64 	%fd546, %fd5, %fd947;
	neg.f64 	%fd548, %fd6;
	fma.rn.f64 	%fd549, %fd548, %fd948, %fd546;
	mul.f64 	%fd550, %fd6, %fd947;
	fma.rn.f64 	%fd551, %fd5, %fd948, %fd550;
	mul.f64 	%fd552, %fd9, 0d403921FB54442D18;
	div.rn.f64 	%fd76, %fd552, %fd10;
	setp.eq.f64 	%p14, %fd76, %fd12;
	setp.eq.f64 	%p15, %fd76, %fd13;
	or.pred  	%p151, %p14, %p15;
	mov.f64 	%fd935, %fd97;
	mov.f64 	%fd936, %fd98;
	mov.f64 	%fd937, %fd247;
	mov.f64 	%fd938, %fd249;
	mov.f64 	%fd939, %fd398;
	mov.f64 	%fd940, %fd400;
	mov.f64 	%fd941, %fd549;
	mov.f64 	%fd942, %fd551;
	@%p151 bra 	BB11_225;

	// inline asm
	abs.f64 	%fd553, %fd76;
	// inline asm
	setp.gt.f64 	%p152, %fd553, 0d41E0000000000000;
	@%p152 bra 	BB11_205;

	mov.f64 	%fd568, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd555, %fd76, %fd568;
	// inline asm
	cvt.rni.s32.f64 	%r471, %fd555;
	// inline asm
	cvt.rn.f64.s32 	%fd569, %r471;
	neg.f64 	%fd565, %fd569;
	mov.f64 	%fd558, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd556, %fd565, %fd558, %fd76;
	// inline asm
	mov.f64 	%fd562, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd560, %fd565, %fd562, %fd556;
	// inline asm
	mov.f64 	%fd566, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd564, %fd565, %fd566, %fd560;
	// inline asm
	mov.u32 	%r581, %r471;
	mov.f64 	%fd929, %fd564;
	bra.uni 	BB11_221;

BB11_205:
	mov.b64 	 %rl193, %fd76;
	and.b64  	%rl648, %rl193, -9223372036854775808;
	shr.u64 	%rl195, %rl193, 52;
	and.b64  	%rl503, %rl195, 2047;
	add.s64 	%rl504, %rl503, 4294966272;
	cvt.u32.u64 	%r144, %rl504;
	shl.b64 	%rl505, %rl193, 11;
	or.b64  	%rl196, %rl505, -9223372036854775808;
	shr.u32 	%r475, %r144, 6;
	mov.u32 	%r476, 16;
	sub.s32 	%r145, %r476, %r475;
	mov.u32 	%r477, 15;
	sub.s32 	%r579, %r477, %r475;
	mov.u32 	%r478, 19;
	sub.s32 	%r147, %r478, %r475;
	mov.u32 	%r473, 18;
	// inline asm
	min.s32 	%r472, %r473, %r147;
	// inline asm
	setp.lt.s32 	%p153, %r579, %r472;
	@%p153 bra 	BB11_207;

	mov.u64 	%rl645, 0;
	bra.uni 	BB11_209;

BB11_207:
	mov.u32 	%r479, 1;
	sub.s32 	%r148, %r479, %r145;
	mov.u64 	%rl645, 0;

BB11_208:
	.pragma "nounroll";
	shl.b32 	%r483, %r579, 3;
	mov.u32 	%r484, __internal_i2opi_d;
	add.s32 	%r485, %r484, %r483;
	ld.const.u64 	%rl509, [%r485];
	mul.lo.s64 	%rl511, %rl509, %rl196;
	// inline asm
	mul.hi.u64 	%rl508, %rl509, %rl196;
	// inline asm
	mad.lo.s64 	%rl512, %rl509, %rl196, %rl645;
	setp.lt.u64 	%p154, %rl512, %rl511;
	selp.u64 	%rl513, 1, 0, %p154;
	add.s64 	%rl645, %rl513, %rl508;
	add.s32 	%r486, %r148, %r579;
	shl.b32 	%r487, %r486, 3;
	add.s32 	%r489, %r43, %r487;
	st.local.u64 	[%r489], %rl512;
	// inline asm
	min.s32 	%r480, %r473, %r147;
	// inline asm
	add.s32 	%r579, %r579, 1;
	setp.lt.s32 	%p155, %r579, %r480;
	@%p155 bra 	BB11_208;

BB11_209:
	mov.u32 	%r490, 1;
	sub.s32 	%r491, %r490, %r145;
	add.s32 	%r492, %r491, %r579;
	shl.b32 	%r493, %r492, 3;
	add.s32 	%r495, %r43, %r493;
	st.local.u64 	[%r495], %rl645;
	ld.local.u64 	%rl646, [%r43+24];
	ld.local.u64 	%rl647, [%r43+16];
	and.b32  	%r496, %r144, 63;
	setp.eq.s32 	%p156, %r496, 0;
	@%p156 bra 	BB11_211;

	and.b64  	%rl514, %rl195, 63;
	cvt.u32.u64 	%r497, %rl514;
	shl.b64 	%rl515, %rl646, %r497;
	neg.s32 	%r498, %r144;
	and.b32  	%r499, %r498, 63;
	shr.u64 	%rl516, %rl647, %r499;
	or.b64  	%rl646, %rl516, %rl515;
	shl.b64 	%rl517, %rl647, %r497;
	ld.local.u64 	%rl518, [%r43+8];
	shr.u64 	%rl519, %rl518, %r499;
	or.b64  	%rl647, %rl519, %rl517;

BB11_211:
	shr.u64 	%rl520, %rl646, 62;
	cvt.u32.u64 	%r500, %rl520;
	shr.u64 	%rl521, %rl647, 62;
	shl.b64 	%rl522, %rl646, 2;
	or.b64  	%rl652, %rl521, %rl522;
	shl.b64 	%rl207, %rl647, 2;
	setp.ne.s64 	%p157, %rl207, 0;
	selp.u64 	%rl523, 1, 0, %p157;
	or.b64  	%rl524, %rl523, %rl652;
	setp.gt.u64 	%p158, %rl524, -9223372036854775808;
	selp.u32 	%r501, 1, 0, %p158;
	add.s32 	%r502, %r501, %r500;
	neg.s32 	%r503, %r502;
	setp.lt.s64 	%p159, %rl193, 0;
	selp.b32 	%r581, %r503, %r502, %p159;
	@%p158 bra 	BB11_213;

	mov.u64 	%rl651, %rl207;
	bra.uni 	BB11_214;

BB11_213:
	not.b64 	%rl525, %rl652;
	neg.s64 	%rl208, %rl207;
	setp.eq.s64 	%p160, %rl207, 0;
	selp.u64 	%rl526, 1, 0, %p160;
	add.s64 	%rl652, %rl526, %rl525;
	xor.b64  	%rl648, %rl648, -9223372036854775808;
	mov.u64 	%rl651, %rl208;

BB11_214:
	mov.u64 	%rl650, %rl651;
	setp.gt.s64 	%p161, %rl652, 0;
	@%p161 bra 	BB11_216;

	mov.u32 	%r580, 0;
	bra.uni 	BB11_218;

BB11_216:
	mov.u32 	%r580, 0;

BB11_217:
	shr.u64 	%rl527, %rl650, 63;
	shl.b64 	%rl528, %rl652, 1;
	or.b64  	%rl652, %rl527, %rl528;
	shl.b64 	%rl650, %rl650, 1;
	add.s32 	%r580, %r580, -1;
	setp.gt.s64 	%p162, %rl652, 0;
	@%p162 bra 	BB11_217;

BB11_218:
	mul.lo.s64 	%rl654, %rl652, -3958705157555305931;
	mov.u64 	%rl531, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl529, %rl652, %rl531;
	// inline asm
	setp.gt.s64 	%p163, %rl529, 0;
	mov.u64 	%rl653, %rl529;
	@%p163 bra 	BB11_219;
	bra.uni 	BB11_220;

BB11_219:
	shl.b64 	%rl532, %rl529, 1;
	shr.u64 	%rl533, %rl654, 63;
	or.b64  	%rl653, %rl532, %rl533;
	mul.lo.s64 	%rl654, %rl652, -7917410315110611862;
	add.s32 	%r580, %r580, -1;

BB11_220:
	setp.ne.s64 	%p164, %rl654, 0;
	selp.u64 	%rl534, 1, 0, %p164;
	add.s64 	%rl535, %rl534, %rl653;
	add.s32 	%r506, %r580, 1022;
	cvt.u64.u32 	%rl536, %r506;
	shl.b64 	%rl537, %rl536, 52;
	shr.u64 	%rl538, %rl535, 11;
	shr.u64 	%rl539, %rl535, 10;
	and.b64  	%rl540, %rl539, 1;
	add.s64 	%rl541, %rl537, %rl538;
	add.s64 	%rl542, %rl541, %rl540;
	or.b64  	%rl543, %rl542, %rl648;
	mov.b64 	 %fd929, %rl543;

BB11_221:
	add.s32 	%r159, %r581, 1;
	and.b32  	%r507, %r159, 1;
	setp.eq.s32 	%p165, %r507, 0;
	mul.rn.f64 	%fd80, %fd929, %fd929;
	@%p165 bra 	BB11_223;

	mov.f64 	%fd571, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd573, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd570, %fd571, %fd80, %fd573;
	// inline asm
	mov.f64 	%fd577, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd574, %fd570, %fd80, %fd577;
	// inline asm
	mov.f64 	%fd581, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd578, %fd574, %fd80, %fd581;
	// inline asm
	mov.f64 	%fd585, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd582, %fd578, %fd80, %fd585;
	// inline asm
	mov.f64 	%fd589, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd586, %fd582, %fd80, %fd589;
	// inline asm
	mov.f64 	%fd593, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd590, %fd586, %fd80, %fd593;
	// inline asm
	mov.f64 	%fd597, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd594, %fd590, %fd80, %fd597;
	// inline asm
	mov.f64 	%fd930, %fd594;
	bra.uni 	BB11_224;

BB11_223:
	mov.f64 	%fd599, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd601, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd598, %fd599, %fd80, %fd601;
	// inline asm
	mov.f64 	%fd605, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd602, %fd598, %fd80, %fd605;
	// inline asm
	mov.f64 	%fd609, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd606, %fd602, %fd80, %fd609;
	// inline asm
	mov.f64 	%fd613, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd610, %fd606, %fd80, %fd613;
	// inline asm
	mov.f64 	%fd617, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd614, %fd610, %fd80, %fd617;
	// inline asm
	mul.rn.f64 	%fd619, %fd614, %fd80;
	// inline asm
	fma.rn.f64 	%fd618, %fd619, %fd929, %fd929;
	// inline asm
	mov.f64 	%fd930, %fd618;

BB11_224:
	and.b32  	%r508, %r159, 2;
	setp.eq.s32 	%p166, %r508, 0;
	neg.f64 	%fd622, %fd930;
	selp.f64 	%fd931, %fd930, %fd622, %p166;
	bra.uni 	BB11_226;

BB11_225:
	mov.f64 	%fd931, 0dFFF8000000000000;

BB11_226:
	setp.eq.f64 	%p167, %fd76, 0d0000000000000000;
	or.pred  	%p168, %p15, %p167;
	or.pred  	%p169, %p14, %p168;
	@%p169 bra 	BB11_249;

	// inline asm
	abs.f64 	%fd624, %fd76;
	// inline asm
	setp.gt.f64 	%p170, %fd624, 0d41E0000000000000;
	@%p170 bra 	BB11_229;

	mov.f64 	%fd639, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd626, %fd76, %fd639;
	// inline asm
	cvt.rni.s32.f64 	%r509, %fd626;
	// inline asm
	cvt.rn.f64.s32 	%fd640, %r509;
	neg.f64 	%fd636, %fd640;
	mov.f64 	%fd629, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd627, %fd636, %fd629, %fd76;
	// inline asm
	mov.f64 	%fd633, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd631, %fd636, %fd633, %fd627;
	// inline asm
	mov.f64 	%fd637, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd635, %fd636, %fd637, %fd631;
	// inline asm
	mov.u32 	%r584, %r509;
	mov.f64 	%fd932, %fd635;
	bra.uni 	BB11_245;

BB11_229:
	mov.b64 	 %rl225, %fd76;
	and.b64  	%rl658, %rl225, -9223372036854775808;
	shr.u64 	%rl227, %rl225, 52;
	and.b64  	%rl544, %rl227, 2047;
	add.s64 	%rl545, %rl544, 4294966272;
	cvt.u32.u64 	%r161, %rl545;
	shl.b64 	%rl546, %rl225, 11;
	or.b64  	%rl228, %rl546, -9223372036854775808;
	shr.u32 	%r513, %r161, 6;
	mov.u32 	%r514, 16;
	sub.s32 	%r162, %r514, %r513;
	mov.u32 	%r515, 15;
	sub.s32 	%r582, %r515, %r513;
	mov.u32 	%r516, 19;
	sub.s32 	%r164, %r516, %r513;
	mov.u32 	%r511, 18;
	// inline asm
	min.s32 	%r510, %r511, %r164;
	// inline asm
	setp.lt.s32 	%p171, %r582, %r510;
	@%p171 bra 	BB11_231;

	mov.u64 	%rl655, 0;
	bra.uni 	BB11_233;

BB11_231:
	mov.u32 	%r517, 1;
	sub.s32 	%r165, %r517, %r162;
	mov.u64 	%rl655, 0;

BB11_232:
	.pragma "nounroll";
	shl.b32 	%r521, %r582, 3;
	mov.u32 	%r522, __internal_i2opi_d;
	add.s32 	%r523, %r522, %r521;
	ld.const.u64 	%rl550, [%r523];
	mul.lo.s64 	%rl552, %rl550, %rl228;
	// inline asm
	mul.hi.u64 	%rl549, %rl550, %rl228;
	// inline asm
	mad.lo.s64 	%rl553, %rl550, %rl228, %rl655;
	setp.lt.u64 	%p172, %rl553, %rl552;
	selp.u64 	%rl554, 1, 0, %p172;
	add.s64 	%rl655, %rl554, %rl549;
	add.s32 	%r524, %r165, %r582;
	shl.b32 	%r525, %r524, 3;
	add.s32 	%r527, %r43, %r525;
	st.local.u64 	[%r527], %rl553;
	// inline asm
	min.s32 	%r518, %r511, %r164;
	// inline asm
	add.s32 	%r582, %r582, 1;
	setp.lt.s32 	%p173, %r582, %r518;
	@%p173 bra 	BB11_232;

BB11_233:
	mov.u32 	%r528, 1;
	sub.s32 	%r529, %r528, %r162;
	add.s32 	%r530, %r529, %r582;
	shl.b32 	%r531, %r530, 3;
	add.s32 	%r533, %r43, %r531;
	st.local.u64 	[%r533], %rl655;
	ld.local.u64 	%rl656, [%r43+24];
	ld.local.u64 	%rl657, [%r43+16];
	and.b32  	%r534, %r161, 63;
	setp.eq.s32 	%p174, %r534, 0;
	@%p174 bra 	BB11_235;

	and.b64  	%rl555, %rl227, 63;
	cvt.u32.u64 	%r535, %rl555;
	shl.b64 	%rl556, %rl656, %r535;
	neg.s32 	%r536, %r161;
	and.b32  	%r537, %r536, 63;
	shr.u64 	%rl557, %rl657, %r537;
	or.b64  	%rl656, %rl557, %rl556;
	shl.b64 	%rl558, %rl657, %r535;
	ld.local.u64 	%rl559, [%r43+8];
	shr.u64 	%rl560, %rl559, %r537;
	or.b64  	%rl657, %rl560, %rl558;

BB11_235:
	shr.u64 	%rl561, %rl656, 62;
	cvt.u32.u64 	%r538, %rl561;
	shr.u64 	%rl562, %rl657, 62;
	shl.b64 	%rl563, %rl656, 2;
	or.b64  	%rl662, %rl562, %rl563;
	shl.b64 	%rl239, %rl657, 2;
	setp.ne.s64 	%p175, %rl239, 0;
	selp.u64 	%rl564, 1, 0, %p175;
	or.b64  	%rl565, %rl564, %rl662;
	setp.gt.u64 	%p176, %rl565, -9223372036854775808;
	selp.u32 	%r539, 1, 0, %p176;
	add.s32 	%r540, %r539, %r538;
	neg.s32 	%r541, %r540;
	setp.lt.s64 	%p177, %rl225, 0;
	selp.b32 	%r584, %r541, %r540, %p177;
	@%p176 bra 	BB11_237;

	mov.u64 	%rl661, %rl239;
	bra.uni 	BB11_238;

BB11_237:
	not.b64 	%rl566, %rl662;
	neg.s64 	%rl240, %rl239;
	setp.eq.s64 	%p178, %rl239, 0;
	selp.u64 	%rl567, 1, 0, %p178;
	add.s64 	%rl662, %rl567, %rl566;
	xor.b64  	%rl658, %rl658, -9223372036854775808;
	mov.u64 	%rl661, %rl240;

BB11_238:
	mov.u64 	%rl660, %rl661;
	setp.gt.s64 	%p179, %rl662, 0;
	@%p179 bra 	BB11_240;

	mov.u32 	%r583, 0;
	bra.uni 	BB11_242;

BB11_240:
	mov.u32 	%r583, 0;

BB11_241:
	shr.u64 	%rl568, %rl660, 63;
	shl.b64 	%rl569, %rl662, 1;
	or.b64  	%rl662, %rl568, %rl569;
	shl.b64 	%rl660, %rl660, 1;
	add.s32 	%r583, %r583, -1;
	setp.gt.s64 	%p180, %rl662, 0;
	@%p180 bra 	BB11_241;

BB11_242:
	mul.lo.s64 	%rl664, %rl662, -3958705157555305931;
	mov.u64 	%rl572, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl570, %rl662, %rl572;
	// inline asm
	setp.gt.s64 	%p181, %rl570, 0;
	mov.u64 	%rl663, %rl570;
	@%p181 bra 	BB11_243;
	bra.uni 	BB11_244;

BB11_243:
	shl.b64 	%rl573, %rl570, 1;
	shr.u64 	%rl574, %rl664, 63;
	or.b64  	%rl663, %rl573, %rl574;
	mul.lo.s64 	%rl664, %rl662, -7917410315110611862;
	add.s32 	%r583, %r583, -1;

BB11_244:
	setp.ne.s64 	%p182, %rl664, 0;
	selp.u64 	%rl575, 1, 0, %p182;
	add.s64 	%rl576, %rl575, %rl663;
	add.s32 	%r544, %r583, 1022;
	cvt.u64.u32 	%rl577, %r544;
	shl.b64 	%rl578, %rl577, 52;
	shr.u64 	%rl579, %rl576, 11;
	shr.u64 	%rl580, %rl576, 10;
	and.b64  	%rl581, %rl580, 1;
	add.s64 	%rl582, %rl578, %rl579;
	add.s64 	%rl583, %rl582, %rl581;
	or.b64  	%rl584, %rl583, %rl658;
	mov.b64 	 %fd932, %rl584;

BB11_245:
	and.b32  	%r545, %r584, 1;
	setp.eq.s32 	%p183, %r545, 0;
	mul.rn.f64 	%fd90, %fd932, %fd932;
	@%p183 bra 	BB11_247;

	mov.f64 	%fd642, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd644, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd641, %fd642, %fd90, %fd644;
	// inline asm
	mov.f64 	%fd648, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd645, %fd641, %fd90, %fd648;
	// inline asm
	mov.f64 	%fd652, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd649, %fd645, %fd90, %fd652;
	// inline asm
	mov.f64 	%fd656, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd653, %fd649, %fd90, %fd656;
	// inline asm
	mov.f64 	%fd660, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd657, %fd653, %fd90, %fd660;
	// inline asm
	mov.f64 	%fd664, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd661, %fd657, %fd90, %fd664;
	// inline asm
	mov.f64 	%fd668, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd665, %fd661, %fd90, %fd668;
	// inline asm
	mov.f64 	%fd933, %fd665;
	bra.uni 	BB11_248;

BB11_247:
	mov.f64 	%fd670, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd672, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd669, %fd670, %fd90, %fd672;
	// inline asm
	mov.f64 	%fd676, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd673, %fd669, %fd90, %fd676;
	// inline asm
	mov.f64 	%fd680, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd677, %fd673, %fd90, %fd680;
	// inline asm
	mov.f64 	%fd684, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd681, %fd677, %fd90, %fd684;
	// inline asm
	mov.f64 	%fd688, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd685, %fd681, %fd90, %fd688;
	// inline asm
	mul.rn.f64 	%fd690, %fd685, %fd90;
	// inline asm
	fma.rn.f64 	%fd689, %fd690, %fd932, %fd932;
	// inline asm
	mov.f64 	%fd933, %fd689;

BB11_248:
	and.b32  	%r546, %r584, 2;
	setp.eq.s32 	%p184, %r546, 0;
	neg.f64 	%fd693, %fd933;
	selp.f64 	%fd934, %fd933, %fd693, %p184;
	bra.uni 	BB11_250;

BB11_249:
	mov.f64 	%fd694, 0d0000000000000000;
	mul.rn.f64 	%fd934, %fd76, %fd694;

BB11_250:
	neg.f64 	%fd695, %fd934;
	mov.f64 	%fd945, %fd931;
	mov.f64 	%fd946, %fd695;
	@%p9 bra 	BB11_251;
	bra.uni 	BB11_252;

BB11_251:
	mov.f64 	%fd945, %fd931;
	mov.f64 	%fd946, %fd934;

BB11_252:
	mul.f64 	%fd697, %fd7, %fd945;
	neg.f64 	%fd699, %fd8;
	fma.rn.f64 	%fd700, %fd699, %fd946, %fd697;
	mul.f64 	%fd701, %fd8, %fd945;
	fma.rn.f64 	%fd702, %fd7, %fd946, %fd701;
	mov.f64 	%fd943, %fd700;
	mov.f64 	%fd944, %fd702;

BB11_253:
	add.f64 	%fd705, %fd935, %fd937;
	add.f64 	%fd707, %fd705, %fd939;
	add.f64 	%fd709, %fd707, %fd941;
	add.f64 	%fd711, %fd709, %fd943;
	st.global.f64 	[%r38], %fd711;
	add.f64 	%fd714, %fd936, %fd938;
	add.f64 	%fd716, %fd714, %fd940;
	add.f64 	%fd718, %fd716, %fd942;
	add.f64 	%fd720, %fd718, %fd944;
	st.global.f64 	[%r38+8], %fd720;
	mul.f64 	%fd721, %fd937, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd722, %fd938, 0d3FEE6F0E1344FF84;
	mov.f64 	%fd723, 0d3FEE6F0E1344FF84;
	fma.rn.f64 	%fd724, %fd937, 0d3FD3C6EF3736CC3B, %fd722;
	add.f64 	%fd725, %fd935, %fd724;
	mul.f64 	%fd726, %fd939, 0dBFE9E3779B9B661D;
	mul.f64 	%fd727, %fd940, 0d3FE2CF2304766332;
	mov.f64 	%fd728, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd729, %fd939, 0dBFE9E3779B9B661D, %fd727;
	add.f64 	%fd730, %fd725, %fd729;
	mul.f64 	%fd731, %fd941, 0dBFE9E3779B9B661D;
	mul.f64 	%fd732, %fd942, 0d3FE2CF2304766332;
	neg.f64 	%fd733, %fd942;
	fma.rn.f64 	%fd734, %fd733, %fd728, %fd731;
	add.f64 	%fd735, %fd730, %fd734;
	mul.f64 	%fd736, %fd943, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd737, %fd944, 0d3FEE6F0E1344FF84;
	neg.f64 	%fd738, %fd944;
	fma.rn.f64 	%fd739, %fd738, %fd723, %fd736;
	add.f64 	%fd740, %fd735, %fd739;
	st.global.f64 	[%r39], %fd740;
	mul.f64 	%fd741, %fd938, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd742, %fd937, 0d3FEE6F0E1344FF84;
	neg.f64 	%fd743, %fd937;
	fma.rn.f64 	%fd744, %fd743, %fd723, %fd741;
	add.f64 	%fd745, %fd936, %fd744;
	mul.f64 	%fd746, %fd940, 0dBFE9E3779B9B661D;
	mul.f64 	%fd747, %fd939, 0d3FE2CF2304766332;
	neg.f64 	%fd748, %fd939;
	fma.rn.f64 	%fd749, %fd748, %fd728, %fd746;
	add.f64 	%fd750, %fd745, %fd749;
	mul.f64 	%fd751, %fd942, 0dBFE9E3779B9B661D;
	mul.f64 	%fd752, %fd941, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd753, %fd942, 0dBFE9E3779B9B661D, %fd752;
	add.f64 	%fd754, %fd750, %fd753;
	mul.f64 	%fd755, %fd944, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd756, %fd943, 0d3FEE6F0E1344FF84;
	fma.rn.f64 	%fd757, %fd944, 0d3FD3C6EF3736CC3B, %fd756;
	add.f64 	%fd758, %fd754, %fd757;
	st.global.f64 	[%r39+8], %fd758;
	mul.f64 	%fd759, %fd937, 0dBFE9E3779B9B661D;
	mul.f64 	%fd760, %fd938, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd761, %fd937, 0dBFE9E3779B9B661D, %fd760;
	add.f64 	%fd762, %fd935, %fd761;
	mul.f64 	%fd763, %fd939, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd764, %fd940, 0d3FEE6F0E1344FF84;
	neg.f64 	%fd765, %fd940;
	fma.rn.f64 	%fd766, %fd765, %fd723, %fd763;
	add.f64 	%fd767, %fd762, %fd766;
	mul.f64 	%fd768, %fd941, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd769, %fd942, 0d3FEE6F0E1344FF84;
	fma.rn.f64 	%fd770, %fd941, 0d3FD3C6EF3736CC3B, %fd769;
	add.f64 	%fd771, %fd767, %fd770;
	mul.f64 	%fd772, %fd943, 0dBFE9E3779B9B661D;
	mul.f64 	%fd773, %fd944, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd774, %fd738, %fd728, %fd772;
	add.f64 	%fd775, %fd771, %fd774;
	st.global.f64 	[%r40], %fd775;
	mul.f64 	%fd776, %fd938, 0dBFE9E3779B9B661D;
	mul.f64 	%fd777, %fd937, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd778, %fd743, %fd728, %fd776;
	add.f64 	%fd779, %fd936, %fd778;
	mul.f64 	%fd780, %fd940, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd781, %fd939, 0d3FEE6F0E1344FF84;
	fma.rn.f64 	%fd782, %fd940, 0d3FD3C6EF3736CC3B, %fd781;
	add.f64 	%fd783, %fd779, %fd782;
	mul.f64 	%fd784, %fd942, 0d3FD3C6EF3736CC3B;
	mul.f64 	%fd785, %fd941, 0d3FEE6F0E1344FF84;
	neg.f64 	%fd786, %fd941;
	fma.rn.f64 	%fd787, %fd786, %fd723, %fd784;
	add.f64 	%fd788, %fd783, %fd787;
	mul.f64 	%fd789, %fd944, 0dBFE9E3779B9B661D;
	mul.f64 	%fd790, %fd943, 0d3FE2CF2304766332;
	fma.rn.f64 	%fd791, %fd944, 0dBFE9E3779B9B661D, %fd790;
	add.f64 	%fd792, %fd788, %fd791;
	st.global.f64 	[%r40+8], %fd792;
	neg.f64 	%fd793, %fd938;
	fma.rn.f64 	%fd794, %fd793, %fd728, %fd759;
	add.f64 	%fd795, %fd935, %fd794;
	fma.rn.f64 	%fd796, %fd939, 0d3FD3C6EF3736CC3B, %fd764;
	add.f64 	%fd797, %fd795, %fd796;
	fma.rn.f64 	%fd798, %fd733, %fd723, %fd768;
	add.f64 	%fd799, %fd797, %fd798;
	fma.rn.f64 	%fd800, %fd943, 0dBFE9E3779B9B661D, %fd773;
	add.f64 	%fd801, %fd799, %fd800;
	st.global.f64 	[%r41], %fd801;
	fma.rn.f64 	%fd802, %fd938, 0dBFE9E3779B9B661D, %fd777;
	add.f64 	%fd803, %fd936, %fd802;
	fma.rn.f64 	%fd804, %fd748, %fd723, %fd780;
	add.f64 	%fd805, %fd803, %fd804;
	fma.rn.f64 	%fd806, %fd942, 0d3FD3C6EF3736CC3B, %fd785;
	add.f64 	%fd807, %fd805, %fd806;
	neg.f64 	%fd808, %fd943;
	fma.rn.f64 	%fd809, %fd808, %fd728, %fd789;
	add.f64 	%fd810, %fd807, %fd809;
	st.global.f64 	[%r41+8], %fd810;
	fma.rn.f64 	%fd811, %fd793, %fd723, %fd721;
	add.f64 	%fd812, %fd935, %fd811;
	fma.rn.f64 	%fd813, %fd765, %fd728, %fd726;
	add.f64 	%fd814, %fd812, %fd813;
	fma.rn.f64 	%fd815, %fd941, 0dBFE9E3779B9B661D, %fd732;
	add.f64 	%fd816, %fd814, %fd815;
	fma.rn.f64 	%fd817, %fd943, 0d3FD3C6EF3736CC3B, %fd737;
	add.f64 	%fd818, %fd816, %fd817;
	st.global.f64 	[%r42], %fd818;
	fma.rn.f64 	%fd819, %fd938, 0d3FD3C6EF3736CC3B, %fd742;
	add.f64 	%fd820, %fd936, %fd819;
	fma.rn.f64 	%fd821, %fd940, 0dBFE9E3779B9B661D, %fd747;
	add.f64 	%fd822, %fd820, %fd821;
	fma.rn.f64 	%fd823, %fd786, %fd728, %fd751;
	add.f64 	%fd824, %fd822, %fd823;
	fma.rn.f64 	%fd825, %fd808, %fd723, %fd755;
	add.f64 	%fd826, %fd824, %fd825;
	st.global.f64 	[%r42+8], %fd826;
	ret;
}

.entry DIT7C2CM(
	.param .u32 .ptr .global .align 8 DIT7C2CM_param_0,
	.param .u32 DIT7C2CM_param_1,
	.param .u32 DIT7C2CM_param_2,
	.param .u32 DIT7C2CM_param_3,
	.param .u32 DIT7C2CM_param_4,
	.param .u32 DIT7C2CM_param_5
)
{
	.local .align 8 .b8 	__local_depot12[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<1480>;
	.reg .pred 	%p<260>;
	.reg .s32 	%r<818>;
	.reg .s64 	%rl<1006>;


	mov.u32 	%SP, __local_depot12;
	ld.param.u32 	%r252, [DIT7C2CM_param_3];
	// inline asm
	mov.u32 	%r244, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r245, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r246, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r247, %tid.x;
	// inline asm
	add.s32 	%r253, %r247, %r244;
	mad.lo.s32 	%r5, %r246, %r245, %r253;
	// inline asm
	mov.u32 	%r248, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r249, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r250, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r251, %tid.y;
	// inline asm
	add.s32 	%r254, %r251, %r248;
	mad.lo.s32 	%r6, %r250, %r249, %r254;
	mul.hi.s32 	%r255, %r252, 780903145;
	shr.u32 	%r256, %r255, 31;
	shr.s32 	%r257, %r255, 1;
	add.s32 	%r7, %r257, %r256;
	mul.lo.s32 	%r258, %r7, 11;
	sub.s32 	%r8, %r252, %r258;
	setp.gt.s32 	%p17, %r252, 10;
	@%p17 bra 	BB12_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB12_23;

BB12_2:
	mov.f32 	%f1, 0f41300000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40E00000;
	add.f32 	%f2, %f47, 0f41300000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r775, 0;
	mov.u32 	%r774, 1;

BB12_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p18, %p3, %p3;
	@%p18 bra 	BB12_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mov.f32 	%f55, 0f40000000;
	mul.rn.f32 	%f56, %f55, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p19, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p20, %f1, %f52;
	and.pred  	%p4, %p19, %p20;
	setp.eq.f32 	%p21, %f48, 0f00000000;
	@%p21 bra 	BB12_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r11, %f58;
	shr.u32 	%r261, %r11, 23;
	and.b32  	%r262, %r261, 255;
	add.s32 	%r776, %r262, -127;
	setp.eq.s32 	%p22, %r262, 0;
	mov.f32 	%f280, %f58;
	@%p22 bra 	BB12_6;
	bra.uni 	BB12_7;

BB12_6:
	and.b32  	%r263, %r11, -2139095041;
	or.b32  	%r264, %r263, 1065353216;
	mov.b32 	 %f60, %r264;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r265, %f61;
	shr.u32 	%r266, %r265, 23;
	and.b32  	%r267, %r266, 255;
	add.s32 	%r776, %r267, -253;
	and.b32  	%r268, %r265, -2139095041;
	or.b32  	%r269, %r268, 1065353216;
	mov.b32 	 %f280, %r269;

BB12_7:
	mov.b32 	 %r270, %f280;
	and.b32  	%r271, %r270, -2139095041;
	or.b32  	%r272, %r271, 1065353216;
	mov.b32 	 %f281, %r272;
	setp.gt.f32 	%p23, %f281, 0f3FB504F3;
	@%p23 bra 	BB12_8;
	bra.uni 	BB12_9;

BB12_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r776, %r776, 1;

BB12_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f55, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r273, %f73;
	and.b32  	%r274, %r273, -4096;
	mov.b32 	 %f81, %r274;
	mov.b32 	 %r275, %f64;
	and.b32  	%r276, %r275, -4096;
	mov.b32 	 %f82, %r276;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f55, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r776;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p24, %f69, 0f77F684DF;
	@%p24 bra 	BB12_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB12_12;

BB12_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB12_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r17, %f131;
	setp.eq.s32 	%p25, %r17, 1118925336;
	@%p25 bra 	BB12_13;
	bra.uni 	BB12_14;

BB12_13:
	add.s32 	%r277, %r17, -1;
	mov.b32 	 %f133, %r277;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB12_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p26, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p26;
	setp.gt.f32 	%p27, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p27;
	setp.neu.f32 	%p28, %f19, %f3;
	@%p28 bra 	BB12_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB12_21;

BB12_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB12_21;

BB12_17:
	@%p3 bra 	BB12_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB12_21;

BB12_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB12_21;

BB12_20:
	mov.f32 	%f283, %f7;

BB12_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r774;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r774, %f157;
	add.s32 	%r775, %r775, 1;
	setp.lt.s32 	%p29, %r775, %r7;
	@%p29 bra 	BB12_3;

	cvt.rn.f32.s32 	%f284, %r774;

BB12_23:
	mov.f32 	%f159, 0f40E00000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r8;
	setp.eq.f32 	%p30, %f287, 0f00000000;
	@%p30 bra 	BB12_45;

	setp.nan.f32 	%p31, %f287, %f287;
	@%p31 bra 	BB12_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p32, %f287, 0f7F800000;
	setp.eq.f32 	%p33, %f287, 0fFF800000;
	or.pred  	%p34, %p32, %p33;
	@%p34 bra 	BB12_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p37, %f158, 0f00000000;
	@%p37 bra 	BB12_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r20, %f168;
	shr.u32 	%r278, %r20, 23;
	and.b32  	%r279, %r278, 255;
	add.s32 	%r777, %r279, -127;
	setp.eq.s32 	%p38, %r279, 0;
	mov.f32 	%f285, %f168;
	@%p38 bra 	BB12_28;
	bra.uni 	BB12_29;

BB12_28:
	and.b32  	%r280, %r20, -2139095041;
	or.b32  	%r281, %r280, 1065353216;
	mov.b32 	 %f170, %r281;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r282, %f171;
	shr.u32 	%r283, %r282, 23;
	and.b32  	%r284, %r283, 255;
	add.s32 	%r777, %r284, -253;
	and.b32  	%r285, %r282, -2139095041;
	or.b32  	%r286, %r285, 1065353216;
	mov.b32 	 %f285, %r286;

BB12_29:
	mov.b32 	 %r287, %f285;
	and.b32  	%r288, %r287, -2139095041;
	or.b32  	%r289, %r288, 1065353216;
	mov.b32 	 %f286, %r289;
	setp.gt.f32 	%p39, %f286, 0f3FB504F3;
	@%p39 bra 	BB12_30;
	bra.uni 	BB12_31;

BB12_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r777, %r777, 1;

BB12_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mov.f32 	%f182, 0f40000000;
	mul.rn.f32 	%f183, %f182, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r290, %f183;
	and.b32  	%r291, %r290, -4096;
	mov.b32 	 %f191, %r291;
	mov.b32 	 %r292, %f174;
	and.b32  	%r293, %r292, -4096;
	mov.b32 	 %f192, %r293;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f182, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r777;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p40, %f179, 0f77F684DF;
	@%p40 bra 	BB12_32;
	bra.uni 	BB12_33;

BB12_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB12_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r26, %f241;
	setp.eq.s32 	%p41, %r26, 1118925336;
	@%p41 bra 	BB12_34;
	bra.uni 	BB12_35;

BB12_34:
	add.s32 	%r294, %r26, -1;
	mov.b32 	 %f243, %r294;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB12_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p42, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p42;
	setp.gt.f32 	%p43, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p43;
	setp.neu.f32 	%p44, %f39, %f27;
	@%p44 bra 	BB12_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB12_46;

BB12_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB12_46;

BB12_38:
	setp.lt.f32 	%p45, %f287, 0f00000000;
	@%p45 bra 	BB12_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB12_46;

BB12_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB12_46;

BB12_41:
	setp.lt.f32 	%p46, %f158, 0f3F800000;
	mov.b32 	 %r295, %f287;
	setp.lt.s32 	%p6, %r295, 0;
	@%p46 bra 	BB12_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB12_46;

BB12_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB12_46;

BB12_44:
	add.f32 	%f288, %f287, 0f40E00000;
	bra.uni 	BB12_46;

BB12_45:
	mov.f32 	%f288, 0f3F800000;

BB12_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r27, %f267;
	mul.hi.s32 	%r296, %r27, -1840700269;
	add.s32 	%r297, %r296, %r27;
	shr.u32 	%r298, %r297, 31;
	shr.s32 	%r299, %r297, 2;
	add.s32 	%r28, %r299, %r298;
	ld.param.u32 	%r773, [DIT7C2CM_param_5];
	setp.eq.s32 	%p47, %r773, 0;
	@%p47 bra 	BB12_49;

	ld.param.u32 	%r772, [DIT7C2CM_param_5];
	setp.ne.s32 	%p48, %r772, 1;
	@%p48 bra 	BB12_50;

	div.s32 	%r778, %r6, %r28;
	rem.s32 	%r779, %r6, %r28;
	ld.param.u32 	%r781, [DIT7C2CM_param_1];
	mov.u32 	%r780, %r5;
	bra.uni 	BB12_51;

BB12_49:
	ld.param.u32 	%r770, [DIT7C2CM_param_1];
	mul.lo.s32 	%r31, %r6, %r770;
	div.s32 	%r778, %r5, %r28;
	rem.s32 	%r779, %r5, %r28;
	mov.u32 	%r781, 1;
	mov.u32 	%r780, %r31;
	bra.uni 	BB12_51;

BB12_50:
	mov.u32 	%r781, 1;
	mov.u32 	%r780, 0;
	mov.u32 	%r779, %r305;
	mov.u32 	%r778, %r306;

BB12_51:
	mad.lo.s32 	%r307, %r778, %r27, %r779;
	mad.lo.s32 	%r308, %r781, %r307, %r780;
	shl.b32 	%r309, %r308, 4;
	ld.param.u32 	%r768, [DIT7C2CM_param_0];
	add.s32 	%r38, %r768, %r309;
	ld.global.f64 	%fd141, [%r38];
	ld.global.f64 	%fd142, [%r38+8];
	mul.lo.s32 	%r310, %r781, %r28;
	shl.b32 	%r311, %r310, 4;
	add.s32 	%r39, %r38, %r311;
	ld.global.f64 	%fd1, [%r39];
	ld.global.f64 	%fd2, [%r39+8];
	add.s32 	%r40, %r39, %r311;
	ld.global.f64 	%fd3, [%r40];
	ld.global.f64 	%fd4, [%r40+8];
	add.s32 	%r41, %r40, %r311;
	ld.global.f64 	%fd5, [%r41];
	ld.global.f64 	%fd6, [%r41+8];
	add.s32 	%r42, %r41, %r311;
	ld.global.f64 	%fd7, [%r42];
	ld.global.f64 	%fd8, [%r42+8];
	mov.f64 	%fd1462, %fd7;
	mov.f64 	%fd1463, %fd8;
	add.s32 	%r43, %r42, %r311;
	ld.global.f64 	%fd9, [%r43];
	ld.global.f64 	%fd10, [%r43+8];
	mov.f64 	%fd1464, %fd9;
	mov.f64 	%fd1465, %fd10;
	add.s32 	%r44, %r43, %r311;
	ld.global.f64 	%fd11, [%r44];
	ld.global.f64 	%fd12, [%r44+8];
	mov.f64 	%fd1466, %fd11;
	mov.f64 	%fd1467, %fd12;
	setp.eq.s32 	%p49, %r779, 0;
	mov.f64 	%fd1454, %fd141;
	mov.f64 	%fd1455, %fd142;
	mov.f64 	%fd1456, %fd1;
	mov.f64 	%fd1457, %fd2;
	mov.f64 	%fd1458, %fd3;
	mov.f64 	%fd1459, %fd4;
	mov.f64 	%fd1460, %fd5;
	mov.f64 	%fd1461, %fd6;
	@%p49 bra 	BB12_353;

	cvt.rn.f64.s32 	%fd13, %r779;
	mul.f64 	%fd143, %fd13, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd14, %r27;
	div.rn.f64 	%fd15, %fd143, %fd14;
	setp.eq.f64 	%p50, %fd15, 0d7FF0000000000000;
	setp.eq.f64 	%p51, %fd15, 0dFFF0000000000000;
	or.pred  	%p52, %p50, %p51;
	add.u32 	%r45, %SP, 0;
	@%p52 bra 	BB12_75;

	// inline asm
	abs.f64 	%fd144, %fd15;
	// inline asm
	setp.gt.f64 	%p53, %fd144, 0d41E0000000000000;
	@%p53 bra 	BB12_55;

	mov.f64 	%fd159, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd146, %fd15, %fd159;
	// inline asm
	cvt.rni.s32.f64 	%r312, %fd146;
	// inline asm
	cvt.rn.f64.s32 	%fd160, %r312;
	neg.f64 	%fd156, %fd160;
	mov.f64 	%fd149, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd147, %fd156, %fd149, %fd15;
	// inline asm
	mov.f64 	%fd153, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd151, %fd156, %fd153, %fd147;
	// inline asm
	mov.f64 	%fd157, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd155, %fd156, %fd157, %fd151;
	// inline asm
	mov.u32 	%r784, %r312;
	mov.f64 	%fd1418, %fd155;
	bra.uni 	BB12_71;

BB12_55:
	mov.b64 	 %rl381, %fd15;
	shr.u64 	%rl382, %rl381, 52;
	and.b64  	%rl383, %rl382, 2047;
	add.s64 	%rl384, %rl383, 4294966272;
	cvt.u32.u64 	%r47, %rl384;
	shl.b64 	%rl385, %rl381, 11;
	or.b64  	%rl1, %rl385, -9223372036854775808;
	shr.u32 	%r316, %r47, 6;
	mov.u32 	%r317, 16;
	sub.s32 	%r48, %r317, %r316;
	mov.u32 	%r318, 15;
	sub.s32 	%r782, %r318, %r316;
	mov.u32 	%r319, 19;
	sub.s32 	%r50, %r319, %r316;
	mov.u32 	%r314, 18;
	// inline asm
	min.s32 	%r313, %r314, %r50;
	// inline asm
	setp.lt.s32 	%p54, %r782, %r313;
	@%p54 bra 	BB12_57;

	mov.u64 	%rl886, 0;
	bra.uni 	BB12_59;

BB12_57:
	mov.u32 	%r320, 1;
	sub.s32 	%r51, %r320, %r48;
	mov.u64 	%rl886, 0;

BB12_58:
	.pragma "nounroll";
	shl.b32 	%r324, %r782, 3;
	mov.u32 	%r325, __internal_i2opi_d;
	add.s32 	%r326, %r325, %r324;
	ld.const.u64 	%rl389, [%r326];
	mul.lo.s64 	%rl391, %rl389, %rl1;
	// inline asm
	mul.hi.u64 	%rl388, %rl389, %rl1;
	// inline asm
	mad.lo.s64 	%rl392, %rl389, %rl1, %rl886;
	setp.lt.u64 	%p55, %rl392, %rl391;
	selp.u64 	%rl393, 1, 0, %p55;
	add.s64 	%rl886, %rl393, %rl388;
	add.s32 	%r327, %r51, %r782;
	shl.b32 	%r328, %r327, 3;
	add.s32 	%r330, %r45, %r328;
	st.local.u64 	[%r330], %rl392;
	// inline asm
	min.s32 	%r321, %r314, %r50;
	// inline asm
	add.s32 	%r782, %r782, 1;
	setp.lt.s32 	%p56, %r782, %r321;
	@%p56 bra 	BB12_58;

BB12_59:
	mov.u32 	%r331, 1;
	sub.s32 	%r332, %r331, %r48;
	add.s32 	%r333, %r332, %r782;
	shl.b32 	%r334, %r333, 3;
	add.s32 	%r336, %r45, %r334;
	st.local.u64 	[%r336], %rl886;
	ld.local.u64 	%rl887, [%r45+24];
	ld.local.u64 	%rl888, [%r45+16];
	and.b32  	%r337, %r47, 63;
	setp.eq.s32 	%p57, %r337, 0;
	@%p57 bra 	BB12_61;

	and.b64  	%rl396, %rl382, 63;
	cvt.u32.u64 	%r338, %rl396;
	shl.b64 	%rl397, %rl887, %r338;
	neg.s32 	%r339, %r47;
	and.b32  	%r340, %r339, 63;
	shr.u64 	%rl398, %rl888, %r340;
	or.b64  	%rl887, %rl398, %rl397;
	shl.b64 	%rl399, %rl888, %r338;
	ld.local.u64 	%rl400, [%r45+8];
	shr.u64 	%rl401, %rl400, %r340;
	or.b64  	%rl888, %rl401, %rl399;

BB12_61:
	shr.u64 	%rl402, %rl887, 62;
	cvt.u32.u64 	%r341, %rl402;
	shr.u64 	%rl403, %rl888, 62;
	shl.b64 	%rl404, %rl887, 2;
	or.b64  	%rl893, %rl403, %rl404;
	shl.b64 	%rl12, %rl888, 2;
	setp.ne.s64 	%p58, %rl12, 0;
	selp.u64 	%rl405, 1, 0, %p58;
	or.b64  	%rl406, %rl405, %rl893;
	setp.gt.u64 	%p59, %rl406, -9223372036854775808;
	selp.u32 	%r342, 1, 0, %p59;
	add.s32 	%r343, %r342, %r341;
	setp.lt.s64 	%p60, %rl381, 0;
	neg.s32 	%r344, %r343;
	selp.b32 	%r784, %r344, %r343, %p60;
	@%p59 bra 	BB12_63;

	and.b64  	%rl889, %rl381, -9223372036854775808;
	mov.u64 	%rl892, %rl12;
	bra.uni 	BB12_64;

BB12_63:
	not.b64 	%rl409, %rl893;
	neg.s64 	%rl14, %rl12;
	setp.eq.s64 	%p61, %rl12, 0;
	selp.u64 	%rl410, 1, 0, %p61;
	add.s64 	%rl893, %rl410, %rl409;
	and.b64  	%rl412, %rl381, -9223372036854775808;
	xor.b64  	%rl889, %rl412, -9223372036854775808;
	mov.u64 	%rl892, %rl14;

BB12_64:
	mov.u64 	%rl891, %rl892;
	setp.gt.s64 	%p62, %rl893, 0;
	@%p62 bra 	BB12_66;

	mov.u32 	%r783, 0;
	bra.uni 	BB12_68;

BB12_66:
	mov.u32 	%r783, 0;

BB12_67:
	shr.u64 	%rl413, %rl891, 63;
	shl.b64 	%rl414, %rl893, 1;
	or.b64  	%rl893, %rl413, %rl414;
	shl.b64 	%rl891, %rl891, 1;
	add.s32 	%r783, %r783, -1;
	setp.gt.s64 	%p63, %rl893, 0;
	@%p63 bra 	BB12_67;

BB12_68:
	mul.lo.s64 	%rl895, %rl893, -3958705157555305931;
	mov.u64 	%rl417, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl415, %rl893, %rl417;
	// inline asm
	setp.gt.s64 	%p64, %rl415, 0;
	mov.u64 	%rl894, %rl415;
	@%p64 bra 	BB12_69;
	bra.uni 	BB12_70;

BB12_69:
	shl.b64 	%rl418, %rl415, 1;
	shr.u64 	%rl419, %rl895, 63;
	or.b64  	%rl894, %rl418, %rl419;
	mul.lo.s64 	%rl895, %rl893, -7917410315110611862;
	add.s32 	%r783, %r783, -1;

BB12_70:
	setp.ne.s64 	%p65, %rl895, 0;
	selp.u64 	%rl420, 1, 0, %p65;
	add.s64 	%rl421, %rl420, %rl894;
	add.s32 	%r347, %r783, 1022;
	cvt.u64.u32 	%rl422, %r347;
	shl.b64 	%rl423, %rl422, 52;
	shr.u64 	%rl424, %rl421, 11;
	shr.u64 	%rl425, %rl421, 10;
	and.b64  	%rl426, %rl425, 1;
	add.s64 	%rl427, %rl423, %rl424;
	add.s64 	%rl428, %rl427, %rl426;
	or.b64  	%rl429, %rl428, %rl889;
	mov.b64 	 %fd1418, %rl429;

BB12_71:
	add.s32 	%r62, %r784, 1;
	and.b32  	%r348, %r62, 1;
	setp.eq.s32 	%p66, %r348, 0;
	mul.rn.f64 	%fd19, %fd1418, %fd1418;
	@%p66 bra 	BB12_73;

	mov.f64 	%fd162, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd164, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd161, %fd162, %fd19, %fd164;
	// inline asm
	mov.f64 	%fd168, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd165, %fd161, %fd19, %fd168;
	// inline asm
	mov.f64 	%fd172, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd169, %fd165, %fd19, %fd172;
	// inline asm
	mov.f64 	%fd176, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd173, %fd169, %fd19, %fd176;
	// inline asm
	mov.f64 	%fd180, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd177, %fd173, %fd19, %fd180;
	// inline asm
	mov.f64 	%fd184, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd181, %fd177, %fd19, %fd184;
	// inline asm
	mov.f64 	%fd188, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd185, %fd181, %fd19, %fd188;
	// inline asm
	mov.f64 	%fd1419, %fd185;
	bra.uni 	BB12_74;

BB12_73:
	mov.f64 	%fd190, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd192, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd189, %fd190, %fd19, %fd192;
	// inline asm
	mov.f64 	%fd196, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd193, %fd189, %fd19, %fd196;
	// inline asm
	mov.f64 	%fd200, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd197, %fd193, %fd19, %fd200;
	// inline asm
	mov.f64 	%fd204, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd201, %fd197, %fd19, %fd204;
	// inline asm
	mov.f64 	%fd208, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd205, %fd201, %fd19, %fd208;
	// inline asm
	mul.rn.f64 	%fd210, %fd205, %fd19;
	// inline asm
	fma.rn.f64 	%fd209, %fd210, %fd1418, %fd1418;
	// inline asm
	mov.f64 	%fd1419, %fd209;

BB12_74:
	and.b32  	%r349, %r62, 2;
	setp.eq.s32 	%p67, %r349, 0;
	neg.f64 	%fd213, %fd1419;
	selp.f64 	%fd1420, %fd1419, %fd213, %p67;
	bra.uni 	BB12_76;

BB12_75:
	mov.f64 	%fd1420, 0dFFF8000000000000;

BB12_76:
	setp.eq.f64 	%p69, %fd15, 0d0000000000000000;
	or.pred  	%p70, %p51, %p69;
	or.pred  	%p72, %p50, %p70;
	@%p72 bra 	BB12_99;

	// inline asm
	abs.f64 	%fd215, %fd15;
	// inline asm
	setp.gt.f64 	%p73, %fd215, 0d41E0000000000000;
	@%p73 bra 	BB12_79;

	mov.f64 	%fd230, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd217, %fd15, %fd230;
	// inline asm
	cvt.rni.s32.f64 	%r350, %fd217;
	// inline asm
	cvt.rn.f64.s32 	%fd231, %r350;
	neg.f64 	%fd227, %fd231;
	mov.f64 	%fd220, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd218, %fd227, %fd220, %fd15;
	// inline asm
	mov.f64 	%fd224, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd222, %fd227, %fd224, %fd218;
	// inline asm
	mov.f64 	%fd228, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd226, %fd227, %fd228, %fd222;
	// inline asm
	mov.u32 	%r787, %r350;
	mov.f64 	%fd1421, %fd226;
	bra.uni 	BB12_95;

BB12_79:
	mov.b64 	 %rl31, %fd15;
	and.b64  	%rl899, %rl31, -9223372036854775808;
	shr.u64 	%rl33, %rl31, 52;
	and.b64  	%rl430, %rl33, 2047;
	add.s64 	%rl431, %rl430, 4294966272;
	cvt.u32.u64 	%r64, %rl431;
	shl.b64 	%rl432, %rl31, 11;
	or.b64  	%rl34, %rl432, -9223372036854775808;
	shr.u32 	%r354, %r64, 6;
	mov.u32 	%r355, 16;
	sub.s32 	%r65, %r355, %r354;
	mov.u32 	%r356, 15;
	sub.s32 	%r785, %r356, %r354;
	mov.u32 	%r357, 19;
	sub.s32 	%r67, %r357, %r354;
	mov.u32 	%r352, 18;
	// inline asm
	min.s32 	%r351, %r352, %r67;
	// inline asm
	setp.lt.s32 	%p74, %r785, %r351;
	@%p74 bra 	BB12_81;

	mov.u64 	%rl896, 0;
	bra.uni 	BB12_83;

BB12_81:
	mov.u32 	%r358, 1;
	sub.s32 	%r68, %r358, %r65;
	mov.u64 	%rl896, 0;

BB12_82:
	.pragma "nounroll";
	shl.b32 	%r362, %r785, 3;
	mov.u32 	%r363, __internal_i2opi_d;
	add.s32 	%r364, %r363, %r362;
	ld.const.u64 	%rl436, [%r364];
	mul.lo.s64 	%rl438, %rl436, %rl34;
	// inline asm
	mul.hi.u64 	%rl435, %rl436, %rl34;
	// inline asm
	mad.lo.s64 	%rl439, %rl436, %rl34, %rl896;
	setp.lt.u64 	%p75, %rl439, %rl438;
	selp.u64 	%rl440, 1, 0, %p75;
	add.s64 	%rl896, %rl440, %rl435;
	add.s32 	%r365, %r68, %r785;
	shl.b32 	%r366, %r365, 3;
	add.s32 	%r368, %r45, %r366;
	st.local.u64 	[%r368], %rl439;
	// inline asm
	min.s32 	%r359, %r352, %r67;
	// inline asm
	add.s32 	%r785, %r785, 1;
	setp.lt.s32 	%p76, %r785, %r359;
	@%p76 bra 	BB12_82;

BB12_83:
	mov.u32 	%r369, 1;
	sub.s32 	%r370, %r369, %r65;
	add.s32 	%r371, %r370, %r785;
	shl.b32 	%r372, %r371, 3;
	add.s32 	%r374, %r45, %r372;
	st.local.u64 	[%r374], %rl896;
	ld.local.u64 	%rl897, [%r45+24];
	ld.local.u64 	%rl898, [%r45+16];
	and.b32  	%r375, %r64, 63;
	setp.eq.s32 	%p77, %r375, 0;
	@%p77 bra 	BB12_85;

	and.b64  	%rl441, %rl33, 63;
	cvt.u32.u64 	%r376, %rl441;
	shl.b64 	%rl442, %rl897, %r376;
	neg.s32 	%r377, %r64;
	and.b32  	%r378, %r377, 63;
	shr.u64 	%rl443, %rl898, %r378;
	or.b64  	%rl897, %rl443, %rl442;
	shl.b64 	%rl444, %rl898, %r376;
	ld.local.u64 	%rl445, [%r45+8];
	shr.u64 	%rl446, %rl445, %r378;
	or.b64  	%rl898, %rl446, %rl444;

BB12_85:
	shr.u64 	%rl447, %rl897, 62;
	cvt.u32.u64 	%r379, %rl447;
	shr.u64 	%rl448, %rl898, 62;
	shl.b64 	%rl449, %rl897, 2;
	or.b64  	%rl903, %rl448, %rl449;
	shl.b64 	%rl45, %rl898, 2;
	setp.ne.s64 	%p78, %rl45, 0;
	selp.u64 	%rl450, 1, 0, %p78;
	or.b64  	%rl451, %rl450, %rl903;
	setp.gt.u64 	%p79, %rl451, -9223372036854775808;
	selp.u32 	%r380, 1, 0, %p79;
	add.s32 	%r381, %r380, %r379;
	neg.s32 	%r382, %r381;
	setp.lt.s64 	%p80, %rl31, 0;
	selp.b32 	%r787, %r382, %r381, %p80;
	@%p79 bra 	BB12_87;

	mov.u64 	%rl902, %rl45;
	bra.uni 	BB12_88;

BB12_87:
	not.b64 	%rl452, %rl903;
	neg.s64 	%rl46, %rl45;
	setp.eq.s64 	%p81, %rl45, 0;
	selp.u64 	%rl453, 1, 0, %p81;
	add.s64 	%rl903, %rl453, %rl452;
	xor.b64  	%rl899, %rl899, -9223372036854775808;
	mov.u64 	%rl902, %rl46;

BB12_88:
	mov.u64 	%rl901, %rl902;
	setp.gt.s64 	%p82, %rl903, 0;
	@%p82 bra 	BB12_90;

	mov.u32 	%r786, 0;
	bra.uni 	BB12_92;

BB12_90:
	mov.u32 	%r786, 0;

BB12_91:
	shr.u64 	%rl454, %rl901, 63;
	shl.b64 	%rl455, %rl903, 1;
	or.b64  	%rl903, %rl454, %rl455;
	shl.b64 	%rl901, %rl901, 1;
	add.s32 	%r786, %r786, -1;
	setp.gt.s64 	%p83, %rl903, 0;
	@%p83 bra 	BB12_91;

BB12_92:
	mul.lo.s64 	%rl905, %rl903, -3958705157555305931;
	mov.u64 	%rl458, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl456, %rl903, %rl458;
	// inline asm
	setp.gt.s64 	%p84, %rl456, 0;
	mov.u64 	%rl904, %rl456;
	@%p84 bra 	BB12_93;
	bra.uni 	BB12_94;

BB12_93:
	shl.b64 	%rl459, %rl456, 1;
	shr.u64 	%rl460, %rl905, 63;
	or.b64  	%rl904, %rl459, %rl460;
	mul.lo.s64 	%rl905, %rl903, -7917410315110611862;
	add.s32 	%r786, %r786, -1;

BB12_94:
	setp.ne.s64 	%p85, %rl905, 0;
	selp.u64 	%rl461, 1, 0, %p85;
	add.s64 	%rl462, %rl461, %rl904;
	add.s32 	%r385, %r786, 1022;
	cvt.u64.u32 	%rl463, %r385;
	shl.b64 	%rl464, %rl463, 52;
	shr.u64 	%rl465, %rl462, 11;
	shr.u64 	%rl466, %rl462, 10;
	and.b64  	%rl467, %rl466, 1;
	add.s64 	%rl468, %rl464, %rl465;
	add.s64 	%rl469, %rl468, %rl467;
	or.b64  	%rl470, %rl469, %rl899;
	mov.b64 	 %fd1421, %rl470;

BB12_95:
	and.b32  	%r386, %r787, 1;
	setp.eq.s32 	%p86, %r386, 0;
	mul.rn.f64 	%fd29, %fd1421, %fd1421;
	@%p86 bra 	BB12_97;

	mov.f64 	%fd233, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd235, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd232, %fd233, %fd29, %fd235;
	// inline asm
	mov.f64 	%fd239, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd236, %fd232, %fd29, %fd239;
	// inline asm
	mov.f64 	%fd243, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd240, %fd236, %fd29, %fd243;
	// inline asm
	mov.f64 	%fd247, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd244, %fd240, %fd29, %fd247;
	// inline asm
	mov.f64 	%fd251, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd248, %fd244, %fd29, %fd251;
	// inline asm
	mov.f64 	%fd255, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd252, %fd248, %fd29, %fd255;
	// inline asm
	mov.f64 	%fd259, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd256, %fd252, %fd29, %fd259;
	// inline asm
	mov.f64 	%fd1422, %fd256;
	bra.uni 	BB12_98;

BB12_97:
	mov.f64 	%fd261, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd263, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd260, %fd261, %fd29, %fd263;
	// inline asm
	mov.f64 	%fd267, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd264, %fd260, %fd29, %fd267;
	// inline asm
	mov.f64 	%fd271, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd268, %fd264, %fd29, %fd271;
	// inline asm
	mov.f64 	%fd275, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd272, %fd268, %fd29, %fd275;
	// inline asm
	mov.f64 	%fd279, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd276, %fd272, %fd29, %fd279;
	// inline asm
	mul.rn.f64 	%fd281, %fd276, %fd29;
	// inline asm
	fma.rn.f64 	%fd280, %fd281, %fd1421, %fd1421;
	// inline asm
	mov.f64 	%fd1422, %fd280;

BB12_98:
	and.b32  	%r387, %r787, 2;
	setp.eq.s32 	%p87, %r387, 0;
	neg.f64 	%fd284, %fd1422;
	selp.f64 	%fd1423, %fd1422, %fd284, %p87;
	bra.uni 	BB12_100;

BB12_99:
	mov.f64 	%fd285, 0d0000000000000000;
	mul.rn.f64 	%fd1423, %fd15, %fd285;

BB12_100:
	neg.f64 	%fd286, %fd1423;
	mov.f64 	%fd1478, %fd1420;
	mov.f64 	%fd1479, %fd286;
	ld.param.u32 	%r771, [DIT7C2CM_param_4];
	setp.eq.s32 	%p7, %r771, 0;
	@%p7 bra 	BB12_101;
	bra.uni 	BB12_102;

BB12_101:
	mov.f64 	%fd1478, %fd1420;
	mov.f64 	%fd1479, %fd1423;

BB12_102:
	mul.f64 	%fd288, %fd1, %fd1478;
	neg.f64 	%fd290, %fd2;
	fma.rn.f64 	%fd291, %fd290, %fd1479, %fd288;
	mul.f64 	%fd292, %fd2, %fd1478;
	fma.rn.f64 	%fd293, %fd1, %fd1479, %fd292;
	mul.f64 	%fd294, %fd13, 0d402921FB54442D18;
	div.rn.f64 	%fd36, %fd294, %fd14;
	setp.eq.f64 	%p8, %fd36, 0d7FF0000000000000;
	setp.eq.f64 	%p88, %fd36, 0dFFF0000000000000;
	or.pred  	%p89, %p8, %p88;
	@%p89 bra 	BB12_125;

	// inline asm
	abs.f64 	%fd295, %fd36;
	// inline asm
	setp.gt.f64 	%p90, %fd295, 0d41E0000000000000;
	@%p90 bra 	BB12_105;

	mov.f64 	%fd310, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd297, %fd36, %fd310;
	// inline asm
	cvt.rni.s32.f64 	%r388, %fd297;
	// inline asm
	cvt.rn.f64.s32 	%fd311, %r388;
	neg.f64 	%fd307, %fd311;
	mov.f64 	%fd300, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd298, %fd307, %fd300, %fd36;
	// inline asm
	mov.f64 	%fd304, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd302, %fd307, %fd304, %fd298;
	// inline asm
	mov.f64 	%fd308, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd306, %fd307, %fd308, %fd302;
	// inline asm
	mov.u32 	%r790, %r388;
	mov.f64 	%fd1424, %fd306;
	bra.uni 	BB12_121;

BB12_105:
	mov.b64 	 %rl471, %fd36;
	and.b64  	%rl909, %rl471, -9223372036854775808;
	shr.u64 	%rl472, %rl471, 52;
	and.b64  	%rl473, %rl472, 2047;
	add.s64 	%rl474, %rl473, 4294966272;
	cvt.u32.u64 	%r80, %rl474;
	shl.b64 	%rl475, %rl471, 11;
	or.b64  	%rl64, %rl475, -9223372036854775808;
	shr.u32 	%r392, %r80, 6;
	mov.u32 	%r393, 16;
	sub.s32 	%r81, %r393, %r392;
	mov.u32 	%r394, 15;
	sub.s32 	%r788, %r394, %r392;
	mov.u32 	%r395, 19;
	sub.s32 	%r83, %r395, %r392;
	mov.u32 	%r390, 18;
	// inline asm
	min.s32 	%r389, %r390, %r83;
	// inline asm
	setp.lt.s32 	%p91, %r788, %r389;
	@%p91 bra 	BB12_107;

	mov.u64 	%rl906, 0;
	bra.uni 	BB12_109;

BB12_107:
	mov.u32 	%r396, 1;
	sub.s32 	%r84, %r396, %r81;
	mov.u64 	%rl906, 0;

BB12_108:
	.pragma "nounroll";
	shl.b32 	%r400, %r788, 3;
	mov.u32 	%r401, __internal_i2opi_d;
	add.s32 	%r402, %r401, %r400;
	ld.const.u64 	%rl479, [%r402];
	mul.lo.s64 	%rl481, %rl479, %rl64;
	// inline asm
	mul.hi.u64 	%rl478, %rl479, %rl64;
	// inline asm
	mad.lo.s64 	%rl482, %rl479, %rl64, %rl906;
	setp.lt.u64 	%p92, %rl482, %rl481;
	selp.u64 	%rl483, 1, 0, %p92;
	add.s64 	%rl906, %rl483, %rl478;
	add.s32 	%r403, %r84, %r788;
	shl.b32 	%r404, %r403, 3;
	add.s32 	%r406, %r45, %r404;
	st.local.u64 	[%r406], %rl482;
	// inline asm
	min.s32 	%r397, %r390, %r83;
	// inline asm
	add.s32 	%r788, %r788, 1;
	setp.lt.s32 	%p93, %r788, %r397;
	@%p93 bra 	BB12_108;

BB12_109:
	mov.u32 	%r407, 1;
	sub.s32 	%r408, %r407, %r81;
	add.s32 	%r409, %r408, %r788;
	shl.b32 	%r410, %r409, 3;
	add.s32 	%r412, %r45, %r410;
	st.local.u64 	[%r412], %rl906;
	ld.local.u64 	%rl907, [%r45+24];
	ld.local.u64 	%rl908, [%r45+16];
	and.b32  	%r413, %r80, 63;
	setp.eq.s32 	%p94, %r413, 0;
	@%p94 bra 	BB12_111;

	and.b64  	%rl486, %rl472, 63;
	cvt.u32.u64 	%r414, %rl486;
	shl.b64 	%rl487, %rl907, %r414;
	neg.s32 	%r415, %r80;
	and.b32  	%r416, %r415, 63;
	shr.u64 	%rl488, %rl908, %r416;
	or.b64  	%rl907, %rl488, %rl487;
	shl.b64 	%rl489, %rl908, %r414;
	ld.local.u64 	%rl490, [%r45+8];
	shr.u64 	%rl491, %rl490, %r416;
	or.b64  	%rl908, %rl491, %rl489;

BB12_111:
	shr.u64 	%rl492, %rl907, 62;
	cvt.u32.u64 	%r417, %rl492;
	shr.u64 	%rl493, %rl908, 62;
	shl.b64 	%rl494, %rl907, 2;
	or.b64  	%rl913, %rl493, %rl494;
	shl.b64 	%rl75, %rl908, 2;
	setp.ne.s64 	%p95, %rl75, 0;
	selp.u64 	%rl495, 1, 0, %p95;
	or.b64  	%rl496, %rl495, %rl913;
	setp.gt.u64 	%p96, %rl496, -9223372036854775808;
	selp.u32 	%r418, 1, 0, %p96;
	add.s32 	%r419, %r418, %r417;
	setp.lt.s64 	%p97, %rl471, 0;
	neg.s32 	%r420, %r419;
	selp.b32 	%r790, %r420, %r419, %p97;
	@%p96 bra 	BB12_113;

	mov.u64 	%rl912, %rl75;
	bra.uni 	BB12_114;

BB12_113:
	not.b64 	%rl498, %rl913;
	neg.s64 	%rl76, %rl75;
	setp.eq.s64 	%p98, %rl75, 0;
	selp.u64 	%rl499, 1, 0, %p98;
	add.s64 	%rl913, %rl499, %rl498;
	xor.b64  	%rl909, %rl909, -9223372036854775808;
	mov.u64 	%rl912, %rl76;

BB12_114:
	mov.u64 	%rl911, %rl912;
	setp.gt.s64 	%p99, %rl913, 0;
	@%p99 bra 	BB12_116;

	mov.u32 	%r789, 0;
	bra.uni 	BB12_118;

BB12_116:
	mov.u32 	%r789, 0;

BB12_117:
	shr.u64 	%rl500, %rl911, 63;
	shl.b64 	%rl501, %rl913, 1;
	or.b64  	%rl913, %rl500, %rl501;
	shl.b64 	%rl911, %rl911, 1;
	add.s32 	%r789, %r789, -1;
	setp.gt.s64 	%p100, %rl913, 0;
	@%p100 bra 	BB12_117;

BB12_118:
	mul.lo.s64 	%rl915, %rl913, -3958705157555305931;
	mov.u64 	%rl504, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl502, %rl913, %rl504;
	// inline asm
	setp.gt.s64 	%p101, %rl502, 0;
	mov.u64 	%rl914, %rl502;
	@%p101 bra 	BB12_119;
	bra.uni 	BB12_120;

BB12_119:
	shl.b64 	%rl505, %rl502, 1;
	shr.u64 	%rl506, %rl915, 63;
	or.b64  	%rl914, %rl505, %rl506;
	mul.lo.s64 	%rl915, %rl913, -7917410315110611862;
	add.s32 	%r789, %r789, -1;

BB12_120:
	setp.ne.s64 	%p102, %rl915, 0;
	selp.u64 	%rl507, 1, 0, %p102;
	add.s64 	%rl508, %rl507, %rl914;
	add.s32 	%r423, %r789, 1022;
	cvt.u64.u32 	%rl509, %r423;
	shl.b64 	%rl510, %rl509, 52;
	shr.u64 	%rl511, %rl508, 11;
	shr.u64 	%rl512, %rl508, 10;
	and.b64  	%rl513, %rl512, 1;
	add.s64 	%rl514, %rl510, %rl511;
	add.s64 	%rl515, %rl514, %rl513;
	or.b64  	%rl516, %rl515, %rl909;
	mov.b64 	 %fd1424, %rl516;

BB12_121:
	add.s32 	%r95, %r790, 1;
	and.b32  	%r424, %r95, 1;
	setp.eq.s32 	%p103, %r424, 0;
	mul.rn.f64 	%fd40, %fd1424, %fd1424;
	@%p103 bra 	BB12_123;

	mov.f64 	%fd313, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd315, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd312, %fd313, %fd40, %fd315;
	// inline asm
	mov.f64 	%fd319, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd316, %fd312, %fd40, %fd319;
	// inline asm
	mov.f64 	%fd323, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd320, %fd316, %fd40, %fd323;
	// inline asm
	mov.f64 	%fd327, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd324, %fd320, %fd40, %fd327;
	// inline asm
	mov.f64 	%fd331, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd328, %fd324, %fd40, %fd331;
	// inline asm
	mov.f64 	%fd335, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd332, %fd328, %fd40, %fd335;
	// inline asm
	mov.f64 	%fd339, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd336, %fd332, %fd40, %fd339;
	// inline asm
	mov.f64 	%fd1425, %fd336;
	bra.uni 	BB12_124;

BB12_123:
	mov.f64 	%fd341, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd343, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd340, %fd341, %fd40, %fd343;
	// inline asm
	mov.f64 	%fd347, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd344, %fd340, %fd40, %fd347;
	// inline asm
	mov.f64 	%fd351, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd348, %fd344, %fd40, %fd351;
	// inline asm
	mov.f64 	%fd355, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd352, %fd348, %fd40, %fd355;
	// inline asm
	mov.f64 	%fd359, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd356, %fd352, %fd40, %fd359;
	// inline asm
	mul.rn.f64 	%fd361, %fd356, %fd40;
	// inline asm
	fma.rn.f64 	%fd360, %fd361, %fd1424, %fd1424;
	// inline asm
	mov.f64 	%fd1425, %fd360;

BB12_124:
	and.b32  	%r425, %r95, 2;
	setp.eq.s32 	%p104, %r425, 0;
	neg.f64 	%fd364, %fd1425;
	selp.f64 	%fd1426, %fd1425, %fd364, %p104;
	bra.uni 	BB12_126;

BB12_125:
	mov.f64 	%fd1426, 0dFFF8000000000000;

BB12_126:
	setp.eq.f64 	%p106, %fd36, 0d0000000000000000;
	or.pred  	%p107, %p88, %p106;
	or.pred  	%p108, %p8, %p107;
	@%p108 bra 	BB12_149;

	// inline asm
	abs.f64 	%fd366, %fd36;
	// inline asm
	setp.gt.f64 	%p109, %fd366, 0d41E0000000000000;
	@%p109 bra 	BB12_129;

	mov.f64 	%fd381, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd368, %fd36, %fd381;
	// inline asm
	cvt.rni.s32.f64 	%r426, %fd368;
	// inline asm
	cvt.rn.f64.s32 	%fd382, %r426;
	neg.f64 	%fd378, %fd382;
	mov.f64 	%fd371, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd369, %fd378, %fd371, %fd36;
	// inline asm
	mov.f64 	%fd375, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd373, %fd378, %fd375, %fd369;
	// inline asm
	mov.f64 	%fd379, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd377, %fd378, %fd379, %fd373;
	// inline asm
	mov.u32 	%r793, %r426;
	mov.f64 	%fd1427, %fd377;
	bra.uni 	BB12_145;

BB12_129:
	mov.b64 	 %rl93, %fd36;
	and.b64  	%rl919, %rl93, -9223372036854775808;
	shr.u64 	%rl95, %rl93, 52;
	and.b64  	%rl517, %rl95, 2047;
	add.s64 	%rl518, %rl517, 4294966272;
	cvt.u32.u64 	%r97, %rl518;
	shl.b64 	%rl519, %rl93, 11;
	or.b64  	%rl96, %rl519, -9223372036854775808;
	shr.u32 	%r430, %r97, 6;
	mov.u32 	%r431, 16;
	sub.s32 	%r98, %r431, %r430;
	mov.u32 	%r432, 15;
	sub.s32 	%r791, %r432, %r430;
	mov.u32 	%r433, 19;
	sub.s32 	%r100, %r433, %r430;
	mov.u32 	%r428, 18;
	// inline asm
	min.s32 	%r427, %r428, %r100;
	// inline asm
	setp.lt.s32 	%p110, %r791, %r427;
	@%p110 bra 	BB12_131;

	mov.u64 	%rl916, 0;
	bra.uni 	BB12_133;

BB12_131:
	mov.u32 	%r434, 1;
	sub.s32 	%r101, %r434, %r98;
	mov.u64 	%rl916, 0;

BB12_132:
	.pragma "nounroll";
	shl.b32 	%r438, %r791, 3;
	mov.u32 	%r439, __internal_i2opi_d;
	add.s32 	%r440, %r439, %r438;
	ld.const.u64 	%rl523, [%r440];
	mul.lo.s64 	%rl525, %rl523, %rl96;
	// inline asm
	mul.hi.u64 	%rl522, %rl523, %rl96;
	// inline asm
	mad.lo.s64 	%rl526, %rl523, %rl96, %rl916;
	setp.lt.u64 	%p111, %rl526, %rl525;
	selp.u64 	%rl527, 1, 0, %p111;
	add.s64 	%rl916, %rl527, %rl522;
	add.s32 	%r441, %r101, %r791;
	shl.b32 	%r442, %r441, 3;
	add.s32 	%r444, %r45, %r442;
	st.local.u64 	[%r444], %rl526;
	// inline asm
	min.s32 	%r435, %r428, %r100;
	// inline asm
	add.s32 	%r791, %r791, 1;
	setp.lt.s32 	%p112, %r791, %r435;
	@%p112 bra 	BB12_132;

BB12_133:
	mov.u32 	%r445, 1;
	sub.s32 	%r446, %r445, %r98;
	add.s32 	%r447, %r446, %r791;
	shl.b32 	%r448, %r447, 3;
	add.s32 	%r450, %r45, %r448;
	st.local.u64 	[%r450], %rl916;
	ld.local.u64 	%rl917, [%r45+24];
	ld.local.u64 	%rl918, [%r45+16];
	and.b32  	%r451, %r97, 63;
	setp.eq.s32 	%p113, %r451, 0;
	@%p113 bra 	BB12_135;

	and.b64  	%rl528, %rl95, 63;
	cvt.u32.u64 	%r452, %rl528;
	shl.b64 	%rl529, %rl917, %r452;
	neg.s32 	%r453, %r97;
	and.b32  	%r454, %r453, 63;
	shr.u64 	%rl530, %rl918, %r454;
	or.b64  	%rl917, %rl530, %rl529;
	shl.b64 	%rl531, %rl918, %r452;
	ld.local.u64 	%rl532, [%r45+8];
	shr.u64 	%rl533, %rl532, %r454;
	or.b64  	%rl918, %rl533, %rl531;

BB12_135:
	shr.u64 	%rl534, %rl917, 62;
	cvt.u32.u64 	%r455, %rl534;
	shr.u64 	%rl535, %rl918, 62;
	shl.b64 	%rl536, %rl917, 2;
	or.b64  	%rl923, %rl535, %rl536;
	shl.b64 	%rl107, %rl918, 2;
	setp.ne.s64 	%p114, %rl107, 0;
	selp.u64 	%rl537, 1, 0, %p114;
	or.b64  	%rl538, %rl537, %rl923;
	setp.gt.u64 	%p115, %rl538, -9223372036854775808;
	selp.u32 	%r456, 1, 0, %p115;
	add.s32 	%r457, %r456, %r455;
	neg.s32 	%r458, %r457;
	setp.lt.s64 	%p116, %rl93, 0;
	selp.b32 	%r793, %r458, %r457, %p116;
	@%p115 bra 	BB12_137;

	mov.u64 	%rl922, %rl107;
	bra.uni 	BB12_138;

BB12_137:
	not.b64 	%rl539, %rl923;
	neg.s64 	%rl108, %rl107;
	setp.eq.s64 	%p117, %rl107, 0;
	selp.u64 	%rl540, 1, 0, %p117;
	add.s64 	%rl923, %rl540, %rl539;
	xor.b64  	%rl919, %rl919, -9223372036854775808;
	mov.u64 	%rl922, %rl108;

BB12_138:
	mov.u64 	%rl921, %rl922;
	setp.gt.s64 	%p118, %rl923, 0;
	@%p118 bra 	BB12_140;

	mov.u32 	%r792, 0;
	bra.uni 	BB12_142;

BB12_140:
	mov.u32 	%r792, 0;

BB12_141:
	shr.u64 	%rl541, %rl921, 63;
	shl.b64 	%rl542, %rl923, 1;
	or.b64  	%rl923, %rl541, %rl542;
	shl.b64 	%rl921, %rl921, 1;
	add.s32 	%r792, %r792, -1;
	setp.gt.s64 	%p119, %rl923, 0;
	@%p119 bra 	BB12_141;

BB12_142:
	mul.lo.s64 	%rl925, %rl923, -3958705157555305931;
	mov.u64 	%rl545, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl543, %rl923, %rl545;
	// inline asm
	setp.gt.s64 	%p120, %rl543, 0;
	mov.u64 	%rl924, %rl543;
	@%p120 bra 	BB12_143;
	bra.uni 	BB12_144;

BB12_143:
	shl.b64 	%rl546, %rl543, 1;
	shr.u64 	%rl547, %rl925, 63;
	or.b64  	%rl924, %rl546, %rl547;
	mul.lo.s64 	%rl925, %rl923, -7917410315110611862;
	add.s32 	%r792, %r792, -1;

BB12_144:
	setp.ne.s64 	%p121, %rl925, 0;
	selp.u64 	%rl548, 1, 0, %p121;
	add.s64 	%rl549, %rl548, %rl924;
	add.s32 	%r461, %r792, 1022;
	cvt.u64.u32 	%rl550, %r461;
	shl.b64 	%rl551, %rl550, 52;
	shr.u64 	%rl552, %rl549, 11;
	shr.u64 	%rl553, %rl549, 10;
	and.b64  	%rl554, %rl553, 1;
	add.s64 	%rl555, %rl551, %rl552;
	add.s64 	%rl556, %rl555, %rl554;
	or.b64  	%rl557, %rl556, %rl919;
	mov.b64 	 %fd1427, %rl557;

BB12_145:
	and.b32  	%r462, %r793, 1;
	setp.eq.s32 	%p122, %r462, 0;
	mul.rn.f64 	%fd50, %fd1427, %fd1427;
	@%p122 bra 	BB12_147;

	mov.f64 	%fd384, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd386, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd383, %fd384, %fd50, %fd386;
	// inline asm
	mov.f64 	%fd390, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd387, %fd383, %fd50, %fd390;
	// inline asm
	mov.f64 	%fd394, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd391, %fd387, %fd50, %fd394;
	// inline asm
	mov.f64 	%fd398, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd395, %fd391, %fd50, %fd398;
	// inline asm
	mov.f64 	%fd402, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd399, %fd395, %fd50, %fd402;
	// inline asm
	mov.f64 	%fd406, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd403, %fd399, %fd50, %fd406;
	// inline asm
	mov.f64 	%fd410, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd407, %fd403, %fd50, %fd410;
	// inline asm
	mov.f64 	%fd1428, %fd407;
	bra.uni 	BB12_148;

BB12_147:
	mov.f64 	%fd412, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd414, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd411, %fd412, %fd50, %fd414;
	// inline asm
	mov.f64 	%fd418, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd415, %fd411, %fd50, %fd418;
	// inline asm
	mov.f64 	%fd422, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd419, %fd415, %fd50, %fd422;
	// inline asm
	mov.f64 	%fd426, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd423, %fd419, %fd50, %fd426;
	// inline asm
	mov.f64 	%fd430, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd427, %fd423, %fd50, %fd430;
	// inline asm
	mul.rn.f64 	%fd432, %fd427, %fd50;
	// inline asm
	fma.rn.f64 	%fd431, %fd432, %fd1427, %fd1427;
	// inline asm
	mov.f64 	%fd1428, %fd431;

BB12_148:
	and.b32  	%r463, %r793, 2;
	setp.eq.s32 	%p123, %r463, 0;
	neg.f64 	%fd435, %fd1428;
	selp.f64 	%fd1429, %fd1428, %fd435, %p123;
	bra.uni 	BB12_150;

BB12_149:
	mov.f64 	%fd436, 0d0000000000000000;
	mul.rn.f64 	%fd1429, %fd36, %fd436;

BB12_150:
	neg.f64 	%fd437, %fd1429;
	mov.f64 	%fd1476, %fd1426;
	mov.f64 	%fd1477, %fd437;
	@%p7 bra 	BB12_151;
	bra.uni 	BB12_152;

BB12_151:
	mov.f64 	%fd1476, %fd1426;
	mov.f64 	%fd1477, %fd1429;

BB12_152:
	mul.f64 	%fd439, %fd3, %fd1476;
	neg.f64 	%fd441, %fd4;
	fma.rn.f64 	%fd442, %fd441, %fd1477, %fd439;
	mul.f64 	%fd443, %fd4, %fd1476;
	fma.rn.f64 	%fd444, %fd3, %fd1477, %fd443;
	mul.f64 	%fd445, %fd13, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd57, %fd445, %fd14;
	setp.eq.f64 	%p9, %fd57, 0d7FF0000000000000;
	setp.eq.f64 	%p10, %fd57, 0dFFF0000000000000;
	or.pred  	%p124, %p9, %p10;
	@%p124 bra 	BB12_175;

	// inline asm
	abs.f64 	%fd446, %fd57;
	// inline asm
	setp.gt.f64 	%p125, %fd446, 0d41E0000000000000;
	@%p125 bra 	BB12_155;

	mov.f64 	%fd461, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd448, %fd57, %fd461;
	// inline asm
	cvt.rni.s32.f64 	%r464, %fd448;
	// inline asm
	cvt.rn.f64.s32 	%fd462, %r464;
	neg.f64 	%fd458, %fd462;
	mov.f64 	%fd451, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd449, %fd458, %fd451, %fd57;
	// inline asm
	mov.f64 	%fd455, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd453, %fd458, %fd455, %fd449;
	// inline asm
	mov.f64 	%fd459, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd457, %fd458, %fd459, %fd453;
	// inline asm
	mov.u32 	%r796, %r464;
	mov.f64 	%fd1430, %fd457;
	bra.uni 	BB12_171;

BB12_155:
	mov.b64 	 %rl125, %fd57;
	and.b64  	%rl929, %rl125, -9223372036854775808;
	shr.u64 	%rl127, %rl125, 52;
	and.b64  	%rl558, %rl127, 2047;
	add.s64 	%rl559, %rl558, 4294966272;
	cvt.u32.u64 	%r113, %rl559;
	shl.b64 	%rl560, %rl125, 11;
	or.b64  	%rl128, %rl560, -9223372036854775808;
	shr.u32 	%r468, %r113, 6;
	mov.u32 	%r469, 16;
	sub.s32 	%r114, %r469, %r468;
	mov.u32 	%r470, 15;
	sub.s32 	%r794, %r470, %r468;
	mov.u32 	%r471, 19;
	sub.s32 	%r116, %r471, %r468;
	mov.u32 	%r466, 18;
	// inline asm
	min.s32 	%r465, %r466, %r116;
	// inline asm
	setp.lt.s32 	%p126, %r794, %r465;
	@%p126 bra 	BB12_157;

	mov.u64 	%rl926, 0;
	bra.uni 	BB12_159;

BB12_157:
	mov.u32 	%r472, 1;
	sub.s32 	%r117, %r472, %r114;
	mov.u64 	%rl926, 0;

BB12_158:
	.pragma "nounroll";
	shl.b32 	%r476, %r794, 3;
	mov.u32 	%r477, __internal_i2opi_d;
	add.s32 	%r478, %r477, %r476;
	ld.const.u64 	%rl564, [%r478];
	mul.lo.s64 	%rl566, %rl564, %rl128;
	// inline asm
	mul.hi.u64 	%rl563, %rl564, %rl128;
	// inline asm
	mad.lo.s64 	%rl567, %rl564, %rl128, %rl926;
	setp.lt.u64 	%p127, %rl567, %rl566;
	selp.u64 	%rl568, 1, 0, %p127;
	add.s64 	%rl926, %rl568, %rl563;
	add.s32 	%r479, %r117, %r794;
	shl.b32 	%r480, %r479, 3;
	add.s32 	%r482, %r45, %r480;
	st.local.u64 	[%r482], %rl567;
	// inline asm
	min.s32 	%r473, %r466, %r116;
	// inline asm
	add.s32 	%r794, %r794, 1;
	setp.lt.s32 	%p128, %r794, %r473;
	@%p128 bra 	BB12_158;

BB12_159:
	mov.u32 	%r483, 1;
	sub.s32 	%r484, %r483, %r114;
	add.s32 	%r485, %r484, %r794;
	shl.b32 	%r486, %r485, 3;
	add.s32 	%r488, %r45, %r486;
	st.local.u64 	[%r488], %rl926;
	ld.local.u64 	%rl927, [%r45+24];
	ld.local.u64 	%rl928, [%r45+16];
	and.b32  	%r489, %r113, 63;
	setp.eq.s32 	%p129, %r489, 0;
	@%p129 bra 	BB12_161;

	and.b64  	%rl569, %rl127, 63;
	cvt.u32.u64 	%r490, %rl569;
	shl.b64 	%rl570, %rl927, %r490;
	neg.s32 	%r491, %r113;
	and.b32  	%r492, %r491, 63;
	shr.u64 	%rl571, %rl928, %r492;
	or.b64  	%rl927, %rl571, %rl570;
	shl.b64 	%rl572, %rl928, %r490;
	ld.local.u64 	%rl573, [%r45+8];
	shr.u64 	%rl574, %rl573, %r492;
	or.b64  	%rl928, %rl574, %rl572;

BB12_161:
	shr.u64 	%rl575, %rl927, 62;
	cvt.u32.u64 	%r493, %rl575;
	shr.u64 	%rl576, %rl928, 62;
	shl.b64 	%rl577, %rl927, 2;
	or.b64  	%rl933, %rl576, %rl577;
	shl.b64 	%rl139, %rl928, 2;
	setp.ne.s64 	%p130, %rl139, 0;
	selp.u64 	%rl578, 1, 0, %p130;
	or.b64  	%rl579, %rl578, %rl933;
	setp.gt.u64 	%p131, %rl579, -9223372036854775808;
	selp.u32 	%r494, 1, 0, %p131;
	add.s32 	%r495, %r494, %r493;
	neg.s32 	%r496, %r495;
	setp.lt.s64 	%p132, %rl125, 0;
	selp.b32 	%r796, %r496, %r495, %p132;
	@%p131 bra 	BB12_163;

	mov.u64 	%rl932, %rl139;
	bra.uni 	BB12_164;

BB12_163:
	not.b64 	%rl580, %rl933;
	neg.s64 	%rl140, %rl139;
	setp.eq.s64 	%p133, %rl139, 0;
	selp.u64 	%rl581, 1, 0, %p133;
	add.s64 	%rl933, %rl581, %rl580;
	xor.b64  	%rl929, %rl929, -9223372036854775808;
	mov.u64 	%rl932, %rl140;

BB12_164:
	mov.u64 	%rl931, %rl932;
	setp.gt.s64 	%p134, %rl933, 0;
	@%p134 bra 	BB12_166;

	mov.u32 	%r795, 0;
	bra.uni 	BB12_168;

BB12_166:
	mov.u32 	%r795, 0;

BB12_167:
	shr.u64 	%rl582, %rl931, 63;
	shl.b64 	%rl583, %rl933, 1;
	or.b64  	%rl933, %rl582, %rl583;
	shl.b64 	%rl931, %rl931, 1;
	add.s32 	%r795, %r795, -1;
	setp.gt.s64 	%p135, %rl933, 0;
	@%p135 bra 	BB12_167;

BB12_168:
	mul.lo.s64 	%rl935, %rl933, -3958705157555305931;
	mov.u64 	%rl586, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl584, %rl933, %rl586;
	// inline asm
	setp.gt.s64 	%p136, %rl584, 0;
	mov.u64 	%rl934, %rl584;
	@%p136 bra 	BB12_169;
	bra.uni 	BB12_170;

BB12_169:
	shl.b64 	%rl587, %rl584, 1;
	shr.u64 	%rl588, %rl935, 63;
	or.b64  	%rl934, %rl587, %rl588;
	mul.lo.s64 	%rl935, %rl933, -7917410315110611862;
	add.s32 	%r795, %r795, -1;

BB12_170:
	setp.ne.s64 	%p137, %rl935, 0;
	selp.u64 	%rl589, 1, 0, %p137;
	add.s64 	%rl590, %rl589, %rl934;
	add.s32 	%r499, %r795, 1022;
	cvt.u64.u32 	%rl591, %r499;
	shl.b64 	%rl592, %rl591, 52;
	shr.u64 	%rl593, %rl590, 11;
	shr.u64 	%rl594, %rl590, 10;
	and.b64  	%rl595, %rl594, 1;
	add.s64 	%rl596, %rl592, %rl593;
	add.s64 	%rl597, %rl596, %rl595;
	or.b64  	%rl598, %rl597, %rl929;
	mov.b64 	 %fd1430, %rl598;

BB12_171:
	add.s32 	%r128, %r796, 1;
	and.b32  	%r500, %r128, 1;
	setp.eq.s32 	%p138, %r500, 0;
	mul.rn.f64 	%fd61, %fd1430, %fd1430;
	@%p138 bra 	BB12_173;

	mov.f64 	%fd464, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd466, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd463, %fd464, %fd61, %fd466;
	// inline asm
	mov.f64 	%fd470, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd467, %fd463, %fd61, %fd470;
	// inline asm
	mov.f64 	%fd474, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd471, %fd467, %fd61, %fd474;
	// inline asm
	mov.f64 	%fd478, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd475, %fd471, %fd61, %fd478;
	// inline asm
	mov.f64 	%fd482, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd479, %fd475, %fd61, %fd482;
	// inline asm
	mov.f64 	%fd486, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd483, %fd479, %fd61, %fd486;
	// inline asm
	mov.f64 	%fd490, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd487, %fd483, %fd61, %fd490;
	// inline asm
	mov.f64 	%fd1431, %fd487;
	bra.uni 	BB12_174;

BB12_173:
	mov.f64 	%fd492, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd494, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd491, %fd492, %fd61, %fd494;
	// inline asm
	mov.f64 	%fd498, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd495, %fd491, %fd61, %fd498;
	// inline asm
	mov.f64 	%fd502, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd499, %fd495, %fd61, %fd502;
	// inline asm
	mov.f64 	%fd506, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd503, %fd499, %fd61, %fd506;
	// inline asm
	mov.f64 	%fd510, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd507, %fd503, %fd61, %fd510;
	// inline asm
	mul.rn.f64 	%fd512, %fd507, %fd61;
	// inline asm
	fma.rn.f64 	%fd511, %fd512, %fd1430, %fd1430;
	// inline asm
	mov.f64 	%fd1431, %fd511;

BB12_174:
	and.b32  	%r501, %r128, 2;
	setp.eq.s32 	%p139, %r501, 0;
	neg.f64 	%fd515, %fd1431;
	selp.f64 	%fd1432, %fd1431, %fd515, %p139;
	bra.uni 	BB12_176;

BB12_175:
	mov.f64 	%fd1432, 0dFFF8000000000000;

BB12_176:
	setp.eq.f64 	%p140, %fd57, 0d0000000000000000;
	or.pred  	%p141, %p10, %p140;
	or.pred  	%p142, %p9, %p141;
	@%p142 bra 	BB12_199;

	// inline asm
	abs.f64 	%fd517, %fd57;
	// inline asm
	setp.gt.f64 	%p143, %fd517, 0d41E0000000000000;
	@%p143 bra 	BB12_179;

	mov.f64 	%fd532, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd519, %fd57, %fd532;
	// inline asm
	cvt.rni.s32.f64 	%r502, %fd519;
	// inline asm
	cvt.rn.f64.s32 	%fd533, %r502;
	neg.f64 	%fd529, %fd533;
	mov.f64 	%fd522, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd520, %fd529, %fd522, %fd57;
	// inline asm
	mov.f64 	%fd526, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd524, %fd529, %fd526, %fd520;
	// inline asm
	mov.f64 	%fd530, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd528, %fd529, %fd530, %fd524;
	// inline asm
	mov.u32 	%r799, %r502;
	mov.f64 	%fd1433, %fd528;
	bra.uni 	BB12_195;

BB12_179:
	mov.b64 	 %rl157, %fd57;
	and.b64  	%rl939, %rl157, -9223372036854775808;
	shr.u64 	%rl159, %rl157, 52;
	and.b64  	%rl599, %rl159, 2047;
	add.s64 	%rl600, %rl599, 4294966272;
	cvt.u32.u64 	%r130, %rl600;
	shl.b64 	%rl601, %rl157, 11;
	or.b64  	%rl160, %rl601, -9223372036854775808;
	shr.u32 	%r506, %r130, 6;
	mov.u32 	%r507, 16;
	sub.s32 	%r131, %r507, %r506;
	mov.u32 	%r508, 15;
	sub.s32 	%r797, %r508, %r506;
	mov.u32 	%r509, 19;
	sub.s32 	%r133, %r509, %r506;
	mov.u32 	%r504, 18;
	// inline asm
	min.s32 	%r503, %r504, %r133;
	// inline asm
	setp.lt.s32 	%p144, %r797, %r503;
	@%p144 bra 	BB12_181;

	mov.u64 	%rl936, 0;
	bra.uni 	BB12_183;

BB12_181:
	mov.u32 	%r510, 1;
	sub.s32 	%r134, %r510, %r131;
	mov.u64 	%rl936, 0;

BB12_182:
	.pragma "nounroll";
	shl.b32 	%r514, %r797, 3;
	mov.u32 	%r515, __internal_i2opi_d;
	add.s32 	%r516, %r515, %r514;
	ld.const.u64 	%rl605, [%r516];
	mul.lo.s64 	%rl607, %rl605, %rl160;
	// inline asm
	mul.hi.u64 	%rl604, %rl605, %rl160;
	// inline asm
	mad.lo.s64 	%rl608, %rl605, %rl160, %rl936;
	setp.lt.u64 	%p145, %rl608, %rl607;
	selp.u64 	%rl609, 1, 0, %p145;
	add.s64 	%rl936, %rl609, %rl604;
	add.s32 	%r517, %r134, %r797;
	shl.b32 	%r518, %r517, 3;
	add.s32 	%r520, %r45, %r518;
	st.local.u64 	[%r520], %rl608;
	// inline asm
	min.s32 	%r511, %r504, %r133;
	// inline asm
	add.s32 	%r797, %r797, 1;
	setp.lt.s32 	%p146, %r797, %r511;
	@%p146 bra 	BB12_182;

BB12_183:
	mov.u32 	%r521, 1;
	sub.s32 	%r522, %r521, %r131;
	add.s32 	%r523, %r522, %r797;
	shl.b32 	%r524, %r523, 3;
	add.s32 	%r526, %r45, %r524;
	st.local.u64 	[%r526], %rl936;
	ld.local.u64 	%rl937, [%r45+24];
	ld.local.u64 	%rl938, [%r45+16];
	and.b32  	%r527, %r130, 63;
	setp.eq.s32 	%p147, %r527, 0;
	@%p147 bra 	BB12_185;

	and.b64  	%rl610, %rl159, 63;
	cvt.u32.u64 	%r528, %rl610;
	shl.b64 	%rl611, %rl937, %r528;
	neg.s32 	%r529, %r130;
	and.b32  	%r530, %r529, 63;
	shr.u64 	%rl612, %rl938, %r530;
	or.b64  	%rl937, %rl612, %rl611;
	shl.b64 	%rl613, %rl938, %r528;
	ld.local.u64 	%rl614, [%r45+8];
	shr.u64 	%rl615, %rl614, %r530;
	or.b64  	%rl938, %rl615, %rl613;

BB12_185:
	shr.u64 	%rl616, %rl937, 62;
	cvt.u32.u64 	%r531, %rl616;
	shr.u64 	%rl617, %rl938, 62;
	shl.b64 	%rl618, %rl937, 2;
	or.b64  	%rl943, %rl617, %rl618;
	shl.b64 	%rl171, %rl938, 2;
	setp.ne.s64 	%p148, %rl171, 0;
	selp.u64 	%rl619, 1, 0, %p148;
	or.b64  	%rl620, %rl619, %rl943;
	setp.gt.u64 	%p149, %rl620, -9223372036854775808;
	selp.u32 	%r532, 1, 0, %p149;
	add.s32 	%r533, %r532, %r531;
	neg.s32 	%r534, %r533;
	setp.lt.s64 	%p150, %rl157, 0;
	selp.b32 	%r799, %r534, %r533, %p150;
	@%p149 bra 	BB12_187;

	mov.u64 	%rl942, %rl171;
	bra.uni 	BB12_188;

BB12_187:
	not.b64 	%rl621, %rl943;
	neg.s64 	%rl172, %rl171;
	setp.eq.s64 	%p151, %rl171, 0;
	selp.u64 	%rl622, 1, 0, %p151;
	add.s64 	%rl943, %rl622, %rl621;
	xor.b64  	%rl939, %rl939, -9223372036854775808;
	mov.u64 	%rl942, %rl172;

BB12_188:
	mov.u64 	%rl941, %rl942;
	setp.gt.s64 	%p152, %rl943, 0;
	@%p152 bra 	BB12_190;

	mov.u32 	%r798, 0;
	bra.uni 	BB12_192;

BB12_190:
	mov.u32 	%r798, 0;

BB12_191:
	shr.u64 	%rl623, %rl941, 63;
	shl.b64 	%rl624, %rl943, 1;
	or.b64  	%rl943, %rl623, %rl624;
	shl.b64 	%rl941, %rl941, 1;
	add.s32 	%r798, %r798, -1;
	setp.gt.s64 	%p153, %rl943, 0;
	@%p153 bra 	BB12_191;

BB12_192:
	mul.lo.s64 	%rl945, %rl943, -3958705157555305931;
	mov.u64 	%rl627, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl625, %rl943, %rl627;
	// inline asm
	setp.gt.s64 	%p154, %rl625, 0;
	mov.u64 	%rl944, %rl625;
	@%p154 bra 	BB12_193;
	bra.uni 	BB12_194;

BB12_193:
	shl.b64 	%rl628, %rl625, 1;
	shr.u64 	%rl629, %rl945, 63;
	or.b64  	%rl944, %rl628, %rl629;
	mul.lo.s64 	%rl945, %rl943, -7917410315110611862;
	add.s32 	%r798, %r798, -1;

BB12_194:
	setp.ne.s64 	%p155, %rl945, 0;
	selp.u64 	%rl630, 1, 0, %p155;
	add.s64 	%rl631, %rl630, %rl944;
	add.s32 	%r537, %r798, 1022;
	cvt.u64.u32 	%rl632, %r537;
	shl.b64 	%rl633, %rl632, 52;
	shr.u64 	%rl634, %rl631, 11;
	shr.u64 	%rl635, %rl631, 10;
	and.b64  	%rl636, %rl635, 1;
	add.s64 	%rl637, %rl633, %rl634;
	add.s64 	%rl638, %rl637, %rl636;
	or.b64  	%rl639, %rl638, %rl939;
	mov.b64 	 %fd1433, %rl639;

BB12_195:
	and.b32  	%r538, %r799, 1;
	setp.eq.s32 	%p156, %r538, 0;
	mul.rn.f64 	%fd71, %fd1433, %fd1433;
	@%p156 bra 	BB12_197;

	mov.f64 	%fd535, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd537, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd534, %fd535, %fd71, %fd537;
	// inline asm
	mov.f64 	%fd541, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd538, %fd534, %fd71, %fd541;
	// inline asm
	mov.f64 	%fd545, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd542, %fd538, %fd71, %fd545;
	// inline asm
	mov.f64 	%fd549, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd546, %fd542, %fd71, %fd549;
	// inline asm
	mov.f64 	%fd553, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd550, %fd546, %fd71, %fd553;
	// inline asm
	mov.f64 	%fd557, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd554, %fd550, %fd71, %fd557;
	// inline asm
	mov.f64 	%fd561, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd558, %fd554, %fd71, %fd561;
	// inline asm
	mov.f64 	%fd1434, %fd558;
	bra.uni 	BB12_198;

BB12_197:
	mov.f64 	%fd563, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd565, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd562, %fd563, %fd71, %fd565;
	// inline asm
	mov.f64 	%fd569, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd566, %fd562, %fd71, %fd569;
	// inline asm
	mov.f64 	%fd573, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd570, %fd566, %fd71, %fd573;
	// inline asm
	mov.f64 	%fd577, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd574, %fd570, %fd71, %fd577;
	// inline asm
	mov.f64 	%fd581, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd578, %fd574, %fd71, %fd581;
	// inline asm
	mul.rn.f64 	%fd583, %fd578, %fd71;
	// inline asm
	fma.rn.f64 	%fd582, %fd583, %fd1433, %fd1433;
	// inline asm
	mov.f64 	%fd1434, %fd582;

BB12_198:
	and.b32  	%r539, %r799, 2;
	setp.eq.s32 	%p157, %r539, 0;
	neg.f64 	%fd586, %fd1434;
	selp.f64 	%fd1435, %fd1434, %fd586, %p157;
	bra.uni 	BB12_200;

BB12_199:
	mov.f64 	%fd587, 0d0000000000000000;
	mul.rn.f64 	%fd1435, %fd57, %fd587;

BB12_200:
	neg.f64 	%fd588, %fd1435;
	mov.f64 	%fd1474, %fd1432;
	mov.f64 	%fd1475, %fd588;
	@%p7 bra 	BB12_201;
	bra.uni 	BB12_202;

BB12_201:
	mov.f64 	%fd1474, %fd1432;
	mov.f64 	%fd1475, %fd1435;

BB12_202:
	mul.f64 	%fd590, %fd5, %fd1474;
	neg.f64 	%fd592, %fd6;
	fma.rn.f64 	%fd593, %fd592, %fd1475, %fd590;
	mul.f64 	%fd594, %fd6, %fd1474;
	fma.rn.f64 	%fd595, %fd5, %fd1475, %fd594;
	mul.f64 	%fd596, %fd13, 0d403921FB54442D18;
	div.rn.f64 	%fd78, %fd596, %fd14;
	setp.eq.f64 	%p11, %fd78, 0d7FF0000000000000;
	setp.eq.f64 	%p12, %fd78, 0dFFF0000000000000;
	or.pred  	%p158, %p11, %p12;
	mov.f64 	%fd1454, %fd141;
	mov.f64 	%fd1455, %fd142;
	mov.f64 	%fd1456, %fd291;
	mov.f64 	%fd1457, %fd293;
	mov.f64 	%fd1458, %fd442;
	mov.f64 	%fd1459, %fd444;
	mov.f64 	%fd1460, %fd593;
	mov.f64 	%fd1461, %fd595;
	@%p158 bra 	BB12_225;

	// inline asm
	abs.f64 	%fd597, %fd78;
	// inline asm
	setp.gt.f64 	%p159, %fd597, 0d41E0000000000000;
	@%p159 bra 	BB12_205;

	mov.f64 	%fd612, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd599, %fd78, %fd612;
	// inline asm
	cvt.rni.s32.f64 	%r540, %fd599;
	// inline asm
	cvt.rn.f64.s32 	%fd613, %r540;
	neg.f64 	%fd609, %fd613;
	mov.f64 	%fd602, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd600, %fd609, %fd602, %fd78;
	// inline asm
	mov.f64 	%fd606, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd604, %fd609, %fd606, %fd600;
	// inline asm
	mov.f64 	%fd610, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd608, %fd609, %fd610, %fd604;
	// inline asm
	mov.u32 	%r802, %r540;
	mov.f64 	%fd1436, %fd608;
	bra.uni 	BB12_221;

BB12_205:
	mov.b64 	 %rl189, %fd78;
	and.b64  	%rl949, %rl189, -9223372036854775808;
	shr.u64 	%rl191, %rl189, 52;
	and.b64  	%rl640, %rl191, 2047;
	add.s64 	%rl641, %rl640, 4294966272;
	cvt.u32.u64 	%r146, %rl641;
	shl.b64 	%rl642, %rl189, 11;
	or.b64  	%rl192, %rl642, -9223372036854775808;
	shr.u32 	%r544, %r146, 6;
	mov.u32 	%r545, 16;
	sub.s32 	%r147, %r545, %r544;
	mov.u32 	%r546, 15;
	sub.s32 	%r800, %r546, %r544;
	mov.u32 	%r547, 19;
	sub.s32 	%r149, %r547, %r544;
	mov.u32 	%r542, 18;
	// inline asm
	min.s32 	%r541, %r542, %r149;
	// inline asm
	setp.lt.s32 	%p160, %r800, %r541;
	@%p160 bra 	BB12_207;

	mov.u64 	%rl946, 0;
	bra.uni 	BB12_209;

BB12_207:
	mov.u32 	%r548, 1;
	sub.s32 	%r150, %r548, %r147;
	mov.u64 	%rl946, 0;

BB12_208:
	.pragma "nounroll";
	shl.b32 	%r552, %r800, 3;
	mov.u32 	%r553, __internal_i2opi_d;
	add.s32 	%r554, %r553, %r552;
	ld.const.u64 	%rl646, [%r554];
	mul.lo.s64 	%rl648, %rl646, %rl192;
	// inline asm
	mul.hi.u64 	%rl645, %rl646, %rl192;
	// inline asm
	mad.lo.s64 	%rl649, %rl646, %rl192, %rl946;
	setp.lt.u64 	%p161, %rl649, %rl648;
	selp.u64 	%rl650, 1, 0, %p161;
	add.s64 	%rl946, %rl650, %rl645;
	add.s32 	%r555, %r150, %r800;
	shl.b32 	%r556, %r555, 3;
	add.s32 	%r558, %r45, %r556;
	st.local.u64 	[%r558], %rl649;
	// inline asm
	min.s32 	%r549, %r542, %r149;
	// inline asm
	add.s32 	%r800, %r800, 1;
	setp.lt.s32 	%p162, %r800, %r549;
	@%p162 bra 	BB12_208;

BB12_209:
	mov.u32 	%r559, 1;
	sub.s32 	%r560, %r559, %r147;
	add.s32 	%r561, %r560, %r800;
	shl.b32 	%r562, %r561, 3;
	add.s32 	%r564, %r45, %r562;
	st.local.u64 	[%r564], %rl946;
	ld.local.u64 	%rl947, [%r45+24];
	ld.local.u64 	%rl948, [%r45+16];
	and.b32  	%r565, %r146, 63;
	setp.eq.s32 	%p163, %r565, 0;
	@%p163 bra 	BB12_211;

	and.b64  	%rl651, %rl191, 63;
	cvt.u32.u64 	%r566, %rl651;
	shl.b64 	%rl652, %rl947, %r566;
	neg.s32 	%r567, %r146;
	and.b32  	%r568, %r567, 63;
	shr.u64 	%rl653, %rl948, %r568;
	or.b64  	%rl947, %rl653, %rl652;
	shl.b64 	%rl654, %rl948, %r566;
	ld.local.u64 	%rl655, [%r45+8];
	shr.u64 	%rl656, %rl655, %r568;
	or.b64  	%rl948, %rl656, %rl654;

BB12_211:
	shr.u64 	%rl657, %rl947, 62;
	cvt.u32.u64 	%r569, %rl657;
	shr.u64 	%rl658, %rl948, 62;
	shl.b64 	%rl659, %rl947, 2;
	or.b64  	%rl953, %rl658, %rl659;
	shl.b64 	%rl203, %rl948, 2;
	setp.ne.s64 	%p164, %rl203, 0;
	selp.u64 	%rl660, 1, 0, %p164;
	or.b64  	%rl661, %rl660, %rl953;
	setp.gt.u64 	%p165, %rl661, -9223372036854775808;
	selp.u32 	%r570, 1, 0, %p165;
	add.s32 	%r571, %r570, %r569;
	neg.s32 	%r572, %r571;
	setp.lt.s64 	%p166, %rl189, 0;
	selp.b32 	%r802, %r572, %r571, %p166;
	@%p165 bra 	BB12_213;

	mov.u64 	%rl952, %rl203;
	bra.uni 	BB12_214;

BB12_213:
	not.b64 	%rl662, %rl953;
	neg.s64 	%rl204, %rl203;
	setp.eq.s64 	%p167, %rl203, 0;
	selp.u64 	%rl663, 1, 0, %p167;
	add.s64 	%rl953, %rl663, %rl662;
	xor.b64  	%rl949, %rl949, -9223372036854775808;
	mov.u64 	%rl952, %rl204;

BB12_214:
	mov.u64 	%rl951, %rl952;
	setp.gt.s64 	%p168, %rl953, 0;
	@%p168 bra 	BB12_216;

	mov.u32 	%r801, 0;
	bra.uni 	BB12_218;

BB12_216:
	mov.u32 	%r801, 0;

BB12_217:
	shr.u64 	%rl664, %rl951, 63;
	shl.b64 	%rl665, %rl953, 1;
	or.b64  	%rl953, %rl664, %rl665;
	shl.b64 	%rl951, %rl951, 1;
	add.s32 	%r801, %r801, -1;
	setp.gt.s64 	%p169, %rl953, 0;
	@%p169 bra 	BB12_217;

BB12_218:
	mul.lo.s64 	%rl955, %rl953, -3958705157555305931;
	mov.u64 	%rl668, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl666, %rl953, %rl668;
	// inline asm
	setp.gt.s64 	%p170, %rl666, 0;
	mov.u64 	%rl954, %rl666;
	@%p170 bra 	BB12_219;
	bra.uni 	BB12_220;

BB12_219:
	shl.b64 	%rl669, %rl666, 1;
	shr.u64 	%rl670, %rl955, 63;
	or.b64  	%rl954, %rl669, %rl670;
	mul.lo.s64 	%rl955, %rl953, -7917410315110611862;
	add.s32 	%r801, %r801, -1;

BB12_220:
	setp.ne.s64 	%p171, %rl955, 0;
	selp.u64 	%rl671, 1, 0, %p171;
	add.s64 	%rl672, %rl671, %rl954;
	add.s32 	%r575, %r801, 1022;
	cvt.u64.u32 	%rl673, %r575;
	shl.b64 	%rl674, %rl673, 52;
	shr.u64 	%rl675, %rl672, 11;
	shr.u64 	%rl676, %rl672, 10;
	and.b64  	%rl677, %rl676, 1;
	add.s64 	%rl678, %rl674, %rl675;
	add.s64 	%rl679, %rl678, %rl677;
	or.b64  	%rl680, %rl679, %rl949;
	mov.b64 	 %fd1436, %rl680;

BB12_221:
	add.s32 	%r161, %r802, 1;
	and.b32  	%r576, %r161, 1;
	setp.eq.s32 	%p172, %r576, 0;
	mul.rn.f64 	%fd82, %fd1436, %fd1436;
	@%p172 bra 	BB12_223;

	mov.f64 	%fd615, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd617, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd614, %fd615, %fd82, %fd617;
	// inline asm
	mov.f64 	%fd621, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd618, %fd614, %fd82, %fd621;
	// inline asm
	mov.f64 	%fd625, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd622, %fd618, %fd82, %fd625;
	// inline asm
	mov.f64 	%fd629, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd626, %fd622, %fd82, %fd629;
	// inline asm
	mov.f64 	%fd633, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd630, %fd626, %fd82, %fd633;
	// inline asm
	mov.f64 	%fd637, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd634, %fd630, %fd82, %fd637;
	// inline asm
	mov.f64 	%fd641, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd638, %fd634, %fd82, %fd641;
	// inline asm
	mov.f64 	%fd1437, %fd638;
	bra.uni 	BB12_224;

BB12_223:
	mov.f64 	%fd643, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd645, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd642, %fd643, %fd82, %fd645;
	// inline asm
	mov.f64 	%fd649, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd646, %fd642, %fd82, %fd649;
	// inline asm
	mov.f64 	%fd653, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd650, %fd646, %fd82, %fd653;
	// inline asm
	mov.f64 	%fd657, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd654, %fd650, %fd82, %fd657;
	// inline asm
	mov.f64 	%fd661, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd658, %fd654, %fd82, %fd661;
	// inline asm
	mul.rn.f64 	%fd663, %fd658, %fd82;
	// inline asm
	fma.rn.f64 	%fd662, %fd663, %fd1436, %fd1436;
	// inline asm
	mov.f64 	%fd1437, %fd662;

BB12_224:
	and.b32  	%r577, %r161, 2;
	setp.eq.s32 	%p173, %r577, 0;
	neg.f64 	%fd666, %fd1437;
	selp.f64 	%fd1438, %fd1437, %fd666, %p173;
	bra.uni 	BB12_226;

BB12_225:
	mov.f64 	%fd1438, 0dFFF8000000000000;

BB12_226:
	setp.eq.f64 	%p174, %fd78, 0d0000000000000000;
	or.pred  	%p175, %p12, %p174;
	or.pred  	%p176, %p11, %p175;
	@%p176 bra 	BB12_249;

	// inline asm
	abs.f64 	%fd668, %fd78;
	// inline asm
	setp.gt.f64 	%p177, %fd668, 0d41E0000000000000;
	@%p177 bra 	BB12_229;

	mov.f64 	%fd683, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd670, %fd78, %fd683;
	// inline asm
	cvt.rni.s32.f64 	%r578, %fd670;
	// inline asm
	cvt.rn.f64.s32 	%fd684, %r578;
	neg.f64 	%fd680, %fd684;
	mov.f64 	%fd673, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd671, %fd680, %fd673, %fd78;
	// inline asm
	mov.f64 	%fd677, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd675, %fd680, %fd677, %fd671;
	// inline asm
	mov.f64 	%fd681, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd679, %fd680, %fd681, %fd675;
	// inline asm
	mov.u32 	%r805, %r578;
	mov.f64 	%fd1439, %fd679;
	bra.uni 	BB12_245;

BB12_229:
	mov.b64 	 %rl221, %fd78;
	and.b64  	%rl959, %rl221, -9223372036854775808;
	shr.u64 	%rl223, %rl221, 52;
	and.b64  	%rl681, %rl223, 2047;
	add.s64 	%rl682, %rl681, 4294966272;
	cvt.u32.u64 	%r163, %rl682;
	shl.b64 	%rl683, %rl221, 11;
	or.b64  	%rl224, %rl683, -9223372036854775808;
	shr.u32 	%r582, %r163, 6;
	mov.u32 	%r583, 16;
	sub.s32 	%r164, %r583, %r582;
	mov.u32 	%r584, 15;
	sub.s32 	%r803, %r584, %r582;
	mov.u32 	%r585, 19;
	sub.s32 	%r166, %r585, %r582;
	mov.u32 	%r580, 18;
	// inline asm
	min.s32 	%r579, %r580, %r166;
	// inline asm
	setp.lt.s32 	%p178, %r803, %r579;
	@%p178 bra 	BB12_231;

	mov.u64 	%rl956, 0;
	bra.uni 	BB12_233;

BB12_231:
	mov.u32 	%r586, 1;
	sub.s32 	%r167, %r586, %r164;
	mov.u64 	%rl956, 0;

BB12_232:
	.pragma "nounroll";
	shl.b32 	%r590, %r803, 3;
	mov.u32 	%r591, __internal_i2opi_d;
	add.s32 	%r592, %r591, %r590;
	ld.const.u64 	%rl687, [%r592];
	mul.lo.s64 	%rl689, %rl687, %rl224;
	// inline asm
	mul.hi.u64 	%rl686, %rl687, %rl224;
	// inline asm
	mad.lo.s64 	%rl690, %rl687, %rl224, %rl956;
	setp.lt.u64 	%p179, %rl690, %rl689;
	selp.u64 	%rl691, 1, 0, %p179;
	add.s64 	%rl956, %rl691, %rl686;
	add.s32 	%r593, %r167, %r803;
	shl.b32 	%r594, %r593, 3;
	add.s32 	%r596, %r45, %r594;
	st.local.u64 	[%r596], %rl690;
	// inline asm
	min.s32 	%r587, %r580, %r166;
	// inline asm
	add.s32 	%r803, %r803, 1;
	setp.lt.s32 	%p180, %r803, %r587;
	@%p180 bra 	BB12_232;

BB12_233:
	mov.u32 	%r597, 1;
	sub.s32 	%r598, %r597, %r164;
	add.s32 	%r599, %r598, %r803;
	shl.b32 	%r600, %r599, 3;
	add.s32 	%r602, %r45, %r600;
	st.local.u64 	[%r602], %rl956;
	ld.local.u64 	%rl957, [%r45+24];
	ld.local.u64 	%rl958, [%r45+16];
	and.b32  	%r603, %r163, 63;
	setp.eq.s32 	%p181, %r603, 0;
	@%p181 bra 	BB12_235;

	and.b64  	%rl692, %rl223, 63;
	cvt.u32.u64 	%r604, %rl692;
	shl.b64 	%rl693, %rl957, %r604;
	neg.s32 	%r605, %r163;
	and.b32  	%r606, %r605, 63;
	shr.u64 	%rl694, %rl958, %r606;
	or.b64  	%rl957, %rl694, %rl693;
	shl.b64 	%rl695, %rl958, %r604;
	ld.local.u64 	%rl696, [%r45+8];
	shr.u64 	%rl697, %rl696, %r606;
	or.b64  	%rl958, %rl697, %rl695;

BB12_235:
	shr.u64 	%rl698, %rl957, 62;
	cvt.u32.u64 	%r607, %rl698;
	shr.u64 	%rl699, %rl958, 62;
	shl.b64 	%rl700, %rl957, 2;
	or.b64  	%rl963, %rl699, %rl700;
	shl.b64 	%rl235, %rl958, 2;
	setp.ne.s64 	%p182, %rl235, 0;
	selp.u64 	%rl701, 1, 0, %p182;
	or.b64  	%rl702, %rl701, %rl963;
	setp.gt.u64 	%p183, %rl702, -9223372036854775808;
	selp.u32 	%r608, 1, 0, %p183;
	add.s32 	%r609, %r608, %r607;
	neg.s32 	%r610, %r609;
	setp.lt.s64 	%p184, %rl221, 0;
	selp.b32 	%r805, %r610, %r609, %p184;
	@%p183 bra 	BB12_237;

	mov.u64 	%rl962, %rl235;
	bra.uni 	BB12_238;

BB12_237:
	not.b64 	%rl703, %rl963;
	neg.s64 	%rl236, %rl235;
	setp.eq.s64 	%p185, %rl235, 0;
	selp.u64 	%rl704, 1, 0, %p185;
	add.s64 	%rl963, %rl704, %rl703;
	xor.b64  	%rl959, %rl959, -9223372036854775808;
	mov.u64 	%rl962, %rl236;

BB12_238:
	mov.u64 	%rl961, %rl962;
	setp.gt.s64 	%p186, %rl963, 0;
	@%p186 bra 	BB12_240;

	mov.u32 	%r804, 0;
	bra.uni 	BB12_242;

BB12_240:
	mov.u32 	%r804, 0;

BB12_241:
	shr.u64 	%rl705, %rl961, 63;
	shl.b64 	%rl706, %rl963, 1;
	or.b64  	%rl963, %rl705, %rl706;
	shl.b64 	%rl961, %rl961, 1;
	add.s32 	%r804, %r804, -1;
	setp.gt.s64 	%p187, %rl963, 0;
	@%p187 bra 	BB12_241;

BB12_242:
	mul.lo.s64 	%rl965, %rl963, -3958705157555305931;
	mov.u64 	%rl709, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl707, %rl963, %rl709;
	// inline asm
	setp.gt.s64 	%p188, %rl707, 0;
	mov.u64 	%rl964, %rl707;
	@%p188 bra 	BB12_243;
	bra.uni 	BB12_244;

BB12_243:
	shl.b64 	%rl710, %rl707, 1;
	shr.u64 	%rl711, %rl965, 63;
	or.b64  	%rl964, %rl710, %rl711;
	mul.lo.s64 	%rl965, %rl963, -7917410315110611862;
	add.s32 	%r804, %r804, -1;

BB12_244:
	setp.ne.s64 	%p189, %rl965, 0;
	selp.u64 	%rl712, 1, 0, %p189;
	add.s64 	%rl713, %rl712, %rl964;
	add.s32 	%r613, %r804, 1022;
	cvt.u64.u32 	%rl714, %r613;
	shl.b64 	%rl715, %rl714, 52;
	shr.u64 	%rl716, %rl713, 11;
	shr.u64 	%rl717, %rl713, 10;
	and.b64  	%rl718, %rl717, 1;
	add.s64 	%rl719, %rl715, %rl716;
	add.s64 	%rl720, %rl719, %rl718;
	or.b64  	%rl721, %rl720, %rl959;
	mov.b64 	 %fd1439, %rl721;

BB12_245:
	and.b32  	%r614, %r805, 1;
	setp.eq.s32 	%p190, %r614, 0;
	mul.rn.f64 	%fd92, %fd1439, %fd1439;
	@%p190 bra 	BB12_247;

	mov.f64 	%fd686, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd688, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd685, %fd686, %fd92, %fd688;
	// inline asm
	mov.f64 	%fd692, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd689, %fd685, %fd92, %fd692;
	// inline asm
	mov.f64 	%fd696, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd693, %fd689, %fd92, %fd696;
	// inline asm
	mov.f64 	%fd700, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd697, %fd693, %fd92, %fd700;
	// inline asm
	mov.f64 	%fd704, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd701, %fd697, %fd92, %fd704;
	// inline asm
	mov.f64 	%fd708, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd705, %fd701, %fd92, %fd708;
	// inline asm
	mov.f64 	%fd712, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd709, %fd705, %fd92, %fd712;
	// inline asm
	mov.f64 	%fd1440, %fd709;
	bra.uni 	BB12_248;

BB12_247:
	mov.f64 	%fd714, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd716, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd713, %fd714, %fd92, %fd716;
	// inline asm
	mov.f64 	%fd720, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd717, %fd713, %fd92, %fd720;
	// inline asm
	mov.f64 	%fd724, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd721, %fd717, %fd92, %fd724;
	// inline asm
	mov.f64 	%fd728, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd725, %fd721, %fd92, %fd728;
	// inline asm
	mov.f64 	%fd732, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd729, %fd725, %fd92, %fd732;
	// inline asm
	mul.rn.f64 	%fd734, %fd729, %fd92;
	// inline asm
	fma.rn.f64 	%fd733, %fd734, %fd1439, %fd1439;
	// inline asm
	mov.f64 	%fd1440, %fd733;

BB12_248:
	and.b32  	%r615, %r805, 2;
	setp.eq.s32 	%p191, %r615, 0;
	neg.f64 	%fd737, %fd1440;
	selp.f64 	%fd1441, %fd1440, %fd737, %p191;
	bra.uni 	BB12_250;

BB12_249:
	mov.f64 	%fd738, 0d0000000000000000;
	mul.rn.f64 	%fd1441, %fd78, %fd738;

BB12_250:
	neg.f64 	%fd739, %fd1441;
	mov.f64 	%fd1472, %fd1438;
	mov.f64 	%fd1473, %fd739;
	@%p7 bra 	BB12_251;
	bra.uni 	BB12_252;

BB12_251:
	mov.f64 	%fd1472, %fd1438;
	mov.f64 	%fd1473, %fd1441;

BB12_252:
	mul.f64 	%fd741, %fd7, %fd1472;
	neg.f64 	%fd743, %fd8;
	fma.rn.f64 	%fd744, %fd743, %fd1473, %fd741;
	mul.f64 	%fd745, %fd8, %fd1472;
	fma.rn.f64 	%fd746, %fd7, %fd1473, %fd745;
	mov.f64 	%fd1462, %fd744;
	mov.f64 	%fd1463, %fd746;
	mul.f64 	%fd747, %fd13, 0d403F6A7A2955385E;
	div.rn.f64 	%fd99, %fd747, %fd14;
	setp.eq.f64 	%p13, %fd99, 0d7FF0000000000000;
	setp.eq.f64 	%p14, %fd99, 0dFFF0000000000000;
	or.pred  	%p192, %p13, %p14;
	@%p192 bra 	BB12_275;

	// inline asm
	abs.f64 	%fd748, %fd99;
	// inline asm
	setp.gt.f64 	%p193, %fd748, 0d41E0000000000000;
	@%p193 bra 	BB12_255;

	mov.f64 	%fd763, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd750, %fd99, %fd763;
	// inline asm
	cvt.rni.s32.f64 	%r616, %fd750;
	// inline asm
	cvt.rn.f64.s32 	%fd764, %r616;
	neg.f64 	%fd760, %fd764;
	mov.f64 	%fd753, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd751, %fd760, %fd753, %fd99;
	// inline asm
	mov.f64 	%fd757, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd755, %fd760, %fd757, %fd751;
	// inline asm
	mov.f64 	%fd761, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd759, %fd760, %fd761, %fd755;
	// inline asm
	mov.u32 	%r808, %r616;
	mov.f64 	%fd1442, %fd759;
	bra.uni 	BB12_271;

BB12_255:
	mov.b64 	 %rl253, %fd99;
	and.b64  	%rl969, %rl253, -9223372036854775808;
	shr.u64 	%rl255, %rl253, 52;
	and.b64  	%rl722, %rl255, 2047;
	add.s64 	%rl723, %rl722, 4294966272;
	cvt.u32.u64 	%r179, %rl723;
	shl.b64 	%rl724, %rl253, 11;
	or.b64  	%rl256, %rl724, -9223372036854775808;
	shr.u32 	%r620, %r179, 6;
	mov.u32 	%r621, 16;
	sub.s32 	%r180, %r621, %r620;
	mov.u32 	%r622, 15;
	sub.s32 	%r806, %r622, %r620;
	mov.u32 	%r623, 19;
	sub.s32 	%r182, %r623, %r620;
	mov.u32 	%r618, 18;
	// inline asm
	min.s32 	%r617, %r618, %r182;
	// inline asm
	setp.lt.s32 	%p194, %r806, %r617;
	@%p194 bra 	BB12_257;

	mov.u64 	%rl966, 0;
	bra.uni 	BB12_259;

BB12_257:
	mov.u32 	%r624, 1;
	sub.s32 	%r183, %r624, %r180;
	mov.u64 	%rl966, 0;

BB12_258:
	.pragma "nounroll";
	shl.b32 	%r628, %r806, 3;
	mov.u32 	%r629, __internal_i2opi_d;
	add.s32 	%r630, %r629, %r628;
	ld.const.u64 	%rl728, [%r630];
	mul.lo.s64 	%rl730, %rl728, %rl256;
	// inline asm
	mul.hi.u64 	%rl727, %rl728, %rl256;
	// inline asm
	mad.lo.s64 	%rl731, %rl728, %rl256, %rl966;
	setp.lt.u64 	%p195, %rl731, %rl730;
	selp.u64 	%rl732, 1, 0, %p195;
	add.s64 	%rl966, %rl732, %rl727;
	add.s32 	%r631, %r183, %r806;
	shl.b32 	%r632, %r631, 3;
	add.s32 	%r634, %r45, %r632;
	st.local.u64 	[%r634], %rl731;
	// inline asm
	min.s32 	%r625, %r618, %r182;
	// inline asm
	add.s32 	%r806, %r806, 1;
	setp.lt.s32 	%p196, %r806, %r625;
	@%p196 bra 	BB12_258;

BB12_259:
	mov.u32 	%r635, 1;
	sub.s32 	%r636, %r635, %r180;
	add.s32 	%r637, %r636, %r806;
	shl.b32 	%r638, %r637, 3;
	add.s32 	%r640, %r45, %r638;
	st.local.u64 	[%r640], %rl966;
	ld.local.u64 	%rl967, [%r45+24];
	ld.local.u64 	%rl968, [%r45+16];
	and.b32  	%r641, %r179, 63;
	setp.eq.s32 	%p197, %r641, 0;
	@%p197 bra 	BB12_261;

	and.b64  	%rl733, %rl255, 63;
	cvt.u32.u64 	%r642, %rl733;
	shl.b64 	%rl734, %rl967, %r642;
	neg.s32 	%r643, %r179;
	and.b32  	%r644, %r643, 63;
	shr.u64 	%rl735, %rl968, %r644;
	or.b64  	%rl967, %rl735, %rl734;
	shl.b64 	%rl736, %rl968, %r642;
	ld.local.u64 	%rl737, [%r45+8];
	shr.u64 	%rl738, %rl737, %r644;
	or.b64  	%rl968, %rl738, %rl736;

BB12_261:
	shr.u64 	%rl739, %rl967, 62;
	cvt.u32.u64 	%r645, %rl739;
	shr.u64 	%rl740, %rl968, 62;
	shl.b64 	%rl741, %rl967, 2;
	or.b64  	%rl973, %rl740, %rl741;
	shl.b64 	%rl267, %rl968, 2;
	setp.ne.s64 	%p198, %rl267, 0;
	selp.u64 	%rl742, 1, 0, %p198;
	or.b64  	%rl743, %rl742, %rl973;
	setp.gt.u64 	%p199, %rl743, -9223372036854775808;
	selp.u32 	%r646, 1, 0, %p199;
	add.s32 	%r647, %r646, %r645;
	neg.s32 	%r648, %r647;
	setp.lt.s64 	%p200, %rl253, 0;
	selp.b32 	%r808, %r648, %r647, %p200;
	@%p199 bra 	BB12_263;

	mov.u64 	%rl972, %rl267;
	bra.uni 	BB12_264;

BB12_263:
	not.b64 	%rl744, %rl973;
	neg.s64 	%rl268, %rl267;
	setp.eq.s64 	%p201, %rl267, 0;
	selp.u64 	%rl745, 1, 0, %p201;
	add.s64 	%rl973, %rl745, %rl744;
	xor.b64  	%rl969, %rl969, -9223372036854775808;
	mov.u64 	%rl972, %rl268;

BB12_264:
	mov.u64 	%rl971, %rl972;
	setp.gt.s64 	%p202, %rl973, 0;
	@%p202 bra 	BB12_266;

	mov.u32 	%r807, 0;
	bra.uni 	BB12_268;

BB12_266:
	mov.u32 	%r807, 0;

BB12_267:
	shr.u64 	%rl746, %rl971, 63;
	shl.b64 	%rl747, %rl973, 1;
	or.b64  	%rl973, %rl746, %rl747;
	shl.b64 	%rl971, %rl971, 1;
	add.s32 	%r807, %r807, -1;
	setp.gt.s64 	%p203, %rl973, 0;
	@%p203 bra 	BB12_267;

BB12_268:
	mul.lo.s64 	%rl975, %rl973, -3958705157555305931;
	mov.u64 	%rl750, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl748, %rl973, %rl750;
	// inline asm
	setp.gt.s64 	%p204, %rl748, 0;
	mov.u64 	%rl974, %rl748;
	@%p204 bra 	BB12_269;
	bra.uni 	BB12_270;

BB12_269:
	shl.b64 	%rl751, %rl748, 1;
	shr.u64 	%rl752, %rl975, 63;
	or.b64  	%rl974, %rl751, %rl752;
	mul.lo.s64 	%rl975, %rl973, -7917410315110611862;
	add.s32 	%r807, %r807, -1;

BB12_270:
	setp.ne.s64 	%p205, %rl975, 0;
	selp.u64 	%rl753, 1, 0, %p205;
	add.s64 	%rl754, %rl753, %rl974;
	add.s32 	%r651, %r807, 1022;
	cvt.u64.u32 	%rl755, %r651;
	shl.b64 	%rl756, %rl755, 52;
	shr.u64 	%rl757, %rl754, 11;
	shr.u64 	%rl758, %rl754, 10;
	and.b64  	%rl759, %rl758, 1;
	add.s64 	%rl760, %rl756, %rl757;
	add.s64 	%rl761, %rl760, %rl759;
	or.b64  	%rl762, %rl761, %rl969;
	mov.b64 	 %fd1442, %rl762;

BB12_271:
	add.s32 	%r194, %r808, 1;
	and.b32  	%r652, %r194, 1;
	setp.eq.s32 	%p206, %r652, 0;
	mul.rn.f64 	%fd103, %fd1442, %fd1442;
	@%p206 bra 	BB12_273;

	mov.f64 	%fd766, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd768, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd765, %fd766, %fd103, %fd768;
	// inline asm
	mov.f64 	%fd772, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd769, %fd765, %fd103, %fd772;
	// inline asm
	mov.f64 	%fd776, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd773, %fd769, %fd103, %fd776;
	// inline asm
	mov.f64 	%fd780, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd777, %fd773, %fd103, %fd780;
	// inline asm
	mov.f64 	%fd784, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd781, %fd777, %fd103, %fd784;
	// inline asm
	mov.f64 	%fd788, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd785, %fd781, %fd103, %fd788;
	// inline asm
	mov.f64 	%fd792, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd789, %fd785, %fd103, %fd792;
	// inline asm
	mov.f64 	%fd1443, %fd789;
	bra.uni 	BB12_274;

BB12_273:
	mov.f64 	%fd794, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd796, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd793, %fd794, %fd103, %fd796;
	// inline asm
	mov.f64 	%fd800, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd797, %fd793, %fd103, %fd800;
	// inline asm
	mov.f64 	%fd804, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd801, %fd797, %fd103, %fd804;
	// inline asm
	mov.f64 	%fd808, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd805, %fd801, %fd103, %fd808;
	// inline asm
	mov.f64 	%fd812, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd809, %fd805, %fd103, %fd812;
	// inline asm
	mul.rn.f64 	%fd814, %fd809, %fd103;
	// inline asm
	fma.rn.f64 	%fd813, %fd814, %fd1442, %fd1442;
	// inline asm
	mov.f64 	%fd1443, %fd813;

BB12_274:
	and.b32  	%r653, %r194, 2;
	setp.eq.s32 	%p207, %r653, 0;
	neg.f64 	%fd817, %fd1443;
	selp.f64 	%fd1444, %fd1443, %fd817, %p207;
	bra.uni 	BB12_276;

BB12_275:
	mov.f64 	%fd1444, 0dFFF8000000000000;

BB12_276:
	setp.eq.f64 	%p208, %fd99, 0d0000000000000000;
	or.pred  	%p209, %p14, %p208;
	or.pred  	%p210, %p13, %p209;
	@%p210 bra 	BB12_299;

	// inline asm
	abs.f64 	%fd819, %fd99;
	// inline asm
	setp.gt.f64 	%p211, %fd819, 0d41E0000000000000;
	@%p211 bra 	BB12_279;

	mov.f64 	%fd834, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd821, %fd99, %fd834;
	// inline asm
	cvt.rni.s32.f64 	%r654, %fd821;
	// inline asm
	cvt.rn.f64.s32 	%fd835, %r654;
	neg.f64 	%fd831, %fd835;
	mov.f64 	%fd824, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd822, %fd831, %fd824, %fd99;
	// inline asm
	mov.f64 	%fd828, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd826, %fd831, %fd828, %fd822;
	// inline asm
	mov.f64 	%fd832, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd830, %fd831, %fd832, %fd826;
	// inline asm
	mov.u32 	%r811, %r654;
	mov.f64 	%fd1445, %fd830;
	bra.uni 	BB12_295;

BB12_279:
	mov.b64 	 %rl285, %fd99;
	and.b64  	%rl979, %rl285, -9223372036854775808;
	shr.u64 	%rl287, %rl285, 52;
	and.b64  	%rl763, %rl287, 2047;
	add.s64 	%rl764, %rl763, 4294966272;
	cvt.u32.u64 	%r196, %rl764;
	shl.b64 	%rl765, %rl285, 11;
	or.b64  	%rl288, %rl765, -9223372036854775808;
	shr.u32 	%r658, %r196, 6;
	mov.u32 	%r659, 16;
	sub.s32 	%r197, %r659, %r658;
	mov.u32 	%r660, 15;
	sub.s32 	%r809, %r660, %r658;
	mov.u32 	%r661, 19;
	sub.s32 	%r199, %r661, %r658;
	mov.u32 	%r656, 18;
	// inline asm
	min.s32 	%r655, %r656, %r199;
	// inline asm
	setp.lt.s32 	%p212, %r809, %r655;
	@%p212 bra 	BB12_281;

	mov.u64 	%rl976, 0;
	bra.uni 	BB12_283;

BB12_281:
	mov.u32 	%r662, 1;
	sub.s32 	%r200, %r662, %r197;
	mov.u64 	%rl976, 0;

BB12_282:
	.pragma "nounroll";
	shl.b32 	%r666, %r809, 3;
	mov.u32 	%r667, __internal_i2opi_d;
	add.s32 	%r668, %r667, %r666;
	ld.const.u64 	%rl769, [%r668];
	mul.lo.s64 	%rl771, %rl769, %rl288;
	// inline asm
	mul.hi.u64 	%rl768, %rl769, %rl288;
	// inline asm
	mad.lo.s64 	%rl772, %rl769, %rl288, %rl976;
	setp.lt.u64 	%p213, %rl772, %rl771;
	selp.u64 	%rl773, 1, 0, %p213;
	add.s64 	%rl976, %rl773, %rl768;
	add.s32 	%r669, %r200, %r809;
	shl.b32 	%r670, %r669, 3;
	add.s32 	%r672, %r45, %r670;
	st.local.u64 	[%r672], %rl772;
	// inline asm
	min.s32 	%r663, %r656, %r199;
	// inline asm
	add.s32 	%r809, %r809, 1;
	setp.lt.s32 	%p214, %r809, %r663;
	@%p214 bra 	BB12_282;

BB12_283:
	mov.u32 	%r673, 1;
	sub.s32 	%r674, %r673, %r197;
	add.s32 	%r675, %r674, %r809;
	shl.b32 	%r676, %r675, 3;
	add.s32 	%r678, %r45, %r676;
	st.local.u64 	[%r678], %rl976;
	ld.local.u64 	%rl977, [%r45+24];
	ld.local.u64 	%rl978, [%r45+16];
	and.b32  	%r679, %r196, 63;
	setp.eq.s32 	%p215, %r679, 0;
	@%p215 bra 	BB12_285;

	and.b64  	%rl774, %rl287, 63;
	cvt.u32.u64 	%r680, %rl774;
	shl.b64 	%rl775, %rl977, %r680;
	neg.s32 	%r681, %r196;
	and.b32  	%r682, %r681, 63;
	shr.u64 	%rl776, %rl978, %r682;
	or.b64  	%rl977, %rl776, %rl775;
	shl.b64 	%rl777, %rl978, %r680;
	ld.local.u64 	%rl778, [%r45+8];
	shr.u64 	%rl779, %rl778, %r682;
	or.b64  	%rl978, %rl779, %rl777;

BB12_285:
	shr.u64 	%rl780, %rl977, 62;
	cvt.u32.u64 	%r683, %rl780;
	shr.u64 	%rl781, %rl978, 62;
	shl.b64 	%rl782, %rl977, 2;
	or.b64  	%rl983, %rl781, %rl782;
	shl.b64 	%rl299, %rl978, 2;
	setp.ne.s64 	%p216, %rl299, 0;
	selp.u64 	%rl783, 1, 0, %p216;
	or.b64  	%rl784, %rl783, %rl983;
	setp.gt.u64 	%p217, %rl784, -9223372036854775808;
	selp.u32 	%r684, 1, 0, %p217;
	add.s32 	%r685, %r684, %r683;
	neg.s32 	%r686, %r685;
	setp.lt.s64 	%p218, %rl285, 0;
	selp.b32 	%r811, %r686, %r685, %p218;
	@%p217 bra 	BB12_287;

	mov.u64 	%rl982, %rl299;
	bra.uni 	BB12_288;

BB12_287:
	not.b64 	%rl785, %rl983;
	neg.s64 	%rl300, %rl299;
	setp.eq.s64 	%p219, %rl299, 0;
	selp.u64 	%rl786, 1, 0, %p219;
	add.s64 	%rl983, %rl786, %rl785;
	xor.b64  	%rl979, %rl979, -9223372036854775808;
	mov.u64 	%rl982, %rl300;

BB12_288:
	mov.u64 	%rl981, %rl982;
	setp.gt.s64 	%p220, %rl983, 0;
	@%p220 bra 	BB12_290;

	mov.u32 	%r810, 0;
	bra.uni 	BB12_292;

BB12_290:
	mov.u32 	%r810, 0;

BB12_291:
	shr.u64 	%rl787, %rl981, 63;
	shl.b64 	%rl788, %rl983, 1;
	or.b64  	%rl983, %rl787, %rl788;
	shl.b64 	%rl981, %rl981, 1;
	add.s32 	%r810, %r810, -1;
	setp.gt.s64 	%p221, %rl983, 0;
	@%p221 bra 	BB12_291;

BB12_292:
	mul.lo.s64 	%rl985, %rl983, -3958705157555305931;
	mov.u64 	%rl791, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl789, %rl983, %rl791;
	// inline asm
	setp.gt.s64 	%p222, %rl789, 0;
	mov.u64 	%rl984, %rl789;
	@%p222 bra 	BB12_293;
	bra.uni 	BB12_294;

BB12_293:
	shl.b64 	%rl792, %rl789, 1;
	shr.u64 	%rl793, %rl985, 63;
	or.b64  	%rl984, %rl792, %rl793;
	mul.lo.s64 	%rl985, %rl983, -7917410315110611862;
	add.s32 	%r810, %r810, -1;

BB12_294:
	setp.ne.s64 	%p223, %rl985, 0;
	selp.u64 	%rl794, 1, 0, %p223;
	add.s64 	%rl795, %rl794, %rl984;
	add.s32 	%r689, %r810, 1022;
	cvt.u64.u32 	%rl796, %r689;
	shl.b64 	%rl797, %rl796, 52;
	shr.u64 	%rl798, %rl795, 11;
	shr.u64 	%rl799, %rl795, 10;
	and.b64  	%rl800, %rl799, 1;
	add.s64 	%rl801, %rl797, %rl798;
	add.s64 	%rl802, %rl801, %rl800;
	or.b64  	%rl803, %rl802, %rl979;
	mov.b64 	 %fd1445, %rl803;

BB12_295:
	and.b32  	%r690, %r811, 1;
	setp.eq.s32 	%p224, %r690, 0;
	mul.rn.f64 	%fd113, %fd1445, %fd1445;
	@%p224 bra 	BB12_297;

	mov.f64 	%fd837, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd839, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd836, %fd837, %fd113, %fd839;
	// inline asm
	mov.f64 	%fd843, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd840, %fd836, %fd113, %fd843;
	// inline asm
	mov.f64 	%fd847, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd844, %fd840, %fd113, %fd847;
	// inline asm
	mov.f64 	%fd851, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd848, %fd844, %fd113, %fd851;
	// inline asm
	mov.f64 	%fd855, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd852, %fd848, %fd113, %fd855;
	// inline asm
	mov.f64 	%fd859, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd856, %fd852, %fd113, %fd859;
	// inline asm
	mov.f64 	%fd863, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd860, %fd856, %fd113, %fd863;
	// inline asm
	mov.f64 	%fd1446, %fd860;
	bra.uni 	BB12_298;

BB12_297:
	mov.f64 	%fd865, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd867, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd864, %fd865, %fd113, %fd867;
	// inline asm
	mov.f64 	%fd871, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd868, %fd864, %fd113, %fd871;
	// inline asm
	mov.f64 	%fd875, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd872, %fd868, %fd113, %fd875;
	// inline asm
	mov.f64 	%fd879, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd876, %fd872, %fd113, %fd879;
	// inline asm
	mov.f64 	%fd883, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd880, %fd876, %fd113, %fd883;
	// inline asm
	mul.rn.f64 	%fd885, %fd880, %fd113;
	// inline asm
	fma.rn.f64 	%fd884, %fd885, %fd1445, %fd1445;
	// inline asm
	mov.f64 	%fd1446, %fd884;

BB12_298:
	and.b32  	%r691, %r811, 2;
	setp.eq.s32 	%p225, %r691, 0;
	neg.f64 	%fd888, %fd1446;
	selp.f64 	%fd1447, %fd1446, %fd888, %p225;
	bra.uni 	BB12_300;

BB12_299:
	mov.f64 	%fd889, 0d0000000000000000;
	mul.rn.f64 	%fd1447, %fd99, %fd889;

BB12_300:
	neg.f64 	%fd890, %fd1447;
	mov.f64 	%fd1470, %fd1444;
	mov.f64 	%fd1471, %fd890;
	@%p7 bra 	BB12_301;
	bra.uni 	BB12_302;

BB12_301:
	mov.f64 	%fd1470, %fd1444;
	mov.f64 	%fd1471, %fd1447;

BB12_302:
	mul.f64 	%fd892, %fd9, %fd1470;
	neg.f64 	%fd894, %fd10;
	fma.rn.f64 	%fd895, %fd894, %fd1471, %fd892;
	mul.f64 	%fd896, %fd10, %fd1470;
	fma.rn.f64 	%fd897, %fd9, %fd1471, %fd896;
	mov.f64 	%fd1464, %fd895;
	mov.f64 	%fd1465, %fd897;
	mul.f64 	%fd898, %fd13, 0d4042D97C7F3321D2;
	div.rn.f64 	%fd120, %fd898, %fd14;
	setp.eq.f64 	%p15, %fd120, 0d7FF0000000000000;
	setp.eq.f64 	%p16, %fd120, 0dFFF0000000000000;
	or.pred  	%p226, %p15, %p16;
	@%p226 bra 	BB12_325;

	// inline asm
	abs.f64 	%fd899, %fd120;
	// inline asm
	setp.gt.f64 	%p227, %fd899, 0d41E0000000000000;
	@%p227 bra 	BB12_305;

	mov.f64 	%fd914, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd901, %fd120, %fd914;
	// inline asm
	cvt.rni.s32.f64 	%r692, %fd901;
	// inline asm
	cvt.rn.f64.s32 	%fd915, %r692;
	neg.f64 	%fd911, %fd915;
	mov.f64 	%fd904, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd902, %fd911, %fd904, %fd120;
	// inline asm
	mov.f64 	%fd908, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd906, %fd911, %fd908, %fd902;
	// inline asm
	mov.f64 	%fd912, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd910, %fd911, %fd912, %fd906;
	// inline asm
	mov.u32 	%r814, %r692;
	mov.f64 	%fd1448, %fd910;
	bra.uni 	BB12_321;

BB12_305:
	mov.b64 	 %rl317, %fd120;
	and.b64  	%rl989, %rl317, -9223372036854775808;
	shr.u64 	%rl319, %rl317, 52;
	and.b64  	%rl804, %rl319, 2047;
	add.s64 	%rl805, %rl804, 4294966272;
	cvt.u32.u64 	%r212, %rl805;
	shl.b64 	%rl806, %rl317, 11;
	or.b64  	%rl320, %rl806, -9223372036854775808;
	shr.u32 	%r696, %r212, 6;
	mov.u32 	%r697, 16;
	sub.s32 	%r213, %r697, %r696;
	mov.u32 	%r698, 15;
	sub.s32 	%r812, %r698, %r696;
	mov.u32 	%r699, 19;
	sub.s32 	%r215, %r699, %r696;
	mov.u32 	%r694, 18;
	// inline asm
	min.s32 	%r693, %r694, %r215;
	// inline asm
	setp.lt.s32 	%p228, %r812, %r693;
	@%p228 bra 	BB12_307;

	mov.u64 	%rl986, 0;
	bra.uni 	BB12_309;

BB12_307:
	mov.u32 	%r700, 1;
	sub.s32 	%r216, %r700, %r213;
	mov.u64 	%rl986, 0;

BB12_308:
	.pragma "nounroll";
	shl.b32 	%r704, %r812, 3;
	mov.u32 	%r705, __internal_i2opi_d;
	add.s32 	%r706, %r705, %r704;
	ld.const.u64 	%rl810, [%r706];
	mul.lo.s64 	%rl812, %rl810, %rl320;
	// inline asm
	mul.hi.u64 	%rl809, %rl810, %rl320;
	// inline asm
	mad.lo.s64 	%rl813, %rl810, %rl320, %rl986;
	setp.lt.u64 	%p229, %rl813, %rl812;
	selp.u64 	%rl814, 1, 0, %p229;
	add.s64 	%rl986, %rl814, %rl809;
	add.s32 	%r707, %r216, %r812;
	shl.b32 	%r708, %r707, 3;
	add.s32 	%r710, %r45, %r708;
	st.local.u64 	[%r710], %rl813;
	// inline asm
	min.s32 	%r701, %r694, %r215;
	// inline asm
	add.s32 	%r812, %r812, 1;
	setp.lt.s32 	%p230, %r812, %r701;
	@%p230 bra 	BB12_308;

BB12_309:
	mov.u32 	%r711, 1;
	sub.s32 	%r712, %r711, %r213;
	add.s32 	%r713, %r712, %r812;
	shl.b32 	%r714, %r713, 3;
	add.s32 	%r716, %r45, %r714;
	st.local.u64 	[%r716], %rl986;
	ld.local.u64 	%rl987, [%r45+24];
	ld.local.u64 	%rl988, [%r45+16];
	and.b32  	%r717, %r212, 63;
	setp.eq.s32 	%p231, %r717, 0;
	@%p231 bra 	BB12_311;

	and.b64  	%rl815, %rl319, 63;
	cvt.u32.u64 	%r718, %rl815;
	shl.b64 	%rl816, %rl987, %r718;
	neg.s32 	%r719, %r212;
	and.b32  	%r720, %r719, 63;
	shr.u64 	%rl817, %rl988, %r720;
	or.b64  	%rl987, %rl817, %rl816;
	shl.b64 	%rl818, %rl988, %r718;
	ld.local.u64 	%rl819, [%r45+8];
	shr.u64 	%rl820, %rl819, %r720;
	or.b64  	%rl988, %rl820, %rl818;

BB12_311:
	shr.u64 	%rl821, %rl987, 62;
	cvt.u32.u64 	%r721, %rl821;
	shr.u64 	%rl822, %rl988, 62;
	shl.b64 	%rl823, %rl987, 2;
	or.b64  	%rl993, %rl822, %rl823;
	shl.b64 	%rl331, %rl988, 2;
	setp.ne.s64 	%p232, %rl331, 0;
	selp.u64 	%rl824, 1, 0, %p232;
	or.b64  	%rl825, %rl824, %rl993;
	setp.gt.u64 	%p233, %rl825, -9223372036854775808;
	selp.u32 	%r722, 1, 0, %p233;
	add.s32 	%r723, %r722, %r721;
	neg.s32 	%r724, %r723;
	setp.lt.s64 	%p234, %rl317, 0;
	selp.b32 	%r814, %r724, %r723, %p234;
	@%p233 bra 	BB12_313;

	mov.u64 	%rl992, %rl331;
	bra.uni 	BB12_314;

BB12_313:
	not.b64 	%rl826, %rl993;
	neg.s64 	%rl332, %rl331;
	setp.eq.s64 	%p235, %rl331, 0;
	selp.u64 	%rl827, 1, 0, %p235;
	add.s64 	%rl993, %rl827, %rl826;
	xor.b64  	%rl989, %rl989, -9223372036854775808;
	mov.u64 	%rl992, %rl332;

BB12_314:
	mov.u64 	%rl991, %rl992;
	setp.gt.s64 	%p236, %rl993, 0;
	@%p236 bra 	BB12_316;

	mov.u32 	%r813, 0;
	bra.uni 	BB12_318;

BB12_316:
	mov.u32 	%r813, 0;

BB12_317:
	shr.u64 	%rl828, %rl991, 63;
	shl.b64 	%rl829, %rl993, 1;
	or.b64  	%rl993, %rl828, %rl829;
	shl.b64 	%rl991, %rl991, 1;
	add.s32 	%r813, %r813, -1;
	setp.gt.s64 	%p237, %rl993, 0;
	@%p237 bra 	BB12_317;

BB12_318:
	mul.lo.s64 	%rl995, %rl993, -3958705157555305931;
	mov.u64 	%rl832, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl830, %rl993, %rl832;
	// inline asm
	setp.gt.s64 	%p238, %rl830, 0;
	mov.u64 	%rl994, %rl830;
	@%p238 bra 	BB12_319;
	bra.uni 	BB12_320;

BB12_319:
	shl.b64 	%rl833, %rl830, 1;
	shr.u64 	%rl834, %rl995, 63;
	or.b64  	%rl994, %rl833, %rl834;
	mul.lo.s64 	%rl995, %rl993, -7917410315110611862;
	add.s32 	%r813, %r813, -1;

BB12_320:
	setp.ne.s64 	%p239, %rl995, 0;
	selp.u64 	%rl835, 1, 0, %p239;
	add.s64 	%rl836, %rl835, %rl994;
	add.s32 	%r727, %r813, 1022;
	cvt.u64.u32 	%rl837, %r727;
	shl.b64 	%rl838, %rl837, 52;
	shr.u64 	%rl839, %rl836, 11;
	shr.u64 	%rl840, %rl836, 10;
	and.b64  	%rl841, %rl840, 1;
	add.s64 	%rl842, %rl838, %rl839;
	add.s64 	%rl843, %rl842, %rl841;
	or.b64  	%rl844, %rl843, %rl989;
	mov.b64 	 %fd1448, %rl844;

BB12_321:
	add.s32 	%r227, %r814, 1;
	and.b32  	%r728, %r227, 1;
	setp.eq.s32 	%p240, %r728, 0;
	mul.rn.f64 	%fd124, %fd1448, %fd1448;
	@%p240 bra 	BB12_323;

	mov.f64 	%fd917, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd919, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd916, %fd917, %fd124, %fd919;
	// inline asm
	mov.f64 	%fd923, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd920, %fd916, %fd124, %fd923;
	// inline asm
	mov.f64 	%fd927, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd924, %fd920, %fd124, %fd927;
	// inline asm
	mov.f64 	%fd931, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd928, %fd924, %fd124, %fd931;
	// inline asm
	mov.f64 	%fd935, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd932, %fd928, %fd124, %fd935;
	// inline asm
	mov.f64 	%fd939, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd936, %fd932, %fd124, %fd939;
	// inline asm
	mov.f64 	%fd943, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd940, %fd936, %fd124, %fd943;
	// inline asm
	mov.f64 	%fd1449, %fd940;
	bra.uni 	BB12_324;

BB12_323:
	mov.f64 	%fd945, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd947, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd944, %fd945, %fd124, %fd947;
	// inline asm
	mov.f64 	%fd951, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd948, %fd944, %fd124, %fd951;
	// inline asm
	mov.f64 	%fd955, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd952, %fd948, %fd124, %fd955;
	// inline asm
	mov.f64 	%fd959, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd956, %fd952, %fd124, %fd959;
	// inline asm
	mov.f64 	%fd963, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd960, %fd956, %fd124, %fd963;
	// inline asm
	mul.rn.f64 	%fd965, %fd960, %fd124;
	// inline asm
	fma.rn.f64 	%fd964, %fd965, %fd1448, %fd1448;
	// inline asm
	mov.f64 	%fd1449, %fd964;

BB12_324:
	and.b32  	%r729, %r227, 2;
	setp.eq.s32 	%p241, %r729, 0;
	neg.f64 	%fd968, %fd1449;
	selp.f64 	%fd1450, %fd1449, %fd968, %p241;
	bra.uni 	BB12_326;

BB12_325:
	mov.f64 	%fd1450, 0dFFF8000000000000;

BB12_326:
	setp.eq.f64 	%p242, %fd120, 0d0000000000000000;
	or.pred  	%p243, %p16, %p242;
	or.pred  	%p244, %p15, %p243;
	@%p244 bra 	BB12_349;

	// inline asm
	abs.f64 	%fd970, %fd120;
	// inline asm
	setp.gt.f64 	%p245, %fd970, 0d41E0000000000000;
	@%p245 bra 	BB12_329;

	mov.f64 	%fd985, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd972, %fd120, %fd985;
	// inline asm
	cvt.rni.s32.f64 	%r730, %fd972;
	// inline asm
	cvt.rn.f64.s32 	%fd986, %r730;
	neg.f64 	%fd982, %fd986;
	mov.f64 	%fd975, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd973, %fd982, %fd975, %fd120;
	// inline asm
	mov.f64 	%fd979, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd977, %fd982, %fd979, %fd973;
	// inline asm
	mov.f64 	%fd983, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd981, %fd982, %fd983, %fd977;
	// inline asm
	mov.u32 	%r817, %r730;
	mov.f64 	%fd1451, %fd981;
	bra.uni 	BB12_345;

BB12_329:
	mov.b64 	 %rl349, %fd120;
	and.b64  	%rl999, %rl349, -9223372036854775808;
	shr.u64 	%rl351, %rl349, 52;
	and.b64  	%rl845, %rl351, 2047;
	add.s64 	%rl846, %rl845, 4294966272;
	cvt.u32.u64 	%r229, %rl846;
	shl.b64 	%rl847, %rl349, 11;
	or.b64  	%rl352, %rl847, -9223372036854775808;
	shr.u32 	%r734, %r229, 6;
	mov.u32 	%r735, 16;
	sub.s32 	%r230, %r735, %r734;
	mov.u32 	%r736, 15;
	sub.s32 	%r815, %r736, %r734;
	mov.u32 	%r737, 19;
	sub.s32 	%r232, %r737, %r734;
	mov.u32 	%r732, 18;
	// inline asm
	min.s32 	%r731, %r732, %r232;
	// inline asm
	setp.lt.s32 	%p246, %r815, %r731;
	@%p246 bra 	BB12_331;

	mov.u64 	%rl996, 0;
	bra.uni 	BB12_333;

BB12_331:
	mov.u32 	%r738, 1;
	sub.s32 	%r233, %r738, %r230;
	mov.u64 	%rl996, 0;

BB12_332:
	.pragma "nounroll";
	shl.b32 	%r742, %r815, 3;
	mov.u32 	%r743, __internal_i2opi_d;
	add.s32 	%r744, %r743, %r742;
	ld.const.u64 	%rl851, [%r744];
	mul.lo.s64 	%rl853, %rl851, %rl352;
	// inline asm
	mul.hi.u64 	%rl850, %rl851, %rl352;
	// inline asm
	mad.lo.s64 	%rl854, %rl851, %rl352, %rl996;
	setp.lt.u64 	%p247, %rl854, %rl853;
	selp.u64 	%rl855, 1, 0, %p247;
	add.s64 	%rl996, %rl855, %rl850;
	add.s32 	%r745, %r233, %r815;
	shl.b32 	%r746, %r745, 3;
	add.s32 	%r748, %r45, %r746;
	st.local.u64 	[%r748], %rl854;
	// inline asm
	min.s32 	%r739, %r732, %r232;
	// inline asm
	add.s32 	%r815, %r815, 1;
	setp.lt.s32 	%p248, %r815, %r739;
	@%p248 bra 	BB12_332;

BB12_333:
	mov.u32 	%r749, 1;
	sub.s32 	%r750, %r749, %r230;
	add.s32 	%r751, %r750, %r815;
	shl.b32 	%r752, %r751, 3;
	add.s32 	%r754, %r45, %r752;
	st.local.u64 	[%r754], %rl996;
	ld.local.u64 	%rl997, [%r45+24];
	ld.local.u64 	%rl998, [%r45+16];
	and.b32  	%r755, %r229, 63;
	setp.eq.s32 	%p249, %r755, 0;
	@%p249 bra 	BB12_335;

	and.b64  	%rl856, %rl351, 63;
	cvt.u32.u64 	%r756, %rl856;
	shl.b64 	%rl857, %rl997, %r756;
	neg.s32 	%r757, %r229;
	and.b32  	%r758, %r757, 63;
	shr.u64 	%rl858, %rl998, %r758;
	or.b64  	%rl997, %rl858, %rl857;
	shl.b64 	%rl859, %rl998, %r756;
	ld.local.u64 	%rl860, [%r45+8];
	shr.u64 	%rl861, %rl860, %r758;
	or.b64  	%rl998, %rl861, %rl859;

BB12_335:
	shr.u64 	%rl862, %rl997, 62;
	cvt.u32.u64 	%r759, %rl862;
	shr.u64 	%rl863, %rl998, 62;
	shl.b64 	%rl864, %rl997, 2;
	or.b64  	%rl1003, %rl863, %rl864;
	shl.b64 	%rl363, %rl998, 2;
	setp.ne.s64 	%p250, %rl363, 0;
	selp.u64 	%rl865, 1, 0, %p250;
	or.b64  	%rl866, %rl865, %rl1003;
	setp.gt.u64 	%p251, %rl866, -9223372036854775808;
	selp.u32 	%r760, 1, 0, %p251;
	add.s32 	%r761, %r760, %r759;
	neg.s32 	%r762, %r761;
	setp.lt.s64 	%p252, %rl349, 0;
	selp.b32 	%r817, %r762, %r761, %p252;
	@%p251 bra 	BB12_337;

	mov.u64 	%rl1002, %rl363;
	bra.uni 	BB12_338;

BB12_337:
	not.b64 	%rl867, %rl1003;
	neg.s64 	%rl364, %rl363;
	setp.eq.s64 	%p253, %rl363, 0;
	selp.u64 	%rl868, 1, 0, %p253;
	add.s64 	%rl1003, %rl868, %rl867;
	xor.b64  	%rl999, %rl999, -9223372036854775808;
	mov.u64 	%rl1002, %rl364;

BB12_338:
	mov.u64 	%rl1001, %rl1002;
	setp.gt.s64 	%p254, %rl1003, 0;
	@%p254 bra 	BB12_340;

	mov.u32 	%r816, 0;
	bra.uni 	BB12_342;

BB12_340:
	mov.u32 	%r816, 0;

BB12_341:
	shr.u64 	%rl869, %rl1001, 63;
	shl.b64 	%rl870, %rl1003, 1;
	or.b64  	%rl1003, %rl869, %rl870;
	shl.b64 	%rl1001, %rl1001, 1;
	add.s32 	%r816, %r816, -1;
	setp.gt.s64 	%p255, %rl1003, 0;
	@%p255 bra 	BB12_341;

BB12_342:
	mul.lo.s64 	%rl1005, %rl1003, -3958705157555305931;
	mov.u64 	%rl873, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl871, %rl1003, %rl873;
	// inline asm
	setp.gt.s64 	%p256, %rl871, 0;
	mov.u64 	%rl1004, %rl871;
	@%p256 bra 	BB12_343;
	bra.uni 	BB12_344;

BB12_343:
	shl.b64 	%rl874, %rl871, 1;
	shr.u64 	%rl875, %rl1005, 63;
	or.b64  	%rl1004, %rl874, %rl875;
	mul.lo.s64 	%rl1005, %rl1003, -7917410315110611862;
	add.s32 	%r816, %r816, -1;

BB12_344:
	setp.ne.s64 	%p257, %rl1005, 0;
	selp.u64 	%rl876, 1, 0, %p257;
	add.s64 	%rl877, %rl876, %rl1004;
	add.s32 	%r765, %r816, 1022;
	cvt.u64.u32 	%rl878, %r765;
	shl.b64 	%rl879, %rl878, 52;
	shr.u64 	%rl880, %rl877, 11;
	shr.u64 	%rl881, %rl877, 10;
	and.b64  	%rl882, %rl881, 1;
	add.s64 	%rl883, %rl879, %rl880;
	add.s64 	%rl884, %rl883, %rl882;
	or.b64  	%rl885, %rl884, %rl999;
	mov.b64 	 %fd1451, %rl885;

BB12_345:
	and.b32  	%r766, %r817, 1;
	setp.eq.s32 	%p258, %r766, 0;
	mul.rn.f64 	%fd134, %fd1451, %fd1451;
	@%p258 bra 	BB12_347;

	mov.f64 	%fd988, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd990, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd987, %fd988, %fd134, %fd990;
	// inline asm
	mov.f64 	%fd994, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd991, %fd987, %fd134, %fd994;
	// inline asm
	mov.f64 	%fd998, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd995, %fd991, %fd134, %fd998;
	// inline asm
	mov.f64 	%fd1002, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd999, %fd995, %fd134, %fd1002;
	// inline asm
	mov.f64 	%fd1006, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd1003, %fd999, %fd134, %fd1006;
	// inline asm
	mov.f64 	%fd1010, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd1007, %fd1003, %fd134, %fd1010;
	// inline asm
	mov.f64 	%fd1014, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd1011, %fd1007, %fd134, %fd1014;
	// inline asm
	mov.f64 	%fd1452, %fd1011;
	bra.uni 	BB12_348;

BB12_347:
	mov.f64 	%fd1016, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd1018, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd1015, %fd1016, %fd134, %fd1018;
	// inline asm
	mov.f64 	%fd1022, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd1019, %fd1015, %fd134, %fd1022;
	// inline asm
	mov.f64 	%fd1026, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd1023, %fd1019, %fd134, %fd1026;
	// inline asm
	mov.f64 	%fd1030, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd1027, %fd1023, %fd134, %fd1030;
	// inline asm
	mov.f64 	%fd1034, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd1031, %fd1027, %fd134, %fd1034;
	// inline asm
	mul.rn.f64 	%fd1036, %fd1031, %fd134;
	// inline asm
	fma.rn.f64 	%fd1035, %fd1036, %fd1451, %fd1451;
	// inline asm
	mov.f64 	%fd1452, %fd1035;

BB12_348:
	and.b32  	%r767, %r817, 2;
	setp.eq.s32 	%p259, %r767, 0;
	neg.f64 	%fd1039, %fd1452;
	selp.f64 	%fd1453, %fd1452, %fd1039, %p259;
	bra.uni 	BB12_350;

BB12_349:
	mov.f64 	%fd1040, 0d0000000000000000;
	mul.rn.f64 	%fd1453, %fd120, %fd1040;

BB12_350:
	neg.f64 	%fd1041, %fd1453;
	mov.f64 	%fd1468, %fd1450;
	mov.f64 	%fd1469, %fd1041;
	@%p7 bra 	BB12_351;
	bra.uni 	BB12_352;

BB12_351:
	mov.f64 	%fd1468, %fd1450;
	mov.f64 	%fd1469, %fd1453;

BB12_352:
	mul.f64 	%fd1043, %fd11, %fd1468;
	neg.f64 	%fd1045, %fd12;
	fma.rn.f64 	%fd1046, %fd1045, %fd1469, %fd1043;
	mul.f64 	%fd1047, %fd12, %fd1468;
	fma.rn.f64 	%fd1048, %fd11, %fd1469, %fd1047;
	mov.f64 	%fd1466, %fd1046;
	mov.f64 	%fd1467, %fd1048;

BB12_353:
	add.f64 	%fd1051, %fd1454, %fd1456;
	add.f64 	%fd1053, %fd1051, %fd1458;
	add.f64 	%fd1055, %fd1053, %fd1460;
	add.f64 	%fd1057, %fd1055, %fd1462;
	add.f64 	%fd1059, %fd1057, %fd1464;
	add.f64 	%fd1061, %fd1059, %fd1466;
	st.global.f64 	[%r38], %fd1061;
	add.f64 	%fd1064, %fd1455, %fd1457;
	add.f64 	%fd1066, %fd1064, %fd1459;
	add.f64 	%fd1068, %fd1066, %fd1461;
	add.f64 	%fd1070, %fd1068, %fd1463;
	add.f64 	%fd1072, %fd1070, %fd1465;
	add.f64 	%fd1074, %fd1072, %fd1467;
	st.global.f64 	[%r38+8], %fd1074;
	mul.f64 	%fd1075, %fd1456, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1076, %fd1457, 0d3FE904C37505DE4D;
	mov.f64 	%fd1077, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1078, %fd1456, 0d3FE3F3A0E28BEDD6, %fd1076;
	add.f64 	%fd1079, %fd1454, %fd1078;
	mul.f64 	%fd1080, %fd1458, 0dBFCC7B90E3024574;
	mul.f64 	%fd1081, %fd1459, 0d3FEF329C0558E96C;
	mov.f64 	%fd1082, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1083, %fd1458, 0dBFCC7B90E3024574, %fd1081;
	add.f64 	%fd1084, %fd1079, %fd1083;
	mul.f64 	%fd1085, %fd1460, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1086, %fd1461, 0d3FDBC4C04D71ABBF;
	mov.f64 	%fd1087, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1088, %fd1460, 0dBFECD4BCA9CB5C70, %fd1086;
	add.f64 	%fd1089, %fd1084, %fd1088;
	mul.f64 	%fd1090, %fd1462, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1091, %fd1463, 0d3FDBC4C04D71ABBF;
	neg.f64 	%fd1092, %fd1463;
	fma.rn.f64 	%fd1093, %fd1092, %fd1087, %fd1090;
	add.f64 	%fd1094, %fd1089, %fd1093;
	mul.f64 	%fd1095, %fd1464, 0dBFCC7B90E3024574;
	mul.f64 	%fd1096, %fd1465, 0d3FEF329C0558E96C;
	neg.f64 	%fd1097, %fd1465;
	fma.rn.f64 	%fd1098, %fd1097, %fd1082, %fd1095;
	add.f64 	%fd1099, %fd1094, %fd1098;
	mul.f64 	%fd1100, %fd1466, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1101, %fd1467, 0d3FE904C37505DE4D;
	neg.f64 	%fd1102, %fd1467;
	fma.rn.f64 	%fd1103, %fd1102, %fd1077, %fd1100;
	add.f64 	%fd1104, %fd1099, %fd1103;
	st.global.f64 	[%r39], %fd1104;
	mul.f64 	%fd1105, %fd1457, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1106, %fd1456, 0d3FE904C37505DE4D;
	neg.f64 	%fd1107, %fd1456;
	fma.rn.f64 	%fd1108, %fd1107, %fd1077, %fd1105;
	add.f64 	%fd1109, %fd1455, %fd1108;
	mul.f64 	%fd1110, %fd1459, 0dBFCC7B90E3024574;
	mul.f64 	%fd1111, %fd1458, 0d3FEF329C0558E96C;
	neg.f64 	%fd1112, %fd1458;
	fma.rn.f64 	%fd1113, %fd1112, %fd1082, %fd1110;
	add.f64 	%fd1114, %fd1109, %fd1113;
	mul.f64 	%fd1115, %fd1461, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1116, %fd1460, 0d3FDBC4C04D71ABBF;
	neg.f64 	%fd1117, %fd1460;
	fma.rn.f64 	%fd1118, %fd1117, %fd1087, %fd1115;
	add.f64 	%fd1119, %fd1114, %fd1118;
	mul.f64 	%fd1120, %fd1463, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1121, %fd1462, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1122, %fd1463, 0dBFECD4BCA9CB5C70, %fd1121;
	add.f64 	%fd1123, %fd1119, %fd1122;
	mul.f64 	%fd1124, %fd1465, 0dBFCC7B90E3024574;
	mul.f64 	%fd1125, %fd1464, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1126, %fd1465, 0dBFCC7B90E3024574, %fd1125;
	add.f64 	%fd1127, %fd1123, %fd1126;
	mul.f64 	%fd1128, %fd1467, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1129, %fd1466, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1130, %fd1467, 0d3FE3F3A0E28BEDD6, %fd1129;
	add.f64 	%fd1131, %fd1127, %fd1130;
	st.global.f64 	[%r39+8], %fd1131;
	mul.f64 	%fd1132, %fd1456, 0dBFCC7B90E3024574;
	mul.f64 	%fd1133, %fd1457, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1134, %fd1456, 0dBFCC7B90E3024574, %fd1133;
	add.f64 	%fd1135, %fd1454, %fd1134;
	mul.f64 	%fd1136, %fd1458, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1137, %fd1459, 0d3FDBC4C04D71ABBF;
	neg.f64 	%fd1138, %fd1459;
	fma.rn.f64 	%fd1139, %fd1138, %fd1087, %fd1136;
	add.f64 	%fd1140, %fd1135, %fd1139;
	mul.f64 	%fd1141, %fd1460, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1142, %fd1461, 0d3FE904C37505DE4D;
	neg.f64 	%fd1143, %fd1461;
	fma.rn.f64 	%fd1144, %fd1143, %fd1077, %fd1141;
	add.f64 	%fd1145, %fd1140, %fd1144;
	mul.f64 	%fd1146, %fd1462, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1147, %fd1463, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1148, %fd1462, 0d3FE3F3A0E28BEDD6, %fd1147;
	add.f64 	%fd1149, %fd1145, %fd1148;
	mul.f64 	%fd1150, %fd1464, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1151, %fd1465, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1152, %fd1464, 0dBFECD4BCA9CB5C70, %fd1151;
	add.f64 	%fd1153, %fd1149, %fd1152;
	mul.f64 	%fd1154, %fd1466, 0dBFCC7B90E3024574;
	mul.f64 	%fd1155, %fd1467, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1156, %fd1102, %fd1082, %fd1154;
	add.f64 	%fd1157, %fd1153, %fd1156;
	st.global.f64 	[%r40], %fd1157;
	mul.f64 	%fd1158, %fd1457, 0dBFCC7B90E3024574;
	mul.f64 	%fd1159, %fd1456, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1160, %fd1107, %fd1082, %fd1158;
	add.f64 	%fd1161, %fd1455, %fd1160;
	mul.f64 	%fd1162, %fd1459, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1163, %fd1458, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1164, %fd1459, 0dBFECD4BCA9CB5C70, %fd1163;
	add.f64 	%fd1165, %fd1161, %fd1164;
	mul.f64 	%fd1166, %fd1461, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1167, %fd1460, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1168, %fd1461, 0d3FE3F3A0E28BEDD6, %fd1167;
	add.f64 	%fd1169, %fd1165, %fd1168;
	mul.f64 	%fd1170, %fd1463, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1171, %fd1462, 0d3FE904C37505DE4D;
	neg.f64 	%fd1172, %fd1462;
	fma.rn.f64 	%fd1173, %fd1172, %fd1077, %fd1170;
	add.f64 	%fd1174, %fd1169, %fd1173;
	mul.f64 	%fd1175, %fd1465, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1176, %fd1464, 0d3FDBC4C04D71ABBF;
	neg.f64 	%fd1177, %fd1464;
	fma.rn.f64 	%fd1178, %fd1177, %fd1087, %fd1175;
	add.f64 	%fd1179, %fd1174, %fd1178;
	mul.f64 	%fd1180, %fd1467, 0dBFCC7B90E3024574;
	mul.f64 	%fd1181, %fd1466, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1182, %fd1467, 0dBFCC7B90E3024574, %fd1181;
	add.f64 	%fd1183, %fd1179, %fd1182;
	st.global.f64 	[%r40+8], %fd1183;
	mul.f64 	%fd1184, %fd1456, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1185, %fd1457, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1186, %fd1456, 0dBFECD4BCA9CB5C70, %fd1185;
	add.f64 	%fd1187, %fd1454, %fd1186;
	mul.f64 	%fd1188, %fd1458, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1189, %fd1459, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1190, %fd1138, %fd1077, %fd1188;
	add.f64 	%fd1191, %fd1187, %fd1190;
	mul.f64 	%fd1192, %fd1460, 0dBFCC7B90E3024574;
	mul.f64 	%fd1193, %fd1461, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1194, %fd1460, 0dBFCC7B90E3024574, %fd1193;
	add.f64 	%fd1195, %fd1191, %fd1194;
	mul.f64 	%fd1196, %fd1462, 0dBFCC7B90E3024574;
	mul.f64 	%fd1197, %fd1463, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1198, %fd1092, %fd1082, %fd1196;
	add.f64 	%fd1199, %fd1195, %fd1198;
	mul.f64 	%fd1200, %fd1464, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1201, %fd1465, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1202, %fd1464, 0d3FE3F3A0E28BEDD6, %fd1201;
	add.f64 	%fd1203, %fd1199, %fd1202;
	mul.f64 	%fd1204, %fd1466, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1205, %fd1467, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1206, %fd1102, %fd1087, %fd1204;
	add.f64 	%fd1207, %fd1203, %fd1206;
	st.global.f64 	[%r41], %fd1207;
	mul.f64 	%fd1208, %fd1457, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1209, %fd1456, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1210, %fd1107, %fd1087, %fd1208;
	add.f64 	%fd1211, %fd1455, %fd1210;
	mul.f64 	%fd1212, %fd1459, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1213, %fd1458, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1214, %fd1459, 0d3FE3F3A0E28BEDD6, %fd1213;
	add.f64 	%fd1215, %fd1211, %fd1214;
	mul.f64 	%fd1216, %fd1461, 0dBFCC7B90E3024574;
	mul.f64 	%fd1217, %fd1460, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1218, %fd1117, %fd1082, %fd1216;
	add.f64 	%fd1219, %fd1215, %fd1218;
	mul.f64 	%fd1220, %fd1463, 0dBFCC7B90E3024574;
	mul.f64 	%fd1221, %fd1462, 0d3FEF329C0558E96C;
	fma.rn.f64 	%fd1222, %fd1463, 0dBFCC7B90E3024574, %fd1221;
	add.f64 	%fd1223, %fd1219, %fd1222;
	mul.f64 	%fd1224, %fd1465, 0d3FE3F3A0E28BEDD6;
	mul.f64 	%fd1225, %fd1464, 0d3FE904C37505DE4D;
	fma.rn.f64 	%fd1226, %fd1177, %fd1077, %fd1224;
	add.f64 	%fd1227, %fd1223, %fd1226;
	mul.f64 	%fd1228, %fd1467, 0dBFECD4BCA9CB5C70;
	mul.f64 	%fd1229, %fd1466, 0d3FDBC4C04D71ABBF;
	fma.rn.f64 	%fd1230, %fd1467, 0dBFECD4BCA9CB5C70, %fd1229;
	add.f64 	%fd1231, %fd1227, %fd1230;
	st.global.f64 	[%r41+8], %fd1231;
	neg.f64 	%fd1232, %fd1457;
	fma.rn.f64 	%fd1233, %fd1232, %fd1087, %fd1184;
	add.f64 	%fd1234, %fd1454, %fd1233;
	fma.rn.f64 	%fd1235, %fd1458, 0d3FE3F3A0E28BEDD6, %fd1189;
	add.f64 	%fd1236, %fd1234, %fd1235;
	fma.rn.f64 	%fd1237, %fd1143, %fd1082, %fd1192;
	add.f64 	%fd1238, %fd1236, %fd1237;
	fma.rn.f64 	%fd1239, %fd1462, 0dBFCC7B90E3024574, %fd1197;
	add.f64 	%fd1240, %fd1238, %fd1239;
	fma.rn.f64 	%fd1241, %fd1097, %fd1077, %fd1200;
	add.f64 	%fd1242, %fd1240, %fd1241;
	fma.rn.f64 	%fd1243, %fd1466, 0dBFECD4BCA9CB5C70, %fd1205;
	add.f64 	%fd1244, %fd1242, %fd1243;
	st.global.f64 	[%r42], %fd1244;
	fma.rn.f64 	%fd1245, %fd1457, 0dBFECD4BCA9CB5C70, %fd1209;
	add.f64 	%fd1246, %fd1455, %fd1245;
	fma.rn.f64 	%fd1247, %fd1112, %fd1077, %fd1212;
	add.f64 	%fd1248, %fd1246, %fd1247;
	fma.rn.f64 	%fd1249, %fd1461, 0dBFCC7B90E3024574, %fd1217;
	add.f64 	%fd1250, %fd1248, %fd1249;
	fma.rn.f64 	%fd1251, %fd1172, %fd1082, %fd1220;
	add.f64 	%fd1252, %fd1250, %fd1251;
	fma.rn.f64 	%fd1253, %fd1465, 0d3FE3F3A0E28BEDD6, %fd1225;
	add.f64 	%fd1254, %fd1252, %fd1253;
	neg.f64 	%fd1255, %fd1466;
	fma.rn.f64 	%fd1256, %fd1255, %fd1087, %fd1228;
	add.f64 	%fd1257, %fd1254, %fd1256;
	st.global.f64 	[%r42+8], %fd1257;
	fma.rn.f64 	%fd1258, %fd1232, %fd1082, %fd1132;
	add.f64 	%fd1259, %fd1454, %fd1258;
	fma.rn.f64 	%fd1260, %fd1458, 0dBFECD4BCA9CB5C70, %fd1137;
	add.f64 	%fd1261, %fd1259, %fd1260;
	fma.rn.f64 	%fd1262, %fd1460, 0d3FE3F3A0E28BEDD6, %fd1142;
	add.f64 	%fd1263, %fd1261, %fd1262;
	fma.rn.f64 	%fd1264, %fd1092, %fd1077, %fd1146;
	add.f64 	%fd1265, %fd1263, %fd1264;
	fma.rn.f64 	%fd1266, %fd1097, %fd1087, %fd1150;
	add.f64 	%fd1267, %fd1265, %fd1266;
	fma.rn.f64 	%fd1268, %fd1466, 0dBFCC7B90E3024574, %fd1155;
	add.f64 	%fd1269, %fd1267, %fd1268;
	st.global.f64 	[%r43], %fd1269;
	fma.rn.f64 	%fd1270, %fd1457, 0dBFCC7B90E3024574, %fd1159;
	add.f64 	%fd1271, %fd1455, %fd1270;
	fma.rn.f64 	%fd1272, %fd1112, %fd1087, %fd1162;
	add.f64 	%fd1273, %fd1271, %fd1272;
	fma.rn.f64 	%fd1274, %fd1117, %fd1077, %fd1166;
	add.f64 	%fd1275, %fd1273, %fd1274;
	fma.rn.f64 	%fd1276, %fd1463, 0d3FE3F3A0E28BEDD6, %fd1171;
	add.f64 	%fd1277, %fd1275, %fd1276;
	fma.rn.f64 	%fd1278, %fd1465, 0dBFECD4BCA9CB5C70, %fd1176;
	add.f64 	%fd1279, %fd1277, %fd1278;
	fma.rn.f64 	%fd1280, %fd1255, %fd1082, %fd1180;
	add.f64 	%fd1281, %fd1279, %fd1280;
	st.global.f64 	[%r43+8], %fd1281;
	fma.rn.f64 	%fd1282, %fd1232, %fd1077, %fd1075;
	add.f64 	%fd1283, %fd1454, %fd1282;
	fma.rn.f64 	%fd1284, %fd1138, %fd1082, %fd1080;
	add.f64 	%fd1285, %fd1283, %fd1284;
	fma.rn.f64 	%fd1286, %fd1143, %fd1087, %fd1085;
	add.f64 	%fd1287, %fd1285, %fd1286;
	fma.rn.f64 	%fd1288, %fd1462, 0dBFECD4BCA9CB5C70, %fd1091;
	add.f64 	%fd1289, %fd1287, %fd1288;
	fma.rn.f64 	%fd1290, %fd1464, 0dBFCC7B90E3024574, %fd1096;
	add.f64 	%fd1291, %fd1289, %fd1290;
	fma.rn.f64 	%fd1292, %fd1466, 0d3FE3F3A0E28BEDD6, %fd1101;
	add.f64 	%fd1293, %fd1291, %fd1292;
	st.global.f64 	[%r44], %fd1293;
	fma.rn.f64 	%fd1294, %fd1457, 0d3FE3F3A0E28BEDD6, %fd1106;
	add.f64 	%fd1295, %fd1455, %fd1294;
	fma.rn.f64 	%fd1296, %fd1459, 0dBFCC7B90E3024574, %fd1111;
	add.f64 	%fd1297, %fd1295, %fd1296;
	fma.rn.f64 	%fd1298, %fd1461, 0dBFECD4BCA9CB5C70, %fd1116;
	add.f64 	%fd1299, %fd1297, %fd1298;
	fma.rn.f64 	%fd1300, %fd1172, %fd1087, %fd1120;
	add.f64 	%fd1301, %fd1299, %fd1300;
	fma.rn.f64 	%fd1302, %fd1177, %fd1082, %fd1124;
	add.f64 	%fd1303, %fd1301, %fd1302;
	fma.rn.f64 	%fd1304, %fd1255, %fd1077, %fd1128;
	add.f64 	%fd1305, %fd1303, %fd1304;
	st.global.f64 	[%r44+8], %fd1305;
	ret;
}

.entry DIT8C2CM(
	.param .u32 .ptr .global .align 8 DIT8C2CM_param_0,
	.param .u32 DIT8C2CM_param_1,
	.param .u32 DIT8C2CM_param_2,
	.param .u32 DIT8C2CM_param_3,
	.param .u32 DIT8C2CM_param_4,
	.param .u32 DIT8C2CM_param_5
)
{
	.local .align 8 .b8 	__local_depot13[40];
	.reg .b32 	%SP;
	.reg .f64 	%fd<1105>;
	.reg .pred 	%p<99>;
	.reg .s32 	%r<320>;
	.reg .s64 	%rl<456>;


	mov.u32 	%SP, __local_depot13;
	ld.param.u32 	%r4, [DIT8C2CM_param_5];
	// inline asm
	mov.u32 	%r102, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r103, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r104, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r105, %tid.x;
	// inline asm
	add.s32 	%r110, %r105, %r102;
	mad.lo.s32 	%r291, %r104, %r103, %r110;
	// inline asm
	mov.u32 	%r106, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r107, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r108, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r109, %tid.y;
	// inline asm
	add.s32 	%r111, %r109, %r106;
	mad.lo.s32 	%r6, %r108, %r107, %r111;
	setp.eq.s32 	%p2, %r4, 0;
	@%p2 bra 	BB13_166;

	ld.param.u32 	%r290, [DIT8C2CM_param_5];
	setp.ne.s32 	%p3, %r290, 1;
	@%p3 bra 	BB13_2;
	bra.uni 	BB13_3;

BB13_2:
	mov.u32 	%r291, 0;

BB13_3:
	shl.b32 	%r113, %r291, 4;
	ld.param.u32 	%r283, [DIT8C2CM_param_0];
	add.s32 	%r9, %r283, %r113;
	ld.global.f64 	%fd1, [%r9];
	ld.global.f64 	%fd2, [%r9+8];
	cvt.rn.f64.s32 	%fd3, %r114;
	mul.f64 	%fd96, %fd3, 0d401921FB54442D18;
	mov.f64 	%fd4, 0d4010000000000000;
	div.rn.f64 	%fd95, %fd96, 0d4010000000000000;
	// inline asm
	abs.f64 	%fd94, %fd95;
	// inline asm
	setp.gt.f64 	%p4, %fd94, 0d41E0000000000000;
	add.u32 	%r10, %SP, 0;
	@%p4 bra 	BB13_5;

	mov.f64 	%fd112, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd99, %fd95, %fd112;
	// inline asm
	cvt.rni.s32.f64 	%r115, %fd99;
	// inline asm
	cvt.rn.f64.s32 	%fd113, %r115;
	neg.f64 	%fd109, %fd113;
	mov.f64 	%fd102, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd100, %fd109, %fd102, %fd95;
	// inline asm
	mov.f64 	%fd106, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd104, %fd109, %fd106, %fd100;
	// inline asm
	mov.f64 	%fd110, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd108, %fd109, %fd110, %fd104;
	// inline asm
	mov.u32 	%r295, %r115;
	mov.f64 	%fd1057, %fd108;
	bra.uni 	BB13_20;

BB13_5:
	mov.u32 	%r117, 18;
	mov.u32 	%r118, -67108829;
	// inline asm
	min.s32 	%r116, %r117, %r118;
	// inline asm
	setp.gt.s32 	%p5, %r116, -67108833;
	@%p5 bra 	BB13_7;

	mov.u64 	%rl400, 0;
	mov.u32 	%r293, 0;
	bra.uni 	BB13_10;

BB13_7:
	mov.u64 	%rl400, 0;
	mov.u32 	%r292, -67108833;

BB13_8:
	.pragma "nounroll";
	mov.u32 	%r12, %r292;
	shl.b32 	%r124, %r12, 3;
	mov.u32 	%r125, __internal_i2opi_d;
	add.s32 	%r126, %r125, %r124;
	ld.const.u64 	%rl151, [%r126];
	shl.b64 	%rl153, %rl151, 63;
	mov.u64 	%rl152, -9223372036854775808;
	// inline asm
	mul.hi.u64 	%rl150, %rl151, %rl152;
	// inline asm
	add.s64 	%rl154, %rl153, %rl400;
	setp.lt.u64 	%p6, %rl154, %rl153;
	selp.u64 	%rl155, 1, 0, %p6;
	add.s64 	%rl400, %rl155, %rl150;
	add.s32 	%r128, %r124, %r10;
	st.local.u64 	[%r128+536870664], %rl154;
	// inline asm
	min.s32 	%r121, %r117, %r118;
	// inline asm
	add.s32 	%r13, %r12, 1;
	setp.lt.s32 	%p7, %r13, %r121;
	mov.u32 	%r292, %r13;
	@%p7 bra 	BB13_8;

	add.s32 	%r293, %r12, 67108834;

BB13_10:
	shl.b32 	%r129, %r293, 3;
	add.s32 	%r131, %r10, %r129;
	st.local.u64 	[%r131], %rl400;
	ld.local.u64 	%rl156, [%r10+24];
	shr.u64 	%rl157, %rl156, 62;
	cvt.u32.u64 	%r132, %rl157;
	shl.b64 	%rl158, %rl156, 2;
	ld.local.u64 	%rl159, [%r10+16];
	shr.u64 	%rl160, %rl159, 62;
	or.b64  	%rl405, %rl160, %rl158;
	shl.b64 	%rl5, %rl159, 2;
	setp.ne.s64 	%p8, %rl5, 0;
	selp.u64 	%rl161, 1, 0, %p8;
	or.b64  	%rl162, %rl161, %rl405;
	setp.gt.u64 	%p9, %rl162, -9223372036854775808;
	selp.u32 	%r133, 1, 0, %p9;
	add.s32 	%r295, %r133, %r132;
	@%p9 bra 	BB13_12;

	mov.u64 	%rl401, 0;
	mov.u64 	%rl404, %rl5;
	bra.uni 	BB13_13;

BB13_12:
	not.b64 	%rl165, %rl405;
	neg.s64 	%rl6, %rl5;
	setp.eq.s64 	%p10, %rl5, 0;
	selp.u64 	%rl166, 1, 0, %p10;
	add.s64 	%rl405, %rl166, %rl165;
	mov.u64 	%rl401, -9223372036854775808;
	mov.u64 	%rl404, %rl6;

BB13_13:
	mov.u64 	%rl403, %rl404;
	setp.gt.s64 	%p11, %rl405, 0;
	@%p11 bra 	BB13_15;

	mov.u32 	%r294, 0;
	bra.uni 	BB13_17;

BB13_15:
	mov.u32 	%r294, 0;

BB13_16:
	shr.u64 	%rl167, %rl403, 63;
	shl.b64 	%rl168, %rl405, 1;
	or.b64  	%rl405, %rl167, %rl168;
	shl.b64 	%rl403, %rl403, 1;
	add.s32 	%r294, %r294, -1;
	setp.gt.s64 	%p12, %rl405, 0;
	@%p12 bra 	BB13_16;

BB13_17:
	mul.lo.s64 	%rl407, %rl405, -3958705157555305931;
	mov.u64 	%rl171, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl169, %rl405, %rl171;
	// inline asm
	setp.gt.s64 	%p13, %rl169, 0;
	mov.u64 	%rl406, %rl169;
	@%p13 bra 	BB13_18;
	bra.uni 	BB13_19;

BB13_18:
	shl.b64 	%rl172, %rl169, 1;
	shr.u64 	%rl173, %rl407, 63;
	or.b64  	%rl406, %rl172, %rl173;
	mul.lo.s64 	%rl407, %rl405, -7917410315110611862;
	add.s32 	%r294, %r294, -1;

BB13_19:
	setp.ne.s64 	%p14, %rl407, 0;
	selp.u64 	%rl174, 1, 0, %p14;
	add.s64 	%rl175, %rl174, %rl406;
	add.s32 	%r136, %r294, 1022;
	cvt.u64.u32 	%rl176, %r136;
	shl.b64 	%rl177, %rl176, 52;
	shr.u64 	%rl178, %rl175, 11;
	shr.u64 	%rl179, %rl175, 10;
	and.b64  	%rl180, %rl179, 1;
	add.s64 	%rl181, %rl177, %rl178;
	add.s64 	%rl182, %rl181, %rl180;
	or.b64  	%rl183, %rl182, %rl401;
	mov.b64 	 %fd1057, %rl183;

BB13_20:
	add.s32 	%r23, %r295, 1;
	and.b32  	%r137, %r23, 1;
	setp.eq.s32 	%p15, %r137, 0;
	mul.rn.f64 	%fd9, %fd1057, %fd1057;
	@%p15 bra 	BB13_22;

	mov.f64 	%fd115, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd117, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd114, %fd115, %fd9, %fd117;
	// inline asm
	mov.f64 	%fd121, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd118, %fd114, %fd9, %fd121;
	// inline asm
	mov.f64 	%fd125, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd122, %fd118, %fd9, %fd125;
	// inline asm
	mov.f64 	%fd129, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd126, %fd122, %fd9, %fd129;
	// inline asm
	mov.f64 	%fd133, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd130, %fd126, %fd9, %fd133;
	// inline asm
	mov.f64 	%fd137, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd134, %fd130, %fd9, %fd137;
	// inline asm
	mov.f64 	%fd141, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd138, %fd134, %fd9, %fd141;
	// inline asm
	mov.f64 	%fd1058, %fd138;
	bra.uni 	BB13_23;

BB13_22:
	mov.f64 	%fd143, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd145, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd142, %fd143, %fd9, %fd145;
	// inline asm
	mov.f64 	%fd149, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd146, %fd142, %fd9, %fd149;
	// inline asm
	mov.f64 	%fd153, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd150, %fd146, %fd9, %fd153;
	// inline asm
	mov.f64 	%fd157, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd154, %fd150, %fd9, %fd157;
	// inline asm
	mov.f64 	%fd161, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd158, %fd154, %fd9, %fd161;
	// inline asm
	mul.rn.f64 	%fd163, %fd158, %fd9;
	// inline asm
	fma.rn.f64 	%fd162, %fd163, %fd1057, %fd1057;
	// inline asm
	mov.f64 	%fd1058, %fd162;

BB13_23:
	and.b32  	%r138, %r23, 2;
	setp.eq.s32 	%p16, %r138, 0;
	neg.f64 	%fd166, %fd1058;
	selp.f64 	%fd167, %fd1058, %fd166, %p16;
	mov.f64 	%fd168, 0d0000000000000000;
	mul.rn.f64 	%fd13, %fd95, %fd168;
	mov.f64 	%fd1103, %fd167;
	mov.f64 	%fd1104, %fd13;
	ld.param.u32 	%r289, [DIT8C2CM_param_4];
	setp.eq.s32 	%p1, %r289, 0;
	@%p1 bra 	BB13_24;
	bra.uni 	BB13_25;

BB13_24:
	neg.f64 	%fd169, %fd13;
	mov.f64 	%fd1103, %fd167;
	mov.f64 	%fd1104, %fd169;

BB13_25:
	mul.f64 	%fd173, %fd1, %fd1103;
	neg.f64 	%fd175, %fd2;
	fma.rn.f64 	%fd14, %fd175, %fd1104, %fd173;
	mul.f64 	%fd176, %fd2, %fd1103;
	fma.rn.f64 	%fd15, %fd1, %fd1104, %fd176;
	mul.f64 	%fd177, %fd3, 0d402921FB54442D18;
	div.rn.f64 	%fd171, %fd177, %fd4;
	// inline asm
	abs.f64 	%fd170, %fd171;
	// inline asm
	setp.gt.f64 	%p17, %fd170, 0d41E0000000000000;
	@%p17 bra 	BB13_27;

	mov.f64 	%fd191, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd178, %fd171, %fd191;
	// inline asm
	cvt.rni.s32.f64 	%r139, %fd178;
	// inline asm
	cvt.rn.f64.s32 	%fd192, %r139;
	neg.f64 	%fd188, %fd192;
	mov.f64 	%fd181, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd179, %fd188, %fd181, %fd171;
	// inline asm
	mov.f64 	%fd185, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd183, %fd188, %fd185, %fd179;
	// inline asm
	mov.f64 	%fd189, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd187, %fd188, %fd189, %fd183;
	// inline asm
	mov.u32 	%r299, %r139;
	mov.f64 	%fd1059, %fd187;
	bra.uni 	BB13_42;

BB13_27:
	mov.u32 	%r141, 18;
	mov.u32 	%r142, -67108829;
	// inline asm
	min.s32 	%r140, %r141, %r142;
	// inline asm
	setp.gt.s32 	%p18, %r140, -67108833;
	@%p18 bra 	BB13_29;

	mov.u64 	%rl408, 0;
	mov.u32 	%r297, 0;
	bra.uni 	BB13_32;

BB13_29:
	mov.u64 	%rl408, 0;
	mov.u32 	%r296, -67108833;

BB13_30:
	.pragma "nounroll";
	mov.u32 	%r25, %r296;
	shl.b32 	%r148, %r25, 3;
	mov.u32 	%r149, __internal_i2opi_d;
	add.s32 	%r150, %r149, %r148;
	ld.const.u64 	%rl187, [%r150];
	shl.b64 	%rl189, %rl187, 63;
	mov.u64 	%rl188, -9223372036854775808;
	// inline asm
	mul.hi.u64 	%rl186, %rl187, %rl188;
	// inline asm
	add.s64 	%rl190, %rl189, %rl408;
	setp.lt.u64 	%p19, %rl190, %rl189;
	selp.u64 	%rl191, 1, 0, %p19;
	add.s64 	%rl408, %rl191, %rl186;
	add.s32 	%r152, %r148, %r10;
	st.local.u64 	[%r152+536870664], %rl190;
	// inline asm
	min.s32 	%r145, %r141, %r142;
	// inline asm
	add.s32 	%r26, %r25, 1;
	setp.lt.s32 	%p20, %r26, %r145;
	mov.u32 	%r296, %r26;
	@%p20 bra 	BB13_30;

	add.s32 	%r297, %r25, 67108834;

BB13_32:
	shl.b32 	%r153, %r297, 3;
	add.s32 	%r155, %r10, %r153;
	st.local.u64 	[%r155], %rl408;
	ld.local.u64 	%rl192, [%r10+24];
	shr.u64 	%rl193, %rl192, 62;
	cvt.u32.u64 	%r156, %rl193;
	shl.b64 	%rl194, %rl192, 2;
	ld.local.u64 	%rl195, [%r10+16];
	shr.u64 	%rl196, %rl195, 62;
	or.b64  	%rl413, %rl196, %rl194;
	shl.b64 	%rl26, %rl195, 2;
	setp.ne.s64 	%p21, %rl26, 0;
	selp.u64 	%rl197, 1, 0, %p21;
	or.b64  	%rl198, %rl197, %rl413;
	setp.gt.u64 	%p22, %rl198, -9223372036854775808;
	selp.u32 	%r157, 1, 0, %p22;
	add.s32 	%r299, %r157, %r156;
	@%p22 bra 	BB13_34;

	mov.u64 	%rl409, 0;
	mov.u64 	%rl412, %rl26;
	bra.uni 	BB13_35;

BB13_34:
	not.b64 	%rl201, %rl413;
	neg.s64 	%rl27, %rl26;
	setp.eq.s64 	%p23, %rl26, 0;
	selp.u64 	%rl202, 1, 0, %p23;
	add.s64 	%rl413, %rl202, %rl201;
	mov.u64 	%rl409, -9223372036854775808;
	mov.u64 	%rl412, %rl27;

BB13_35:
	mov.u64 	%rl411, %rl412;
	setp.gt.s64 	%p24, %rl413, 0;
	@%p24 bra 	BB13_37;

	mov.u32 	%r298, 0;
	bra.uni 	BB13_39;

BB13_37:
	mov.u32 	%r298, 0;

BB13_38:
	shr.u64 	%rl203, %rl411, 63;
	shl.b64 	%rl204, %rl413, 1;
	or.b64  	%rl413, %rl203, %rl204;
	shl.b64 	%rl411, %rl411, 1;
	add.s32 	%r298, %r298, -1;
	setp.gt.s64 	%p25, %rl413, 0;
	@%p25 bra 	BB13_38;

BB13_39:
	mul.lo.s64 	%rl415, %rl413, -3958705157555305931;
	mov.u64 	%rl207, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl205, %rl413, %rl207;
	// inline asm
	setp.gt.s64 	%p26, %rl205, 0;
	mov.u64 	%rl414, %rl205;
	@%p26 bra 	BB13_40;
	bra.uni 	BB13_41;

BB13_40:
	shl.b64 	%rl208, %rl205, 1;
	shr.u64 	%rl209, %rl415, 63;
	or.b64  	%rl414, %rl208, %rl209;
	mul.lo.s64 	%rl415, %rl413, -7917410315110611862;
	add.s32 	%r298, %r298, -1;

BB13_41:
	setp.ne.s64 	%p27, %rl415, 0;
	selp.u64 	%rl210, 1, 0, %p27;
	add.s64 	%rl211, %rl210, %rl414;
	add.s32 	%r160, %r298, 1022;
	cvt.u64.u32 	%rl212, %r160;
	shl.b64 	%rl213, %rl212, 52;
	shr.u64 	%rl214, %rl211, 11;
	shr.u64 	%rl215, %rl211, 10;
	and.b64  	%rl216, %rl215, 1;
	add.s64 	%rl217, %rl213, %rl214;
	add.s64 	%rl218, %rl217, %rl216;
	or.b64  	%rl219, %rl218, %rl409;
	mov.b64 	 %fd1059, %rl219;

BB13_42:
	add.s32 	%r36, %r299, 1;
	and.b32  	%r161, %r36, 1;
	setp.eq.s32 	%p28, %r161, 0;
	mul.rn.f64 	%fd20, %fd1059, %fd1059;
	@%p28 bra 	BB13_44;

	mov.f64 	%fd194, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd196, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd193, %fd194, %fd20, %fd196;
	// inline asm
	mov.f64 	%fd200, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd197, %fd193, %fd20, %fd200;
	// inline asm
	mov.f64 	%fd204, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd201, %fd197, %fd20, %fd204;
	// inline asm
	mov.f64 	%fd208, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd205, %fd201, %fd20, %fd208;
	// inline asm
	mov.f64 	%fd212, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd209, %fd205, %fd20, %fd212;
	// inline asm
	mov.f64 	%fd216, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd213, %fd209, %fd20, %fd216;
	// inline asm
	mov.f64 	%fd220, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd217, %fd213, %fd20, %fd220;
	// inline asm
	mov.f64 	%fd1060, %fd217;
	bra.uni 	BB13_45;

BB13_44:
	mov.f64 	%fd222, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd224, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd221, %fd222, %fd20, %fd224;
	// inline asm
	mov.f64 	%fd228, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd225, %fd221, %fd20, %fd228;
	// inline asm
	mov.f64 	%fd232, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd229, %fd225, %fd20, %fd232;
	// inline asm
	mov.f64 	%fd236, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd233, %fd229, %fd20, %fd236;
	// inline asm
	mov.f64 	%fd240, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd237, %fd233, %fd20, %fd240;
	// inline asm
	mul.rn.f64 	%fd242, %fd237, %fd20;
	// inline asm
	fma.rn.f64 	%fd241, %fd242, %fd1059, %fd1059;
	// inline asm
	mov.f64 	%fd1060, %fd241;

BB13_45:
	and.b32  	%r162, %r36, 2;
	setp.eq.s32 	%p29, %r162, 0;
	neg.f64 	%fd245, %fd1060;
	selp.f64 	%fd246, %fd1060, %fd245, %p29;
	mul.rn.f64 	%fd24, %fd171, %fd168;
	mov.f64 	%fd1101, %fd246;
	mov.f64 	%fd1102, %fd24;
	@%p1 bra 	BB13_46;
	bra.uni 	BB13_47;

BB13_46:
	neg.f64 	%fd248, %fd24;
	mov.f64 	%fd1101, %fd246;
	mov.f64 	%fd1102, %fd248;

BB13_47:
	mul.f64 	%fd252, %fd1, %fd1101;
	fma.rn.f64 	%fd25, %fd175, %fd1102, %fd252;
	mul.f64 	%fd255, %fd2, %fd1101;
	fma.rn.f64 	%fd26, %fd1, %fd1102, %fd255;
	mul.f64 	%fd256, %fd3, 0d4032D97C7F3321D2;
	div.rn.f64 	%fd250, %fd256, %fd4;
	// inline asm
	abs.f64 	%fd249, %fd250;
	// inline asm
	setp.gt.f64 	%p30, %fd249, 0d41E0000000000000;
	@%p30 bra 	BB13_49;

	mov.f64 	%fd270, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd257, %fd250, %fd270;
	// inline asm
	cvt.rni.s32.f64 	%r163, %fd257;
	// inline asm
	cvt.rn.f64.s32 	%fd271, %r163;
	neg.f64 	%fd267, %fd271;
	mov.f64 	%fd260, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd258, %fd267, %fd260, %fd250;
	// inline asm
	mov.f64 	%fd264, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd262, %fd267, %fd264, %fd258;
	// inline asm
	mov.f64 	%fd268, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd266, %fd267, %fd268, %fd262;
	// inline asm
	mov.u32 	%r303, %r163;
	mov.f64 	%fd1061, %fd266;
	bra.uni 	BB13_64;

BB13_49:
	mov.u32 	%r165, 18;
	mov.u32 	%r166, -67108829;
	// inline asm
	min.s32 	%r164, %r165, %r166;
	// inline asm
	setp.gt.s32 	%p31, %r164, -67108833;
	@%p31 bra 	BB13_51;

	mov.u64 	%rl416, 0;
	mov.u32 	%r301, 0;
	bra.uni 	BB13_54;

BB13_51:
	mov.u64 	%rl416, 0;
	mov.u32 	%r300, -67108833;

BB13_52:
	.pragma "nounroll";
	mov.u32 	%r38, %r300;
	shl.b32 	%r172, %r38, 3;
	mov.u32 	%r173, __internal_i2opi_d;
	add.s32 	%r174, %r173, %r172;
	ld.const.u64 	%rl223, [%r174];
	shl.b64 	%rl225, %rl223, 63;
	mov.u64 	%rl224, -9223372036854775808;
	// inline asm
	mul.hi.u64 	%rl222, %rl223, %rl224;
	// inline asm
	add.s64 	%rl226, %rl225, %rl416;
	setp.lt.u64 	%p32, %rl226, %rl225;
	selp.u64 	%rl227, 1, 0, %p32;
	add.s64 	%rl416, %rl227, %rl222;
	add.s32 	%r176, %r172, %r10;
	st.local.u64 	[%r176+536870664], %rl226;
	// inline asm
	min.s32 	%r169, %r165, %r166;
	// inline asm
	add.s32 	%r39, %r38, 1;
	setp.lt.s32 	%p33, %r39, %r169;
	mov.u32 	%r300, %r39;
	@%p33 bra 	BB13_52;

	add.s32 	%r301, %r38, 67108834;

BB13_54:
	shl.b32 	%r177, %r301, 3;
	add.s32 	%r179, %r10, %r177;
	st.local.u64 	[%r179], %rl416;
	ld.local.u64 	%rl228, [%r10+24];
	shr.u64 	%rl229, %rl228, 62;
	cvt.u32.u64 	%r180, %rl229;
	shl.b64 	%rl230, %rl228, 2;
	ld.local.u64 	%rl231, [%r10+16];
	shr.u64 	%rl232, %rl231, 62;
	or.b64  	%rl421, %rl232, %rl230;
	shl.b64 	%rl47, %rl231, 2;
	setp.ne.s64 	%p34, %rl47, 0;
	selp.u64 	%rl233, 1, 0, %p34;
	or.b64  	%rl234, %rl233, %rl421;
	setp.gt.u64 	%p35, %rl234, -9223372036854775808;
	selp.u32 	%r181, 1, 0, %p35;
	add.s32 	%r303, %r181, %r180;
	@%p35 bra 	BB13_56;

	mov.u64 	%rl417, 0;
	mov.u64 	%rl420, %rl47;
	bra.uni 	BB13_57;

BB13_56:
	not.b64 	%rl237, %rl421;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p36, %rl47, 0;
	selp.u64 	%rl238, 1, 0, %p36;
	add.s64 	%rl421, %rl238, %rl237;
	mov.u64 	%rl417, -9223372036854775808;
	mov.u64 	%rl420, %rl48;

BB13_57:
	mov.u64 	%rl419, %rl420;
	setp.gt.s64 	%p37, %rl421, 0;
	@%p37 bra 	BB13_59;

	mov.u32 	%r302, 0;
	bra.uni 	BB13_61;

BB13_59:
	mov.u32 	%r302, 0;

BB13_60:
	shr.u64 	%rl239, %rl419, 63;
	shl.b64 	%rl240, %rl421, 1;
	or.b64  	%rl421, %rl239, %rl240;
	shl.b64 	%rl419, %rl419, 1;
	add.s32 	%r302, %r302, -1;
	setp.gt.s64 	%p38, %rl421, 0;
	@%p38 bra 	BB13_60;

BB13_61:
	mul.lo.s64 	%rl423, %rl421, -3958705157555305931;
	mov.u64 	%rl243, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl241, %rl421, %rl243;
	// inline asm
	setp.gt.s64 	%p39, %rl241, 0;
	mov.u64 	%rl422, %rl241;
	@%p39 bra 	BB13_62;
	bra.uni 	BB13_63;

BB13_62:
	shl.b64 	%rl244, %rl241, 1;
	shr.u64 	%rl245, %rl423, 63;
	or.b64  	%rl422, %rl244, %rl245;
	mul.lo.s64 	%rl423, %rl421, -7917410315110611862;
	add.s32 	%r302, %r302, -1;

BB13_63:
	setp.ne.s64 	%p40, %rl423, 0;
	selp.u64 	%rl246, 1, 0, %p40;
	add.s64 	%rl247, %rl246, %rl422;
	add.s32 	%r184, %r302, 1022;
	cvt.u64.u32 	%rl248, %r184;
	shl.b64 	%rl249, %rl248, 52;
	shr.u64 	%rl250, %rl247, 11;
	shr.u64 	%rl251, %rl247, 10;
	and.b64  	%rl252, %rl251, 1;
	add.s64 	%rl253, %rl249, %rl250;
	add.s64 	%rl254, %rl253, %rl252;
	or.b64  	%rl255, %rl254, %rl417;
	mov.b64 	 %fd1061, %rl255;

BB13_64:
	add.s32 	%r49, %r303, 1;
	and.b32  	%r185, %r49, 1;
	setp.eq.s32 	%p41, %r185, 0;
	mul.rn.f64 	%fd31, %fd1061, %fd1061;
	@%p41 bra 	BB13_66;

	mov.f64 	%fd273, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd275, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd272, %fd273, %fd31, %fd275;
	// inline asm
	mov.f64 	%fd279, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd276, %fd272, %fd31, %fd279;
	// inline asm
	mov.f64 	%fd283, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd280, %fd276, %fd31, %fd283;
	// inline asm
	mov.f64 	%fd287, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd284, %fd280, %fd31, %fd287;
	// inline asm
	mov.f64 	%fd291, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd288, %fd284, %fd31, %fd291;
	// inline asm
	mov.f64 	%fd295, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd292, %fd288, %fd31, %fd295;
	// inline asm
	mov.f64 	%fd299, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd296, %fd292, %fd31, %fd299;
	// inline asm
	mov.f64 	%fd1062, %fd296;
	bra.uni 	BB13_67;

BB13_66:
	mov.f64 	%fd301, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd303, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd300, %fd301, %fd31, %fd303;
	// inline asm
	mov.f64 	%fd307, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd304, %fd300, %fd31, %fd307;
	// inline asm
	mov.f64 	%fd311, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd308, %fd304, %fd31, %fd311;
	// inline asm
	mov.f64 	%fd315, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd312, %fd308, %fd31, %fd315;
	// inline asm
	mov.f64 	%fd319, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd316, %fd312, %fd31, %fd319;
	// inline asm
	mul.rn.f64 	%fd321, %fd316, %fd31;
	// inline asm
	fma.rn.f64 	%fd320, %fd321, %fd1061, %fd1061;
	// inline asm
	mov.f64 	%fd1062, %fd320;

BB13_67:
	and.b32  	%r186, %r49, 2;
	setp.eq.s32 	%p42, %r186, 0;
	neg.f64 	%fd324, %fd1062;
	selp.f64 	%fd325, %fd1062, %fd324, %p42;
	mul.rn.f64 	%fd35, %fd250, %fd168;
	mov.f64 	%fd1099, %fd325;
	mov.f64 	%fd1100, %fd35;
	@%p1 bra 	BB13_68;
	bra.uni 	BB13_69;

BB13_68:
	neg.f64 	%fd327, %fd35;
	mov.f64 	%fd1099, %fd325;
	mov.f64 	%fd1100, %fd327;

BB13_69:
	mul.f64 	%fd331, %fd1, %fd1099;
	fma.rn.f64 	%fd36, %fd175, %fd1100, %fd331;
	mul.f64 	%fd334, %fd2, %fd1099;
	fma.rn.f64 	%fd37, %fd1, %fd1100, %fd334;
	mul.f64 	%fd335, %fd3, 0d403921FB54442D18;
	div.rn.f64 	%fd329, %fd335, %fd4;
	// inline asm
	abs.f64 	%fd328, %fd329;
	// inline asm
	setp.gt.f64 	%p43, %fd328, 0d41E0000000000000;
	@%p43 bra 	BB13_71;

	mov.f64 	%fd349, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd336, %fd329, %fd349;
	// inline asm
	cvt.rni.s32.f64 	%r187, %fd336;
	// inline asm
	cvt.rn.f64.s32 	%fd350, %r187;
	neg.f64 	%fd346, %fd350;
	mov.f64 	%fd339, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd337, %fd346, %fd339, %fd329;
	// inline asm
	mov.f64 	%fd343, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd341, %fd346, %fd343, %fd337;
	// inline asm
	mov.f64 	%fd347, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd345, %fd346, %fd347, %fd341;
	// inline asm
	mov.u32 	%r307, %r187;
	mov.f64 	%fd1063, %fd345;
	bra.uni 	BB13_86;

BB13_71:
	mov.u32 	%r189, 18;
	mov.u32 	%r190, -67108829;
	// inline asm
	min.s32 	%r188, %r189, %r190;
	// inline asm
	setp.gt.s32 	%p44, %r188, -67108833;
	@%p44 bra 	BB13_73;

	mov.u64 	%rl424, 0;
	mov.u32 	%r305, 0;
	bra.uni 	BB13_76;

BB13_73:
	mov.u64 	%rl424, 0;
	mov.u32 	%r304, -67108833;

BB13_74:
	.pragma "nounroll";
	mov.u32 	%r51, %r304;
	shl.b32 	%r196, %r51, 3;
	mov.u32 	%r197, __internal_i2opi_d;
	add.s32 	%r198, %r197, %r196;
	ld.const.u64 	%rl259, [%r198];
	shl.b64 	%rl261, %rl259, 63;
	mov.u64 	%rl260, -9223372036854775808;
	// inline asm
	mul.hi.u64 	%rl258, %rl259, %rl260;
	// inline asm
	add.s64 	%rl262, %rl261, %rl424;
	setp.lt.u64 	%p45, %rl262, %rl261;
	selp.u64 	%rl263, 1, 0, %p45;
	add.s64 	%rl424, %rl263, %rl258;
	add.s32 	%r200, %r196, %r10;
	st.local.u64 	[%r200+536870664], %rl262;
	// inline asm
	min.s32 	%r193, %r189, %r190;
	// inline asm
	add.s32 	%r52, %r51, 1;
	setp.lt.s32 	%p46, %r52, %r193;
	mov.u32 	%r304, %r52;
	@%p46 bra 	BB13_74;

	add.s32 	%r305, %r51, 67108834;

BB13_76:
	shl.b32 	%r201, %r305, 3;
	add.s32 	%r203, %r10, %r201;
	st.local.u64 	[%r203], %rl424;
	ld.local.u64 	%rl264, [%r10+24];
	shr.u64 	%rl265, %rl264, 62;
	cvt.u32.u64 	%r204, %rl265;
	shl.b64 	%rl266, %rl264, 2;
	ld.local.u64 	%rl267, [%r10+16];
	shr.u64 	%rl268, %rl267, 62;
	or.b64  	%rl429, %rl268, %rl266;
	shl.b64 	%rl68, %rl267, 2;
	setp.ne.s64 	%p47, %rl68, 0;
	selp.u64 	%rl269, 1, 0, %p47;
	or.b64  	%rl270, %rl269, %rl429;
	setp.gt.u64 	%p48, %rl270, -9223372036854775808;
	selp.u32 	%r205, 1, 0, %p48;
	add.s32 	%r307, %r205, %r204;
	@%p48 bra 	BB13_78;

	mov.u64 	%rl425, 0;
	mov.u64 	%rl428, %rl68;
	bra.uni 	BB13_79;

BB13_78:
	not.b64 	%rl273, %rl429;
	neg.s64 	%rl69, %rl68;
	setp.eq.s64 	%p49, %rl68, 0;
	selp.u64 	%rl274, 1, 0, %p49;
	add.s64 	%rl429, %rl274, %rl273;
	mov.u64 	%rl425, -9223372036854775808;
	mov.u64 	%rl428, %rl69;

BB13_79:
	mov.u64 	%rl427, %rl428;
	setp.gt.s64 	%p50, %rl429, 0;
	@%p50 bra 	BB13_81;

	mov.u32 	%r306, 0;
	bra.uni 	BB13_83;

BB13_81:
	mov.u32 	%r306, 0;

BB13_82:
	shr.u64 	%rl275, %rl427, 63;
	shl.b64 	%rl276, %rl429, 1;
	or.b64  	%rl429, %rl275, %rl276;
	shl.b64 	%rl427, %rl427, 1;
	add.s32 	%r306, %r306, -1;
	setp.gt.s64 	%p51, %rl429, 0;
	@%p51 bra 	BB13_82;

BB13_83:
	mul.lo.s64 	%rl431, %rl429, -3958705157555305931;
	mov.u64 	%rl279, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl277, %rl429, %rl279;
	// inline asm
	setp.gt.s64 	%p52, %rl277, 0;
	mov.u64 	%rl430, %rl277;
	@%p52 bra 	BB13_84;
	bra.uni 	BB13_85;

BB13_84:
	shl.b64 	%rl280, %rl277, 1;
	shr.u64 	%rl281, %rl431, 63;
	or.b64  	%rl430, %rl280, %rl281;
	mul.lo.s64 	%rl431, %rl429, -7917410315110611862;
	add.s32 	%r306, %r306, -1;

BB13_85:
	setp.ne.s64 	%p53, %rl431, 0;
	selp.u64 	%rl282, 1, 0, %p53;
	add.s64 	%rl283, %rl282, %rl430;
	add.s32 	%r208, %r306, 1022;
	cvt.u64.u32 	%rl284, %r208;
	shl.b64 	%rl285, %rl284, 52;
	shr.u64 	%rl286, %rl283, 11;
	shr.u64 	%rl287, %rl283, 10;
	and.b64  	%rl288, %rl287, 1;
	add.s64 	%rl289, %rl285, %rl286;
	add.s64 	%rl290, %rl289, %rl288;
	or.b64  	%rl291, %rl290, %rl425;
	mov.b64 	 %fd1063, %rl291;

BB13_86:
	add.s32 	%r62, %r307, 1;
	and.b32  	%r209, %r62, 1;
	setp.eq.s32 	%p54, %r209, 0;
	mul.rn.f64 	%fd42, %fd1063, %fd1063;
	@%p54 bra 	BB13_88;

	mov.f64 	%fd352, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd354, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd351, %fd352, %fd42, %fd354;
	// inline asm
	mov.f64 	%fd358, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd355, %fd351, %fd42, %fd358;
	// inline asm
	mov.f64 	%fd362, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd359, %fd355, %fd42, %fd362;
	// inline asm
	mov.f64 	%fd366, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd363, %fd359, %fd42, %fd366;
	// inline asm
	mov.f64 	%fd370, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd367, %fd363, %fd42, %fd370;
	// inline asm
	mov.f64 	%fd374, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd371, %fd367, %fd42, %fd374;
	// inline asm
	mov.f64 	%fd378, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd375, %fd371, %fd42, %fd378;
	// inline asm
	mov.f64 	%fd1064, %fd375;
	bra.uni 	BB13_89;

BB13_88:
	mov.f64 	%fd380, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd382, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd379, %fd380, %fd42, %fd382;
	// inline asm
	mov.f64 	%fd386, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd383, %fd379, %fd42, %fd386;
	// inline asm
	mov.f64 	%fd390, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd387, %fd383, %fd42, %fd390;
	// inline asm
	mov.f64 	%fd394, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd391, %fd387, %fd42, %fd394;
	// inline asm
	mov.f64 	%fd398, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd395, %fd391, %fd42, %fd398;
	// inline asm
	mul.rn.f64 	%fd400, %fd395, %fd42;
	// inline asm
	fma.rn.f64 	%fd399, %fd400, %fd1063, %fd1063;
	// inline asm
	mov.f64 	%fd1064, %fd399;

BB13_89:
	and.b32  	%r210, %r62, 2;
	setp.eq.s32 	%p55, %r210, 0;
	neg.f64 	%fd403, %fd1064;
	selp.f64 	%fd404, %fd1064, %fd403, %p55;
	mul.rn.f64 	%fd46, %fd329, %fd168;
	mov.f64 	%fd1097, %fd404;
	mov.f64 	%fd1098, %fd46;
	@%p1 bra 	BB13_90;
	bra.uni 	BB13_91;

BB13_90:
	neg.f64 	%fd406, %fd46;
	mov.f64 	%fd1097, %fd404;
	mov.f64 	%fd1098, %fd406;

BB13_91:
	mul.f64 	%fd410, %fd1, %fd1097;
	fma.rn.f64 	%fd47, %fd175, %fd1098, %fd410;
	mul.f64 	%fd413, %fd2, %fd1097;
	fma.rn.f64 	%fd48, %fd1, %fd1098, %fd413;
	mul.f64 	%fd414, %fd3, 0d403F6A7A2955385E;
	div.rn.f64 	%fd408, %fd414, %fd4;
	// inline asm
	abs.f64 	%fd407, %fd408;
	// inline asm
	setp.gt.f64 	%p56, %fd407, 0d41E0000000000000;
	@%p56 bra 	BB13_93;

	mov.f64 	%fd428, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd415, %fd408, %fd428;
	// inline asm
	cvt.rni.s32.f64 	%r211, %fd415;
	// inline asm
	cvt.rn.f64.s32 	%fd429, %r211;
	neg.f64 	%fd425, %fd429;
	mov.f64 	%fd418, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd416, %fd425, %fd418, %fd408;
	// inline asm
	mov.f64 	%fd422, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd420, %fd425, %fd422, %fd416;
	// inline asm
	mov.f64 	%fd426, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd424, %fd425, %fd426, %fd420;
	// inline asm
	mov.u32 	%r311, %r211;
	mov.f64 	%fd1065, %fd424;
	bra.uni 	BB13_108;

BB13_93:
	mov.u32 	%r213, 18;
	mov.u32 	%r214, -67108829;
	// inline asm
	min.s32 	%r212, %r213, %r214;
	// inline asm
	setp.gt.s32 	%p57, %r212, -67108833;
	@%p57 bra 	BB13_95;

	mov.u64 	%rl432, 0;
	mov.u32 	%r309, 0;
	bra.uni 	BB13_98;

BB13_95:
	mov.u64 	%rl432, 0;
	mov.u32 	%r308, -67108833;

BB13_96:
	.pragma "nounroll";
	mov.u32 	%r64, %r308;
	shl.b32 	%r220, %r64, 3;
	mov.u32 	%r221, __internal_i2opi_d;
	add.s32 	%r222, %r221, %r220;
	ld.const.u64 	%rl295, [%r222];
	shl.b64 	%rl297, %rl295, 63;
	mov.u64 	%rl296, -9223372036854775808;
	// inline asm
	mul.hi.u64 	%rl294, %rl295, %rl296;
	// inline asm
	add.s64 	%rl298, %rl297, %rl432;
	setp.lt.u64 	%p58, %rl298, %rl297;
	selp.u64 	%rl299, 1, 0, %p58;
	add.s64 	%rl432, %rl299, %rl294;
	add.s32 	%r224, %r220, %r10;
	st.local.u64 	[%r224+536870664], %rl298;
	// inline asm
	min.s32 	%r217, %r213, %r214;
	// inline asm
	add.s32 	%r65, %r64, 1;
	setp.lt.s32 	%p59, %r65, %r217;
	mov.u32 	%r308, %r65;
	@%p59 bra 	BB13_96;

	add.s32 	%r309, %r64, 67108834;

BB13_98:
	shl.b32 	%r225, %r309, 3;
	add.s32 	%r227, %r10, %r225;
	st.local.u64 	[%r227], %rl432;
	ld.local.u64 	%rl300, [%r10+24];
	shr.u64 	%rl301, %rl300, 62;
	cvt.u32.u64 	%r228, %rl301;
	shl.b64 	%rl302, %rl300, 2;
	ld.local.u64 	%rl303, [%r10+16];
	shr.u64 	%rl304, %rl303, 62;
	or.b64  	%rl437, %rl304, %rl302;
	shl.b64 	%rl89, %rl303, 2;
	setp.ne.s64 	%p60, %rl89, 0;
	selp.u64 	%rl305, 1, 0, %p60;
	or.b64  	%rl306, %rl305, %rl437;
	setp.gt.u64 	%p61, %rl306, -9223372036854775808;
	selp.u32 	%r229, 1, 0, %p61;
	add.s32 	%r311, %r229, %r228;
	@%p61 bra 	BB13_100;

	mov.u64 	%rl433, 0;
	mov.u64 	%rl436, %rl89;
	bra.uni 	BB13_101;

BB13_100:
	not.b64 	%rl309, %rl437;
	neg.s64 	%rl90, %rl89;
	setp.eq.s64 	%p62, %rl89, 0;
	selp.u64 	%rl310, 1, 0, %p62;
	add.s64 	%rl437, %rl310, %rl309;
	mov.u64 	%rl433, -9223372036854775808;
	mov.u64 	%rl436, %rl90;

BB13_101:
	mov.u64 	%rl435, %rl436;
	setp.gt.s64 	%p63, %rl437, 0;
	@%p63 bra 	BB13_103;

	mov.u32 	%r310, 0;
	bra.uni 	BB13_105;

BB13_103:
	mov.u32 	%r310, 0;

BB13_104:
	shr.u64 	%rl311, %rl435, 63;
	shl.b64 	%rl312, %rl437, 1;
	or.b64  	%rl437, %rl311, %rl312;
	shl.b64 	%rl435, %rl435, 1;
	add.s32 	%r310, %r310, -1;
	setp.gt.s64 	%p64, %rl437, 0;
	@%p64 bra 	BB13_104;

BB13_105:
	mul.lo.s64 	%rl439, %rl437, -3958705157555305931;
	mov.u64 	%rl315, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl313, %rl437, %rl315;
	// inline asm
	setp.gt.s64 	%p65, %rl313, 0;
	mov.u64 	%rl438, %rl313;
	@%p65 bra 	BB13_106;
	bra.uni 	BB13_107;

BB13_106:
	shl.b64 	%rl316, %rl313, 1;
	shr.u64 	%rl317, %rl439, 63;
	or.b64  	%rl438, %rl316, %rl317;
	mul.lo.s64 	%rl439, %rl437, -7917410315110611862;
	add.s32 	%r310, %r310, -1;

BB13_107:
	setp.ne.s64 	%p66, %rl439, 0;
	selp.u64 	%rl318, 1, 0, %p66;
	add.s64 	%rl319, %rl318, %rl438;
	add.s32 	%r232, %r310, 1022;
	cvt.u64.u32 	%rl320, %r232;
	shl.b64 	%rl321, %rl320, 52;
	shr.u64 	%rl322, %rl319, 11;
	shr.u64 	%rl323, %rl319, 10;
	and.b64  	%rl324, %rl323, 1;
	add.s64 	%rl325, %rl321, %rl322;
	add.s64 	%rl326, %rl325, %rl324;
	or.b64  	%rl327, %rl326, %rl433;
	mov.b64 	 %fd1065, %rl327;

BB13_108:
	add.s32 	%r75, %r311, 1;
	and.b32  	%r233, %r75, 1;
	setp.eq.s32 	%p67, %r233, 0;
	mul.rn.f64 	%fd53, %fd1065, %fd1065;
	@%p67 bra 	BB13_110;

	mov.f64 	%fd431, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd433, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd430, %fd431, %fd53, %fd433;
	// inline asm
	mov.f64 	%fd437, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd434, %fd430, %fd53, %fd437;
	// inline asm
	mov.f64 	%fd441, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd438, %fd434, %fd53, %fd441;
	// inline asm
	mov.f64 	%fd445, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd442, %fd438, %fd53, %fd445;
	// inline asm
	mov.f64 	%fd449, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd446, %fd442, %fd53, %fd449;
	// inline asm
	mov.f64 	%fd453, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd450, %fd446, %fd53, %fd453;
	// inline asm
	mov.f64 	%fd457, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd454, %fd450, %fd53, %fd457;
	// inline asm
	mov.f64 	%fd1066, %fd454;
	bra.uni 	BB13_111;

BB13_110:
	mov.f64 	%fd459, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd461, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd458, %fd459, %fd53, %fd461;
	// inline asm
	mov.f64 	%fd465, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd462, %fd458, %fd53, %fd465;
	// inline asm
	mov.f64 	%fd469, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd466, %fd462, %fd53, %fd469;
	// inline asm
	mov.f64 	%fd473, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd470, %fd466, %fd53, %fd473;
	// inline asm
	mov.f64 	%fd477, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd474, %fd470, %fd53, %fd477;
	// inline asm
	mul.rn.f64 	%fd479, %fd474, %fd53;
	// inline asm
	fma.rn.f64 	%fd478, %fd479, %fd1065, %fd1065;
	// inline asm
	mov.f64 	%fd1066, %fd478;

BB13_111:
	and.b32  	%r234, %r75, 2;
	setp.eq.s32 	%p68, %r234, 0;
	neg.f64 	%fd482, %fd1066;
	selp.f64 	%fd483, %fd1066, %fd482, %p68;
	mul.rn.f64 	%fd57, %fd408, %fd168;
	mov.f64 	%fd1095, %fd483;
	mov.f64 	%fd1096, %fd57;
	@%p1 bra 	BB13_112;
	bra.uni 	BB13_113;

BB13_112:
	neg.f64 	%fd485, %fd57;
	mov.f64 	%fd1095, %fd483;
	mov.f64 	%fd1096, %fd485;

BB13_113:
	mul.f64 	%fd489, %fd1, %fd1095;
	fma.rn.f64 	%fd58, %fd175, %fd1096, %fd489;
	mul.f64 	%fd492, %fd2, %fd1095;
	fma.rn.f64 	%fd59, %fd1, %fd1096, %fd492;
	mul.f64 	%fd493, %fd3, 0d4042D97C7F3321D2;
	div.rn.f64 	%fd487, %fd493, %fd4;
	// inline asm
	abs.f64 	%fd486, %fd487;
	// inline asm
	setp.gt.f64 	%p69, %fd486, 0d41E0000000000000;
	@%p69 bra 	BB13_115;

	mov.f64 	%fd507, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd494, %fd487, %fd507;
	// inline asm
	cvt.rni.s32.f64 	%r235, %fd494;
	// inline asm
	cvt.rn.f64.s32 	%fd508, %r235;
	neg.f64 	%fd504, %fd508;
	mov.f64 	%fd497, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd495, %fd504, %fd497, %fd487;
	// inline asm
	mov.f64 	%fd501, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd499, %fd504, %fd501, %fd495;
	// inline asm
	mov.f64 	%fd505, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd503, %fd504, %fd505, %fd499;
	// inline asm
	mov.u32 	%r315, %r235;
	mov.f64 	%fd1067, %fd503;
	bra.uni 	BB13_130;

BB13_115:
	mov.u32 	%r237, 18;
	mov.u32 	%r238, -67108829;
	// inline asm
	min.s32 	%r236, %r237, %r238;
	// inline asm
	setp.gt.s32 	%p70, %r236, -67108833;
	@%p70 bra 	BB13_117;

	mov.u64 	%rl440, 0;
	mov.u32 	%r313, 0;
	bra.uni 	BB13_120;

BB13_117:
	mov.u64 	%rl440, 0;
	mov.u32 	%r312, -67108833;

BB13_118:
	.pragma "nounroll";
	mov.u32 	%r77, %r312;
	shl.b32 	%r244, %r77, 3;
	mov.u32 	%r245, __internal_i2opi_d;
	add.s32 	%r246, %r245, %r244;
	ld.const.u64 	%rl331, [%r246];
	shl.b64 	%rl333, %rl331, 63;
	mov.u64 	%rl332, -9223372036854775808;
	// inline asm
	mul.hi.u64 	%rl330, %rl331, %rl332;
	// inline asm
	add.s64 	%rl334, %rl333, %rl440;
	setp.lt.u64 	%p71, %rl334, %rl333;
	selp.u64 	%rl335, 1, 0, %p71;
	add.s64 	%rl440, %rl335, %rl330;
	add.s32 	%r248, %r244, %r10;
	st.local.u64 	[%r248+536870664], %rl334;
	// inline asm
	min.s32 	%r241, %r237, %r238;
	// inline asm
	add.s32 	%r78, %r77, 1;
	setp.lt.s32 	%p72, %r78, %r241;
	mov.u32 	%r312, %r78;
	@%p72 bra 	BB13_118;

	add.s32 	%r313, %r77, 67108834;

BB13_120:
	shl.b32 	%r249, %r313, 3;
	add.s32 	%r251, %r10, %r249;
	st.local.u64 	[%r251], %rl440;
	ld.local.u64 	%rl336, [%r10+24];
	shr.u64 	%rl337, %rl336, 62;
	cvt.u32.u64 	%r252, %rl337;
	shl.b64 	%rl338, %rl336, 2;
	ld.local.u64 	%rl339, [%r10+16];
	shr.u64 	%rl340, %rl339, 62;
	or.b64  	%rl445, %rl340, %rl338;
	shl.b64 	%rl110, %rl339, 2;
	setp.ne.s64 	%p73, %rl110, 0;
	selp.u64 	%rl341, 1, 0, %p73;
	or.b64  	%rl342, %rl341, %rl445;
	setp.gt.u64 	%p74, %rl342, -9223372036854775808;
	selp.u32 	%r253, 1, 0, %p74;
	add.s32 	%r315, %r253, %r252;
	@%p74 bra 	BB13_122;

	mov.u64 	%rl441, 0;
	mov.u64 	%rl444, %rl110;
	bra.uni 	BB13_123;

BB13_122:
	not.b64 	%rl345, %rl445;
	neg.s64 	%rl111, %rl110;
	setp.eq.s64 	%p75, %rl110, 0;
	selp.u64 	%rl346, 1, 0, %p75;
	add.s64 	%rl445, %rl346, %rl345;
	mov.u64 	%rl441, -9223372036854775808;
	mov.u64 	%rl444, %rl111;

BB13_123:
	mov.u64 	%rl443, %rl444;
	setp.gt.s64 	%p76, %rl445, 0;
	@%p76 bra 	BB13_125;

	mov.u32 	%r314, 0;
	bra.uni 	BB13_127;

BB13_125:
	mov.u32 	%r314, 0;

BB13_126:
	shr.u64 	%rl347, %rl443, 63;
	shl.b64 	%rl348, %rl445, 1;
	or.b64  	%rl445, %rl347, %rl348;
	shl.b64 	%rl443, %rl443, 1;
	add.s32 	%r314, %r314, -1;
	setp.gt.s64 	%p77, %rl445, 0;
	@%p77 bra 	BB13_126;

BB13_127:
	mul.lo.s64 	%rl447, %rl445, -3958705157555305931;
	mov.u64 	%rl351, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl349, %rl445, %rl351;
	// inline asm
	setp.gt.s64 	%p78, %rl349, 0;
	mov.u64 	%rl446, %rl349;
	@%p78 bra 	BB13_128;
	bra.uni 	BB13_129;

BB13_128:
	shl.b64 	%rl352, %rl349, 1;
	shr.u64 	%rl353, %rl447, 63;
	or.b64  	%rl446, %rl352, %rl353;
	mul.lo.s64 	%rl447, %rl445, -7917410315110611862;
	add.s32 	%r314, %r314, -1;

BB13_129:
	setp.ne.s64 	%p79, %rl447, 0;
	selp.u64 	%rl354, 1, 0, %p79;
	add.s64 	%rl355, %rl354, %rl446;
	add.s32 	%r256, %r314, 1022;
	cvt.u64.u32 	%rl356, %r256;
	shl.b64 	%rl357, %rl356, 52;
	shr.u64 	%rl358, %rl355, 11;
	shr.u64 	%rl359, %rl355, 10;
	and.b64  	%rl360, %rl359, 1;
	add.s64 	%rl361, %rl357, %rl358;
	add.s64 	%rl362, %rl361, %rl360;
	or.b64  	%rl363, %rl362, %rl441;
	mov.b64 	 %fd1067, %rl363;

BB13_130:
	add.s32 	%r88, %r315, 1;
	and.b32  	%r257, %r88, 1;
	setp.eq.s32 	%p80, %r257, 0;
	mul.rn.f64 	%fd64, %fd1067, %fd1067;
	@%p80 bra 	BB13_132;

	mov.f64 	%fd510, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd512, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd509, %fd510, %fd64, %fd512;
	// inline asm
	mov.f64 	%fd516, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd513, %fd509, %fd64, %fd516;
	// inline asm
	mov.f64 	%fd520, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd517, %fd513, %fd64, %fd520;
	// inline asm
	mov.f64 	%fd524, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd521, %fd517, %fd64, %fd524;
	// inline asm
	mov.f64 	%fd528, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd525, %fd521, %fd64, %fd528;
	// inline asm
	mov.f64 	%fd532, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd529, %fd525, %fd64, %fd532;
	// inline asm
	mov.f64 	%fd536, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd533, %fd529, %fd64, %fd536;
	// inline asm
	mov.f64 	%fd1068, %fd533;
	bra.uni 	BB13_133;

BB13_132:
	mov.f64 	%fd538, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd540, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd537, %fd538, %fd64, %fd540;
	// inline asm
	mov.f64 	%fd544, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd541, %fd537, %fd64, %fd544;
	// inline asm
	mov.f64 	%fd548, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd545, %fd541, %fd64, %fd548;
	// inline asm
	mov.f64 	%fd552, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd549, %fd545, %fd64, %fd552;
	// inline asm
	mov.f64 	%fd556, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd553, %fd549, %fd64, %fd556;
	// inline asm
	mul.rn.f64 	%fd558, %fd553, %fd64;
	// inline asm
	fma.rn.f64 	%fd557, %fd558, %fd1067, %fd1067;
	// inline asm
	mov.f64 	%fd1068, %fd557;

BB13_133:
	and.b32  	%r258, %r88, 2;
	setp.eq.s32 	%p81, %r258, 0;
	neg.f64 	%fd561, %fd1068;
	selp.f64 	%fd562, %fd1068, %fd561, %p81;
	mul.rn.f64 	%fd68, %fd487, %fd168;
	mov.f64 	%fd1093, %fd562;
	mov.f64 	%fd1094, %fd68;
	@%p1 bra 	BB13_134;
	bra.uni 	BB13_135;

BB13_134:
	neg.f64 	%fd564, %fd68;
	mov.f64 	%fd1093, %fd562;
	mov.f64 	%fd1094, %fd564;

BB13_135:
	mul.f64 	%fd568, %fd1, %fd1093;
	fma.rn.f64 	%fd69, %fd175, %fd1094, %fd568;
	mul.f64 	%fd571, %fd2, %fd1093;
	fma.rn.f64 	%fd70, %fd1, %fd1094, %fd571;
	mul.f64 	%fd572, %fd3, 0d4045FDBBE9BBA775;
	div.rn.f64 	%fd566, %fd572, %fd4;
	// inline asm
	abs.f64 	%fd565, %fd566;
	// inline asm
	setp.gt.f64 	%p82, %fd565, 0d41E0000000000000;
	@%p82 bra 	BB13_137;

	mov.f64 	%fd586, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd573, %fd566, %fd586;
	// inline asm
	cvt.rni.s32.f64 	%r259, %fd573;
	// inline asm
	cvt.rn.f64.s32 	%fd587, %r259;
	neg.f64 	%fd583, %fd587;
	mov.f64 	%fd576, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd574, %fd583, %fd576, %fd566;
	// inline asm
	mov.f64 	%fd580, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd578, %fd583, %fd580, %fd574;
	// inline asm
	mov.f64 	%fd584, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd582, %fd583, %fd584, %fd578;
	// inline asm
	mov.u32 	%r319, %r259;
	mov.f64 	%fd1069, %fd582;
	bra.uni 	BB13_152;

BB13_137:
	mov.u32 	%r261, 18;
	mov.u32 	%r262, -67108829;
	// inline asm
	min.s32 	%r260, %r261, %r262;
	// inline asm
	setp.gt.s32 	%p83, %r260, -67108833;
	@%p83 bra 	BB13_139;

	mov.u64 	%rl448, 0;
	mov.u32 	%r317, 0;
	bra.uni 	BB13_142;

BB13_139:
	mov.u64 	%rl448, 0;
	mov.u32 	%r316, -67108833;

BB13_140:
	.pragma "nounroll";
	mov.u32 	%r90, %r316;
	shl.b32 	%r268, %r90, 3;
	mov.u32 	%r269, __internal_i2opi_d;
	add.s32 	%r270, %r269, %r268;
	ld.const.u64 	%rl367, [%r270];
	shl.b64 	%rl369, %rl367, 63;
	mov.u64 	%rl368, -9223372036854775808;
	// inline asm
	mul.hi.u64 	%rl366, %rl367, %rl368;
	// inline asm
	add.s64 	%rl370, %rl369, %rl448;
	setp.lt.u64 	%p84, %rl370, %rl369;
	selp.u64 	%rl371, 1, 0, %p84;
	add.s64 	%rl448, %rl371, %rl366;
	add.s32 	%r272, %r268, %r10;
	st.local.u64 	[%r272+536870664], %rl370;
	// inline asm
	min.s32 	%r265, %r261, %r262;
	// inline asm
	add.s32 	%r91, %r90, 1;
	setp.lt.s32 	%p85, %r91, %r265;
	mov.u32 	%r316, %r91;
	@%p85 bra 	BB13_140;

	add.s32 	%r317, %r90, 67108834;

BB13_142:
	shl.b32 	%r273, %r317, 3;
	add.s32 	%r275, %r10, %r273;
	st.local.u64 	[%r275], %rl448;
	ld.local.u64 	%rl372, [%r10+24];
	shr.u64 	%rl373, %rl372, 62;
	cvt.u32.u64 	%r276, %rl373;
	shl.b64 	%rl374, %rl372, 2;
	ld.local.u64 	%rl375, [%r10+16];
	shr.u64 	%rl376, %rl375, 62;
	or.b64  	%rl453, %rl376, %rl374;
	shl.b64 	%rl131, %rl375, 2;
	setp.ne.s64 	%p86, %rl131, 0;
	selp.u64 	%rl377, 1, 0, %p86;
	or.b64  	%rl378, %rl377, %rl453;
	setp.gt.u64 	%p87, %rl378, -9223372036854775808;
	selp.u32 	%r277, 1, 0, %p87;
	add.s32 	%r319, %r277, %r276;
	@%p87 bra 	BB13_144;

	mov.u64 	%rl449, 0;
	mov.u64 	%rl452, %rl131;
	bra.uni 	BB13_145;

BB13_144:
	not.b64 	%rl381, %rl453;
	neg.s64 	%rl132, %rl131;
	setp.eq.s64 	%p88, %rl131, 0;
	selp.u64 	%rl382, 1, 0, %p88;
	add.s64 	%rl453, %rl382, %rl381;
	mov.u64 	%rl449, -9223372036854775808;
	mov.u64 	%rl452, %rl132;

BB13_145:
	mov.u64 	%rl451, %rl452;
	setp.gt.s64 	%p89, %rl453, 0;
	@%p89 bra 	BB13_147;

	mov.u32 	%r318, 0;
	bra.uni 	BB13_149;

BB13_147:
	mov.u32 	%r318, 0;

BB13_148:
	shr.u64 	%rl383, %rl451, 63;
	shl.b64 	%rl384, %rl453, 1;
	or.b64  	%rl453, %rl383, %rl384;
	shl.b64 	%rl451, %rl451, 1;
	add.s32 	%r318, %r318, -1;
	setp.gt.s64 	%p90, %rl453, 0;
	@%p90 bra 	BB13_148;

BB13_149:
	mul.lo.s64 	%rl455, %rl453, -3958705157555305931;
	mov.u64 	%rl387, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl385, %rl453, %rl387;
	// inline asm
	setp.gt.s64 	%p91, %rl385, 0;
	mov.u64 	%rl454, %rl385;
	@%p91 bra 	BB13_150;
	bra.uni 	BB13_151;

BB13_150:
	shl.b64 	%rl388, %rl385, 1;
	shr.u64 	%rl389, %rl455, 63;
	or.b64  	%rl454, %rl388, %rl389;
	mul.lo.s64 	%rl455, %rl453, -7917410315110611862;
	add.s32 	%r318, %r318, -1;

BB13_151:
	setp.ne.s64 	%p92, %rl455, 0;
	selp.u64 	%rl390, 1, 0, %p92;
	add.s64 	%rl391, %rl390, %rl454;
	add.s32 	%r280, %r318, 1022;
	cvt.u64.u32 	%rl392, %r280;
	shl.b64 	%rl393, %rl392, 52;
	shr.u64 	%rl394, %rl391, 11;
	shr.u64 	%rl395, %rl391, 10;
	and.b64  	%rl396, %rl395, 1;
	add.s64 	%rl397, %rl393, %rl394;
	add.s64 	%rl398, %rl397, %rl396;
	or.b64  	%rl399, %rl398, %rl449;
	mov.b64 	 %fd1069, %rl399;

BB13_152:
	add.s32 	%r101, %r319, 1;
	and.b32  	%r281, %r101, 1;
	setp.eq.s32 	%p93, %r281, 0;
	mul.rn.f64 	%fd75, %fd1069, %fd1069;
	@%p93 bra 	BB13_154;

	mov.f64 	%fd589, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd591, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd588, %fd589, %fd75, %fd591;
	// inline asm
	mov.f64 	%fd595, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd592, %fd588, %fd75, %fd595;
	// inline asm
	mov.f64 	%fd599, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd596, %fd592, %fd75, %fd599;
	// inline asm
	mov.f64 	%fd603, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd600, %fd596, %fd75, %fd603;
	// inline asm
	mov.f64 	%fd607, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd604, %fd600, %fd75, %fd607;
	// inline asm
	mov.f64 	%fd611, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd608, %fd604, %fd75, %fd611;
	// inline asm
	mov.f64 	%fd615, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd612, %fd608, %fd75, %fd615;
	// inline asm
	mov.f64 	%fd1070, %fd612;
	bra.uni 	BB13_155;

BB13_154:
	mov.f64 	%fd617, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd619, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd616, %fd617, %fd75, %fd619;
	// inline asm
	mov.f64 	%fd623, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd620, %fd616, %fd75, %fd623;
	// inline asm
	mov.f64 	%fd627, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd624, %fd620, %fd75, %fd627;
	// inline asm
	mov.f64 	%fd631, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd628, %fd624, %fd75, %fd631;
	// inline asm
	mov.f64 	%fd635, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd632, %fd628, %fd75, %fd635;
	// inline asm
	mul.rn.f64 	%fd637, %fd632, %fd75;
	// inline asm
	fma.rn.f64 	%fd636, %fd637, %fd1069, %fd1069;
	// inline asm
	mov.f64 	%fd1070, %fd636;

BB13_155:
	and.b32  	%r282, %r101, 2;
	setp.eq.s32 	%p94, %r282, 0;
	neg.f64 	%fd640, %fd1070;
	selp.f64 	%fd641, %fd1070, %fd640, %p94;
	mul.rn.f64 	%fd79, %fd566, %fd168;
	mov.f64 	%fd1091, %fd641;
	mov.f64 	%fd1092, %fd79;
	@%p1 bra 	BB13_156;
	bra.uni 	BB13_157;

BB13_156:
	neg.f64 	%fd643, %fd79;
	mov.f64 	%fd1091, %fd641;
	mov.f64 	%fd1092, %fd643;

BB13_157:
	mul.f64 	%fd645, %fd1, %fd1091;
	fma.rn.f64 	%fd648, %fd175, %fd1092, %fd645;
	mul.f64 	%fd649, %fd2, %fd1091;
	fma.rn.f64 	%fd650, %fd1, %fd1092, %fd649;
	add.f64 	%fd651, %fd1, %fd47;
	add.f64 	%fd652, %fd2, %fd48;
	sub.f64 	%fd653, %fd1, %fd47;
	sub.f64 	%fd654, %fd2, %fd48;
	add.f64 	%fd655, %fd25, %fd69;
	add.f64 	%fd656, %fd26, %fd70;
	sub.f64 	%fd80, %fd25, %fd69;
	sub.f64 	%fd81, %fd26, %fd70;
	add.f64 	%fd657, %fd14, %fd58;
	add.f64 	%fd658, %fd15, %fd59;
	sub.f64 	%fd82, %fd14, %fd58;
	sub.f64 	%fd83, %fd15, %fd59;
	add.f64 	%fd659, %fd36, %fd648;
	add.f64 	%fd660, %fd37, %fd650;
	sub.f64 	%fd84, %fd36, %fd648;
	sub.f64 	%fd85, %fd37, %fd650;
	ld.param.u32 	%r288, [DIT8C2CM_param_4];
	setp.eq.s32 	%p95, %r288, 1;
	mov.f64 	%fd1085, %fd82;
	mov.f64 	%fd1086, %fd83;
	mov.f64 	%fd1083, %fd657;
	mov.f64 	%fd1084, %fd658;
	mov.f64 	%fd1081, %fd80;
	mov.f64 	%fd1082, %fd81;
	mov.f64 	%fd1079, %fd655;
	mov.f64 	%fd1080, %fd656;
	mov.f64 	%fd1077, %fd653;
	mov.f64 	%fd1078, %fd654;
	mov.f64 	%fd1075, %fd651;
	mov.f64 	%fd1076, %fd652;
	mov.f64 	%fd1089, %fd84;
	mov.f64 	%fd1090, %fd85;
	mov.f64 	%fd1087, %fd659;
	mov.f64 	%fd1088, %fd660;
	@%p95 bra 	BB13_160;

	ld.param.u32 	%r287, [DIT8C2CM_param_4];
	setp.ne.s32 	%p96, %r287, 0;
	@%p96 bra 	BB13_161;

	sub.f64 	%fd661, %fd82, %fd83;
	mul.f64 	%fd662, %fd661, 0d3FE6A09E667F3BCD;
	add.f64 	%fd663, %fd85, %fd84;
	mul.f64 	%fd664, %fd663, 0dBFE6A09E667F3BCD;
	add.f64 	%fd665, %fd83, %fd82;
	mul.f64 	%fd666, %fd665, 0d3FE6A09E667F3BCD;
	sub.f64 	%fd667, %fd84, %fd85;
	mul.f64 	%fd668, %fd667, 0d3FE6A09E667F3BCD;
	neg.f64 	%fd669, %fd81;
	mov.f64 	%fd1075, %fd651;
	mov.f64 	%fd1076, %fd652;
	mov.f64 	%fd1077, %fd653;
	mov.f64 	%fd1078, %fd654;
	mov.f64 	%fd1079, %fd655;
	mov.f64 	%fd1080, %fd656;
	mov.f64 	%fd1083, %fd657;
	mov.f64 	%fd1084, %fd658;
	mov.f64 	%fd1081, %fd669;
	mov.f64 	%fd1082, %fd80;
	mov.f64 	%fd1087, %fd659;
	mov.f64 	%fd1088, %fd660;
	mov.f64 	%fd1085, %fd662;
	mov.f64 	%fd1086, %fd666;
	mov.f64 	%fd1089, %fd664;
	mov.f64 	%fd1090, %fd668;
	bra.uni 	BB13_161;

BB13_160:
	add.f64 	%fd670, %fd82, %fd83;
	mul.f64 	%fd671, %fd670, 0d3FE6A09E667F3BCD;
	sub.f64 	%fd672, %fd85, %fd84;
	mul.f64 	%fd673, %fd672, 0d3FE6A09E667F3BCD;
	sub.f64 	%fd674, %fd83, %fd82;
	mul.f64 	%fd675, %fd674, 0d3FE6A09E667F3BCD;
	add.f64 	%fd676, %fd85, %fd84;
	mul.f64 	%fd677, %fd676, 0dBFE6A09E667F3BCD;
	neg.f64 	%fd678, %fd80;
	mov.f64 	%fd1075, %fd651;
	mov.f64 	%fd1076, %fd652;
	mov.f64 	%fd1077, %fd653;
	mov.f64 	%fd1078, %fd654;
	mov.f64 	%fd1079, %fd655;
	mov.f64 	%fd1080, %fd656;
	mov.f64 	%fd1083, %fd657;
	mov.f64 	%fd1084, %fd658;
	mov.f64 	%fd1081, %fd81;
	mov.f64 	%fd1082, %fd678;
	mov.f64 	%fd1087, %fd659;
	mov.f64 	%fd1088, %fd660;
	mov.f64 	%fd1085, %fd671;
	mov.f64 	%fd1086, %fd675;
	mov.f64 	%fd1089, %fd673;
	mov.f64 	%fd1090, %fd677;

BB13_161:
	add.f64 	%fd681, %fd1075, %fd1079;
	add.f64 	%fd684, %fd1076, %fd1080;
	add.f64 	%fd687, %fd1077, %fd1081;
	add.f64 	%fd690, %fd1078, %fd1082;
	sub.f64 	%fd691, %fd1075, %fd1079;
	sub.f64 	%fd692, %fd1076, %fd1080;
	sub.f64 	%fd693, %fd1077, %fd1081;
	sub.f64 	%fd694, %fd1078, %fd1082;
	add.f64 	%fd695, %fd1083, %fd1087;
	add.f64 	%fd696, %fd1084, %fd1088;
	add.f64 	%fd697, %fd1085, %fd1089;
	add.f64 	%fd698, %fd1086, %fd1090;
	ld.param.u32 	%r286, [DIT8C2CM_param_4];
	setp.eq.s32 	%p97, %r286, 1;
	mov.f64 	%fd735, %fd691;
	mov.f64 	%fd736, %fd692;
	mov.f64 	%fd719, %fd681;
	mov.f64 	%fd720, %fd684;
	mov.f64 	%fd1073, %fd648;
	mov.f64 	%fd1074, %fd650;
	mov.f64 	%fd1071, %fd693;
	mov.f64 	%fd1072, %fd694;
	mov.f64 	%fd727, %fd687;
	mov.f64 	%fd728, %fd690;
	mov.f64 	%fd751, %fd695;
	mov.f64 	%fd752, %fd696;
	mov.f64 	%fd759, %fd697;
	mov.f64 	%fd760, %fd698;
	@%p97 bra 	BB13_164;

	ld.param.u32 	%r285, [DIT8C2CM_param_4];
	setp.ne.s32 	%p98, %r285, 0;
	@%p98 bra 	BB13_165;

	sub.f64 	%fd699, %fd1088, %fd1084;
	sub.f64 	%fd700, %fd1083, %fd1087;
	sub.f64 	%fd701, %fd1090, %fd1086;
	sub.f64 	%fd702, %fd1085, %fd1089;
	mov.f64 	%fd1071, %fd693;
	mov.f64 	%fd1072, %fd694;
	mov.f64 	%fd763, %fd699;
	mov.f64 	%fd764, %fd700;
	mov.f64 	%fd1073, %fd701;
	mov.f64 	%fd1074, %fd702;
	bra.uni 	BB13_165;

BB13_164:
	sub.f64 	%fd703, %fd1084, %fd1088;
	sub.f64 	%fd704, %fd1087, %fd1083;
	sub.f64 	%fd705, %fd1086, %fd1090;
	sub.f64 	%fd706, %fd1089, %fd1085;
	mov.f64 	%fd1071, %fd693;
	mov.f64 	%fd1072, %fd694;
	mov.f64 	%fd765, %fd703;
	mov.f64 	%fd766, %fd704;
	mov.f64 	%fd1073, %fd705;
	mov.f64 	%fd1074, %fd706;

BB13_165:
	sub.f64 	%fd709, %fd1071, %fd1073;
	st.global.f64 	[%r9], %fd709;
	sub.f64 	%fd712, %fd1072, %fd1074;
	st.global.f64 	[%r9+8], %fd712;
	ret;

BB13_166:
	ld.param.u32 	%r284, [DIT8C2CM_param_1];
	mul.lo.s32 	%r291, %r6, %r284;
	bra.uni 	BB13_3;
}

.entry kernelMUL(
	.param .u32 .ptr .global .align 8 kernelMUL_param_0,
	.param .u32 kernelMUL_param_1,
	.param .u32 kernelMUL_param_2
)
{
	.local .align 8 .b8 	__local_depot14[40];
	.reg .b32 	%SP;
	.reg .f64 	%fd<181>;
	.reg .pred 	%p<37>;
	.reg .s32 	%r<136>;
	.reg .s64 	%rl<167>;


	mov.u32 	%SP, __local_depot14;
	ld.param.u32 	%r44, [kernelMUL_param_0];
	ld.param.u32 	%r45, [kernelMUL_param_1];
	ld.param.u32 	%r46, [kernelMUL_param_2];
	// inline asm
	mov.u32 	%r36, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r37, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r38, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r39, %tid.x;
	// inline asm
	add.s32 	%r47, %r39, %r36;
	mad.lo.s32 	%r48, %r38, %r37, %r47;
	// inline asm
	mov.u32 	%r40, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r41, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r42, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r43, %tid.y;
	// inline asm
	add.s32 	%r49, %r43, %r40;
	mad.lo.s32 	%r50, %r42, %r41, %r49;
	mad.lo.s32 	%r51, %r50, %r45, %r48;
	shl.b32 	%r52, %r51, 4;
	add.s32 	%r1, %r44, %r52;
	ld.global.f64 	%fd1, [%r1];
	ld.global.f64 	%fd2, [%r1+8];
	cvt.rn.f64.s32 	%fd24, %r48;
	mul.f64 	%fd25, %fd24, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd26, %r50;
	mul.f64 	%fd27, %fd25, %fd26;
	mul.lo.s32 	%r53, %r46, %r45;
	cvt.rn.f64.s32 	%fd28, %r53;
	div.rn.f64 	%fd3, %fd27, %fd28;
	setp.eq.f64 	%p1, %fd3, 0d7FF0000000000000;
	setp.eq.f64 	%p2, %fd3, 0dFFF0000000000000;
	or.pred  	%p3, %p1, %p2;
	add.u32 	%r2, %SP, 0;
	@%p3 bra 	BB14_23;

	// inline asm
	abs.f64 	%fd29, %fd3;
	// inline asm
	setp.gt.f64 	%p4, %fd29, 0d41E0000000000000;
	@%p4 bra 	BB14_3;

	mov.f64 	%fd44, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd31, %fd3, %fd44;
	// inline asm
	cvt.rni.s32.f64 	%r54, %fd31;
	// inline asm
	cvt.rn.f64.s32 	%fd45, %r54;
	neg.f64 	%fd41, %fd45;
	mov.f64 	%fd34, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd32, %fd41, %fd34, %fd3;
	// inline asm
	mov.f64 	%fd38, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd36, %fd41, %fd38, %fd32;
	// inline asm
	mov.f64 	%fd42, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd40, %fd41, %fd42, %fd36;
	// inline asm
	mov.u32 	%r132, %r54;
	mov.f64 	%fd175, %fd40;
	bra.uni 	BB14_19;

BB14_3:
	mov.b64 	 %rl1, %fd3;
	and.b64  	%rl150, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl65, %rl3, 2047;
	add.s64 	%rl66, %rl65, 4294966272;
	cvt.u32.u64 	%r4, %rl66;
	shl.b64 	%rl67, %rl1, 11;
	or.b64  	%rl4, %rl67, -9223372036854775808;
	shr.u32 	%r58, %r4, 6;
	mov.u32 	%r59, 16;
	sub.s32 	%r5, %r59, %r58;
	mov.u32 	%r60, 15;
	sub.s32 	%r130, %r60, %r58;
	mov.u32 	%r61, 19;
	sub.s32 	%r7, %r61, %r58;
	mov.u32 	%r56, 18;
	// inline asm
	min.s32 	%r55, %r56, %r7;
	// inline asm
	setp.lt.s32 	%p5, %r130, %r55;
	@%p5 bra 	BB14_5;

	mov.u64 	%rl147, 0;
	bra.uni 	BB14_7;

BB14_5:
	mov.u32 	%r62, 1;
	sub.s32 	%r8, %r62, %r5;
	mov.u64 	%rl147, 0;

BB14_6:
	.pragma "nounroll";
	shl.b32 	%r66, %r130, 3;
	mov.u32 	%r67, __internal_i2opi_d;
	add.s32 	%r68, %r67, %r66;
	ld.const.u64 	%rl71, [%r68];
	mul.lo.s64 	%rl73, %rl71, %rl4;
	// inline asm
	mul.hi.u64 	%rl70, %rl71, %rl4;
	// inline asm
	mad.lo.s64 	%rl74, %rl71, %rl4, %rl147;
	setp.lt.u64 	%p6, %rl74, %rl73;
	selp.u64 	%rl75, 1, 0, %p6;
	add.s64 	%rl147, %rl75, %rl70;
	add.s32 	%r69, %r8, %r130;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r72, %r2, %r70;
	st.local.u64 	[%r72], %rl74;
	// inline asm
	min.s32 	%r63, %r56, %r7;
	// inline asm
	add.s32 	%r130, %r130, 1;
	setp.lt.s32 	%p7, %r130, %r63;
	@%p7 bra 	BB14_6;

BB14_7:
	mov.u32 	%r73, 1;
	sub.s32 	%r74, %r73, %r5;
	add.s32 	%r75, %r74, %r130;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r78, %r2, %r76;
	st.local.u64 	[%r78], %rl147;
	ld.local.u64 	%rl148, [%r2+24];
	ld.local.u64 	%rl149, [%r2+16];
	and.b32  	%r79, %r4, 63;
	setp.eq.s32 	%p8, %r79, 0;
	@%p8 bra 	BB14_9;

	and.b64  	%rl76, %rl3, 63;
	cvt.u32.u64 	%r80, %rl76;
	shl.b64 	%rl77, %rl148, %r80;
	neg.s32 	%r81, %r4;
	and.b32  	%r82, %r81, 63;
	shr.u64 	%rl78, %rl149, %r82;
	or.b64  	%rl148, %rl78, %rl77;
	shl.b64 	%rl79, %rl149, %r80;
	ld.local.u64 	%rl80, [%r2+8];
	shr.u64 	%rl81, %rl80, %r82;
	or.b64  	%rl149, %rl81, %rl79;

BB14_9:
	shr.u64 	%rl82, %rl148, 62;
	cvt.u32.u64 	%r83, %rl82;
	shr.u64 	%rl83, %rl149, 62;
	shl.b64 	%rl84, %rl148, 2;
	or.b64  	%rl154, %rl83, %rl84;
	shl.b64 	%rl15, %rl149, 2;
	setp.ne.s64 	%p9, %rl15, 0;
	selp.u64 	%rl85, 1, 0, %p9;
	or.b64  	%rl86, %rl85, %rl154;
	setp.gt.u64 	%p10, %rl86, -9223372036854775808;
	selp.u32 	%r84, 1, 0, %p10;
	add.s32 	%r85, %r84, %r83;
	neg.s32 	%r86, %r85;
	setp.lt.s64 	%p11, %rl1, 0;
	selp.b32 	%r132, %r86, %r85, %p11;
	@%p10 bra 	BB14_11;

	mov.u64 	%rl153, %rl15;
	bra.uni 	BB14_12;

BB14_11:
	not.b64 	%rl87, %rl154;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p12, %rl15, 0;
	selp.u64 	%rl88, 1, 0, %p12;
	add.s64 	%rl154, %rl88, %rl87;
	xor.b64  	%rl150, %rl150, -9223372036854775808;
	mov.u64 	%rl153, %rl16;

BB14_12:
	mov.u64 	%rl152, %rl153;
	setp.gt.s64 	%p13, %rl154, 0;
	@%p13 bra 	BB14_14;

	mov.u32 	%r131, 0;
	bra.uni 	BB14_16;

BB14_14:
	mov.u32 	%r131, 0;

BB14_15:
	shr.u64 	%rl89, %rl152, 63;
	shl.b64 	%rl90, %rl154, 1;
	or.b64  	%rl154, %rl89, %rl90;
	shl.b64 	%rl152, %rl152, 1;
	add.s32 	%r131, %r131, -1;
	setp.gt.s64 	%p14, %rl154, 0;
	@%p14 bra 	BB14_15;

BB14_16:
	mul.lo.s64 	%rl156, %rl154, -3958705157555305931;
	mov.u64 	%rl93, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl91, %rl154, %rl93;
	// inline asm
	setp.gt.s64 	%p15, %rl91, 0;
	mov.u64 	%rl155, %rl91;
	@%p15 bra 	BB14_17;
	bra.uni 	BB14_18;

BB14_17:
	shl.b64 	%rl94, %rl91, 1;
	shr.u64 	%rl95, %rl156, 63;
	or.b64  	%rl155, %rl94, %rl95;
	mul.lo.s64 	%rl156, %rl154, -7917410315110611862;
	add.s32 	%r131, %r131, -1;

BB14_18:
	setp.ne.s64 	%p16, %rl156, 0;
	selp.u64 	%rl96, 1, 0, %p16;
	add.s64 	%rl97, %rl96, %rl155;
	add.s32 	%r89, %r131, 1022;
	cvt.u64.u32 	%rl98, %r89;
	shl.b64 	%rl99, %rl98, 52;
	shr.u64 	%rl100, %rl97, 11;
	shr.u64 	%rl101, %rl97, 10;
	and.b64  	%rl102, %rl101, 1;
	add.s64 	%rl103, %rl99, %rl100;
	add.s64 	%rl104, %rl103, %rl102;
	or.b64  	%rl105, %rl104, %rl150;
	mov.b64 	 %fd175, %rl105;

BB14_19:
	add.s32 	%r19, %r132, 1;
	and.b32  	%r90, %r19, 1;
	setp.eq.s32 	%p17, %r90, 0;
	mul.rn.f64 	%fd7, %fd175, %fd175;
	@%p17 bra 	BB14_21;

	mov.f64 	%fd47, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd49, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd46, %fd47, %fd7, %fd49;
	// inline asm
	mov.f64 	%fd53, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd50, %fd46, %fd7, %fd53;
	// inline asm
	mov.f64 	%fd57, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd54, %fd50, %fd7, %fd57;
	// inline asm
	mov.f64 	%fd61, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd58, %fd54, %fd7, %fd61;
	// inline asm
	mov.f64 	%fd65, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd62, %fd58, %fd7, %fd65;
	// inline asm
	mov.f64 	%fd69, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd66, %fd62, %fd7, %fd69;
	// inline asm
	mov.f64 	%fd73, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd70, %fd66, %fd7, %fd73;
	// inline asm
	mov.f64 	%fd176, %fd70;
	bra.uni 	BB14_22;

BB14_21:
	mov.f64 	%fd75, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd77, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd74, %fd75, %fd7, %fd77;
	// inline asm
	mov.f64 	%fd81, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd78, %fd74, %fd7, %fd81;
	// inline asm
	mov.f64 	%fd85, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd82, %fd78, %fd7, %fd85;
	// inline asm
	mov.f64 	%fd89, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd86, %fd82, %fd7, %fd89;
	// inline asm
	mov.f64 	%fd93, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd90, %fd86, %fd7, %fd93;
	// inline asm
	mul.rn.f64 	%fd95, %fd90, %fd7;
	// inline asm
	fma.rn.f64 	%fd94, %fd95, %fd175, %fd175;
	// inline asm
	mov.f64 	%fd176, %fd94;

BB14_22:
	and.b32  	%r91, %r19, 2;
	setp.eq.s32 	%p18, %r91, 0;
	neg.f64 	%fd98, %fd176;
	selp.f64 	%fd177, %fd176, %fd98, %p18;
	bra.uni 	BB14_24;

BB14_23:
	mov.f64 	%fd177, 0dFFF8000000000000;

BB14_24:
	setp.eq.f64 	%p19, %fd3, 0d0000000000000000;
	or.pred  	%p20, %p2, %p19;
	or.pred  	%p21, %p1, %p20;
	@%p21 bra 	BB14_47;

	// inline asm
	abs.f64 	%fd99, %fd3;
	// inline asm
	setp.gt.f64 	%p22, %fd99, 0d41E0000000000000;
	@%p22 bra 	BB14_27;

	mov.f64 	%fd114, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd101, %fd3, %fd114;
	// inline asm
	cvt.rni.s32.f64 	%r92, %fd101;
	// inline asm
	cvt.rn.f64.s32 	%fd115, %r92;
	neg.f64 	%fd111, %fd115;
	mov.f64 	%fd104, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd102, %fd111, %fd104, %fd3;
	// inline asm
	mov.f64 	%fd108, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd106, %fd111, %fd108, %fd102;
	// inline asm
	mov.f64 	%fd112, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd110, %fd111, %fd112, %fd106;
	// inline asm
	mov.u32 	%r135, %r92;
	mov.f64 	%fd178, %fd110;
	bra.uni 	BB14_43;

BB14_27:
	mov.b64 	 %rl33, %fd3;
	and.b64  	%rl160, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl106, %rl35, 2047;
	add.s64 	%rl107, %rl106, 4294966272;
	cvt.u32.u64 	%r21, %rl107;
	shl.b64 	%rl108, %rl33, 11;
	or.b64  	%rl36, %rl108, -9223372036854775808;
	shr.u32 	%r96, %r21, 6;
	mov.u32 	%r97, 16;
	sub.s32 	%r22, %r97, %r96;
	mov.u32 	%r98, 15;
	sub.s32 	%r133, %r98, %r96;
	mov.u32 	%r99, 19;
	sub.s32 	%r24, %r99, %r96;
	mov.u32 	%r94, 18;
	// inline asm
	min.s32 	%r93, %r94, %r24;
	// inline asm
	setp.lt.s32 	%p23, %r133, %r93;
	@%p23 bra 	BB14_29;

	mov.u64 	%rl157, 0;
	bra.uni 	BB14_31;

BB14_29:
	mov.u32 	%r100, 1;
	sub.s32 	%r25, %r100, %r22;
	mov.u64 	%rl157, 0;

BB14_30:
	.pragma "nounroll";
	shl.b32 	%r104, %r133, 3;
	mov.u32 	%r105, __internal_i2opi_d;
	add.s32 	%r106, %r105, %r104;
	ld.const.u64 	%rl112, [%r106];
	mul.lo.s64 	%rl114, %rl112, %rl36;
	// inline asm
	mul.hi.u64 	%rl111, %rl112, %rl36;
	// inline asm
	mad.lo.s64 	%rl115, %rl112, %rl36, %rl157;
	setp.lt.u64 	%p24, %rl115, %rl114;
	selp.u64 	%rl116, 1, 0, %p24;
	add.s64 	%rl157, %rl116, %rl111;
	add.s32 	%r107, %r25, %r133;
	shl.b32 	%r108, %r107, 3;
	add.s32 	%r110, %r2, %r108;
	st.local.u64 	[%r110], %rl115;
	// inline asm
	min.s32 	%r101, %r94, %r24;
	// inline asm
	add.s32 	%r133, %r133, 1;
	setp.lt.s32 	%p25, %r133, %r101;
	@%p25 bra 	BB14_30;

BB14_31:
	mov.u32 	%r111, 1;
	sub.s32 	%r112, %r111, %r22;
	add.s32 	%r113, %r112, %r133;
	shl.b32 	%r114, %r113, 3;
	add.s32 	%r116, %r2, %r114;
	st.local.u64 	[%r116], %rl157;
	ld.local.u64 	%rl158, [%r2+24];
	ld.local.u64 	%rl159, [%r2+16];
	and.b32  	%r117, %r21, 63;
	setp.eq.s32 	%p26, %r117, 0;
	@%p26 bra 	BB14_33;

	and.b64  	%rl117, %rl35, 63;
	cvt.u32.u64 	%r118, %rl117;
	shl.b64 	%rl118, %rl158, %r118;
	neg.s32 	%r119, %r21;
	and.b32  	%r120, %r119, 63;
	shr.u64 	%rl119, %rl159, %r120;
	or.b64  	%rl158, %rl119, %rl118;
	shl.b64 	%rl120, %rl159, %r118;
	ld.local.u64 	%rl121, [%r2+8];
	shr.u64 	%rl122, %rl121, %r120;
	or.b64  	%rl159, %rl122, %rl120;

BB14_33:
	shr.u64 	%rl123, %rl158, 62;
	cvt.u32.u64 	%r121, %rl123;
	shr.u64 	%rl124, %rl159, 62;
	shl.b64 	%rl125, %rl158, 2;
	or.b64  	%rl164, %rl124, %rl125;
	shl.b64 	%rl47, %rl159, 2;
	setp.ne.s64 	%p27, %rl47, 0;
	selp.u64 	%rl126, 1, 0, %p27;
	or.b64  	%rl127, %rl126, %rl164;
	setp.gt.u64 	%p28, %rl127, -9223372036854775808;
	selp.u32 	%r122, 1, 0, %p28;
	add.s32 	%r123, %r122, %r121;
	neg.s32 	%r124, %r123;
	setp.lt.s64 	%p29, %rl33, 0;
	selp.b32 	%r135, %r124, %r123, %p29;
	@%p28 bra 	BB14_35;

	mov.u64 	%rl163, %rl47;
	bra.uni 	BB14_36;

BB14_35:
	not.b64 	%rl128, %rl164;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p30, %rl47, 0;
	selp.u64 	%rl129, 1, 0, %p30;
	add.s64 	%rl164, %rl129, %rl128;
	xor.b64  	%rl160, %rl160, -9223372036854775808;
	mov.u64 	%rl163, %rl48;

BB14_36:
	mov.u64 	%rl162, %rl163;
	setp.gt.s64 	%p31, %rl164, 0;
	@%p31 bra 	BB14_38;

	mov.u32 	%r134, 0;
	bra.uni 	BB14_40;

BB14_38:
	mov.u32 	%r134, 0;

BB14_39:
	shr.u64 	%rl130, %rl162, 63;
	shl.b64 	%rl131, %rl164, 1;
	or.b64  	%rl164, %rl130, %rl131;
	shl.b64 	%rl162, %rl162, 1;
	add.s32 	%r134, %r134, -1;
	setp.gt.s64 	%p32, %rl164, 0;
	@%p32 bra 	BB14_39;

BB14_40:
	mul.lo.s64 	%rl166, %rl164, -3958705157555305931;
	mov.u64 	%rl134, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl132, %rl164, %rl134;
	// inline asm
	setp.gt.s64 	%p33, %rl132, 0;
	mov.u64 	%rl165, %rl132;
	@%p33 bra 	BB14_41;
	bra.uni 	BB14_42;

BB14_41:
	shl.b64 	%rl135, %rl132, 1;
	shr.u64 	%rl136, %rl166, 63;
	or.b64  	%rl165, %rl135, %rl136;
	mul.lo.s64 	%rl166, %rl164, -7917410315110611862;
	add.s32 	%r134, %r134, -1;

BB14_42:
	setp.ne.s64 	%p34, %rl166, 0;
	selp.u64 	%rl137, 1, 0, %p34;
	add.s64 	%rl138, %rl137, %rl165;
	add.s32 	%r127, %r134, 1022;
	cvt.u64.u32 	%rl139, %r127;
	shl.b64 	%rl140, %rl139, 52;
	shr.u64 	%rl141, %rl138, 11;
	shr.u64 	%rl142, %rl138, 10;
	and.b64  	%rl143, %rl142, 1;
	add.s64 	%rl144, %rl140, %rl141;
	add.s64 	%rl145, %rl144, %rl143;
	or.b64  	%rl146, %rl145, %rl160;
	mov.b64 	 %fd178, %rl146;

BB14_43:
	and.b32  	%r128, %r135, 1;
	setp.eq.s32 	%p35, %r128, 0;
	mul.rn.f64 	%fd17, %fd178, %fd178;
	@%p35 bra 	BB14_45;

	mov.f64 	%fd117, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd119, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd116, %fd117, %fd17, %fd119;
	// inline asm
	mov.f64 	%fd123, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd120, %fd116, %fd17, %fd123;
	// inline asm
	mov.f64 	%fd127, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd124, %fd120, %fd17, %fd127;
	// inline asm
	mov.f64 	%fd131, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd128, %fd124, %fd17, %fd131;
	// inline asm
	mov.f64 	%fd135, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd132, %fd128, %fd17, %fd135;
	// inline asm
	mov.f64 	%fd139, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd136, %fd132, %fd17, %fd139;
	// inline asm
	mov.f64 	%fd143, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd140, %fd136, %fd17, %fd143;
	// inline asm
	mov.f64 	%fd179, %fd140;
	bra.uni 	BB14_46;

BB14_45:
	mov.f64 	%fd145, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd147, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd144, %fd145, %fd17, %fd147;
	// inline asm
	mov.f64 	%fd151, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd148, %fd144, %fd17, %fd151;
	// inline asm
	mov.f64 	%fd155, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd152, %fd148, %fd17, %fd155;
	// inline asm
	mov.f64 	%fd159, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd156, %fd152, %fd17, %fd159;
	// inline asm
	mov.f64 	%fd163, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd160, %fd156, %fd17, %fd163;
	// inline asm
	mul.rn.f64 	%fd165, %fd160, %fd17;
	// inline asm
	fma.rn.f64 	%fd164, %fd165, %fd178, %fd178;
	// inline asm
	mov.f64 	%fd179, %fd164;

BB14_46:
	and.b32  	%r129, %r135, 2;
	setp.eq.s32 	%p36, %r129, 0;
	neg.f64 	%fd168, %fd179;
	selp.f64 	%fd180, %fd179, %fd168, %p36;
	bra.uni 	BB14_48;

BB14_47:
	mov.f64 	%fd169, 0d0000000000000000;
	mul.rn.f64 	%fd180, %fd3, %fd169;

BB14_48:
	mul.f64 	%fd170, %fd2, %fd180;
	fma.rn.f64 	%fd171, %fd1, %fd177, %fd170;
	st.global.f64 	[%r1], %fd171;
	mul.f64 	%fd172, %fd2, %fd177;
	neg.f64 	%fd173, %fd1;
	fma.rn.f64 	%fd174, %fd173, %fd180, %fd172;
	st.global.f64 	[%r1+8], %fd174;
	ret;
}

.entry transpose2(
	.param .u32 .ptr .global .align 8 transpose2_param_0,
	.param .u32 transpose2_param_1,
	.param .u32 transpose2_param_2
)
{
	.reg .f64 	%fd<9>;
	.reg .pred 	%p<7>;
	.reg .s32 	%r<45>;


	ld.param.u32 	%r2, [transpose2_param_1];
	// inline asm
	mov.u32 	%r13, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r14, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r15, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r16, %tid.x;
	// inline asm
	// inline asm
	mov.u32 	%r17, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r18, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r19, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r20, %tid.y;
	// inline asm
	add.s32 	%r22, %r20, %r17;
	mad.lo.s32 	%r23, %r19, %r18, %r22;
	mad.lo.s32 	%r24, %r15, %r14, %r13;
	add.s32 	%r25, %r24, %r16;
	mad.lo.s32 	%r4, %r23, %r2, %r25;
	mov.u32 	%r41, 0;
	mov.u32 	%r44, %r4;

BB15_1:
	add.s32 	%r41, %r41, 1;
	ld.param.u32 	%r40, [transpose2_param_2];
	rem.s32 	%r26, %r44, %r40;
	div.s32 	%r27, %r44, %r40;
	ld.param.u32 	%r38, [transpose2_param_1];
	mad.lo.s32 	%r44, %r26, %r38, %r27;
	setp.gt.s32 	%p2, %r44, %r4;
	@%p2 bra 	BB15_1;

	setp.lt.s32 	%p3, %r44, %r4;
	setp.eq.s32 	%p4, %r41, 1;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB15_11;

	shl.b32 	%r28, %r4, 4;
	ld.param.u32 	%r36, [transpose2_param_0];
	add.s32 	%r29, %r36, %r28;
	ld.global.f64 	%fd1, [%r29];
	ld.global.f64 	%fd2, [%r29+8];
	mov.u32 	%r43, %r4;

BB15_4:
	mov.u32 	%r9, %r43;
	ld.param.u32 	%r39, [transpose2_param_2];
	rem.s32 	%r30, %r9, %r39;
	div.s32 	%r31, %r9, %r39;
	ld.param.u32 	%r37, [transpose2_param_1];
	mad.lo.s32 	%r10, %r30, %r37, %r31;
	setp.eq.s32 	%p1, %r10, %r4;
	shl.b32 	%r32, %r10, 4;
	ld.param.u32 	%r35, [transpose2_param_0];
	add.s32 	%r11, %r35, %r32;
	@%p1 bra 	BB15_6;

	ld.global.f64 	%fd3, [%r11];
	mov.f64 	%fd7, %fd3;
	bra.uni 	BB15_7;

BB15_6:
	mov.f64 	%fd7, %fd1;

BB15_7:
	mov.f64 	%fd4, %fd7;
	shl.b32 	%r33, %r9, 4;
	ld.param.u32 	%r34, [transpose2_param_0];
	add.s32 	%r12, %r34, %r33;
	st.global.f64 	[%r12], %fd4;
	@%p1 bra 	BB15_9;

	ld.global.f64 	%fd5, [%r11+8];
	mov.f64 	%fd8, %fd5;
	bra.uni 	BB15_10;

BB15_9:
	mov.f64 	%fd8, %fd2;

BB15_10:
	mov.f64 	%fd6, %fd8;
	st.global.f64 	[%r12+8], %fd6;
	setp.gt.s32 	%p6, %r10, %r4;
	mov.u32 	%r43, %r10;
	@%p6 bra 	BB15_4;

BB15_11:
	ret;
}

.entry swapkernel(
	.param .u32 .ptr .global .align 8 swapkernel_param_0,
	.param .u32 swapkernel_param_1,
	.param .u32 swapkernel_param_2,
	.param .u32 .ptr .global .align 4 swapkernel_param_3,
	.param .u32 .ptr .global .align 4 swapkernel_param_4,
	.param .u32 swapkernel_param_5
)
{
	.reg .f64 	%fd<9>;
	.reg .pred 	%p<5>;
	.reg .s32 	%r<40>;


	ld.param.u32 	%r1, [swapkernel_param_0];
	ld.param.u32 	%r2, [swapkernel_param_1];
	ld.param.u32 	%r5, [swapkernel_param_5];
	// inline asm
	mov.u32 	%r12, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r13, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r14, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r15, %tid.x;
	// inline asm
	add.s32 	%r20, %r15, %r12;
	mad.lo.s32 	%r6, %r14, %r13, %r20;
	// inline asm
	mov.u32 	%r16, %envreg4;
	// inline asm
	// inline asm
	mov.u32 	%r17, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r18, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r19, %tid.y;
	// inline asm
	add.s32 	%r21, %r19, %r16;
	mad.lo.s32 	%r7, %r18, %r17, %r21;
	mul.lo.s32 	%r8, %r7, %r2;
	mad.lo.s32 	%r22, %r7, %r2, %r6;
	shl.b32 	%r23, %r22, 4;
	add.s32 	%r9, %r1, %r23;
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	BB16_4;

	ld.param.u32 	%r39, [swapkernel_param_5];
	setp.ne.s32 	%p2, %r39, 1;
	@%p2 bra 	BB16_6;

	shl.b32 	%r24, %r7, 2;
	ld.param.u32 	%r38, [swapkernel_param_4];
	add.s32 	%r25, %r38, %r24;
	ld.global.u32 	%r10, [%r25];
	setp.ge.s32 	%p3, %r7, %r10;
	@%p3 bra 	BB16_6;

	ld.param.u32 	%r36, [swapkernel_param_1];
	mad.lo.s32 	%r26, %r10, %r36, %r6;
	shl.b32 	%r27, %r26, 4;
	ld.param.u32 	%r35, [swapkernel_param_0];
	add.s32 	%r28, %r35, %r27;
	ld.global.f64 	%fd1, [%r28];
	ld.global.f64 	%fd2, [%r9];
	st.global.f64 	[%r28], %fd2;
	st.global.f64 	[%r9], %fd1;
	ld.global.f64 	%fd3, [%r28+8];
	ld.global.f64 	%fd4, [%r9+8];
	st.global.f64 	[%r28+8], %fd4;
	st.global.f64 	[%r9+8], %fd3;
	ret;

BB16_4:
	shl.b32 	%r29, %r6, 2;
	ld.param.u32 	%r37, [swapkernel_param_3];
	add.s32 	%r30, %r37, %r29;
	ld.global.u32 	%r11, [%r30];
	setp.ge.s32 	%p4, %r6, %r11;
	@%p4 bra 	BB16_6;

	add.s32 	%r31, %r11, %r8;
	shl.b32 	%r32, %r31, 4;
	ld.param.u32 	%r34, [swapkernel_param_0];
	add.s32 	%r33, %r34, %r32;
	ld.global.f64 	%fd5, [%r33];
	ld.global.f64 	%fd6, [%r9];
	st.global.f64 	[%r33], %fd6;
	st.global.f64 	[%r9], %fd5;
	ld.global.f64 	%fd7, [%r33+8];
	ld.global.f64 	%fd8, [%r9+8];
	st.global.f64 	[%r33+8], %fd8;
	st.global.f64 	[%r9+8], %fd7;

BB16_6:
	ret;
}

.entry DIT2C2C(
	.param .u32 .ptr .global .align 8 DIT2C2C_param_0,
	.param .u32 .ptr .global .align 16 DIT2C2C_param_1,
	.param .u32 DIT2C2C_param_2,
	.param .u32 DIT2C2C_param_3,
	.param .u32 DIT2C2C_param_4,
	.param .u32 DIT2C2C_param_5
)
{
	.local .align 8 .b8 	__local_depot17[40];
	.reg .b32 	%SP;
	.reg .f32 	%f<293>;
	.reg .f64 	%fd<231>;
	.reg .pred 	%p<80>;
	.reg .s32 	%r<224>;
	.reg .s64 	%rl<167>;


	mov.u32 	%SP, __local_depot17;
	ld.param.u32 	%r72, [DIT2C2C_param_3];
	// inline asm
	mov.u32 	%r68, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r69, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r70, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r71, %tid.x;
	// inline asm
	add.s32 	%r73, %r71, %r68;
	mad.lo.s32 	%r6, %r70, %r69, %r73;
	mul.hi.s32 	%r74, %r72, 780903145;
	shr.u32 	%r75, %r74, 31;
	shr.s32 	%r76, %r74, 1;
	add.s32 	%r7, %r76, %r75;
	mul.lo.s32 	%r77, %r7, 11;
	sub.s32 	%r8, %r72, %r77;
	setp.gt.s32 	%p9, %r72, 10;
	@%p9 bra 	BB17_2;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB17_23;

BB17_2:
	mov.f32 	%f1, 0f41300000;
	mov.pred 	%p3, 0;
	mov.f32 	%f47, 0f40000000;
	add.f32 	%f2, %f47, 0f41300000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f5, 0f37000000;
	mov.u32 	%r215, 1;
	mov.u32 	%r214, 0;

BB17_3:
	// inline asm
	abs.f32 	%f48, %f47;
	// inline asm
	selp.f32 	%f7, 0f3F800000, %f2, %p3;
	or.pred  	%p10, %p3, %p3;
	@%p10 bra 	BB17_20;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mul.rn.f32 	%f56, %f47, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p11, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p12, %f1, %f52;
	and.pred  	%p4, %p11, %p12;
	setp.eq.f32 	%p13, %f48, 0f00000000;
	@%p13 bra 	BB17_17;

	// inline asm
	abs.f32 	%f58, %f47;
	// inline asm
	mov.b32 	 %r11, %f58;
	shr.u32 	%r80, %r11, 23;
	and.b32  	%r81, %r80, 255;
	add.s32 	%r216, %r81, -127;
	setp.eq.s32 	%p14, %r81, 0;
	mov.f32 	%f280, %f58;
	@%p14 bra 	BB17_6;
	bra.uni 	BB17_7;

BB17_6:
	and.b32  	%r82, %r11, -2139095041;
	or.b32  	%r83, %r82, 1065353216;
	mov.b32 	 %f60, %r83;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r84, %f61;
	shr.u32 	%r85, %r84, 23;
	and.b32  	%r86, %r85, 255;
	add.s32 	%r216, %r86, -253;
	and.b32  	%r87, %r84, -2139095041;
	or.b32  	%r88, %r87, 1065353216;
	mov.b32 	 %f280, %r88;

BB17_7:
	mov.b32 	 %r89, %f280;
	and.b32  	%r90, %r89, -2139095041;
	or.b32  	%r91, %r90, 1065353216;
	mov.b32 	 %f281, %r91;
	setp.gt.f32 	%p15, %f281, 0f3FB504F3;
	@%p15 bra 	BB17_8;
	bra.uni 	BB17_9;

BB17_8:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r216, %r216, 1;

BB17_9:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f47, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r92, %f73;
	and.b32  	%r93, %r92, -4096;
	mov.b32 	 %f81, %r93;
	mov.b32 	 %r94, %f64;
	and.b32  	%r95, %r94, -4096;
	mov.b32 	 %f82, %r95;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f47, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r216;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f14, %f104, %f108;
	sub.f32 	%f109, %f104, %f14;
	add.f32 	%f15, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p16, %f69, 0f77F684DF;
	@%p16 bra 	BB17_11;

	mov.f32 	%f282, %f1;
	bra.uni 	BB17_12;

BB17_11:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f16, %f1, %f110;
	mov.f32 	%f282, %f16;

BB17_12:
	mov.f32 	%f17, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f14, %f111;
	sub.f32 	%f113, %f14, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f14, %f114;
	mul.rn.f32 	%f116, %f17, %f111;
	sub.f32 	%f117, %f17, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f17, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f14, %f17;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f15, %f17;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f18, %f132, %f130;
	mov.f32 	%f291, %f18;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r17, %f131;
	setp.eq.s32 	%p17, %r17, 1118925336;
	@%p17 bra 	BB17_13;
	bra.uni 	BB17_14;

BB17_13:
	add.s32 	%r96, %r17, -1;
	mov.b32 	 %f133, %r96;
	add.f32 	%f134, %f18, %f5;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB17_14:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p18, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p18;
	setp.gt.f32 	%p19, %f292, 0f42D20000;
	selp.f32 	%f19, %f3, %f150, %p19;
	setp.neu.f32 	%p20, %f19, %f3;
	@%p20 bra 	BB17_16;

	mov.f32 	%f283, %f19;
	bra.uni 	BB17_21;

BB17_16:
	// inline asm
	mad.f32 	%f151, %f19, %f291, %f19;
	// inline asm
	mov.f32 	%f20, %f151;
	mov.f32 	%f283, %f20;
	bra.uni 	BB17_21;

BB17_17:
	@%p3 bra 	BB17_19;

	selp.f32 	%f21, %f4, 0f00000000, %p4;
	mov.f32 	%f283, %f21;
	bra.uni 	BB17_21;

BB17_19:
	mov.f32 	%f283, %f3;
	bra.uni 	BB17_21;

BB17_20:
	mov.f32 	%f283, %f7;

BB17_21:
	mov.f32 	%f22, %f283;
	cvt.rn.f32.s32 	%f156, %r215;
	mul.f32 	%f157, %f156, %f22;
	cvt.rzi.s32.f32 	%r215, %f157;
	add.s32 	%r214, %r214, 1;
	setp.lt.s32 	%p21, %r214, %r7;
	@%p21 bra 	BB17_3;

	cvt.rn.f32.s32 	%f284, %r215;

BB17_23:
	mov.f32 	%f159, 0f40000000;
	// inline asm
	abs.f32 	%f158, %f159;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r8;
	setp.eq.f32 	%p22, %f287, 0f00000000;
	@%p22 bra 	BB17_45;

	setp.nan.f32 	%p23, %f287, %f287;
	@%p23 bra 	BB17_44;

	mov.f32 	%f27, 0f7F800000;
	setp.eq.f32 	%p24, %f287, 0f7F800000;
	setp.eq.f32 	%p25, %f287, 0fFF800000;
	or.pred  	%p26, %p24, %p25;
	@%p26 bra 	BB17_41;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p29, %f158, 0f00000000;
	@%p29 bra 	BB17_38;

	// inline asm
	abs.f32 	%f168, %f159;
	// inline asm
	mov.b32 	 %r20, %f168;
	shr.u32 	%r97, %r20, 23;
	and.b32  	%r98, %r97, 255;
	add.s32 	%r217, %r98, -127;
	setp.eq.s32 	%p30, %r98, 0;
	mov.f32 	%f285, %f168;
	@%p30 bra 	BB17_28;
	bra.uni 	BB17_29;

BB17_28:
	and.b32  	%r99, %r20, -2139095041;
	or.b32  	%r100, %r99, 1065353216;
	mov.b32 	 %f170, %r100;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r101, %f171;
	shr.u32 	%r102, %r101, 23;
	and.b32  	%r103, %r102, 255;
	add.s32 	%r217, %r103, -253;
	and.b32  	%r104, %r101, -2139095041;
	or.b32  	%r105, %r104, 1065353216;
	mov.b32 	 %f285, %r105;

BB17_29:
	mov.b32 	 %r106, %f285;
	and.b32  	%r107, %r106, -2139095041;
	or.b32  	%r108, %r107, 1065353216;
	mov.b32 	 %f286, %r108;
	setp.gt.f32 	%p31, %f286, 0f3FB504F3;
	@%p31 bra 	BB17_30;
	bra.uni 	BB17_31;

BB17_30:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r217, %r217, 1;

BB17_31:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mul.rn.f32 	%f183, %f159, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r109, %f183;
	and.b32  	%r110, %r109, -4096;
	mov.b32 	 %f191, %r110;
	mov.b32 	 %r111, %f174;
	and.b32  	%r112, %r111, -4096;
	mov.b32 	 %f192, %r112;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f159, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r217;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p32, %f179, 0f77F684DF;
	@%p32 bra 	BB17_32;
	bra.uni 	BB17_33;

BB17_32:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB17_33:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r26, %f241;
	setp.eq.s32 	%p33, %r26, 1118925336;
	@%p33 bra 	BB17_34;
	bra.uni 	BB17_35;

BB17_34:
	add.s32 	%r113, %r26, -1;
	mov.b32 	 %f243, %r113;
	add.f32 	%f244, %f38, 0f37000000;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB17_35:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p34, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p34;
	setp.gt.f32 	%p35, %f290, 0f42D20000;
	selp.f32 	%f39, %f27, %f260, %p35;
	setp.neu.f32 	%p36, %f39, %f27;
	@%p36 bra 	BB17_37;

	mov.f32 	%f288, %f39;
	bra.uni 	BB17_46;

BB17_37:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f288, %f261;
	bra.uni 	BB17_46;

BB17_38:
	setp.lt.f32 	%p37, %f287, 0f00000000;
	@%p37 bra 	BB17_40;

	mov.f32 	%f288, 0f00000000;
	bra.uni 	BB17_46;

BB17_40:
	mov.f32 	%f288, %f27;
	bra.uni 	BB17_46;

BB17_41:
	setp.lt.f32 	%p38, %f158, 0f3F800000;
	mov.b32 	 %r114, %f287;
	setp.lt.s32 	%p6, %r114, 0;
	@%p38 bra 	BB17_43;

	selp.f32 	%f288, 0f00000000, %f27, %p6;
	bra.uni 	BB17_46;

BB17_43:
	selp.f32 	%f288, %f27, 0f00000000, %p6;
	bra.uni 	BB17_46;

BB17_44:
	add.f32 	%f288, %f287, 0f40000000;
	bra.uni 	BB17_46;

BB17_45:
	mov.f32 	%f288, 0f3F800000;

BB17_46:
	mul.f32 	%f267, %f284, %f288;
	cvt.rzi.s32.f32 	%r27, %f267;
	shr.u32 	%r115, %r27, 31;
	add.s32 	%r116, %r27, %r115;
	shr.s32 	%r117, %r116, 1;
	div.s32 	%r118, %r6, %r117;
	rem.s32 	%r28, %r6, %r117;
	mad.lo.s32 	%r119, %r118, %r27, %r28;
	shl.b32 	%r29, %r119, 1;
	mad.lo.s32 	%r120, %r118, %r27, %r117;
	add.s32 	%r121, %r120, %r28;
	shl.b32 	%r30, %r121, 1;
	ld.param.u32 	%r211, [DIT2C2C_param_2];
	div.s32 	%r122, %r211, %r27;
	mul.lo.s32 	%r123, %r122, %r28;
	shr.s32 	%r124, %r211, 31;
	shr.u32 	%r125, %r124, 30;
	add.s32 	%r126, %r211, %r125;
	shr.s32 	%r127, %r126, 2;
	div.s32 	%r31, %r123, %r127;
	rem.s32 	%r32, %r123, %r127;
	ld.param.u32 	%r213, [DIT2C2C_param_5];
	setp.eq.s32 	%p39, %r213, 1;
	@%p39 bra 	BB17_96;

	cvt.rn.f64.s32 	%fd22, %r28;
	mul.f64 	%fd23, %fd22, 0d401921FB54442D18;
	cvt.rn.f64.s32 	%fd24, %r27;
	div.rn.f64 	%fd1, %fd23, %fd24;
	setp.eq.f64 	%p7, %fd1, 0d7FF0000000000000;
	setp.eq.f64 	%p8, %fd1, 0dFFF0000000000000;
	or.pred  	%p40, %p7, %p8;
	add.u32 	%r33, %SP, 0;
	@%p40 bra 	BB17_70;

	// inline asm
	abs.f64 	%fd25, %fd1;
	// inline asm
	setp.gt.f64 	%p41, %fd25, 0d41E0000000000000;
	@%p41 bra 	BB17_50;

	mov.f64 	%fd40, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd27, %fd1, %fd40;
	// inline asm
	cvt.rni.s32.f64 	%r128, %fd27;
	// inline asm
	cvt.rn.f64.s32 	%fd41, %r128;
	neg.f64 	%fd37, %fd41;
	mov.f64 	%fd30, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd28, %fd37, %fd30, %fd1;
	// inline asm
	mov.f64 	%fd34, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd32, %fd37, %fd34, %fd28;
	// inline asm
	mov.f64 	%fd38, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd36, %fd37, %fd38, %fd32;
	// inline asm
	mov.u32 	%r220, %r128;
	mov.f64 	%fd223, %fd36;
	bra.uni 	BB17_66;

BB17_50:
	mov.b64 	 %rl1, %fd1;
	and.b64  	%rl150, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl65, %rl3, 2047;
	add.s64 	%rl66, %rl65, 4294966272;
	cvt.u32.u64 	%r35, %rl66;
	shl.b64 	%rl67, %rl1, 11;
	or.b64  	%rl4, %rl67, -9223372036854775808;
	shr.u32 	%r132, %r35, 6;
	mov.u32 	%r133, 16;
	sub.s32 	%r36, %r133, %r132;
	mov.u32 	%r134, 15;
	sub.s32 	%r218, %r134, %r132;
	mov.u32 	%r135, 19;
	sub.s32 	%r38, %r135, %r132;
	mov.u32 	%r130, 18;
	// inline asm
	min.s32 	%r129, %r130, %r38;
	// inline asm
	setp.lt.s32 	%p42, %r218, %r129;
	@%p42 bra 	BB17_52;

	mov.u64 	%rl147, 0;
	bra.uni 	BB17_54;

BB17_52:
	mov.u32 	%r136, 1;
	sub.s32 	%r39, %r136, %r36;
	mov.u64 	%rl147, 0;

BB17_53:
	.pragma "nounroll";
	shl.b32 	%r140, %r218, 3;
	mov.u32 	%r141, __internal_i2opi_d;
	add.s32 	%r142, %r141, %r140;
	ld.const.u64 	%rl71, [%r142];
	mul.lo.s64 	%rl73, %rl71, %rl4;
	// inline asm
	mul.hi.u64 	%rl70, %rl71, %rl4;
	// inline asm
	mad.lo.s64 	%rl74, %rl71, %rl4, %rl147;
	setp.lt.u64 	%p43, %rl74, %rl73;
	selp.u64 	%rl75, 1, 0, %p43;
	add.s64 	%rl147, %rl75, %rl70;
	add.s32 	%r143, %r39, %r218;
	shl.b32 	%r144, %r143, 3;
	add.s32 	%r146, %r33, %r144;
	st.local.u64 	[%r146], %rl74;
	// inline asm
	min.s32 	%r137, %r130, %r38;
	// inline asm
	add.s32 	%r218, %r218, 1;
	setp.lt.s32 	%p44, %r218, %r137;
	@%p44 bra 	BB17_53;

BB17_54:
	mov.u32 	%r147, 1;
	sub.s32 	%r148, %r147, %r36;
	add.s32 	%r149, %r148, %r218;
	shl.b32 	%r150, %r149, 3;
	add.s32 	%r152, %r33, %r150;
	st.local.u64 	[%r152], %rl147;
	ld.local.u64 	%rl148, [%r33+24];
	ld.local.u64 	%rl149, [%r33+16];
	and.b32  	%r153, %r35, 63;
	setp.eq.s32 	%p45, %r153, 0;
	@%p45 bra 	BB17_56;

	and.b64  	%rl76, %rl3, 63;
	cvt.u32.u64 	%r154, %rl76;
	shl.b64 	%rl77, %rl148, %r154;
	neg.s32 	%r155, %r35;
	and.b32  	%r156, %r155, 63;
	shr.u64 	%rl78, %rl149, %r156;
	or.b64  	%rl148, %rl78, %rl77;
	shl.b64 	%rl79, %rl149, %r154;
	ld.local.u64 	%rl80, [%r33+8];
	shr.u64 	%rl81, %rl80, %r156;
	or.b64  	%rl149, %rl81, %rl79;

BB17_56:
	shr.u64 	%rl82, %rl148, 62;
	cvt.u32.u64 	%r157, %rl82;
	shr.u64 	%rl83, %rl149, 62;
	shl.b64 	%rl84, %rl148, 2;
	or.b64  	%rl154, %rl83, %rl84;
	shl.b64 	%rl15, %rl149, 2;
	setp.ne.s64 	%p46, %rl15, 0;
	selp.u64 	%rl85, 1, 0, %p46;
	or.b64  	%rl86, %rl85, %rl154;
	setp.gt.u64 	%p47, %rl86, -9223372036854775808;
	selp.u32 	%r158, 1, 0, %p47;
	add.s32 	%r159, %r158, %r157;
	neg.s32 	%r160, %r159;
	setp.lt.s64 	%p48, %rl1, 0;
	selp.b32 	%r220, %r160, %r159, %p48;
	@%p47 bra 	BB17_58;

	mov.u64 	%rl153, %rl15;
	bra.uni 	BB17_59;

BB17_58:
	not.b64 	%rl87, %rl154;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p49, %rl15, 0;
	selp.u64 	%rl88, 1, 0, %p49;
	add.s64 	%rl154, %rl88, %rl87;
	xor.b64  	%rl150, %rl150, -9223372036854775808;
	mov.u64 	%rl153, %rl16;

BB17_59:
	mov.u64 	%rl152, %rl153;
	setp.gt.s64 	%p50, %rl154, 0;
	@%p50 bra 	BB17_61;

	mov.u32 	%r219, 0;
	bra.uni 	BB17_63;

BB17_61:
	mov.u32 	%r219, 0;

BB17_62:
	shr.u64 	%rl89, %rl152, 63;
	shl.b64 	%rl90, %rl154, 1;
	or.b64  	%rl154, %rl89, %rl90;
	shl.b64 	%rl152, %rl152, 1;
	add.s32 	%r219, %r219, -1;
	setp.gt.s64 	%p51, %rl154, 0;
	@%p51 bra 	BB17_62;

BB17_63:
	mul.lo.s64 	%rl156, %rl154, -3958705157555305931;
	mov.u64 	%rl93, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl91, %rl154, %rl93;
	// inline asm
	setp.gt.s64 	%p52, %rl91, 0;
	mov.u64 	%rl155, %rl91;
	@%p52 bra 	BB17_64;
	bra.uni 	BB17_65;

BB17_64:
	shl.b64 	%rl94, %rl91, 1;
	shr.u64 	%rl95, %rl156, 63;
	or.b64  	%rl155, %rl94, %rl95;
	mul.lo.s64 	%rl156, %rl154, -7917410315110611862;
	add.s32 	%r219, %r219, -1;

BB17_65:
	setp.ne.s64 	%p53, %rl156, 0;
	selp.u64 	%rl96, 1, 0, %p53;
	add.s64 	%rl97, %rl96, %rl155;
	add.s32 	%r163, %r219, 1022;
	cvt.u64.u32 	%rl98, %r163;
	shl.b64 	%rl99, %rl98, 52;
	shr.u64 	%rl100, %rl97, 11;
	shr.u64 	%rl101, %rl97, 10;
	and.b64  	%rl102, %rl101, 1;
	add.s64 	%rl103, %rl99, %rl100;
	add.s64 	%rl104, %rl103, %rl102;
	or.b64  	%rl105, %rl104, %rl150;
	mov.b64 	 %fd223, %rl105;

BB17_66:
	add.s32 	%r50, %r220, 1;
	and.b32  	%r164, %r50, 1;
	setp.eq.s32 	%p54, %r164, 0;
	mul.rn.f64 	%fd5, %fd223, %fd223;
	@%p54 bra 	BB17_68;

	mov.f64 	%fd43, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd45, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd42, %fd43, %fd5, %fd45;
	// inline asm
	mov.f64 	%fd49, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd46, %fd42, %fd5, %fd49;
	// inline asm
	mov.f64 	%fd53, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd50, %fd46, %fd5, %fd53;
	// inline asm
	mov.f64 	%fd57, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd54, %fd50, %fd5, %fd57;
	// inline asm
	mov.f64 	%fd61, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd58, %fd54, %fd5, %fd61;
	// inline asm
	mov.f64 	%fd65, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd62, %fd58, %fd5, %fd65;
	// inline asm
	mov.f64 	%fd69, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd66, %fd62, %fd5, %fd69;
	// inline asm
	mov.f64 	%fd224, %fd66;
	bra.uni 	BB17_69;

BB17_68:
	mov.f64 	%fd71, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd73, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd70, %fd71, %fd5, %fd73;
	// inline asm
	mov.f64 	%fd77, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd74, %fd70, %fd5, %fd77;
	// inline asm
	mov.f64 	%fd81, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd78, %fd74, %fd5, %fd81;
	// inline asm
	mov.f64 	%fd85, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd82, %fd78, %fd5, %fd85;
	// inline asm
	mov.f64 	%fd89, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd86, %fd82, %fd5, %fd89;
	// inline asm
	mul.rn.f64 	%fd91, %fd86, %fd5;
	// inline asm
	fma.rn.f64 	%fd90, %fd91, %fd223, %fd223;
	// inline asm
	mov.f64 	%fd224, %fd90;

BB17_69:
	and.b32  	%r165, %r50, 2;
	setp.eq.s32 	%p55, %r165, 0;
	neg.f64 	%fd94, %fd224;
	selp.f64 	%fd225, %fd224, %fd94, %p55;
	bra.uni 	BB17_71;

BB17_70:
	mov.f64 	%fd225, 0dFFF8000000000000;

BB17_71:
	setp.eq.f64 	%p56, %fd1, 0d0000000000000000;
	or.pred  	%p57, %p8, %p56;
	or.pred  	%p58, %p7, %p57;
	@%p58 bra 	BB17_94;

	// inline asm
	abs.f64 	%fd96, %fd1;
	// inline asm
	setp.gt.f64 	%p59, %fd96, 0d41E0000000000000;
	@%p59 bra 	BB17_74;

	mov.f64 	%fd111, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd98, %fd1, %fd111;
	// inline asm
	cvt.rni.s32.f64 	%r166, %fd98;
	// inline asm
	cvt.rn.f64.s32 	%fd112, %r166;
	neg.f64 	%fd108, %fd112;
	mov.f64 	%fd101, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd99, %fd108, %fd101, %fd1;
	// inline asm
	mov.f64 	%fd105, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd103, %fd108, %fd105, %fd99;
	// inline asm
	mov.f64 	%fd109, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd107, %fd108, %fd109, %fd103;
	// inline asm
	mov.u32 	%r223, %r166;
	mov.f64 	%fd226, %fd107;
	bra.uni 	BB17_90;

BB17_74:
	mov.b64 	 %rl33, %fd1;
	and.b64  	%rl160, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl106, %rl35, 2047;
	add.s64 	%rl107, %rl106, 4294966272;
	cvt.u32.u64 	%r52, %rl107;
	shl.b64 	%rl108, %rl33, 11;
	or.b64  	%rl36, %rl108, -9223372036854775808;
	shr.u32 	%r170, %r52, 6;
	mov.u32 	%r171, 16;
	sub.s32 	%r53, %r171, %r170;
	mov.u32 	%r172, 15;
	sub.s32 	%r221, %r172, %r170;
	mov.u32 	%r173, 19;
	sub.s32 	%r55, %r173, %r170;
	mov.u32 	%r168, 18;
	// inline asm
	min.s32 	%r167, %r168, %r55;
	// inline asm
	setp.lt.s32 	%p60, %r221, %r167;
	@%p60 bra 	BB17_76;

	mov.u64 	%rl157, 0;
	bra.uni 	BB17_78;

BB17_76:
	mov.u32 	%r174, 1;
	sub.s32 	%r56, %r174, %r53;
	mov.u64 	%rl157, 0;

BB17_77:
	.pragma "nounroll";
	shl.b32 	%r178, %r221, 3;
	mov.u32 	%r179, __internal_i2opi_d;
	add.s32 	%r180, %r179, %r178;
	ld.const.u64 	%rl112, [%r180];
	mul.lo.s64 	%rl114, %rl112, %rl36;
	// inline asm
	mul.hi.u64 	%rl111, %rl112, %rl36;
	// inline asm
	mad.lo.s64 	%rl115, %rl112, %rl36, %rl157;
	setp.lt.u64 	%p61, %rl115, %rl114;
	selp.u64 	%rl116, 1, 0, %p61;
	add.s64 	%rl157, %rl116, %rl111;
	add.s32 	%r181, %r56, %r221;
	shl.b32 	%r182, %r181, 3;
	add.s32 	%r184, %r33, %r182;
	st.local.u64 	[%r184], %rl115;
	// inline asm
	min.s32 	%r175, %r168, %r55;
	// inline asm
	add.s32 	%r221, %r221, 1;
	setp.lt.s32 	%p62, %r221, %r175;
	@%p62 bra 	BB17_77;

BB17_78:
	mov.u32 	%r185, 1;
	sub.s32 	%r186, %r185, %r53;
	add.s32 	%r187, %r186, %r221;
	shl.b32 	%r188, %r187, 3;
	add.s32 	%r190, %r33, %r188;
	st.local.u64 	[%r190], %rl157;
	ld.local.u64 	%rl158, [%r33+24];
	ld.local.u64 	%rl159, [%r33+16];
	and.b32  	%r191, %r52, 63;
	setp.eq.s32 	%p63, %r191, 0;
	@%p63 bra 	BB17_80;

	and.b64  	%rl117, %rl35, 63;
	cvt.u32.u64 	%r192, %rl117;
	shl.b64 	%rl118, %rl158, %r192;
	neg.s32 	%r193, %r52;
	and.b32  	%r194, %r193, 63;
	shr.u64 	%rl119, %rl159, %r194;
	or.b64  	%rl158, %rl119, %rl118;
	shl.b64 	%rl120, %rl159, %r192;
	ld.local.u64 	%rl121, [%r33+8];
	shr.u64 	%rl122, %rl121, %r194;
	or.b64  	%rl159, %rl122, %rl120;

BB17_80:
	shr.u64 	%rl123, %rl158, 62;
	cvt.u32.u64 	%r195, %rl123;
	shr.u64 	%rl124, %rl159, 62;
	shl.b64 	%rl125, %rl158, 2;
	or.b64  	%rl164, %rl124, %rl125;
	shl.b64 	%rl47, %rl159, 2;
	setp.ne.s64 	%p64, %rl47, 0;
	selp.u64 	%rl126, 1, 0, %p64;
	or.b64  	%rl127, %rl126, %rl164;
	setp.gt.u64 	%p65, %rl127, -9223372036854775808;
	selp.u32 	%r196, 1, 0, %p65;
	add.s32 	%r197, %r196, %r195;
	neg.s32 	%r198, %r197;
	setp.lt.s64 	%p66, %rl33, 0;
	selp.b32 	%r223, %r198, %r197, %p66;
	@%p65 bra 	BB17_82;

	mov.u64 	%rl163, %rl47;
	bra.uni 	BB17_83;

BB17_82:
	not.b64 	%rl128, %rl164;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p67, %rl47, 0;
	selp.u64 	%rl129, 1, 0, %p67;
	add.s64 	%rl164, %rl129, %rl128;
	xor.b64  	%rl160, %rl160, -9223372036854775808;
	mov.u64 	%rl163, %rl48;

BB17_83:
	mov.u64 	%rl162, %rl163;
	setp.gt.s64 	%p68, %rl164, 0;
	@%p68 bra 	BB17_85;

	mov.u32 	%r222, 0;
	bra.uni 	BB17_87;

BB17_85:
	mov.u32 	%r222, 0;

BB17_86:
	shr.u64 	%rl130, %rl162, 63;
	shl.b64 	%rl131, %rl164, 1;
	or.b64  	%rl164, %rl130, %rl131;
	shl.b64 	%rl162, %rl162, 1;
	add.s32 	%r222, %r222, -1;
	setp.gt.s64 	%p69, %rl164, 0;
	@%p69 bra 	BB17_86;

BB17_87:
	mul.lo.s64 	%rl166, %rl164, -3958705157555305931;
	mov.u64 	%rl134, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl132, %rl164, %rl134;
	// inline asm
	setp.gt.s64 	%p70, %rl132, 0;
	mov.u64 	%rl165, %rl132;
	@%p70 bra 	BB17_88;
	bra.uni 	BB17_89;

BB17_88:
	shl.b64 	%rl135, %rl132, 1;
	shr.u64 	%rl136, %rl166, 63;
	or.b64  	%rl165, %rl135, %rl136;
	mul.lo.s64 	%rl166, %rl164, -7917410315110611862;
	add.s32 	%r222, %r222, -1;

BB17_89:
	setp.ne.s64 	%p71, %rl166, 0;
	selp.u64 	%rl137, 1, 0, %p71;
	add.s64 	%rl138, %rl137, %rl165;
	add.s32 	%r201, %r222, 1022;
	cvt.u64.u32 	%rl139, %r201;
	shl.b64 	%rl140, %rl139, 52;
	shr.u64 	%rl141, %rl138, 11;
	shr.u64 	%rl142, %rl138, 10;
	and.b64  	%rl143, %rl142, 1;
	add.s64 	%rl144, %rl140, %rl141;
	add.s64 	%rl145, %rl144, %rl143;
	or.b64  	%rl146, %rl145, %rl160;
	mov.b64 	 %fd226, %rl146;

BB17_90:
	and.b32  	%r202, %r223, 1;
	setp.eq.s32 	%p72, %r202, 0;
	mul.rn.f64 	%fd15, %fd226, %fd226;
	@%p72 bra 	BB17_92;

	mov.f64 	%fd114, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd116, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd113, %fd114, %fd15, %fd116;
	// inline asm
	mov.f64 	%fd120, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd117, %fd113, %fd15, %fd120;
	// inline asm
	mov.f64 	%fd124, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd121, %fd117, %fd15, %fd124;
	// inline asm
	mov.f64 	%fd128, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd125, %fd121, %fd15, %fd128;
	// inline asm
	mov.f64 	%fd132, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd129, %fd125, %fd15, %fd132;
	// inline asm
	mov.f64 	%fd136, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd133, %fd129, %fd15, %fd136;
	// inline asm
	mov.f64 	%fd140, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd137, %fd133, %fd15, %fd140;
	// inline asm
	mov.f64 	%fd227, %fd137;
	bra.uni 	BB17_93;

BB17_92:
	mov.f64 	%fd142, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd144, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd141, %fd142, %fd15, %fd144;
	// inline asm
	mov.f64 	%fd148, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd145, %fd141, %fd15, %fd148;
	// inline asm
	mov.f64 	%fd152, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd149, %fd145, %fd15, %fd152;
	// inline asm
	mov.f64 	%fd156, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd153, %fd149, %fd15, %fd156;
	// inline asm
	mov.f64 	%fd160, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd157, %fd153, %fd15, %fd160;
	// inline asm
	mul.rn.f64 	%fd162, %fd157, %fd15;
	// inline asm
	fma.rn.f64 	%fd161, %fd162, %fd226, %fd226;
	// inline asm
	mov.f64 	%fd227, %fd161;

BB17_93:
	and.b32  	%r203, %r223, 2;
	setp.eq.s32 	%p73, %r203, 0;
	neg.f64 	%fd165, %fd227;
	selp.f64 	%fd228, %fd227, %fd165, %p73;
	bra.uni 	BB17_95;

BB17_94:
	mov.f64 	%fd166, 0d0000000000000000;
	mul.rn.f64 	%fd228, %fd1, %fd166;

BB17_95:
	mov.f64 	%fd229, %fd225;
	mov.f64 	%fd230, %fd228;
	bra.uni 	BB17_105;

BB17_96:
	shl.b32 	%r204, %r32, 4;
	ld.param.u32 	%r210, [DIT2C2C_param_1];
	add.s32 	%r67, %r210, %r204;
	setp.gt.s32 	%p74, %r31, 1;
	@%p74 bra 	BB17_100;

	setp.eq.s32 	%p77, %r31, 0;
	@%p77 bra 	BB17_104;

	setp.eq.s32 	%p78, %r31, 1;
	@%p78 bra 	BB17_99;
	bra.uni 	BB17_105;

BB17_99:
	ld.global.v2.f64 	{%fd213, %fd214}, [%r67];
	neg.f64 	%fd176, %fd213;
	mov.f64 	%fd229, %fd214;
	mov.f64 	%fd230, %fd176;
	bra.uni 	BB17_105;

BB17_100:
	setp.eq.s32 	%p75, %r31, 2;
	@%p75 bra 	BB17_103;

	setp.ne.s32 	%p76, %r31, 3;
	@%p76 bra 	BB17_105;

	ld.global.v2.f64 	{%fd217, %fd218}, [%r67];
	neg.f64 	%fd168, %fd218;
	mov.f64 	%fd229, %fd168;
	mov.f64 	%fd230, %fd217;
	bra.uni 	BB17_105;

BB17_103:
	ld.global.v2.f64 	{%fd215, %fd216}, [%r67];
	neg.f64 	%fd171, %fd215;
	neg.f64 	%fd173, %fd216;
	mov.f64 	%fd229, %fd171;
	mov.f64 	%fd230, %fd173;
	bra.uni 	BB17_105;

BB17_104:
	ld.global.v2.f64 	{%fd229, %fd230}, [%r67];

BB17_105:
	ld.param.u32 	%r212, [DIT2C2C_param_4];
	setp.eq.s32 	%p79, %r212, 0;
	@%p79 bra 	BB17_106;
	bra.uni 	BB17_107;

BB17_106:
	neg.f64 	%fd178, %fd230;
	mov.f64 	%fd229, %fd229;
	mov.f64 	%fd230, %fd178;

BB17_107:
	shl.b32 	%r205, %r29, 3;
	ld.param.u32 	%r209, [DIT2C2C_param_0];
	add.s32 	%r206, %r209, %r205;
	shl.b32 	%r207, %r30, 3;
	add.s32 	%r208, %r209, %r207;
	ld.global.f64 	%fd180, [%r208];
	ld.global.f64 	%fd181, [%r206];
	fma.rn.f64 	%fd182, %fd180, %fd229, %fd181;
	ld.global.f64 	%fd184, [%r208+8];
	neg.f64 	%fd185, %fd184;
	fma.rn.f64 	%fd186, %fd185, %fd230, %fd182;
	ld.global.f64 	%fd187, [%r206+8];
	fma.rn.f64 	%fd188, %fd184, %fd229, %fd187;
	fma.rn.f64 	%fd189, %fd180, %fd230, %fd188;
	neg.f64 	%fd190, %fd180;
	fma.rn.f64 	%fd191, %fd190, %fd229, %fd181;
	fma.rn.f64 	%fd192, %fd184, %fd230, %fd191;
	fma.rn.f64 	%fd193, %fd185, %fd229, %fd187;
	fma.rn.f64 	%fd194, %fd190, %fd230, %fd193;
	st.global.f64 	[%r206], %fd186;
	st.global.f64 	[%r206+8], %fd189;
	st.global.f64 	[%r208], %fd192;
	st.global.f64 	[%r208+8], %fd194;
	ret;
}

.entry divide1D(
	.param .u32 .ptr .global .align 16 divide1D_param_0,
	.param .u32 divide1D_param_1
)
{
	.reg .f64 	%fd<13>;
	.reg .s32 	%r<11>;


	ld.param.u32 	%r5, [divide1D_param_0];
	ld.param.u32 	%r6, [divide1D_param_1];
	// inline asm
	mov.u32 	%r1, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r2, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r3, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r4, %tid.x;
	// inline asm
	add.s32 	%r7, %r4, %r1;
	mad.lo.s32 	%r8, %r3, %r2, %r7;
	cvt.rn.f64.s32 	%fd1, %r6;
	shl.b32 	%r9, %r8, 4;
	add.s32 	%r10, %r5, %r9;
	ld.global.v2.f64 	{%fd9, %fd10}, [%r10];
	div.rn.f64 	%fd11, %fd9, %fd1;
	div.rn.f64 	%fd12, %fd10, %fd1;
	st.global.v2.f64 	[%r10], {%fd11, %fd12};
	ret;
}

.entry swap1D(
	.param .u32 .ptr .global .align 8 swap1D_param_0,
	.param .u32 .ptr .global .align 4 swap1D_param_1
)
{
	.reg .f64 	%fd<5>;
	.reg .pred 	%p<2>;
	.reg .s32 	%r<17>;


	ld.param.u32 	%r8, [swap1D_param_1];
	// inline asm
	mov.u32 	%r4, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r6, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r7, %tid.x;
	// inline asm
	add.s32 	%r9, %r7, %r4;
	mad.lo.s32 	%r2, %r6, %r5, %r9;
	shl.b32 	%r10, %r2, 2;
	add.s32 	%r11, %r8, %r10;
	ld.global.u32 	%r3, [%r11];
	setp.lt.s32 	%p1, %r2, %r3;
	@%p1 bra 	BB19_2;

	ret;

BB19_2:
	shl.b32 	%r12, %r3, 4;
	ld.param.u32 	%r16, [swap1D_param_0];
	add.s32 	%r13, %r16, %r12;
	ld.global.f64 	%fd1, [%r13];
	shl.b32 	%r14, %r2, 4;
	add.s32 	%r15, %r16, %r14;
	ld.global.f64 	%fd2, [%r15];
	st.global.f64 	[%r13], %fd2;
	st.global.f64 	[%r15], %fd1;
	ld.global.f64 	%fd3, [%r13+8];
	ld.global.f64 	%fd4, [%r15+8];
	st.global.f64 	[%r13+8], %fd4;
	st.global.f64 	[%r15+8], %fd3;
	ret;
}

.entry reverse2(
	.param .u32 .ptr .global .align 4 reverse2_param_0,
	.param .u32 reverse2_param_1
)
{
	.reg .f32 	%f<293>;
	.reg .pred 	%p<40>;
	.reg .s32 	%r<103>;


	ld.param.u32 	%r36, [reverse2_param_1];
	// inline asm
	mov.u32 	%r32, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r33, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r34, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r35, %tid.x;
	// inline asm
	add.s32 	%r37, %r35, %r32;
	mad.lo.s32 	%r2, %r34, %r33, %r37;
	add.s32 	%r96, %r36, -1;
	setp.gt.s32 	%p7, %r96, -1;
	@%p7 bra 	BB20_2;

	mov.u32 	%r102, 0;
	bra.uni 	BB20_48;

BB20_2:
	mov.f32 	%f1, 0f41300000;
	mov.pred 	%p3, 0;
	mov.f32 	%f46, 0f40000000;
	add.f32 	%f2, %f46, 0f41300000;
	mov.f32 	%f3, 0f7F800000;
	mov.f32 	%f4, 0fFF800000;
	mov.f32 	%f5, 0f00000000;
	mov.f32 	%f6, 0f37000000;
	mov.u32 	%r97, 0;
	mov.u32 	%r102, %r97;

BB20_3:
	and.b32  	%r41, %r96, 31;
	mov.u32 	%r99, 1;
	shl.b32 	%r43, %r99, %r41;
	and.b32  	%r7, %r43, %r2;
	mul.hi.s32 	%r44, %r97, 780903145;
	shr.u32 	%r45, %r44, 31;
	shr.s32 	%r46, %r44, 1;
	add.s32 	%r8, %r46, %r45;
	mul.lo.s32 	%r47, %r8, 11;
	sub.s32 	%r9, %r97, %r47;
	setp.gt.s32 	%p8, %r97, 10;
	@%p8 bra 	BB20_5;

	mov.f32 	%f284, 0f3F800000;
	bra.uni 	BB20_24;

BB20_5:
	mov.u32 	%r98, 0;

BB20_6:
	// inline asm
	abs.f32 	%f48, %f46;
	// inline asm
	selp.f32 	%f283, 0f3F800000, %f2, %p3;
	or.pred  	%p9, %p3, %p3;
	@%p9 bra 	BB20_22;

	mov.f32 	%f54, 0f3F000000;
	mul.rn.f32 	%f51, %f54, %f1;
	// inline asm
	cvt.rmi.f32.f32 	%f50, %f51;
	// inline asm
	mul.rn.f32 	%f56, %f46, %f50;
	sub.f32 	%f57, %f1, %f56;
	setp.eq.f32 	%p10, %f57, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f52, %f1;
	// inline asm
	setp.eq.f32 	%p11, %f1, %f52;
	and.pred  	%p4, %p10, %p11;
	setp.eq.f32 	%p12, %f48, 0f00000000;
	@%p12 bra 	BB20_19;

	// inline asm
	abs.f32 	%f58, %f46;
	// inline asm
	mov.b32 	 %r12, %f58;
	shr.u32 	%r50, %r12, 23;
	and.b32  	%r51, %r50, 255;
	add.s32 	%r100, %r51, -127;
	setp.eq.s32 	%p13, %r51, 0;
	mov.f32 	%f280, %f58;
	@%p13 bra 	BB20_9;
	bra.uni 	BB20_10;

BB20_9:
	and.b32  	%r52, %r12, -2139095041;
	or.b32  	%r53, %r52, 1065353216;
	mov.b32 	 %f60, %r53;
	add.f32 	%f61, %f60, 0fBF800000;
	mov.b32 	 %r54, %f61;
	shr.u32 	%r55, %r54, 23;
	and.b32  	%r56, %r55, 255;
	add.s32 	%r100, %r56, -253;
	and.b32  	%r57, %r54, -2139095041;
	or.b32  	%r58, %r57, 1065353216;
	mov.b32 	 %f280, %r58;

BB20_10:
	mov.b32 	 %r59, %f280;
	and.b32  	%r60, %r59, -2139095041;
	or.b32  	%r61, %r60, 1065353216;
	mov.b32 	 %f281, %r61;
	setp.gt.f32 	%p14, %f281, 0f3FB504F3;
	@%p14 bra 	BB20_11;
	bra.uni 	BB20_12;

BB20_11:
	mul.rn.f32 	%f281, %f281, %f54;
	add.s32 	%r100, %r100, 1;

BB20_12:
	add.f32 	%f71, %f281, 0f3F800000;
	rcp.approx.f32 	%f65, %f71;
	add.f32 	%f64, %f281, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f63, %f64, %f65;
	// inline asm
	mul.rn.f32 	%f73, %f46, %f63;
	mul.rn.f32 	%f74, %f73, %f73;
	mov.f32 	%f75, 0f3B18F0FE;
	mul.rn.f32 	%f76, %f75, %f74;
	add.f32 	%f77, %f76, 0f3C4CAF63;
	mul.rn.f32 	%f78, %f77, %f74;
	add.f32 	%f79, %f78, 0f3DAAAABD;
	mul.rn.f32 	%f80, %f79, %f74;
	mul.rn.f32 	%f68, %f80, %f73;
	mov.b32 	 %r62, %f73;
	and.b32  	%r63, %r62, -4096;
	mov.b32 	 %f81, %r63;
	mov.b32 	 %r64, %f64;
	and.b32  	%r65, %r64, -4096;
	mov.b32 	 %f82, %r65;
	sub.f32 	%f83, %f64, %f81;
	mul.rn.f32 	%f84, %f46, %f83;
	sub.f32 	%f85, %f64, %f82;
	mul.rn.f32 	%f86, %f81, %f82;
	sub.f32 	%f87, %f84, %f86;
	mul.rn.f32 	%f88, %f81, %f85;
	sub.f32 	%f89, %f87, %f88;
	mul.rn.f32 	%f90, %f65, %f89;
	add.f32 	%f91, %f81, %f90;
	sub.f32 	%f92, %f91, %f81;
	sub.f32 	%f93, %f90, %f92;
	add.f32 	%f94, %f91, %f68;
	sub.f32 	%f67, %f91, %f94;
	// inline asm
	add.rz.f32 	%f66, %f67, %f68;
	// inline asm
	add.f32 	%f95, %f66, %f93;
	add.f32 	%f96, %f94, %f95;
	sub.f32 	%f97, %f94, %f96;
	add.f32 	%f98, %f97, %f95;
	cvt.rn.f32.s32 	%f99, %r100;
	mov.f32 	%f100, 0f3F317200;
	mul.rn.f32 	%f101, %f99, %f100;
	mov.f32 	%f102, 0f35BFBE8E;
	mul.rn.f32 	%f103, %f99, %f102;
	add.f32 	%f104, %f101, %f96;
	sub.f32 	%f105, %f101, %f104;
	add.f32 	%f106, %f105, %f96;
	add.f32 	%f107, %f106, %f98;
	add.f32 	%f108, %f107, %f103;
	add.f32 	%f15, %f104, %f108;
	sub.f32 	%f109, %f104, %f15;
	add.f32 	%f16, %f109, %f108;
	// inline asm
	abs.f32 	%f69, %f1;
	// inline asm
	setp.gt.f32 	%p15, %f69, 0f77F684DF;
	@%p15 bra 	BB20_14;

	mov.f32 	%f282, %f1;
	bra.uni 	BB20_15;

BB20_14:
	mov.f32 	%f110, 0f39000000;
	mul.rn.f32 	%f17, %f1, %f110;
	mov.f32 	%f282, %f17;

BB20_15:
	mov.f32 	%f18, %f282;
	mov.f32 	%f111, 0f45800800;
	mul.rn.f32 	%f112, %f15, %f111;
	sub.f32 	%f113, %f15, %f112;
	add.f32 	%f114, %f113, %f112;
	sub.f32 	%f115, %f15, %f114;
	mul.rn.f32 	%f116, %f18, %f111;
	sub.f32 	%f117, %f18, %f116;
	add.f32 	%f118, %f117, %f116;
	sub.f32 	%f119, %f18, %f118;
	mul.rn.f32 	%f120, %f114, %f118;
	mul.rn.f32 	%f121, %f15, %f18;
	sub.f32 	%f122, %f120, %f121;
	mul.rn.f32 	%f123, %f114, %f119;
	add.f32 	%f124, %f122, %f123;
	mul.rn.f32 	%f125, %f115, %f118;
	add.f32 	%f126, %f124, %f125;
	mul.rn.f32 	%f127, %f115, %f119;
	add.f32 	%f128, %f126, %f127;
	mul.rn.f32 	%f129, %f16, %f18;
	add.f32 	%f130, %f129, %f128;
	add.f32 	%f131, %f121, %f130;
	sub.f32 	%f132, %f121, %f131;
	add.f32 	%f19, %f132, %f130;
	mov.f32 	%f291, %f19;
	mov.f32 	%f292, %f131;
	mov.b32 	 %r18, %f131;
	setp.eq.s32 	%p16, %r18, 1118925336;
	@%p16 bra 	BB20_16;
	bra.uni 	BB20_17;

BB20_16:
	add.s32 	%r66, %r18, -1;
	mov.b32 	 %f133, %r66;
	add.f32 	%f134, %f19, %f6;
	mov.f32 	%f291, %f134;
	mov.f32 	%f292, %f133;

BB20_17:
	mov.f32 	%f142, 0f3FB8AA3B;
	mul.rn.f32 	%f136, %f292, %f142;
	// inline asm
	cvt.rzi.f32.f32 	%f135, %f136;
	// inline asm
	mul.rn.f32 	%f144, %f135, %f100;
	sub.f32 	%f145, %f292, %f144;
	mul.rn.f32 	%f147, %f135, %f102;
	sub.f32 	%f148, %f145, %f147;
	mul.rn.f32 	%f138, %f148, %f142;
	// inline asm
	ex2.approx.f32 	%f137, %f138;
	// inline asm
	add.f32 	%f140, %f135, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f139, %f140;
	// inline asm
	mul.rn.f32 	%f149, %f137, %f139;
	setp.lt.f32 	%p17, %f292, 0fC2D20000;
	selp.f32 	%f150, 0f00000000, %f149, %p17;
	setp.gt.f32 	%p18, %f292, 0f42D20000;
	selp.f32 	%f283, %f3, %f150, %p18;
	setp.neu.f32 	%p19, %f283, %f3;
	@%p19 bra 	BB20_18;
	bra.uni 	BB20_22;

BB20_18:
	// inline asm
	mad.f32 	%f151, %f283, %f291, %f283;
	// inline asm
	mov.f32 	%f283, %f151;
	bra.uni 	BB20_22;

BB20_19:
	@%p3 bra 	BB20_21;

	selp.f32 	%f283, %f5, 0f00000000, %p4;
	bra.uni 	BB20_22;

BB20_21:
	mov.f32 	%f283, 0f7F800000;

BB20_22:
	cvt.rn.f32.s32 	%f156, %r99;
	mul.f32 	%f157, %f156, %f283;
	cvt.rzi.s32.f32 	%r99, %f157;
	add.s32 	%r98, %r98, 1;
	setp.lt.s32 	%p20, %r98, %r8;
	@%p20 bra 	BB20_6;

	cvt.rn.f32.s32 	%f284, %r99;

BB20_24:
	// inline asm
	abs.f32 	%f158, %f46;
	// inline asm
	cvt.rn.f32.s32 	%f287, %r9;
	setp.eq.f32 	%p21, %f287, 0f00000000;
	@%p21 bra 	BB20_46;

	setp.nan.f32 	%p22, %f287, %f287;
	@%p22 bra 	BB20_45;

	setp.eq.f32 	%p23, %f287, %f3;
	setp.eq.f32 	%p24, %f287, %f4;
	or.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB20_42;

	mov.f32 	%f164, 0f3F000000;
	mul.rn.f32 	%f161, %f164, %f287;
	// inline asm
	cvt.rmi.f32.f32 	%f160, %f161;
	// inline asm
	mul.rn.f32 	%f166, %f46, %f160;
	sub.f32 	%f167, %f287, %f166;
	setp.eq.f32 	%p26, %f167, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f162, %f287;
	// inline asm
	setp.eq.f32 	%p27, %f287, %f162;
	and.pred  	%p5, %p26, %p27;
	setp.eq.f32 	%p28, %f158, 0f00000000;
	@%p28 bra 	BB20_39;

	// inline asm
	abs.f32 	%f168, %f46;
	// inline asm
	mov.b32 	 %r21, %f168;
	shr.u32 	%r67, %r21, 23;
	and.b32  	%r68, %r67, 255;
	add.s32 	%r101, %r68, -127;
	setp.eq.s32 	%p29, %r68, 0;
	mov.f32 	%f285, %f168;
	@%p29 bra 	BB20_29;
	bra.uni 	BB20_30;

BB20_29:
	and.b32  	%r69, %r21, -2139095041;
	or.b32  	%r70, %r69, 1065353216;
	mov.b32 	 %f170, %r70;
	add.f32 	%f171, %f170, 0fBF800000;
	mov.b32 	 %r71, %f171;
	shr.u32 	%r72, %r71, 23;
	and.b32  	%r73, %r72, 255;
	add.s32 	%r101, %r73, -253;
	and.b32  	%r74, %r71, -2139095041;
	or.b32  	%r75, %r74, 1065353216;
	mov.b32 	 %f285, %r75;

BB20_30:
	mov.b32 	 %r76, %f285;
	and.b32  	%r77, %r76, -2139095041;
	or.b32  	%r78, %r77, 1065353216;
	mov.b32 	 %f286, %r78;
	setp.gt.f32 	%p30, %f286, 0f3FB504F3;
	@%p30 bra 	BB20_31;
	bra.uni 	BB20_32;

BB20_31:
	mul.rn.f32 	%f286, %f286, %f164;
	add.s32 	%r101, %r101, 1;

BB20_32:
	add.f32 	%f181, %f286, 0f3F800000;
	rcp.approx.f32 	%f175, %f181;
	add.f32 	%f174, %f286, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f173, %f174, %f175;
	// inline asm
	mul.rn.f32 	%f183, %f46, %f173;
	mul.rn.f32 	%f184, %f183, %f183;
	mov.f32 	%f185, 0f3B18F0FE;
	mul.rn.f32 	%f186, %f185, %f184;
	add.f32 	%f187, %f186, 0f3C4CAF63;
	mul.rn.f32 	%f188, %f187, %f184;
	add.f32 	%f189, %f188, 0f3DAAAABD;
	mul.rn.f32 	%f190, %f189, %f184;
	mul.rn.f32 	%f178, %f190, %f183;
	mov.b32 	 %r79, %f183;
	and.b32  	%r80, %r79, -4096;
	mov.b32 	 %f191, %r80;
	mov.b32 	 %r81, %f174;
	and.b32  	%r82, %r81, -4096;
	mov.b32 	 %f192, %r82;
	sub.f32 	%f193, %f174, %f191;
	mul.rn.f32 	%f194, %f46, %f193;
	sub.f32 	%f195, %f174, %f192;
	mul.rn.f32 	%f196, %f191, %f192;
	sub.f32 	%f197, %f194, %f196;
	mul.rn.f32 	%f198, %f191, %f195;
	sub.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f200, %f175, %f199;
	add.f32 	%f201, %f191, %f200;
	sub.f32 	%f202, %f201, %f191;
	sub.f32 	%f203, %f200, %f202;
	add.f32 	%f204, %f201, %f178;
	sub.f32 	%f177, %f201, %f204;
	// inline asm
	add.rz.f32 	%f176, %f177, %f178;
	// inline asm
	add.f32 	%f205, %f176, %f203;
	add.f32 	%f206, %f204, %f205;
	sub.f32 	%f207, %f204, %f206;
	add.f32 	%f208, %f207, %f205;
	cvt.rn.f32.s32 	%f209, %r101;
	mov.f32 	%f210, 0f3F317200;
	mul.rn.f32 	%f211, %f209, %f210;
	mov.f32 	%f212, 0f35BFBE8E;
	mul.rn.f32 	%f213, %f209, %f212;
	add.f32 	%f214, %f211, %f206;
	sub.f32 	%f215, %f211, %f214;
	add.f32 	%f216, %f215, %f206;
	add.f32 	%f217, %f216, %f208;
	add.f32 	%f218, %f217, %f213;
	add.f32 	%f34, %f214, %f218;
	sub.f32 	%f219, %f214, %f34;
	add.f32 	%f35, %f219, %f218;
	// inline asm
	abs.f32 	%f179, %f287;
	// inline asm
	setp.gt.f32 	%p31, %f179, 0f77F684DF;
	@%p31 bra 	BB20_33;
	bra.uni 	BB20_34;

BB20_33:
	mov.f32 	%f220, 0f39000000;
	mul.rn.f32 	%f287, %f287, %f220;

BB20_34:
	mov.f32 	%f221, 0f45800800;
	mul.rn.f32 	%f222, %f34, %f221;
	sub.f32 	%f223, %f34, %f222;
	add.f32 	%f224, %f223, %f222;
	sub.f32 	%f225, %f34, %f224;
	mul.rn.f32 	%f226, %f287, %f221;
	sub.f32 	%f227, %f287, %f226;
	add.f32 	%f228, %f227, %f226;
	sub.f32 	%f229, %f287, %f228;
	mul.rn.f32 	%f230, %f224, %f228;
	mul.rn.f32 	%f231, %f34, %f287;
	sub.f32 	%f232, %f230, %f231;
	mul.rn.f32 	%f233, %f224, %f229;
	add.f32 	%f234, %f232, %f233;
	mul.rn.f32 	%f235, %f225, %f228;
	add.f32 	%f236, %f234, %f235;
	mul.rn.f32 	%f237, %f225, %f229;
	add.f32 	%f238, %f236, %f237;
	mul.rn.f32 	%f239, %f35, %f287;
	add.f32 	%f240, %f239, %f238;
	add.f32 	%f241, %f231, %f240;
	sub.f32 	%f242, %f231, %f241;
	add.f32 	%f38, %f242, %f240;
	mov.f32 	%f289, %f38;
	mov.f32 	%f290, %f241;
	mov.b32 	 %r27, %f241;
	setp.eq.s32 	%p32, %r27, 1118925336;
	@%p32 bra 	BB20_35;
	bra.uni 	BB20_36;

BB20_35:
	add.s32 	%r83, %r27, -1;
	mov.b32 	 %f243, %r83;
	add.f32 	%f244, %f38, %f6;
	mov.f32 	%f289, %f244;
	mov.f32 	%f290, %f243;

BB20_36:
	mov.f32 	%f252, 0f3FB8AA3B;
	mul.rn.f32 	%f246, %f290, %f252;
	// inline asm
	cvt.rzi.f32.f32 	%f245, %f246;
	// inline asm
	mul.rn.f32 	%f254, %f245, %f210;
	sub.f32 	%f255, %f290, %f254;
	mul.rn.f32 	%f257, %f245, %f212;
	sub.f32 	%f258, %f255, %f257;
	mul.rn.f32 	%f248, %f258, %f252;
	// inline asm
	ex2.approx.f32 	%f247, %f248;
	// inline asm
	add.f32 	%f250, %f245, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f249, %f250;
	// inline asm
	mul.rn.f32 	%f259, %f247, %f249;
	setp.lt.f32 	%p33, %f290, 0fC2D20000;
	selp.f32 	%f260, 0f00000000, %f259, %p33;
	setp.gt.f32 	%p34, %f290, 0f42D20000;
	selp.f32 	%f39, %f3, %f260, %p34;
	setp.neu.f32 	%p35, %f39, %f3;
	@%p35 bra 	BB20_38;

	mov.f32 	%f288, %f39;
	bra.uni 	BB20_47;

BB20_38:
	// inline asm
	mad.f32 	%f261, %f39, %f289, %f39;
	// inline asm
	mov.f32 	%f40, %f261;
	mov.f32 	%f288, %f40;
	bra.uni 	BB20_47;

BB20_39:
	setp.lt.f32 	%p36, %f287, 0f00000000;
	@%p36 bra 	BB20_41;

	selp.f32 	%f41, %f5, 0f00000000, %p5;
	mov.f32 	%f288, %f41;
	bra.uni 	BB20_47;

BB20_41:
	mov.f32 	%f288, %f3;
	bra.uni 	BB20_47;

BB20_42:
	setp.lt.f32 	%p37, %f158, 0f3F800000;
	mov.b32 	 %r84, %f287;
	setp.lt.s32 	%p6, %r84, 0;
	@%p37 bra 	BB20_44;

	selp.f32 	%f42, 0f00000000, %f3, %p6;
	mov.f32 	%f288, %f42;
	bra.uni 	BB20_47;

BB20_44:
	selp.f32 	%f43, %f3, 0f00000000, %p6;
	mov.f32 	%f288, %f43;
	bra.uni 	BB20_47;

BB20_45:
	add.f32 	%f44, %f287, 0f40000000;
	mov.f32 	%f288, %f44;
	bra.uni 	BB20_47;

BB20_46:
	mov.f32 	%f266, 0f3F800000;
	mov.f32 	%f288, %f266;

BB20_47:
	mov.f32 	%f45, %f288;
	mul.f32 	%f267, %f284, %f45;
	cvt.rzi.s32.f32 	%r85, %f267;
	setp.eq.s32 	%p38, %r7, 0;
	selp.b32 	%r86, 0, %r85, %p38;
	add.s32 	%r102, %r86, %r102;
	add.s32 	%r97, %r97, 1;
	add.s32 	%r96, %r96, -1;
	setp.gt.s32 	%p39, %r96, -1;
	@%p39 bra 	BB20_3;

BB20_48:
	shl.b32 	%r89, %r2, 2;
	ld.param.u32 	%r95, [reverse2_param_0];
	add.s32 	%r90, %r95, %r89;
	st.global.u32 	[%r90], %r102;
	// inline asm
	mov.u32 	%r87, %envreg6;
	// inline asm
	// inline asm
	mov.u32 	%r88, %ntid.x;
	// inline asm
	mad.lo.s32 	%r91, %r88, %r87, %r2;
	shl.b32 	%r92, %r91, 2;
	add.s32 	%r93, %r95, %r92;
	add.s32 	%r94, %r102, 1;
	st.global.u32 	[%r93], %r94;
	ret;
}

.entry reversen(
	.param .u32 .ptr .global .align 4 reversen_param_0,
	.param .u32 .ptr .shared .align 4 reversen_param_1,
	.param .u32 reversen_param_2,
	.param .u32 reversen_param_3
)
{
	.reg .f32 	%f<313>;
	.reg .pred 	%p<64>;
	.reg .s32 	%r<126>;


	ld.param.u32 	%r3, [reversen_param_2];
	// inline asm
	mov.u32 	%r45, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r46, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r47, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r48, %tid.x;
	// inline asm
	add.s32 	%r50, %r48, %r45;
	mad.lo.s32 	%r5, %r47, %r46, %r50;
	// inline asm
	mov.u32 	%r49, %tid.x;
	// inline asm
	mul.lo.s32 	%r7, %r49, %r3;
	mad.lo.s32 	%r8, %r49, %r3, %r3;
	setp.ge.s32 	%p16, %r7, %r8;
	@%p16 bra 	BB21_3;

	ld.param.u32 	%r113, [reversen_param_2];
	mul.lo.s32 	%r51, %r49, %r113;
	shl.b32 	%r52, %r51, 2;
	ld.param.u32 	%r112, [reversen_param_1];
	add.s32 	%r116, %r112, %r52;
	mov.u32 	%r117, %r7;
	mov.u32 	%r118, %r5;

BB21_2:
	mov.u32 	%r12, %r118;
	mov.u32 	%r11, %r117;
	ld.param.u32 	%r115, [reversen_param_3];
	rem.s32 	%r53, %r12, %r115;
	st.shared.u32 	[%r116], %r53;
	div.s32 	%r13, %r12, %r115;
	add.s32 	%r116, %r116, 4;
	add.s32 	%r15, %r11, 1;
	setp.lt.s32 	%p17, %r15, %r8;
	mov.u32 	%r117, %r15;
	mov.u32 	%r118, %r13;
	@%p17 bra 	BB21_2;

BB21_3:
	add.s32 	%r119, %r8, -1;
	setp.lt.s32 	%p18, %r119, %r7;
	@%p18 bra 	BB21_70;

	ld.param.u32 	%r114, [reversen_param_3];
	cvt.rn.f32.s32 	%f1, %r114;
	setp.eq.f32 	%p1, %f1, 0f3F800000;
	mov.f32 	%f2, 0f40A00000;
	mov.pred 	%p3, 0;
	setp.nan.f32 	%p2, %f1, %f1;
	add.f32 	%f3, %f1, 0f40A00000;
	mov.b32 	 %r56, %f1;
	and.b32  	%r57, %r56, -2147483648;
	or.b32  	%r58, %r57, 2139095040;
	mov.b32 	 %f4, %r58;
	mov.b32 	 %f5, %r57;
	mov.f32 	%f8, 0f7F800000;
	setp.eq.f32 	%p19, %f1, 0f7F800000;
	mov.f32 	%f7, 0fFF800000;
	setp.eq.f32 	%p20, %f1, 0fFF800000;
	or.pred  	%p4, %p19, %p20;
	setp.lt.s32 	%p5, %r56, 0;
	mov.u32 	%r120, 0;
	setp.lt.f32 	%p6, %f1, 0f00000000;
	mov.f32 	%f9, 0f7FFFFFFF;
	mov.f32 	%f10, 0f37000000;
	setp.geu.f32 	%p7, %f1, 0f00000000;
	mov.u32 	%r125, %r120;

BB21_5:
	shl.b32 	%r59, %r119, 2;
	ld.param.u32 	%r111, [reversen_param_1];
	add.s32 	%r60, %r111, %r59;
	ld.shared.u32 	%r20, [%r60];
	mul.hi.s32 	%r61, %r120, 1717986919;
	shr.u32 	%r62, %r61, 31;
	shr.s32 	%r63, %r61, 1;
	add.s32 	%r21, %r63, %r62;
	mul.lo.s32 	%r64, %r21, 5;
	sub.s32 	%r22, %r120, %r64;
	setp.gt.s32 	%p21, %r120, 4;
	@%p21 bra 	BB21_7;

	mov.f32 	%f301, 0f3F800000;
	bra.uni 	BB21_35;

BB21_7:
	mov.u32 	%r122, 1;
	mov.u32 	%r121, 0;

BB21_8:
	// inline asm
	abs.f32 	%f60, %f1;
	// inline asm
	selp.f32 	%f12, 0f3F800000, %f3, %p1;
	or.pred  	%p22, %p1, %p2;
	@%p22 bra 	BB21_32;

	mov.f32 	%f66, 0f3F000000;
	mul.rn.f32 	%f63, %f66, %f2;
	// inline asm
	cvt.rmi.f32.f32 	%f62, %f63;
	// inline asm
	mov.f32 	%f67, 0f40000000;
	mul.rn.f32 	%f68, %f67, %f62;
	sub.f32 	%f69, %f2, %f68;
	setp.eq.f32 	%p8, %f69, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f64, %f2;
	// inline asm
	setp.eq.f32 	%p9, %f2, %f64;
	and.pred  	%p10, %p8, %p9;
	setp.eq.f32 	%p23, %f60, 0f00000000;
	@%p23 bra 	BB21_29;

	@%p4 bra 	BB21_26;

	@!%p6 bra 	BB21_13;

	// inline asm
	cvt.rzi.f32.f32 	%f70, %f2;
	// inline asm
	setp.neu.f32 	%p24, %f2, %f70;
	@%p24 bra 	BB21_25;

BB21_13:
	// inline asm
	abs.f32 	%f72, %f1;
	// inline asm
	mov.b32 	 %r25, %f72;
	shr.u32 	%r67, %r25, 23;
	and.b32  	%r68, %r67, 255;
	add.s32 	%r123, %r68, -127;
	setp.eq.s32 	%p25, %r68, 0;
	mov.f32 	%f297, %f72;
	@%p25 bra 	BB21_14;
	bra.uni 	BB21_15;

BB21_14:
	and.b32  	%r69, %r25, -2139095041;
	or.b32  	%r70, %r69, 1065353216;
	mov.b32 	 %f74, %r70;
	add.f32 	%f75, %f74, 0fBF800000;
	mov.b32 	 %r71, %f75;
	shr.u32 	%r72, %r71, 23;
	and.b32  	%r73, %r72, 255;
	add.s32 	%r123, %r73, -253;
	and.b32  	%r74, %r71, -2139095041;
	or.b32  	%r75, %r74, 1065353216;
	mov.b32 	 %f297, %r75;

BB21_15:
	mov.b32 	 %r76, %f297;
	and.b32  	%r77, %r76, -2139095041;
	or.b32  	%r78, %r77, 1065353216;
	mov.b32 	 %f298, %r78;
	setp.gt.f32 	%p26, %f298, 0f3FB504F3;
	@%p26 bra 	BB21_16;
	bra.uni 	BB21_17;

BB21_16:
	mul.rn.f32 	%f298, %f298, %f66;
	add.s32 	%r123, %r123, 1;

BB21_17:
	add.f32 	%f85, %f298, 0f3F800000;
	rcp.approx.f32 	%f79, %f85;
	add.f32 	%f78, %f298, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f77, %f78, %f79;
	// inline asm
	mul.rn.f32 	%f87, %f67, %f77;
	mul.rn.f32 	%f88, %f87, %f87;
	mov.f32 	%f89, 0f3B18F0FE;
	mul.rn.f32 	%f90, %f89, %f88;
	add.f32 	%f91, %f90, 0f3C4CAF63;
	mul.rn.f32 	%f92, %f91, %f88;
	add.f32 	%f93, %f92, 0f3DAAAABD;
	mul.rn.f32 	%f94, %f93, %f88;
	mul.rn.f32 	%f82, %f94, %f87;
	mov.b32 	 %r79, %f87;
	and.b32  	%r80, %r79, -4096;
	mov.b32 	 %f95, %r80;
	mov.b32 	 %r81, %f78;
	and.b32  	%r82, %r81, -4096;
	mov.b32 	 %f96, %r82;
	sub.f32 	%f97, %f78, %f95;
	mul.rn.f32 	%f98, %f67, %f97;
	sub.f32 	%f99, %f78, %f96;
	mul.rn.f32 	%f100, %f95, %f96;
	sub.f32 	%f101, %f98, %f100;
	mul.rn.f32 	%f102, %f95, %f99;
	sub.f32 	%f103, %f101, %f102;
	mul.rn.f32 	%f104, %f79, %f103;
	add.f32 	%f105, %f95, %f104;
	sub.f32 	%f106, %f105, %f95;
	sub.f32 	%f107, %f104, %f106;
	add.f32 	%f108, %f105, %f82;
	sub.f32 	%f81, %f105, %f108;
	// inline asm
	add.rz.f32 	%f80, %f81, %f82;
	// inline asm
	add.f32 	%f109, %f80, %f107;
	add.f32 	%f110, %f108, %f109;
	sub.f32 	%f111, %f108, %f110;
	add.f32 	%f112, %f111, %f109;
	cvt.rn.f32.s32 	%f113, %r123;
	mov.f32 	%f114, 0f3F317200;
	mul.rn.f32 	%f115, %f113, %f114;
	mov.f32 	%f116, 0f35BFBE8E;
	mul.rn.f32 	%f117, %f113, %f116;
	add.f32 	%f118, %f115, %f110;
	sub.f32 	%f119, %f115, %f118;
	add.f32 	%f120, %f119, %f110;
	add.f32 	%f121, %f120, %f112;
	add.f32 	%f122, %f121, %f117;
	add.f32 	%f19, %f118, %f122;
	sub.f32 	%f123, %f118, %f19;
	add.f32 	%f20, %f123, %f122;
	// inline asm
	abs.f32 	%f83, %f2;
	// inline asm
	setp.gt.f32 	%p27, %f83, 0f77F684DF;
	@%p27 bra 	BB21_19;

	mov.f32 	%f299, %f2;
	bra.uni 	BB21_20;

BB21_19:
	mov.f32 	%f124, 0f39000000;
	mul.rn.f32 	%f21, %f2, %f124;
	mov.f32 	%f299, %f21;

BB21_20:
	mov.f32 	%f22, %f299;
	mov.f32 	%f125, 0f45800800;
	mul.rn.f32 	%f126, %f19, %f125;
	sub.f32 	%f127, %f19, %f126;
	add.f32 	%f128, %f127, %f126;
	sub.f32 	%f129, %f19, %f128;
	mul.rn.f32 	%f130, %f22, %f125;
	sub.f32 	%f131, %f22, %f130;
	add.f32 	%f132, %f131, %f130;
	sub.f32 	%f133, %f22, %f132;
	mul.rn.f32 	%f134, %f128, %f132;
	mul.rn.f32 	%f135, %f19, %f22;
	sub.f32 	%f136, %f134, %f135;
	mul.rn.f32 	%f137, %f128, %f133;
	add.f32 	%f138, %f136, %f137;
	mul.rn.f32 	%f139, %f129, %f132;
	add.f32 	%f140, %f138, %f139;
	mul.rn.f32 	%f141, %f129, %f133;
	add.f32 	%f142, %f140, %f141;
	mul.rn.f32 	%f143, %f20, %f22;
	add.f32 	%f144, %f143, %f142;
	add.f32 	%f145, %f135, %f144;
	sub.f32 	%f146, %f135, %f145;
	add.f32 	%f23, %f146, %f144;
	mov.f32 	%f311, %f23;
	mov.f32 	%f312, %f145;
	mov.b32 	 %r31, %f145;
	setp.eq.s32 	%p28, %r31, 1118925336;
	@%p28 bra 	BB21_21;
	bra.uni 	BB21_22;

BB21_21:
	add.s32 	%r83, %r31, -1;
	mov.b32 	 %f147, %r83;
	add.f32 	%f148, %f23, %f10;
	mov.f32 	%f311, %f148;
	mov.f32 	%f312, %f147;

BB21_22:
	mov.f32 	%f156, 0f3FB8AA3B;
	mul.rn.f32 	%f150, %f312, %f156;
	// inline asm
	cvt.rzi.f32.f32 	%f149, %f150;
	// inline asm
	mul.rn.f32 	%f158, %f149, %f114;
	sub.f32 	%f159, %f312, %f158;
	mul.rn.f32 	%f161, %f149, %f116;
	sub.f32 	%f162, %f159, %f161;
	mul.rn.f32 	%f152, %f162, %f156;
	// inline asm
	ex2.approx.f32 	%f151, %f152;
	// inline asm
	add.f32 	%f154, %f149, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f153, %f154;
	// inline asm
	mul.rn.f32 	%f163, %f151, %f153;
	setp.lt.f32 	%p29, %f312, 0fC2D20000;
	selp.f32 	%f164, 0f00000000, %f163, %p29;
	setp.gt.f32 	%p30, %f312, 0f42D20000;
	selp.f32 	%f300, %f8, %f164, %p30;
	setp.neu.f32 	%p31, %f300, %f8;
	@%p31 bra 	BB21_23;
	bra.uni 	BB21_24;

BB21_23:
	// inline asm
	mad.f32 	%f165, %f300, %f311, %f300;
	// inline asm
	mov.f32 	%f300, %f165;

BB21_24:
	not.pred 	%p33, %p10;
	or.pred  	%p34, %p7, %p33;
	mov.b32 	 %r84, %f300;
	xor.b32  	%r85, %r84, -2147483648;
	mov.b32 	 %f169, %r85;
	selp.f32 	%f27, %f300, %f169, %p34;
	mov.f32 	%f308, %f27;
	bra.uni 	BB21_33;

BB21_25:
	mov.f32 	%f306, %f9;
	mov.f32 	%f308, %f306;
	bra.uni 	BB21_33;

BB21_26:
	not.pred 	%p35, %p5;
	or.pred  	%p36, %p3, %p35;
	@%p36 bra 	BB21_28;

	selp.f32 	%f28, %f7, %f8, %p10;
	mov.f32 	%f308, %f28;
	bra.uni 	BB21_33;

BB21_28:
	mov.f32 	%f308, %f8;
	bra.uni 	BB21_33;

BB21_29:
	@%p3 bra 	BB21_31;

	selp.f32 	%f29, %f5, 0f00000000, %p10;
	mov.f32 	%f308, %f29;
	bra.uni 	BB21_33;

BB21_31:
	mov.f32 	%f308, %f170;
	bra.uni 	BB21_33;

BB21_32:
	mov.f32 	%f308, %f12;

BB21_33:
	mov.f32 	%f30, %f308;
	cvt.rn.f32.s32 	%f171, %r122;
	mul.f32 	%f172, %f171, %f30;
	cvt.rzi.s32.f32 	%r122, %f172;
	add.s32 	%r121, %r121, 1;
	setp.lt.s32 	%p37, %r121, %r21;
	@%p37 bra 	BB21_8;

	cvt.rn.f32.s32 	%f301, %r122;

BB21_35:
	// inline asm
	abs.f32 	%f173, %f1;
	// inline asm
	cvt.rn.f32.s32 	%f304, %r22;
	setp.eq.f32 	%p38, %f304, 0f00000000;
	or.pred  	%p40, %p1, %p38;
	@%p40 bra 	BB21_68;

	setp.neu.f32 	%p41, %f1, 0fBF800000;
	@%p41 bra 	BB21_38;

	setp.eq.f32 	%p42, %f304, %f8;
	setp.eq.f32 	%p43, %f304, 0fFF800000;
	or.pred  	%p44, %p42, %p43;
	@%p44 bra 	BB21_68;

BB21_38:
	setp.nan.f32 	%p45, %f1, %f304;
	@%p45 bra 	BB21_67;

	setp.eq.f32 	%p46, %f304, %f8;
	setp.eq.f32 	%p47, %f304, 0fFF800000;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	BB21_64;

	mov.f32 	%f179, 0f3F000000;
	mul.rn.f32 	%f176, %f179, %f304;
	// inline asm
	cvt.rmi.f32.f32 	%f175, %f176;
	// inline asm
	mov.f32 	%f180, 0f40000000;
	mul.rn.f32 	%f181, %f180, %f175;
	sub.f32 	%f182, %f304, %f181;
	setp.eq.f32 	%p11, %f182, 0f3F800000;
	// inline asm
	cvt.rzi.f32.f32 	%f177, %f304;
	// inline asm
	setp.eq.f32 	%p12, %f304, %f177;
	and.pred  	%p13, %p11, %p12;
	setp.eq.f32 	%p49, %f173, 0f00000000;
	@%p49 bra 	BB21_61;

	@%p4 bra 	BB21_56;

	@!%p6 bra 	BB21_44;

	// inline asm
	cvt.rzi.f32.f32 	%f183, %f304;
	// inline asm
	setp.neu.f32 	%p50, %f304, %f183;
	@%p50 bra 	BB21_55;

BB21_44:
	// inline asm
	abs.f32 	%f185, %f1;
	// inline asm
	mov.b32 	 %r34, %f185;
	shr.u32 	%r86, %r34, 23;
	and.b32  	%r87, %r86, 255;
	add.s32 	%r124, %r87, -127;
	setp.eq.s32 	%p51, %r87, 0;
	mov.f32 	%f302, %f185;
	@%p51 bra 	BB21_45;
	bra.uni 	BB21_46;

BB21_45:
	and.b32  	%r88, %r34, -2139095041;
	or.b32  	%r89, %r88, 1065353216;
	mov.b32 	 %f187, %r89;
	add.f32 	%f188, %f187, 0fBF800000;
	mov.b32 	 %r90, %f188;
	shr.u32 	%r91, %r90, 23;
	and.b32  	%r92, %r91, 255;
	add.s32 	%r124, %r92, -253;
	and.b32  	%r93, %r90, -2139095041;
	or.b32  	%r94, %r93, 1065353216;
	mov.b32 	 %f302, %r94;

BB21_46:
	mov.b32 	 %r95, %f302;
	and.b32  	%r96, %r95, -2139095041;
	or.b32  	%r97, %r96, 1065353216;
	mov.b32 	 %f303, %r97;
	setp.gt.f32 	%p52, %f303, 0f3FB504F3;
	@%p52 bra 	BB21_47;
	bra.uni 	BB21_48;

BB21_47:
	mul.rn.f32 	%f303, %f303, %f179;
	add.s32 	%r124, %r124, 1;

BB21_48:
	add.f32 	%f198, %f303, 0f3F800000;
	rcp.approx.f32 	%f192, %f198;
	add.f32 	%f191, %f303, 0fBF800000;
	// inline asm
	mul.rz.f32 	%f190, %f191, %f192;
	// inline asm
	mul.rn.f32 	%f200, %f180, %f190;
	mul.rn.f32 	%f201, %f200, %f200;
	mov.f32 	%f202, 0f3B18F0FE;
	mul.rn.f32 	%f203, %f202, %f201;
	add.f32 	%f204, %f203, 0f3C4CAF63;
	mul.rn.f32 	%f205, %f204, %f201;
	add.f32 	%f206, %f205, 0f3DAAAABD;
	mul.rn.f32 	%f207, %f206, %f201;
	mul.rn.f32 	%f195, %f207, %f200;
	mov.b32 	 %r98, %f200;
	and.b32  	%r99, %r98, -4096;
	mov.b32 	 %f208, %r99;
	mov.b32 	 %r100, %f191;
	and.b32  	%r101, %r100, -4096;
	mov.b32 	 %f209, %r101;
	sub.f32 	%f210, %f191, %f208;
	mul.rn.f32 	%f211, %f180, %f210;
	sub.f32 	%f212, %f191, %f209;
	mul.rn.f32 	%f213, %f208, %f209;
	sub.f32 	%f214, %f211, %f213;
	mul.rn.f32 	%f215, %f208, %f212;
	sub.f32 	%f216, %f214, %f215;
	mul.rn.f32 	%f217, %f192, %f216;
	add.f32 	%f218, %f208, %f217;
	sub.f32 	%f219, %f218, %f208;
	sub.f32 	%f220, %f217, %f219;
	add.f32 	%f221, %f218, %f195;
	sub.f32 	%f194, %f218, %f221;
	// inline asm
	add.rz.f32 	%f193, %f194, %f195;
	// inline asm
	add.f32 	%f222, %f193, %f220;
	add.f32 	%f223, %f221, %f222;
	sub.f32 	%f224, %f221, %f223;
	add.f32 	%f225, %f224, %f222;
	cvt.rn.f32.s32 	%f226, %r124;
	mov.f32 	%f227, 0f3F317200;
	mul.rn.f32 	%f228, %f226, %f227;
	mov.f32 	%f229, 0f35BFBE8E;
	mul.rn.f32 	%f230, %f226, %f229;
	add.f32 	%f231, %f228, %f223;
	sub.f32 	%f232, %f228, %f231;
	add.f32 	%f233, %f232, %f223;
	add.f32 	%f234, %f233, %f225;
	add.f32 	%f235, %f234, %f230;
	add.f32 	%f41, %f231, %f235;
	sub.f32 	%f236, %f231, %f41;
	add.f32 	%f42, %f236, %f235;
	// inline asm
	abs.f32 	%f196, %f304;
	// inline asm
	setp.gt.f32 	%p53, %f196, 0f77F684DF;
	@%p53 bra 	BB21_49;
	bra.uni 	BB21_50;

BB21_49:
	mov.f32 	%f237, 0f39000000;
	mul.rn.f32 	%f304, %f304, %f237;

BB21_50:
	mov.f32 	%f238, 0f45800800;
	mul.rn.f32 	%f239, %f41, %f238;
	sub.f32 	%f240, %f41, %f239;
	add.f32 	%f241, %f240, %f239;
	sub.f32 	%f242, %f41, %f241;
	mul.rn.f32 	%f243, %f304, %f238;
	sub.f32 	%f244, %f304, %f243;
	add.f32 	%f245, %f244, %f243;
	sub.f32 	%f246, %f304, %f245;
	mul.rn.f32 	%f247, %f241, %f245;
	mul.rn.f32 	%f248, %f41, %f304;
	sub.f32 	%f249, %f247, %f248;
	mul.rn.f32 	%f250, %f241, %f246;
	add.f32 	%f251, %f249, %f250;
	mul.rn.f32 	%f252, %f242, %f245;
	add.f32 	%f253, %f251, %f252;
	mul.rn.f32 	%f254, %f242, %f246;
	add.f32 	%f255, %f253, %f254;
	mul.rn.f32 	%f256, %f42, %f304;
	add.f32 	%f257, %f256, %f255;
	add.f32 	%f258, %f248, %f257;
	sub.f32 	%f259, %f248, %f258;
	add.f32 	%f45, %f259, %f257;
	mov.f32 	%f309, %f45;
	mov.f32 	%f310, %f258;
	mov.b32 	 %r40, %f258;
	setp.eq.s32 	%p54, %r40, 1118925336;
	@%p54 bra 	BB21_51;
	bra.uni 	BB21_52;

BB21_51:
	add.s32 	%r102, %r40, -1;
	mov.b32 	 %f260, %r102;
	add.f32 	%f261, %f45, %f10;
	mov.f32 	%f309, %f261;
	mov.f32 	%f310, %f260;

BB21_52:
	mov.f32 	%f269, 0f3FB8AA3B;
	mul.rn.f32 	%f263, %f310, %f269;
	// inline asm
	cvt.rzi.f32.f32 	%f262, %f263;
	// inline asm
	mul.rn.f32 	%f271, %f262, %f227;
	sub.f32 	%f272, %f310, %f271;
	mul.rn.f32 	%f274, %f262, %f229;
	sub.f32 	%f275, %f272, %f274;
	mul.rn.f32 	%f265, %f275, %f269;
	// inline asm
	ex2.approx.f32 	%f264, %f265;
	// inline asm
	add.f32 	%f267, %f262, 0f00000000;
	// inline asm
	ex2.approx.f32 	%f266, %f267;
	// inline asm
	mul.rn.f32 	%f276, %f264, %f266;
	setp.lt.f32 	%p55, %f310, 0fC2D20000;
	selp.f32 	%f277, 0f00000000, %f276, %p55;
	setp.gt.f32 	%p56, %f310, 0f42D20000;
	selp.f32 	%f305, %f8, %f277, %p56;
	setp.neu.f32 	%p57, %f305, %f8;
	@%p57 bra 	BB21_53;
	bra.uni 	BB21_54;

BB21_53:
	// inline asm
	mad.f32 	%f278, %f305, %f309, %f305;
	// inline asm
	mov.f32 	%f305, %f278;

BB21_54:
	not.pred 	%p59, %p13;
	or.pred  	%p60, %p7, %p59;
	mov.b32 	 %r103, %f305;
	xor.b32  	%r104, %r103, -2147483648;
	mov.b32 	 %f282, %r104;
	selp.f32 	%f49, %f305, %f282, %p60;
	mov.f32 	%f307, %f49;
	bra.uni 	BB21_69;

BB21_55:
	mov.f32 	%f307, %f9;
	bra.uni 	BB21_69;

BB21_56:
	setp.lt.f32 	%p14, %f304, 0f00000000;
	@%p5 bra 	BB21_58;

	selp.f32 	%f50, 0f00000000, %f8, %p14;
	mov.f32 	%f307, %f50;
	bra.uni 	BB21_69;

BB21_58:
	@%p14 bra 	BB21_60;

	selp.f32 	%f51, %f7, %f8, %p13;
	mov.f32 	%f307, %f51;
	bra.uni 	BB21_69;

BB21_60:
	selp.f32 	%f52, 0f80000000, 0f00000000, %p13;
	mov.f32 	%f307, %f52;
	bra.uni 	BB21_69;

BB21_61:
	setp.lt.f32 	%p61, %f304, 0f00000000;
	@%p61 bra 	BB21_63;

	selp.f32 	%f53, %f5, 0f00000000, %p13;
	mov.f32 	%f307, %f53;
	bra.uni 	BB21_69;

BB21_63:
	selp.f32 	%f54, %f4, 0f7F800000, %p13;
	mov.f32 	%f307, %f54;
	bra.uni 	BB21_69;

BB21_64:
	setp.lt.f32 	%p62, %f173, 0f3F800000;
	mov.b32 	 %r105, %f304;
	setp.lt.s32 	%p15, %r105, 0;
	@%p62 bra 	BB21_66;

	selp.f32 	%f55, 0f00000000, %f8, %p15;
	mov.f32 	%f307, %f55;
	bra.uni 	BB21_69;

BB21_66:
	selp.f32 	%f56, %f8, 0f00000000, %p15;
	mov.f32 	%f307, %f56;
	bra.uni 	BB21_69;

BB21_67:
	add.f32 	%f57, %f1, %f304;
	mov.f32 	%f307, %f57;
	bra.uni 	BB21_69;

BB21_68:
	mov.f32 	%f283, 0f3F800000;
	mov.f32 	%f307, %f283;

BB21_69:
	mov.f32 	%f58, %f307;
	mul.f32 	%f284, %f301, %f58;
	cvt.rzi.s32.f32 	%r106, %f284;
	mad.lo.s32 	%r125, %r106, %r20, %r125;
	add.s32 	%r120, %r120, 1;
	add.s32 	%r119, %r119, -1;
	setp.ge.s32 	%p63, %r119, %r7;
	@%p63 bra 	BB21_5;
	bra.uni 	BB21_71;

BB21_70:
	mov.u32 	%r125, 0;

BB21_71:
	shl.b32 	%r108, %r5, 2;
	ld.param.u32 	%r110, [reversen_param_0];
	add.s32 	%r109, %r110, %r108;
	st.global.u32 	[%r109], %r125;
	ret;
}

.entry DFT(
	.param .u32 .ptr .global .align 8 DFT_param_0,
	.param .u32 .ptr .global .align 8 DFT_param_1,
	.param .u32 DFT_param_2,
	.param .u32 DFT_param_3
)
{
	.local .align 8 .b8 	__local_depot22[40];
	.reg .b32 	%SP;
	.reg .f64 	%fd<203>;
	.reg .pred 	%p<39>;
	.reg .s32 	%r<138>;
	.reg .s64 	%rl<167>;


	mov.u32 	%SP, __local_depot22;
	ld.param.u32 	%r3, [DFT_param_2];
	// inline asm
	mov.u32 	%r41, %envreg3;
	// inline asm
	// inline asm
	mov.u32 	%r42, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r43, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r44, %tid.x;
	// inline asm
	add.s32 	%r45, %r44, %r41;
	mad.lo.s32 	%r4, %r43, %r42, %r45;
	setp.gt.s32 	%p3, %r3, 0;
	@%p3 bra 	BB22_2;

	mov.f64 	%fd26, 0d0000000000000000;
	mov.f64 	%fd201, %fd26;
	mov.f64 	%fd202, %fd26;
	bra.uni 	BB22_52;

BB22_2:
	cvt.rn.f64.s32 	%fd27, %r4;
	mul.f64 	%fd1, %fd27, 0d401921FB54442D18;
	ld.param.u32 	%r130, [DFT_param_2];
	cvt.rn.f64.s32 	%fd2, %r130;
	mov.f64 	%fd3, 0d7FF0000000000000;
	mov.f64 	%fd4, 0dFFF0000000000000;
	mov.f64 	%fd5, 0dFFF8000000000000;
	add.u32 	%r5, %SP, 0;
	mov.f64 	%fd28, 0d0000000000000000;
	mov.f64 	%fd201, %fd28;
	mov.f64 	%fd202, %fd28;
	mov.u32 	%r131, 0;

BB22_3:
	cvt.rn.f64.s32 	%fd29, %r131;
	mul.f64 	%fd30, %fd1, %fd29;
	div.rn.f64 	%fd6, %fd30, %fd2;
	setp.eq.f64 	%p1, %fd6, %fd3;
	setp.eq.f64 	%p2, %fd6, %fd4;
	or.pred  	%p4, %p1, %p2;
	@%p4 bra 	BB22_26;

	// inline asm
	abs.f64 	%fd31, %fd6;
	// inline asm
	setp.gt.f64 	%p5, %fd31, 0d41E0000000000000;
	@%p5 bra 	BB22_6;

	mov.f64 	%fd46, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd33, %fd6, %fd46;
	// inline asm
	cvt.rni.s32.f64 	%r47, %fd33;
	// inline asm
	cvt.rn.f64.s32 	%fd47, %r47;
	neg.f64 	%fd43, %fd47;
	mov.f64 	%fd36, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd34, %fd43, %fd36, %fd6;
	// inline asm
	mov.f64 	%fd40, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd38, %fd43, %fd40, %fd34;
	// inline asm
	mov.f64 	%fd44, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd42, %fd43, %fd44, %fd38;
	// inline asm
	mov.u32 	%r134, %r47;
	mov.f64 	%fd195, %fd42;
	bra.uni 	BB22_22;

BB22_6:
	mov.b64 	 %rl1, %fd6;
	and.b64  	%rl150, %rl1, -9223372036854775808;
	shr.u64 	%rl3, %rl1, 52;
	and.b64  	%rl65, %rl3, 2047;
	add.s64 	%rl66, %rl65, 4294966272;
	cvt.u32.u64 	%r8, %rl66;
	shl.b64 	%rl67, %rl1, 11;
	or.b64  	%rl4, %rl67, -9223372036854775808;
	shr.u32 	%r51, %r8, 6;
	mov.u32 	%r52, 16;
	sub.s32 	%r9, %r52, %r51;
	mov.u32 	%r53, 15;
	sub.s32 	%r132, %r53, %r51;
	mov.u32 	%r54, 19;
	sub.s32 	%r11, %r54, %r51;
	mov.u32 	%r49, 18;
	// inline asm
	min.s32 	%r48, %r49, %r11;
	// inline asm
	setp.lt.s32 	%p6, %r132, %r48;
	@%p6 bra 	BB22_8;

	mov.u64 	%rl147, 0;
	bra.uni 	BB22_10;

BB22_8:
	mov.u32 	%r55, 1;
	sub.s32 	%r12, %r55, %r9;
	mov.u64 	%rl147, 0;

BB22_9:
	.pragma "nounroll";
	shl.b32 	%r59, %r132, 3;
	mov.u32 	%r60, __internal_i2opi_d;
	add.s32 	%r61, %r60, %r59;
	ld.const.u64 	%rl71, [%r61];
	mul.lo.s64 	%rl73, %rl71, %rl4;
	// inline asm
	mul.hi.u64 	%rl70, %rl71, %rl4;
	// inline asm
	mad.lo.s64 	%rl74, %rl71, %rl4, %rl147;
	setp.lt.u64 	%p7, %rl74, %rl73;
	selp.u64 	%rl75, 1, 0, %p7;
	add.s64 	%rl147, %rl75, %rl70;
	add.s32 	%r62, %r12, %r132;
	shl.b32 	%r63, %r62, 3;
	add.s32 	%r65, %r5, %r63;
	st.local.u64 	[%r65], %rl74;
	// inline asm
	min.s32 	%r56, %r49, %r11;
	// inline asm
	add.s32 	%r132, %r132, 1;
	setp.lt.s32 	%p8, %r132, %r56;
	@%p8 bra 	BB22_9;

BB22_10:
	mov.u32 	%r66, 1;
	sub.s32 	%r67, %r66, %r9;
	add.s32 	%r68, %r67, %r132;
	shl.b32 	%r69, %r68, 3;
	add.s32 	%r71, %r5, %r69;
	st.local.u64 	[%r71], %rl147;
	ld.local.u64 	%rl148, [%r5+24];
	ld.local.u64 	%rl149, [%r5+16];
	and.b32  	%r72, %r8, 63;
	setp.eq.s32 	%p9, %r72, 0;
	@%p9 bra 	BB22_12;

	and.b64  	%rl76, %rl3, 63;
	cvt.u32.u64 	%r73, %rl76;
	shl.b64 	%rl77, %rl148, %r73;
	neg.s32 	%r74, %r8;
	and.b32  	%r75, %r74, 63;
	shr.u64 	%rl78, %rl149, %r75;
	or.b64  	%rl148, %rl78, %rl77;
	shl.b64 	%rl79, %rl149, %r73;
	ld.local.u64 	%rl80, [%r5+8];
	shr.u64 	%rl81, %rl80, %r75;
	or.b64  	%rl149, %rl81, %rl79;

BB22_12:
	shr.u64 	%rl82, %rl148, 62;
	cvt.u32.u64 	%r76, %rl82;
	shr.u64 	%rl83, %rl149, 62;
	shl.b64 	%rl84, %rl148, 2;
	or.b64  	%rl154, %rl83, %rl84;
	shl.b64 	%rl15, %rl149, 2;
	setp.ne.s64 	%p10, %rl15, 0;
	selp.u64 	%rl85, 1, 0, %p10;
	or.b64  	%rl86, %rl85, %rl154;
	setp.gt.u64 	%p11, %rl86, -9223372036854775808;
	selp.u32 	%r77, 1, 0, %p11;
	add.s32 	%r78, %r77, %r76;
	neg.s32 	%r79, %r78;
	setp.lt.s64 	%p12, %rl1, 0;
	selp.b32 	%r134, %r79, %r78, %p12;
	@%p11 bra 	BB22_14;

	mov.u64 	%rl153, %rl15;
	bra.uni 	BB22_15;

BB22_14:
	not.b64 	%rl87, %rl154;
	neg.s64 	%rl16, %rl15;
	setp.eq.s64 	%p13, %rl15, 0;
	selp.u64 	%rl88, 1, 0, %p13;
	add.s64 	%rl154, %rl88, %rl87;
	xor.b64  	%rl150, %rl150, -9223372036854775808;
	mov.u64 	%rl153, %rl16;

BB22_15:
	mov.u64 	%rl152, %rl153;
	setp.gt.s64 	%p14, %rl154, 0;
	@%p14 bra 	BB22_17;

	mov.u32 	%r133, 0;
	bra.uni 	BB22_19;

BB22_17:
	mov.u32 	%r133, 0;

BB22_18:
	shr.u64 	%rl89, %rl152, 63;
	shl.b64 	%rl90, %rl154, 1;
	or.b64  	%rl154, %rl89, %rl90;
	shl.b64 	%rl152, %rl152, 1;
	add.s32 	%r133, %r133, -1;
	setp.gt.s64 	%p15, %rl154, 0;
	@%p15 bra 	BB22_18;

BB22_19:
	mul.lo.s64 	%rl156, %rl154, -3958705157555305931;
	mov.u64 	%rl93, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl91, %rl154, %rl93;
	// inline asm
	setp.gt.s64 	%p16, %rl91, 0;
	mov.u64 	%rl155, %rl91;
	@%p16 bra 	BB22_20;
	bra.uni 	BB22_21;

BB22_20:
	shl.b64 	%rl94, %rl91, 1;
	shr.u64 	%rl95, %rl156, 63;
	or.b64  	%rl155, %rl94, %rl95;
	mul.lo.s64 	%rl156, %rl154, -7917410315110611862;
	add.s32 	%r133, %r133, -1;

BB22_21:
	setp.ne.s64 	%p17, %rl156, 0;
	selp.u64 	%rl96, 1, 0, %p17;
	add.s64 	%rl97, %rl96, %rl155;
	add.s32 	%r82, %r133, 1022;
	cvt.u64.u32 	%rl98, %r82;
	shl.b64 	%rl99, %rl98, 52;
	shr.u64 	%rl100, %rl97, 11;
	shr.u64 	%rl101, %rl97, 10;
	and.b64  	%rl102, %rl101, 1;
	add.s64 	%rl103, %rl99, %rl100;
	add.s64 	%rl104, %rl103, %rl102;
	or.b64  	%rl105, %rl104, %rl150;
	mov.b64 	 %fd195, %rl105;

BB22_22:
	add.s32 	%r23, %r134, 1;
	and.b32  	%r83, %r23, 1;
	setp.eq.s32 	%p18, %r83, 0;
	mul.rn.f64 	%fd10, %fd195, %fd195;
	@%p18 bra 	BB22_24;

	mov.f64 	%fd49, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd51, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd48, %fd49, %fd10, %fd51;
	// inline asm
	mov.f64 	%fd55, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd52, %fd48, %fd10, %fd55;
	// inline asm
	mov.f64 	%fd59, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd56, %fd52, %fd10, %fd59;
	// inline asm
	mov.f64 	%fd63, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd60, %fd56, %fd10, %fd63;
	// inline asm
	mov.f64 	%fd67, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd64, %fd60, %fd10, %fd67;
	// inline asm
	mov.f64 	%fd71, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd68, %fd64, %fd10, %fd71;
	// inline asm
	mov.f64 	%fd75, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd72, %fd68, %fd10, %fd75;
	// inline asm
	mov.f64 	%fd196, %fd72;
	bra.uni 	BB22_25;

BB22_24:
	mov.f64 	%fd77, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd79, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd76, %fd77, %fd10, %fd79;
	// inline asm
	mov.f64 	%fd83, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd80, %fd76, %fd10, %fd83;
	// inline asm
	mov.f64 	%fd87, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd84, %fd80, %fd10, %fd87;
	// inline asm
	mov.f64 	%fd91, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd88, %fd84, %fd10, %fd91;
	// inline asm
	mov.f64 	%fd95, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd92, %fd88, %fd10, %fd95;
	// inline asm
	mul.rn.f64 	%fd97, %fd92, %fd10;
	// inline asm
	fma.rn.f64 	%fd96, %fd97, %fd195, %fd195;
	// inline asm
	mov.f64 	%fd196, %fd96;

BB22_25:
	and.b32  	%r84, %r23, 2;
	setp.eq.s32 	%p19, %r84, 0;
	neg.f64 	%fd100, %fd196;
	selp.f64 	%fd14, %fd196, %fd100, %p19;
	mov.f64 	%fd197, %fd14;
	bra.uni 	BB22_27;

BB22_26:
	mov.f64 	%fd197, %fd5;

BB22_27:
	mov.f64 	%fd15, %fd197;
	setp.eq.f64 	%p20, %fd6, 0d0000000000000000;
	or.pred  	%p21, %p2, %p20;
	or.pred  	%p22, %p1, %p21;
	@%p22 bra 	BB22_50;

	// inline asm
	abs.f64 	%fd101, %fd6;
	// inline asm
	setp.gt.f64 	%p23, %fd101, 0d41E0000000000000;
	@%p23 bra 	BB22_30;

	mov.f64 	%fd116, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd103, %fd6, %fd116;
	// inline asm
	cvt.rni.s32.f64 	%r85, %fd103;
	// inline asm
	cvt.rn.f64.s32 	%fd117, %r85;
	neg.f64 	%fd113, %fd117;
	mov.f64 	%fd106, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd104, %fd113, %fd106, %fd6;
	// inline asm
	mov.f64 	%fd110, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd108, %fd113, %fd110, %fd104;
	// inline asm
	mov.f64 	%fd114, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd112, %fd113, %fd114, %fd108;
	// inline asm
	mov.u32 	%r137, %r85;
	mov.f64 	%fd198, %fd112;
	bra.uni 	BB22_46;

BB22_30:
	mov.b64 	 %rl33, %fd6;
	and.b64  	%rl160, %rl33, -9223372036854775808;
	shr.u64 	%rl35, %rl33, 52;
	and.b64  	%rl106, %rl35, 2047;
	add.s64 	%rl107, %rl106, 4294966272;
	cvt.u32.u64 	%r25, %rl107;
	shl.b64 	%rl108, %rl33, 11;
	or.b64  	%rl36, %rl108, -9223372036854775808;
	shr.u32 	%r89, %r25, 6;
	mov.u32 	%r90, 16;
	sub.s32 	%r26, %r90, %r89;
	mov.u32 	%r91, 15;
	sub.s32 	%r135, %r91, %r89;
	mov.u32 	%r92, 19;
	sub.s32 	%r28, %r92, %r89;
	mov.u32 	%r87, 18;
	// inline asm
	min.s32 	%r86, %r87, %r28;
	// inline asm
	setp.lt.s32 	%p24, %r135, %r86;
	@%p24 bra 	BB22_32;

	mov.u64 	%rl157, 0;
	bra.uni 	BB22_34;

BB22_32:
	mov.u32 	%r93, 1;
	sub.s32 	%r29, %r93, %r26;
	mov.u64 	%rl157, 0;

BB22_33:
	.pragma "nounroll";
	shl.b32 	%r97, %r135, 3;
	mov.u32 	%r98, __internal_i2opi_d;
	add.s32 	%r99, %r98, %r97;
	ld.const.u64 	%rl112, [%r99];
	mul.lo.s64 	%rl114, %rl112, %rl36;
	// inline asm
	mul.hi.u64 	%rl111, %rl112, %rl36;
	// inline asm
	mad.lo.s64 	%rl115, %rl112, %rl36, %rl157;
	setp.lt.u64 	%p25, %rl115, %rl114;
	selp.u64 	%rl116, 1, 0, %p25;
	add.s64 	%rl157, %rl116, %rl111;
	add.s32 	%r100, %r29, %r135;
	shl.b32 	%r101, %r100, 3;
	add.s32 	%r103, %r5, %r101;
	st.local.u64 	[%r103], %rl115;
	// inline asm
	min.s32 	%r94, %r87, %r28;
	// inline asm
	add.s32 	%r135, %r135, 1;
	setp.lt.s32 	%p26, %r135, %r94;
	@%p26 bra 	BB22_33;

BB22_34:
	mov.u32 	%r104, 1;
	sub.s32 	%r105, %r104, %r26;
	add.s32 	%r106, %r105, %r135;
	shl.b32 	%r107, %r106, 3;
	add.s32 	%r109, %r5, %r107;
	st.local.u64 	[%r109], %rl157;
	ld.local.u64 	%rl158, [%r5+24];
	ld.local.u64 	%rl159, [%r5+16];
	and.b32  	%r110, %r25, 63;
	setp.eq.s32 	%p27, %r110, 0;
	@%p27 bra 	BB22_36;

	and.b64  	%rl117, %rl35, 63;
	cvt.u32.u64 	%r111, %rl117;
	shl.b64 	%rl118, %rl158, %r111;
	neg.s32 	%r112, %r25;
	and.b32  	%r113, %r112, 63;
	shr.u64 	%rl119, %rl159, %r113;
	or.b64  	%rl158, %rl119, %rl118;
	shl.b64 	%rl120, %rl159, %r111;
	ld.local.u64 	%rl121, [%r5+8];
	shr.u64 	%rl122, %rl121, %r113;
	or.b64  	%rl159, %rl122, %rl120;

BB22_36:
	shr.u64 	%rl123, %rl158, 62;
	cvt.u32.u64 	%r114, %rl123;
	shr.u64 	%rl124, %rl159, 62;
	shl.b64 	%rl125, %rl158, 2;
	or.b64  	%rl164, %rl124, %rl125;
	shl.b64 	%rl47, %rl159, 2;
	setp.ne.s64 	%p28, %rl47, 0;
	selp.u64 	%rl126, 1, 0, %p28;
	or.b64  	%rl127, %rl126, %rl164;
	setp.gt.u64 	%p29, %rl127, -9223372036854775808;
	selp.u32 	%r115, 1, 0, %p29;
	add.s32 	%r116, %r115, %r114;
	neg.s32 	%r117, %r116;
	setp.lt.s64 	%p30, %rl33, 0;
	selp.b32 	%r137, %r117, %r116, %p30;
	@%p29 bra 	BB22_38;

	mov.u64 	%rl163, %rl47;
	bra.uni 	BB22_39;

BB22_38:
	not.b64 	%rl128, %rl164;
	neg.s64 	%rl48, %rl47;
	setp.eq.s64 	%p31, %rl47, 0;
	selp.u64 	%rl129, 1, 0, %p31;
	add.s64 	%rl164, %rl129, %rl128;
	xor.b64  	%rl160, %rl160, -9223372036854775808;
	mov.u64 	%rl163, %rl48;

BB22_39:
	mov.u64 	%rl162, %rl163;
	setp.gt.s64 	%p32, %rl164, 0;
	@%p32 bra 	BB22_41;

	mov.u32 	%r136, 0;
	bra.uni 	BB22_43;

BB22_41:
	mov.u32 	%r136, 0;

BB22_42:
	shr.u64 	%rl130, %rl162, 63;
	shl.b64 	%rl131, %rl164, 1;
	or.b64  	%rl164, %rl130, %rl131;
	shl.b64 	%rl162, %rl162, 1;
	add.s32 	%r136, %r136, -1;
	setp.gt.s64 	%p33, %rl164, 0;
	@%p33 bra 	BB22_42;

BB22_43:
	mul.lo.s64 	%rl166, %rl164, -3958705157555305931;
	mov.u64 	%rl134, -3958705157555305931;
	// inline asm
	mul.hi.u64 	%rl132, %rl164, %rl134;
	// inline asm
	setp.gt.s64 	%p34, %rl132, 0;
	mov.u64 	%rl165, %rl132;
	@%p34 bra 	BB22_44;
	bra.uni 	BB22_45;

BB22_44:
	shl.b64 	%rl135, %rl132, 1;
	shr.u64 	%rl136, %rl166, 63;
	or.b64  	%rl165, %rl135, %rl136;
	mul.lo.s64 	%rl166, %rl164, -7917410315110611862;
	add.s32 	%r136, %r136, -1;

BB22_45:
	setp.ne.s64 	%p35, %rl166, 0;
	selp.u64 	%rl137, 1, 0, %p35;
	add.s64 	%rl138, %rl137, %rl165;
	add.s32 	%r120, %r136, 1022;
	cvt.u64.u32 	%rl139, %r120;
	shl.b64 	%rl140, %rl139, 52;
	shr.u64 	%rl141, %rl138, 11;
	shr.u64 	%rl142, %rl138, 10;
	and.b64  	%rl143, %rl142, 1;
	add.s64 	%rl144, %rl140, %rl141;
	add.s64 	%rl145, %rl144, %rl143;
	or.b64  	%rl146, %rl145, %rl160;
	mov.b64 	 %fd198, %rl146;

BB22_46:
	and.b32  	%r121, %r137, 1;
	setp.eq.s32 	%p36, %r121, 0;
	mul.rn.f64 	%fd19, %fd198, %fd198;
	@%p36 bra 	BB22_48;

	mov.f64 	%fd119, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd121, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd118, %fd119, %fd19, %fd121;
	// inline asm
	mov.f64 	%fd125, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd122, %fd118, %fd19, %fd125;
	// inline asm
	mov.f64 	%fd129, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd126, %fd122, %fd19, %fd129;
	// inline asm
	mov.f64 	%fd133, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd130, %fd126, %fd19, %fd133;
	// inline asm
	mov.f64 	%fd137, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd134, %fd130, %fd19, %fd137;
	// inline asm
	mov.f64 	%fd141, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd138, %fd134, %fd19, %fd141;
	// inline asm
	mov.f64 	%fd145, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd142, %fd138, %fd19, %fd145;
	// inline asm
	mov.f64 	%fd199, %fd142;
	bra.uni 	BB22_49;

BB22_48:
	mov.f64 	%fd147, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd149, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd146, %fd147, %fd19, %fd149;
	// inline asm
	mov.f64 	%fd153, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd150, %fd146, %fd19, %fd153;
	// inline asm
	mov.f64 	%fd157, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd154, %fd150, %fd19, %fd157;
	// inline asm
	mov.f64 	%fd161, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd158, %fd154, %fd19, %fd161;
	// inline asm
	mov.f64 	%fd165, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd162, %fd158, %fd19, %fd165;
	// inline asm
	mul.rn.f64 	%fd167, %fd162, %fd19;
	// inline asm
	fma.rn.f64 	%fd166, %fd167, %fd198, %fd198;
	// inline asm
	mov.f64 	%fd199, %fd166;

BB22_49:
	and.b32  	%r122, %r137, 2;
	setp.eq.s32 	%p37, %r122, 0;
	neg.f64 	%fd170, %fd199;
	selp.f64 	%fd200, %fd199, %fd170, %p37;
	bra.uni 	BB22_51;

BB22_50:
	mul.rn.f64 	%fd200, %fd6, %fd28;

BB22_51:
	shl.b32 	%r123, %r131, 4;
	ld.param.u32 	%r127, [DFT_param_0];
	add.s32 	%r124, %r127, %r123;
	ld.global.f64 	%fd172, [%r124];
	ld.global.f64 	%fd173, [%r124+8];
	mul.f64 	%fd174, %fd173, %fd200;
	fma.rn.f64 	%fd175, %fd172, %fd15, %fd174;
	add.f64 	%fd177, %fd201, %fd175;
	mul.f64 	%fd178, %fd173, %fd15;
	neg.f64 	%fd179, %fd172;
	fma.rn.f64 	%fd180, %fd179, %fd200, %fd178;
	add.f64 	%fd182, %fd202, %fd180;
	mov.f64 	%fd201, %fd177;
	mov.f64 	%fd202, %fd182;
	add.s32 	%r131, %r131, 1;
	ld.param.u32 	%r129, [DFT_param_2];
	setp.lt.s32 	%p38, %r131, %r129;
	@%p38 bra 	BB22_3;

BB22_52:
	shl.b32 	%r125, %r4, 4;
	ld.param.u32 	%r128, [DFT_param_1];
	add.s32 	%r126, %r128, %r125;
	st.global.f64 	[%r126], %fd201;
	st.global.f64 	[%r126+8], %fd202;
	ret;
}



